{"question":"I'm comparing waste treatment technologies for my environmental impact study. Can you explain the environmental considerations between using distillation versus gasification processes?","answer":"The key environmental consideration between these processes lies in their operational characteristics and byproducts. Distillation is primarily a physical separation process that doesn't create new chemical compounds during operation, focusing on separating existing components based on their volatility differences. In contrast, gasification presents significant environmental concerns due to its use of oxygen in the process, which can result in the formation of harmful dioxins and furans. These compounds are particularly concerning as they can severely disrupt hormonal balance in living organisms and lead to dangerous diseases. Gasification is also more complex to control environmentally due to its mixture of exothermic and endothermic reactions, while distillation's environmental impact is mainly related to its energy consumption for heating and cooling operations.","context":["Distillation is a process by which a liquid mixture is separated into fractions with higher concentrations of certain components by exploiting differences in relative volatility. In industrial settings such as oil refineries and natural gas processing plants this separation process is undertaken using a distillation column. This article describes the basic principles and operation of a distillation column and the equipment and terminology used when discussing distillation.\nObjective of Distillation\nThe basic objective of distillation is to take a liquid mixture and separate it into two or more streams which have compositions different to the feed stream. In a basic distillation column a feed stream enters in the middle of the column and two streams leave, one at the top and one at the bottom. Components with lower boiling points are concentrated in the stream leaving the top while components with higher boiling points are concentrated in the stream leaving the bottom.\nSeparation is achieved by controlling the column temperature and pressure profiles to take advantage of differences in the relative volatility of the mixture components and therefore tendency to change phase. The lighter, lower boiling point components evaporate and travel up the column to form the top product and the heavier, higher boiling point components condense and travelling down the column to form the bottom product.\nDiscussion of distillation requires an understanding of some basic nomenclature. Listed below are terms that are commonly used to describe the main components of a distillation column complete with a diagrammatic representation of a tray column.\nFractionation - Another term for distillation, or fractional distillation.\nFeed - The liquid and/or gas feed into the distillation column. The tray below the inlet nozzle is called the feed tray.\nHeavy Component - The component with the lower relative volatility, for simple hydrocarbon this is the component with the higher molecular weight. Found in higher concentration in the bottom product of the column.\nLight Component - The component with the higher relative volatility, for simple hydrocarbon this is the component with the lower molecular weight. Found in higher concentration at the top of the column.\nStripping section - The trays between the bottom of the column and the feed tray. In the stripping section the aim is to concentrate the heavier component in the liquid phase.\nRectifying section - The trays between the feed tray and the top of the column. In the rectifying section the aim is to concentrate the lighter component in the vapor phase.\nTop Product - The product which leaves the top of the column, also called distillate. This product is usually passed through a heat exchanger and liquefied.\nBottom Product - The product which leaves through the bottom of the column.\nReflux - A portion of vapor from the top of the column which has been condensed to a liquid and returned to the column as a liquid above the top tray.\nReboiler - A heat exchanger at the bottom of the column which boils some of the liquid leaving the column. The vapor generated returns to the column at the bottom of the stripping section.\nVapor-Liquid Equilibrium (VLE) Curve - A plot of the actual composition of the lighter component in the vapor phase for a given composition in the liquid phase. Usually derived from thermodynamic data.\nPrinciple of Separation\nDistillation takes advantage of the difference in relative volatility of the feed mixture components. Generally for two or more compounds at a given pressure and temperature there will be a difference in the vapour and liquid compositions at equilibrium due to component partial pressure. Distillation exploits this by bringing liquid and gas phases into contact at temperatures and pressures that promote the desired separation. During this contact the components with the lower volatility (typically lower boiling point) will preferentially move into the liquid phase while more volatile components move into the vapor phase.\nA distillation column may use either trays or a packed bed to bring the gas and liquid into contact. For a column using trays we can consider the changes to gas and liquid phase compositions as they both enter and exit a single tray. The liquid entering the tray will contact the gas exiting the tray. The hotter vapor phase will heat the incoming liquid phase as it bubbles through the tray, evaporating the light components which then leave the tray with the vapor phase. Conversely the cooling of the vapor phase by the liquid phase will cause the heavier components of the vapor phase to condense and exit the tray with the liquid phase.\nFor the liquid across the tray:\nFor the vapor through the tray:\nWhere is concentration and is temperature.\nWhen a packing is used rather than trays the principle remains the same, in fact packing is often referenced in terms of height equivalent to a theoretical plate (HETP) i.e. what height of packing is equivalent to one theoretical plate. The packing is just an alternative method to bring the liquid and vapor phases into contact with the liquid generally flowing over the surfaces of the packing material, while the vapor passes up through the space between packing elements.\nTypical Operating Parameters\nThere are some general trends common to the operation of distillation columns. By knowing these trends and why they occur we can improve our understanding of the distillation process.\nThe basic temperature profile of a distillation column is hotter at the bottom and cooler at the top. For a simple two component distillation the temperature at the bottom is just lower than the boiling point of the heavier component. The temperature at the top of the column is just above the boiling point of the lighter component.\nAt the bottom of the column we would like the heavy component to remain as a liquid and the lighter component to stay as a gas. So we set the temperature at the bottom to match this requirement. This temperature is set by adding heat via a heat exchanger called a reboiler. Typically the heat added to the bottom of the column is easy to control, via steam or hot oil flow rates.\nAt the top of the column the situation is reversed. We would like the light component to remain a gas while the heavier component is condensed to a liquid and falls back down the column. The top temperature is set just above the boiling point of the lighter component. The temperature control situation is different here to the bottom of the column, because we usually want the top product to be a liquid when we send it for storage. So we condense all of the gas coming out of the top of the column to liquid. This liquid stream is split with some returning to the column and some going to storage. The top temperature is often controlled by changing the reflux rate, i.e. the flow rate of liquid sent back to the top of the column. A higher reflux rate means more cooler liquid falling down the column against the rising warmer gas, and the top temperature is lower.\nOverall heat is added at the bottom of the column and heat is extracted at the top of the column. Inside the column the temperature balance is created between the hot gas rising up the column and the cooler liquid falling down the column.\nThere is typically a pressure gradient across the column with the pressure being higher at the bottom of the column than the top. This pressure gradient occurs as the liquid coming down the column impedes the flow of vapor up the column and imposes a pressure loss on the flow. In steady state distillations the pressure in the column is held constant, and the temperature is varied to control the composition of the product streams.","The gasification process is conduct similar to pyrolysis (high process temperatures close to 1000 °C, as in the case of high-temperature pyrolysis), thus often both processes are confused with each other. Gasification consists of a series of complex sequential chemical reactions and thermal decomposition reactions.\nThe main difference between the pyrolysis and gasification processes is the addition of a gasifying agent to the gasified waste. It may be oxygen or air, but also water vapor, carbon dioxide or simply exhaust gas. Gasification takes place in the presence of O2, which results in the formation of dioxins and furans harmful to health. The dioxins and furans formed with the participation of oxygen very strongly disturb the hormonal balance of the body, leading to the occurrence of many dangerous diseases.\nSome of the reactions occurring during the gasification process are exothermic (it does not require the supply of heat from the outside) and some of it is endothermic (requires the supply of heat from the outside). The gasification process is therefore more difficult to control, and its course depends on the gasifier used. Pyrolysis is an endothermic process. Gasification can also be seen as a continuation of the pyrolysis process, i.e. both processes may be successive stages, which increases the yield of energy gas. The gas generated in the gasification process is used directly for the production of electricity – burned in gas engines, or subjected to refining to the synthesis gas used in the synthesis of chemicals, including synthetic fuels. The calorific value of gas obtained as a result of gasification depends on the type of oxidizing agent and ranges from 5 MJ/m3 (for air and water vapor) to 10 MJ/m3 (for pure oxygen). For pyrolysis the calorific value of the obtained gas is from about 15 MJ/m3 even to over 30 MJ/m3 (depending on the substrate).\nThe main difference between the pyrolysis and gasification processes is the presence of an oxidizing agent in the process in the case of gasification and the calorific value of the gas obtained – higher calorific value in the case of pyrolysis.\nRys. Fig. Comparison of pyrolysis, gasification and incineration processes\nThe resulting pyrolysis and gasification processes:\n- High temperature pyrolysis (for example EL PIRO):\nIn this process, the main products are gas and biochar, heat is also generated\n- Low temperature pyrolysis\nIn this process, the main products are oil, gas and biochar, heat is also generated\nIn this process, the main products are gas and biochar, heat is also generated, and due to the access of oxygen in the process – dioxins and furans\nIntrocuction to biorafineries and biofuels Assignment 8: comparison of gasification, pyrolysis and combustion, Aalto University School of Chemical Technology 2013\nComparative assessment of municipal sewage sludge incineration, gasification and pyrolysis for a sustainable sludge to energy management in Greece, M.C. Samolada, A.A. Zabaniotou, Waste Management, 2014\nLife cycle assessment of pyrolysis, gasification and inceneration waste-to-energy technologies: Theoretical analysis and case study of commercial plants, J. Dong, Y. Tang, A. Nzihou, Y. Chi, E. Weiss-Hortala, M. Ni, Science of the Total Environment, 2018"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4da2cdc9-9fa7-444f-9c69-c8c77634cdf1>","<urn:uuid:e52020b5-af4c-4293-9ca7-7b48095de205>"],"error":null}
{"question":"Does a total quality management system focus more on prevention or correction of quality issues compared to HACCP?","answer":"Both systems emphasize prevention, but their approaches differ. A total quality management system focuses on prevention through comprehensive quality planning across multiple subsystems including product development, purchase control, and process control, with quality being built into the design phase. HACCP specifically concentrates on preventing food safety hazards through identifying critical control points and establishing preventive measures before hazards occur, rather than detecting them in finished products.","context":["For those food enterprises exporting to U.S. markets, both the quality and safety of their products are serious corporate issues. The following list addresses several subjects or broad activities that can be considered subsystems of an overall “quality system.” The term “system” in this context conforms to its definition: “an orderly and comprehensive assemblage of facts, principles, methods and executions in a particular field, directed to obtain a defined goal.”\nElements of a Quality Assurance (QA) system:\n• Quality management\n• Product development control\n• Purchase material control\n• Process development and manufacturing control\n• Quality measurement\n• Quality data programs\n• Human resources\n• Customer relations\nEach of the subsystems listed has a variety of activities, which are by no means exhaustive or unique; some elements may be modified and expanded, according to the needs of each organization.\nThis subsystem embodies all the activities that set the quality policy, overview and support its implementation, evaluate its performance and report results. The elements of this subsystem are given in a logical sequence for an organization that accepts seriously the quality commitment.\nProduct development control\nIt is important to note that one of the dimensions of quality is quality of design; products must be conceived having the requirements of the final user in mind. This will become the target during manufacturing and distributing. No quality could ever be obtained only by inspections; this is quality compliance, which means to meet established standards.\nPurchase material control\nThis subsystem is intended to increase satisfaction of the purchasing activities. This is aimed at minimizing quality costs and improving service. Commercial aspects of purchasing involve reasonable price contracting and deliveries on time. From a quality point of view, it should be understood that no matter how low the price of an item may be, it is too expensive if it does not perform properly.\nProcess development and manufacturing control\nA well-conceived product development should include the corresponding manufacturing process, where the precise procedure should be established. Each process has points or steps that are essential to the success of the operation. It has also certain points where trouble may arise and affect quality adversely. These are the so called Critical Control Points (CCPs), which the Hazard Analysis and Critical Control Points (HACCP) system for food safety has popularized. The principles of HACCP are not an exclusivity of food safety; they are a logical approach to insure satisfactory and consistent performance of the manufacturing function. CCPs should be identified and their corresponding controls established. A close cooperation of groups such as R&D, manufacturing and QA is essential in establishing a CCP program.\nQuality measurement and control equipment\nThe purpose of this subsystem is to insure that suitable and well-calibrated equipment is available for the needed measurements in quality control.\nQuality data programs\nCareful consideration should be given to the collection of data. In medium to large organizations, there is a tendency to accumulate data for its own sake. This often has little value. Evaluation of what data are really necessary should be done by QA and those who need the information, for example, manufacturing, purchasing, R&D and management. Data that are not needed for taking action or act on its information is generally superfluous.\nThis subsystem deals with the need to obtain full commitment from all personnel to support and participate in the quality system. It has been said that no matter how hard they try, quality control personnel cannot “make quality happen.” Quality is the result of concern, well-planned and judiciously executed actions by all members of the organization and appropriately, it starts with top management. Top management commitment, participation and example, followed then by delegation, assure success. Just a command to establish a quality program or deployment of some resources is insufficient and will not provide lasting and satisfactory results. To attain a quality system, considerable development and training of personnel is needed.\nThis is the final and definitive element of a quality system. We are faced with the need to evaluate how well was the job done. Are we meeting the goals proposed? What is the level of accomplishment? What can be done better? These are all pertinent questions, and we may use a few tools to appraise the situation.\nQuality should not be left to chance; it must be the result of a carefully planned design and the responsible execution of required actions.\nEstablishing the program requires assistance of an individual with experience, and this person can be an employee or a consultant of the company. It is important to have very clear objectives beforehand, and a document should be developed that will include the responsibilities, objectives and goals of all those that are expected to participate. There are many books, manuals and articles (see Bibliography) in which specific information on the preparation and implementation of these programs can be found.\nThe initial “planning document” cannot foresee all possible situations and conditions; thus, it should be kept flexible and reviewed as many times as necessary and modified accordingly.\nA specific example that the author wishes to emphasize is that related to documentation. This, in his experience is one of the areas—together with personal hygiene—that require the most attention and education. For example, the following are some considerations that must be included in the area of documentation of each function (quality control in this case):\n• The operation needs to document actions in the implementation of the quality program to monitor its progress and thus give confidence to its management.\n• Various functional groups, purchasing, production, and marketing, need information about quality control actions. Well-kept records are invaluable in case of an internal or external investigation of incidents that question or challenge the quality of products, whether prompted by customer dissatisfaction or government actions.\n•Well-kept records of pertinent information, when properly analyzed, help to detect trends and establish corrective actions, for instance, changes in compliance of suppliers, variation in quality of competitive products and deterioration in workmanship practices in a unit or shifts in production yields. Error prevention, based on a study of historical data and trends, is a particularly worthwhile application of data gathering.\nReporting As an Established and Necessary Managerial Procedure\nIn the documentation example, the program must include the design of reporting forms, for laboratory work (i.e., time of sampling, sample size, weight, result of test, method used, etc.), manufacturing conditions (i.e., time, temperature, batch weight, product, etc.) and all other operations that are to be documented.\nUnder the above context and systematic development of a quality policy and total quality program for the enterprise, an eventual verification is required to assure that the program fits the needs of the exporter and that the controls and costs are adequate to maintain quality and safety of products to be exported. Normally, this can only be accomplished once the operation is underway and should be considered as a “reality check.” In performing such evaluations, a series of inspections must take place, be it by members of the enterprise or external individuals who are knowledgeable in the process and have the appropriate tools to identify and report problems encountered. Especially at the beginning of operations, these problems should be discussed and actions taken immediately to resolve the issues and allow production to begin. Once the export flow of products is accomplished, the inspections should be at random and if anomalies are noted they also should be corrected.\nHowever, audits/inspections and auditors/inspectors are not created equal. How effective a third-party inspection will be and whether it will “find the skeletons in the closet” depends upon the auditor/inspector and the inspection format being used. The individual who conducts the inspection has to have a level of expertise in the particular operation that will support credibility in his/her findings. Because of this, many companies are utilizing the audit schemes that the Global Food Safety Initiative (GFSI) has approved or the incorporation of the ISO 22000 principles into their food quality/safety management program. The six schemes currently approved under GFSI are the British Retail Consortium (BRC), Safe Quality Food (SQF), International Food Standard (IFS), Dutch HACCP, Food Safety System Certification (FSSC) 22000 and Synergy 22000; most technical personnel—in the food industries in the USA and the EU—believe that the adoption of these audits have enhanced food quality and safety, and reduced recalls. Of course, the question regarding qualified auditors still remains. A good auditor is not simply someone who has attended a course and passed a test. The best auditors have extensive experience with different products, processes and plants, and they understand not only what can create problems but the means for preventing them.\nThe above paragraphs have been included in this paper, because, the Food Safety Modernisation Act (FSMA), by putting the responsibility of the product’s safety in the hands of the importers, has presumed that the “foreign” producing entities will be audited/inspected periodically. Even though these audits/inspections can be performed by local or foreign third-party personnel, their qualifications and expertise should be clearly ascertained.\nAs a last note, most industrial concerns and many food companies are now integrating all of their “operations” (e.g., manufacturing, marketing, sales, advertising and promotion, logistics, human resources, etc.) and reviewing what is the total impact on the enterprise in our environment. They are creating a corporate environmental policy, a written statement of a company’s objectives concerning the management of the effects of its operations on the environment. It is not currently compulsory to have an environmental policy but an increasing number of businesses are opting to have one.\nIn order to be effective and not just a paper exercise, the policy must be implemented at a senior management level and should contain targets and methods for measuring the improvement of environmental performance. And, in the authors’ opinion is the picture of what is to come!\nConclusion and Recommendations\nCompliance with the FSMA for food products that are to be exported to the U.S. is a fact. The best way to assure success is to produce and deliver a product that meets specifications. In order to assure this, it becomes paramount that a total quality management system is implemented, revised and updated constantly, and top management’s involvement, not only the necessary resources but also time and presence, is fundamental to the success and profitability of the operation.\nIn the writing of this document, many of the concepts and ideas mentioned were used from the authors’ publications and presentations, as well as from colleagues who have participated with him in seminars and courses, such as William Saenz, Omar Dary, Ph.D., Richard (Rick) Stier and others.\nHerbert Weinstein, Ph.D., earned his chemical engineering degree from the Universidad Nacional Autonoma de Mexico and his M.Sc. and Ph.D. in Food Science and Technology from Massachusetts Institute of Technology. He has 45+ years of industrial and consulting experience (General Foods, now Kraft Foods, and Unilever) specifically as an expert in Product Development, Quality Control and Quality Assurance and Food Safety. He has traveled extensively to countries where he has been responsible for all technical aspects of food manufacturing, distribution, logistics, product development, quality control, quality assurance and management, both as top manager for his employers, as well as consultant for various clients, governments and United Nations agencies. Food Safety and Food Security have been major topics of his latest assignments as these aspects of global food commerce have become more in the front lines of concern. Today he is a consultant working out of Arlington, VA. He can be contacted at email@example.com.\nCrosby, P.B. 1995. Quality without tears: The art of hassle-Free management. New York: McGraw-Hill.\nIshikawa, K. 1988. What is total quality control? The Japanese way (Business Management). Upper Saddle River, New Jersey: Prentice Hall.\nHeldt, J.J. and D.J. Costa. 1988. Quality pays: Increasing profits through quality cost analysis. Wheaton, IL: Hitchcock Publishing Company.","The importance of an effective food safety system cannot be overstated. According to a study conducted by the Food Marketing Institute (FMI) and the Grocery Manufacturers Association (GMA), food recalls cost companies an average of $10 million per incident. That is why Hazard Analysis Critical Control Points (HACCP) principles are so important. They create the foundation for all higher-level food safety programs and are universally accepted standards. For those new to food safety, begin with an understanding of how HACCP began and why you should be using it in your facility.\nThe History of HACCP\nShortly after NASA was established in 1958, the prospect of manned spaceflight raised an important concern: how could they be sure the food astronauts would be eating was safe? If food poisoning is bad on terra firma, the possibility of such an infection in a Gemini capsule could prove fatal. The process had to change. When food safety practices first began, food safety in industrial production relied heavily on product testing. Samples would be extracted from the processed foods and tested for contamination. While useful in many ways, it was not possible to have a high level of certainty that the entirety of a batch of food was safe to eat just by testing a portion of it. In fact, such a huge portion of each batch needed to be tested that very little final product would be left over for consumption.\nAs a result, NASA partnered with the US Army Laboratories and Pillsbury to develop a more reliable approach to food safety. This new approach focused on preventing the introduction of hazards in the process of manufacturing food, rather than looking for the effects of those hazards in the finished product.\nThe system they developed represented a major improvement in safe food production, and by the 1990s, would come to be the internationally recognized HACCP approach to food safety. HACCP is included in the Codex Alimentarius that is recognized by both the World Health Organization and World Trade Organization for the merit of its guidance on food safety.\nHow To Make HACCP Work For You:\nAs mentioned already, HACCP promotes safe food by attempting to avoid hazards in production. This is accomplished, primarily, through seven principles:\n- Conduct a hazard analysis – Begin by reviewing every step in the entire manufacturing process (from raw material production, procurement, and handling, to manufacturing, distribution, and consumption of the finished product) and consider the potential risks for biological, chemical, and physical contamination.Once those risks are identified, a HACCP management plan calls for controlling the hazards by identifying and implementing preventive measures.\nNot sure how to conduct a hazard analysis? Joining Safe Food Alliance for an in-person training coursetaught by highly knowledgeable trainers who audits systems like yours.\n- Identify critical control points (CCPs) – In many manufacturing environments, there will be one or more points in the process where a failure of Standard Operating Procedures (SOPs) has both a significant chance of occurring and causing harm to a consumer.At such a point, a CCP is established as a process to control the hazard and prevent, eliminate, or reduce the hazard to an acceptable level.\n- Establish critical limits for each critical control point – One of the defining factors of a CCP is that the process has measurable results. Every CCP will have a critical limit: the maximum or minimum value that reliably prevents, eliminates, or reduces a hazard to an acceptable level. In some processes, the critical limit might be a measure of water activity (aw) that prevents microbial growth, or a measure of time and temperature that represents a 5-log reduction in pathogens, such as a juice achieving 160°F for 6 seconds.Don’t forget that critical limits must be science- and evidence-based.\n- Establish critical control point monitoring requirements – After identifying your CCPs and establishing critical limits, you will need to develop and implement a method for monitoring your CCPs. You will need to determine an effective way and frequency to measure your CCP to ensure that it remains within critical limits.Monitoring CCPs will help you to observe when your process is trending out of control so that you may make corrections to keep it in control. Monitoring will also ensure you know if your process has gone out of control by deviating from the critical limit, alerting you that the product is at risk and needs corrective actions.Your monitoring records will also be of critical importance to your verification of the HACCP system.\n- Establish corrective actions Although the HACCP management system is meant to prevent hazards, there are occasions in which the actual process deviates from the plan. With this possibility in mind, a facility must develop corrective actions that will help keep potentially hazardous food away from consumers.\nA corrective action will entail:\na. determining and correcting the root cause of the deviation\nb. determining the disposition of the affected product\nc. keeping a record of the corrective action(s) taken\nA thorough HACCP plan will include instructions on what to do in the case of a deviation, the person(s) responsible for implementing corrective actions, and requiring that the deviation and corrective action be recorded.\n- Establish verification procedures – HACCP management plans depend heavily on good science to produce safe food. This has an impact on the development and maintenance of a HACCP plan in two key ways:\n- The development of the HACCP plan must be based on good scientific evidence in identifying and controlling for risks. For instance, a CCP that is meant to reduce a hazard to an acceptable level should be based on scientific studies (such as those that indicate the cook time and temperature for a 5-log reduction of pathogens in a type of product in general) and verification that the controls in place in your particular facility and process are effective.\n- Your HACCP plan should be routinely verified to ensure that it is effectively preventing and controlling hazards and that it is being followed and applied consistently. This is generally accomplished both through periodic internal audits and external third-party audits.\nNeed to create an internal audit program? Our self-paced online Internal Auditor course offers easy to understand audit training to help you verify the effectiveness of your food safety system.\n- Establish record-keeping and documentation procedures – If the rest of your HACCP plan involves you saying what you will do and doing what you say, the seventh principle requires you to keep records documenting that you do what you say.Clear, consistent records make it possible to demonstrate that the food you manufacture was produced safely. These will not only be production records but will also include items relating to all of the previously mentioned principles. A good record-keeping system will make it possible for an outside auditor like the ones at Safe Food Certifications to easily understand your HACCP plan, the evidence supporting your CCP identification and critical limits, and to see what your plan has looked like in action.\nAs previously stated, HACCP is the foundation for food safety. It is important to not only take the time to learn it but also see its value. With an effective food safety system, your facility will become more efficient, lower the likelihood of recalls, save you money long-term. With the ever-increasing regulations and expectations from consumers, HACCP is the best choice for anyone in the food industry.\nWant to learn more? Preregister for our 8-hour online HACCP course where you will everything from the principles of HACCP to how to write your own HACCP plan. Register Now"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:57dba5e2-71b7-4a7a-99f1-b432a5036952>","<urn:uuid:935d1421-17c3-46fb-9037-262038c7914b>"],"error":null}
{"question":"Please list the sunlight requirements for optimal growth of crabgrass and kikuyu grass.","answer":"Crabgrass requires direct sunlight to germinate, which is why maintaining a dense turf canopy helps prevent its growth. Kikuyu grass needs at least 6+ hours of daily sunlight and has fine shade tolerance, performing best in full sun conditions.","context":["Crabgrass – A Real Pain in the Grass!\nCrabgrass is the number one weed in lawns and the best time to treat is usually before it is seen. This podcast details the best management practices in developing a crabgrass control program.\n- The Virginia Lawn Pest Management Guide\n- Spring and Summer Lawn Management Considerations for Cool-Season Turfgrasses\n- Spring and Summer Lawn Management Considerations for Warm-Season Turfgrasses\nThe number one weed problem in managed turfgrasses is crabgrass (Digitaria sp.). As days get longer in late winter/early spring and soil temperatures warm, crabgrass seed are primed for germination. When soil temperatures reach 55o F for 2 to 3 days, germination is possible and can continue throughout the spring and summer. No matter how well crabgrass has been controlled in previous years, there is still a tremendous seed bank in the soil and any weak spots in the lawn are likely sites to be invaded by this fast growing summer annual. Crabgrass’ rapid emergence and extremely fast growth rate allow it to get a jump start on both cool and warm-season grasses alike. Since crabgrass is a warm-season grass, it is particularly competitive against cool-season grasses (fescues, bluegrasses, ryegrasses etc.). Given that it is such a tough competitor, what are the best management practices to battle this pest?\nA thick, healthy turf is the best method of crabgrass control. Crabgrass requires sunlight to germinate, a requirement that we can use to our advantage in the lawn. Dr. Shawn Askew, Extension Turfgrass Weed Scientist, advocates that the lawn itself controls far more weeds than any chemical ever applied because the most weed control is gained by maintaining a dense turf canopy. If the lawn is reasonably dense entering the spring growing season, crabgrass pressure (and that of any other spring germinating weed) is minimal. It is still feasible to use a standard preemergent (PRE) herbicide (a product that controls germinating plant seedlings), but it is unlikely your lawn will be swamped with weeds. If your turf stand is very sparse, then it is almost guaranteed that you will have significant weed pressure. So there is now a decision to be made—do you apply a PRE herbicide to control weeds OR do you apply new grass seed to fill in the gaps. It is very important to understand that for most standard PRE herbicides available to homeowners, there is no selectivity in control between weed or grass seed. The only exception in a PRE herbicide that can be applied for crabgrass control at seeding of cool-season grasses is Tupersan (siduron). Crabgrass can also be controlled with a specific early postemergent (POST) herbicide such as Drive (quinclorac), and this product has safety on several cool- and warm-season grasses. Many of these approaches are best left (and sometimes they are only available) to professional lawn care operators. Remember – the label is the law when it comes to pesticides. This protects you, the environment, and your lawn.\nWhat PRE herbicides are readily available to homeowners? There are many trade names of products on the market, so while it takes a little extra effort to look for the complicated common chemical names, it’s the safest way to identify the product you are looking for. The following chemicals can most often be found at stores that deal with specialty products for lawns and landscapes, and at least a few of them will likely be available at your big-box retailers. A recent survey of both specialty lawn and garden centers and big box retailers indicated these active ingredients were available: benefin, benefin + trifluralin, dithiopyr, pendimethalin, and prodiamine. Many of these products are formulated on a fertilizer carrier that supplies spring fertility as well. Using them according to label directions should supply the necessary level of active ingredient for weed control, and provide all the fertilizer the grass needs for spring greening. There is also an organic PRE crabgrass control product on the market as well: corn gluten meal (CGM). CGM works by releasing a protein that slows development of weed seedling roots leaving seedlings vulnerable to drought. In periods of extended rainfall CGM will fail to control weeds and its length of activity is very short-lived (perhaps a few weeks) as compared to standard synthetic chemistry which may last 120 days. CGM is not selective to turfgrass seedlings and will control them just as well as crabgrass. CGM works best in cooler climates and in lawns that have good turf density in the spring. Labeled rates of CGM treatments will also deliver approximately 1 pound of water insoluble nitrogen per 1000 sq ft, making CGM another offering in the group of products known as “weed and feed” materials. Repeat applications of CGM for extended weed control will actually supply N that greatly exceeds recommended levels for spring growth. In a transition zone state such as Virginia, CGM has provided acceptable levels of crabgrass control when used as the first spring PRE treatment on moderate density lawns, and where season-long weed control is desired, the second PRE application at 4 weeks after the CGM treatment can be made with a low label rate of a standard synthetic PRE herbicide. For this second PRE treatment, avoid weed and feed products (or at least select those that are very low in N) and try to apply the PRE herbicide alone. This combined approach with CGM and a standard PRE herbicide feeds the plant and reduces the total amount of synthetic chemistry applied.\nTiming of PRE applications? Mother Nature provides a valuable visual tool in the landscape that typically allows us to optimize the timing of PRE herbicides for homeowners: the forsythia. Its blooming can never be taken as an absolute signal of pending crabgrass emergence as Mother Nature is not perfect. However, it works in most years and Dr. Askew’s research has found that the time when forsythia starts to drop its blooms is when PRE herbicides need to be in place in order to achieve optimum crabgrass control. Don’t be alarmed if your lawn care operator has applied earlier in the season; this is simply because with the number of lawns they have to treat, there is no way all applications can be made according to forsythia bloom. The standard PRE herbicides they are using have soil activity for 6-8 weeks that will address a broad window of crabgrass germination potential. Having the PRE products in place ensures no escapes in crabgrass control by missing the appropriate application timing.\nPost-treatment considerations? One thing required for all PRE herbicide applications is to water the product into the soil with either a suitable rainfall or irrigation event. The only way the product works is if it gets into the top of the soil profile to form a chemical barrier that germinating seedlings penetrate. Appropriate moisture is critical to optimize herbicide efficacy. And remember to keep all products on the turf and off hardscapes. This is the easiest way to protect our water resources.\nThis podcast was developed by Mike Goatley and Shawn Askew, Extension Turfgrass Specialists at Virginia Tech. Commercial products are named in this publication for informational purposes only. Virginia Cooperative Extension does not endorse these products and does not intend discrimination against other products which also may be suitable.","Kikuyu Grass Seedx\nKikuyu Grass Seed\nKikuyu is a warm season grass that is hardy, self repairing and drought tolerant. It has a soft, bright green leaf and when mowed regularly makes a beautiful lawn that requires minimal lawn care to remain healthy.\nKikuyu seed is quick to propagate and grows rapidly. Kikuyu has always been the number one choice for any lawn areas which are expected to be under high traffic conditions, including trampling by cattle and vehicles, as this grass repairs itself quicker than almost any other grass.\nAs Kikuyu is a warm season grass the active growing phase is during the warmer months in Spring and Summer. Kikuyu becomes dormant in Winter, however unlike most Kikuyu varieties, our seed will not go yellow after one frost in the winter. It will eventually go yellow after multiple frosts, however it has been shown to be resistant to colder weather in the winter. Kikuyu will return to its green color when the warmer weather returns. Kikuyu can be oversown with a Perennial Ryegrass to provide a year round green lawn and protection of the grass while dormant.\n- New Lawns: 10 grams per square meter (1kg/100m2).\n- Overseeding: 5 grams per square meter (1kg/200m2).\nFor more information see below for Description and Features to see if this grass suits you!\nFROM $ 112Delivery Info\nKikuyu is a runner variety of grass that can spread rapidly via its Rhizomes and Stolons. It may appear that the leaves of the grass are attached to a thick cord, which is more visible than the cords of other warm season runner variety grasses. It is native to East Africa.\nOptimal pH: Kikuyu performs best in soils with moderate acidity (pH 5.5-7.0), but tolerates a pH as low as 4.5\nSowing Tips: Kikuyu prefers fertile, moderately drained, light to medium textured soils. Broadcast the seed at, or very near, the soil surface and incorporate lightly with raking. To ensure best coverage and avoid seed wastage we recommend using an appropriate seed spreader for the size of your area.\nOptimal Planting Time: The best time for sowing Kikuyu is late Spring to late Summer. As long as the temperature is within the optimum temperatures of 18-29°C you can expect germination.\nFor cooler areas of Australia allow enough time after sowing for seedlings to establish to enable them to withstand the first cold days of Winter and frost. A guide for this is approximately 6 weeks post germination. Therefore, sowing early Autumn is usually the latest timeframe recommended for these areas, with Spring to late Summer being preferred.\nGrows best between 15 and 25oC but will tolerate up to 40o C. Growth ceases when exposed to regular heavy frosts.\nGermination: Within 10–21 days fine Kikuyu hairs should appear, however this is condition and climate dependent.\nWater: With the seed so close to the surface, it should be watered quite frequently with light water applications to maintain soil moisture consistently until germination is complete. This will keep the seedlings from drying out as they sprout. Once established it tolerates low water and drought conditions well. It can tolerate some waterlogging as well as moderate salinity levels.\nFertilising: Kikuyu is particularly demanding for Nitrogen (N) and Phosphorus (P). The McKays Slow Release Fertiliser is the perfect fertiliser for the needs of Kikuyu. This can also be used in combination with the seed when sowing.\nFirst Mowing: First mowing should occur when the runners are approximately 20cm long (not high) and have taken root.\nMowing Frequency: Kikuyu should be kept to around 5cm in height to reduce thatching of the grass. It is one of the fastest growing warm season grasses and will have increased mowing requirements through the active growth phase in the warmer months. During dormancy in the winter months the growth of Kikuyu will slow significantly and reduce the mowing requirements.\nDisadvantages of Kikuyu:\n- Pasture dominated by Kikuyu is unsuitable for horses to graze\n- More mowing requirement than some other warm season varieties\n- Can be susceptible to diseases such as Kikuyu Yellows\n- Easily spreads into other vegetation and areas where it is not required to grow, grass clippings should not be used in compost as this can cause seeds collected via mowing to germinate\nSeed & package details: 100% Kikuyu Seed\nIndependently Tested: 85% Min Germination, 98% purity. A seed testing certificate is available upon request.\nIdeal Usage: Residential lawns, paddocks, coastal areas, acreages, sporting fields, playgrounds, airports, pastures, caravan parks.\nWhere can I sow this grass in Australia? Areas: AUSTRALIA WIDE\n**Please note this product can not currently be sent to WA due to quarantine restrictions**\nShade tolerance: Fine\nDaily Sunlight Required: 6+ hours\nDrought tolerance: Excellent\nFrost Resistance: Moderate\nTraffic Tolerance: Excellent\nWarm season grasses thrive during the warmer months in Spring and Summer and may go dormant in cooler climates through winter.\nCool season grasses thrive during the cooler months in Winter\nShade tolerance (Weather)\n- Full Sun\n- Part Shade\n- Heavy Shade\n- Full Shade\nDrought tolerance (Weather)\nTraffic tolerance (Weather)\nAverage days to germination\n- 2-3 Weeks\n- 2-4 Weeks\n- 3-4 Weeks\nFree Shipping On All Orders Over $50\nWe provide free shipping to Australia for all orders where the total cart value exceeds $50.00 AUD\n$9 - Orders are dispatched within 24 hours via regular Australia post(3-5 Business days)\nExpress Post Transport Within Australia\nHave your order sent via AusPost EXPRESS POST for a flat rate of $15\nDue to quarantine restrictions, a $75 fee may apply on any seed products shipped to WA that would need to be cleared by customs. For any WA orders, contact us directly to enquire and place an oder\nPick-up Not Available\nUnfortunately, we do not offer local pick-up"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:80cd23c4-98a8-4aa7-b742-8ed7d76b5d9c>","<urn:uuid:a96f5108-b415-4e9b-aa65-554959a4c203>"],"error":null}
{"question":"What are the key differences in shoulder muscle engagement between proper alignment in Parsvakonasana and poor shoulder positioning in inversions?","answer":"In proper Parsvakonasana alignment, the shoulder muscles work in harmony with rotator cuff muscles being stretched while shoulder muscles like the trapezius and levator scapulae are toned in a balanced way. Conversely, poor shoulder positioning in inversions typically involves harmful hunching where shoulders creep toward the ears, creating a 'banana pose.' This misalignment causes the rhomboids and levator scapulae to become overused and tense, leading to chronic tension and potential rotator cuff problems. The distinction is particularly important as proper alignment in both cases requires the shoulder blades to be drawn down the back, away from the ears, to maintain healthy shoulder function.","context":["A great source for Yoga Postures by Mark Giubarelli.\nYoga Positions – Parsvakonasana\nTranslation: Extended Side Angle Pose\nFrom Tadasana step the legs apart. Turn one leg out and bend the knee over the ankle. From Uttanasana step one leg back and externally rotate it until the heel touches the mat. From the Downward Dog Pose step one leg forward between the hands. Externally rotate the back leg until the heel touches the mat.\nTurn the foot of the back leg 90 degrees or slightly inward. Place one hand on the mat beside the bent knee and the other hand diagonally out. Stretch the side of the body. In easier versions of this pose the elbow can be placed on the bent leg or on a block.\nThere are many theories of how this pose should be modified. The back foot does not come to the mat easily if the angle across the body is one clean line from the back legs to the extended arm and fingers, but it does look and feel good.\nBoth feet have a tendency to Evert (Pronate). This means that all the weight falls onto the inside of the feet. This happens especially in the back leg. Try to get the pinky toes to the mat. To do this you must engage the Tibialis posterior and Tibialisanterior.\nThe extended side angle stretches the side of the body. However it is a pose that involves all the muscles in the body.\nThe Groin and shoulders are stretched. The muscles around the hip of the leading leg are stretched and toned. The gluteal muscles are toned and the muscles around the lateral lower abdomen are stretched. Toward the lower back the quadratus lumborum a big muscle that runs along the top of the pelvis and attaches to the bones in the spine is significantly stretched.\nThe shoulders muscles such as the rotator cuff and closer to the spine the rhomboideus major and minor get a good stretch. Muscles in the shoulders and neck are toned such as the upper trapezius muscles and the levator scapulae. Arm muscles are also toned. The latissimus dorsi is stretched. This is a big muscle that attaches to many bones in the spine (from the sacrum to the lower shoulders) and gathers beside the ribs to attach to the humerus.\nSee other yoga sequences that have Extended Side Angle Pose.\nMuscles involved in the external hip rotation in the back leg are the quadratus femoris, gemellus superior and inferior, obturator internus and externus and the piriformis, they are all strengthened. When they are used the inner leg muscles in the back leg become stretched. They are the pectineus, adductor brevis, adductor magnus, gracilis tendon of the psoas major and further up the pelvis the iliacus on the outer part.\nThe Leg adductors are stretched in the back leg. Some of these are , the pectineus, adductor brevis and adductor longus, the gracilis and adductor magnus.\nVinyasa Yoga postures to transition from and to:\nTadasanaVirabhadrasana 2 Urdhva Virabhadrasana 2 Virabhadrasana 3 Parivrtta Ardha Chandrasana Ardha Chandrasana Anjaneyasana Adho Mukha Svanasana Eka Pada Adho Muka Svanasana Utkatasana Uttanasana Vrksasana Trikonasana Parivrtta Trikonasana Revolved Lunge Virabhadrasana 1 Parsva Dandasana Parsvottanasana\nYoga Teacher Tips\nThis posture requires a flexible groin. Without it the knees has a tendency to turn in. When it is opened out the hip roles in. Therefore to master this work your own groin flexibility and the groin flexibility of the students.\nSun Salutation with Parsvakonasana\nClick to Enlarge Pic","Each time you lift your arms, your shoulder muscles—both big and small—initiate a dance full of subtle nuances. The complex interaction of those muscles, coupled with the unique structure of the shoulder joint, gives your arms a wide range of motion. In fact, the shoulder is one of the loosest joints in the body. But this freedom of movement comes at a price: shoulders are vulnerable to injury both from sudden falls and from repetitive action such as throwing a baseball. The muscles of the rotator cuff, the most delicate movers of the shoulders, are particularly susceptible. But here’s the good news: a regular, targeted asana practice can help you maintain healthy rotator cuffs by bringing awareness to your alignment, strengthening your shoulder muscles, and opening your chest. And several of the poses described in the pages that follow can even encourage the healing of rotator cuffs if you’ve already injured them.\nThe Anatomy of the Shoulder\nLet’s take a look at the special nature of the shoulder joint and, in particular, its relationship to the shoulder blade. Though it is considered a type of ball-and-socket joint, the shoulder is unusual because the rounded “ball” or head of the humerus (i.e., the arm bone) doesn’t have a corresponding socket. Rather, the ends of the collarbone and shoulder blade come together to form a shelf under which the humerus hangs. This shelf is known as the acromion process. Beneath it there is a rounded depression that is part of the shoulder blade. This is as close as the shoulder gets to having a “socket”; the head of the arm bone glides against this surface as it rotates, and the steady contraction of the rotator cuff helps to hold the joint together.\nThe rotator cuff actually comprises four separate muscles—the supraspinatus, the infraspinatus, the teres minor, and the subscapularis—which wrap over, in front of, and behind the head of the humerus and stabilize the joint. These deeper muscles are layered over by larger, stronger muscles that attach directly to the acromion process. The muscles of the rotator cuff guide the actions of the arm bone itself, while other larger muscles control the actions of the shoulder as a whole, with both arm bone and shoulder blade functioning as a unit.\nHow Injuries Occur\nThe most common rotator cuff injury occurs at the outermost corner of the shoulder, beneath the deltoid (the large muscle you use to lift your arm). The injury is to the supraspinatus, a small muscle that attaches directly to the head of the humerus and assists the deltoid in lifting the arm overhead. The very strength of the deltoid is often the cause of injury to the supraspinatus.When you take your arms overhead, the deltoid is able to raise the arm to about 80 degrees from the body. At this point, the deltoid can’t do much more lifting on its own: the arm bone is almost level with the shoulder, and from this angle the deltoid can only pull the arm bone into the joint rather than lift it higher. As the arm continues to rise, the deltoid relaxes somewhat and the supraspinatus jumps in to help: it raises the arm for the next 30 to 40 degrees, after which the deltoid can resume its work.\nIt is within this range of 80 to 120 degrees that the supraspinatus can get hurt. The tendon of the supraspinatus, which is about the size of a large rubber band, is the part of the muscle most often injured, though the muscle itself can also tear. This can happen especially in hasty and aggressive adho mukha svanasana (downward-facing dog) poses, as well as in flamboyant versions of the newly popular vasisthasana (side plank pose), and in advanced arm balances such as tittibhasana.\nSimple accidents can also injure the supraspinatus tendon. For example, if you slip in an icy parking lot and use your arm to break the fall, the humerus gets jammed in the socket, pinching the supraspinatus against the acromion process or even tearing the tendon. The simple repetitive action of raising your arm can also be at fault. When you reach for something on a shelf above you, the deltoid can pull the arm bone up too hard, pressing it against the acromion process, thus pinching the supraspinatus. Over time, these little injuries add up to a more serious problem.\nThe shoulder is built to avoid this pinching, but our patterns of use and everyday life lead to imbalance, pain, or lack of mobility. The problem starts with postural habits: many of us overuse the muscles of the shoulders to support the weight of our arms. The muscles closest to the neck (the rhomboids) and those running from the tops of the shoulder blades up into the neck itself (the levator scapulae) take the brunt of the weight. This is especially problematic during arm-intensive activities such as typing, when your shoulders become set in a perpetual shrug. Chronic tension builds up, pulling the inner corners of your shoulder blades up toward your ears, causing your back to round and your shoulders to hunch. This is the beginning of a vicious cycle: the more your shoulder blades creep up the back from the pull of these muscles, the more your muscles tense and shorten, pulling your shoulder blades up even higher. As a result of this tension and the postural misalignment that ensues, the deltoid is far less likely to relax when it’s supposed to. If your shoulders roll foward and the deltoid remains fully engaged as you lift the arm from 80 to 120 degrees, it can cause the humerus to press against the acromion process, pinching the rotator cuff tendon.\nThere are a variety of yoga poses that can help break the cycle and restore strength and balance to the shoulder muscles—from simple standing poses in which you hold your arms aloft in various positions to those in which your arms directly support the weight of the body. The standing poses described below can help you reestablish the healthy mobility of the shoulder blades as you lift your arms; they will also enable you to activate other muscles to ease the burden on the rhomboids and levator scapulae. The inversions, particularly the headstand, strengthen the shoulder muscles, keeping them more open and stress-free.\nFreeing the Shoulder Blades\nTo begin, extend your arms out to either side in warrior II pose. Make sure your arms are in the same plane as your shoulders or slightly forward of the shoulders. To experience “the shrug,” rotate your hands and arms so your thumbs face downward: feel how the muscles on either side of your neck hunch upward, the deltoids tense, and the shoulders feel blocked.\nNow rotate your hands and arms so the palms face up, even reaching your little fingers upward. The hunching dissipates: the upper inner corners of your shoulder blades release down your back, softening the sides of your neck. Feel how the weight of your arms is supported more by your shoulder blades, which are planted firmly on your back, and less by your neck: you’ll especially feel a firming of the muscles at the outer edges of your shoulder blades, as the deltoids soften and the shoulder joints begin to feel more open and free. Do a few small arm circles to feel the support offered by the shoulder blades.\nThe same hunching tends to happen in parshvakonasana (side angle pose) when you extend the top arm overhead. Many students have trouble straightening the arm: the deltoid is tight, the shoulder is pinched, and the neck feels cramped, making it uncomfortable to turn the head. The problem begins once again with the shoulder blade, which fails to release down the back so that the arm can swing into place in the shoulder joint.\nTo release the shoulder in the side angle pose, take your top arm slightly in front of your body and, while extending out through the little finger, rotate your arm in a tiny arc, making a C shape with your hand, as if you were dipping your little finger in a bowl of ice cream. The shoulder blade will release down your back and away from your ear, and the humerus will swivel into place next to your ear, making space for your head to turn. It’s this simple yet elegant movement of the shoulder blade that opens the shoulder, and also, through a subtle downward pull of deeper muscles in the back and shoulders, prevents pinching of the rotator cuff.\nProtecting the Shoulder Joint\nFreeing the shoulder blades is just the beginning. Protecting and healing the rotator cuffs—the supraspinatus in particular—involves not just realigning the bones, but activating and strengthening the muscles meant to counteract the upward pull of the deltoid. Straight-arm poses such as downward-facing dog and adho mukha vrksasana (handstand) certainly make use of these muscles, but the shoulder is also at its most mobile and vulnerable in these positions. It’s safer to begin with variations on shirsasana (headstand), in which the position of the arms and shoulders is more stable. And since the aim of these headstand preparation exercises is to make the arms more weight-bearing, the neck can remain safe: little, if any, weight needs to be placed upon the head.\nDownward Dog on the Wall\nThis exercise will help you establish correct alignment in your shoulders without putting weight on your arms. Stand facing the wall and place your forearms on the wall in headstand position, with your fingers interlaced and your elbows shoulder-width apart. Keep the palms of your hands separated so that your arms form an upside-down U shape, rather than a V. Walk your feet back as you bend forward from the hips. Maintaining the U shape, let your arms slide down the wall until your body is at more or less a right angle (knees can be bent if necessary), and your head is in line with your upper arms; the top of your head should not touch the wall.\nLightly engage the inner edges of your biceps, drawing energy from your inner elbows toward your armpits. This action stabilizes and protects your shoulders because, when engaged, the biceps draw the arm bones back into the shoulder joints. To keep your shoulders open and prevent pinching in the joints, isometrically squeeze your elbows toward each other while firming the biceps, as if you were squeezing a beach ball between your elbows. Feel how your upper back broadens, much as it did when you turned your little fingers upward in the warrior II exercise.\nPress your forearms into the wall to stretch your upper body through the shoulders and away from the wall. If you are nursing an injury, go only as far as you can without pain or stiffness in your shoulders; your head may only be an inch or two from the wall at first. That’s fine. Notice how pressing through your elbows makes your shoulder blades firm into the back, creating more space within the shoulders. For comparison, press more with your wrists and see how your triceps and deltoids activate, making your shoulders tighten and hunch. Pressing through the elbow when the arm is weight-bearing activates the deeper muscles—latissimus dorsi, subscapularis, and teres major—that pull the head of the arm bone down and back, away from the acromion process; this prevents the pinching of the supraspinatus. While extending back as you press through your forearms into the wall, avoid overly rounding your upper back: let your spine descend from between your shoulder blades toward the floor, while keeping your arms active. Hold the stretch for about 30 seconds.\nHeadstand with a Chair\nIn the next variation, the arms bear more weight. This is where a prop becomes helpful. Place a sturdy chair against a wall so that it will not slide. Sit in front of the chair, facing away from it, and extend your legs so that you can measure a leg’s distance from the chair. When you go into the pose, you will place your elbows where your heels are.\nNow come away from the chair and place your hands and arms in headstand position, with the elbows on the floor at the spot you just measured. With your feet on the floor, toes curled under, lift your hips up as if you were doing downward-facing dog. Lightly engage your inner biceps and push your forearms down and away from you as you lift and stretch your hips back, creating a straight line from your elbows to your hips. Rest the crown of your head on the floor between your hands, pressing through the arms firmly enough so that most of your weight is on your arms, not your head. Hold the stretch for about 30 seconds, keeping the arms engaged.\nIf you can keep most of your weight on your forearms and do not experience pinching in the shoulder joints, then step first one foot and then the other onto the chair to elevate the hips, bringing more weight into your arms. (To protect your neck and build strength in your shoulders, you may want to practice with your head entirely off the floor, lifting so that your head comes in line with your upper arms.) Press the whole forearm into the floor, especially through the elbows.\nIn the next exercise, remove the chair, then measure a leg’s distance from the wall. Rest the crown of your head on the floor between your hands, with your fingers interlaced, your hands cupped around the back of your head, and your wrist bones perpendicular to the floor. If you have neck concerns, you’ll still want to keep your head off the floor, although it will be rather demanding. Come into the downward dog version first, then step your feet up the wall, so that your body is at a right angle—an upside-down version of the exercise with which we began. This pose is sometimes called urdhva dandasana. Press through your arms to take the weight off your neck and to lift your shoulders away from your ears and toward your waist. This engages and strengthens the muscles that pull your arm bones away from the acromion process, keeping the supraspinatus safe and allowing it to heal.\nHow effective are these exercises for mending rotator cuff injuries? In one study published in the International Journal of Yoga Therapy in 2006, 10 people with rotator cuff injuries practiced similar variations of the headstand for 30 seconds daily for six weeks, with follow-up sessions every six weeks, for an average of 4.9 months. Nine out of 10 patients reported improved range of motion in the shoulders and reduced shoulder pain after the initial 30-second session. At the final follow-up, eight patients showed significantly improved range of motion and a 75 percent reduction in pain. None went on to surgery, which is unusual for people with rotator cuff injuries who report a significant amount of pain.\nIf you already practice the full headstand and are aware of the alignments and precautions, you can include this asana in your shoulder-strengthening-and-healing routine. While in the pose, your whole forearm should press fully into the floor to distribute your body’s weight.\nIn the headstand, the shoulders have a tendency to hunch toward the ears. As a result, the body takes on a “banana pose.” If you feel compression in your lower back or neck, it’s a sign that you are indeed in a banana shape. Press a bit more through your elbows. At the same time, with your thighs firm and steady, take your hips back slightly to align them more directly over your shoulders, and take your feet forward, reaching up through your inner heels and the mounds of your big toes. Keep your lower belly firm to steady yourself; you should feel your neck lengthen as you extend down through the crown of your head.\nAs you press your elbows more fully into the floor, you will be able to broaden your shoulders, lifting them away from your ears and toward your waist. Feel how the outer edges of your shoulder blades engage. The muscles you are using are the very muscles which, when awakened and strengthened through practice, create balanced action within your shoulders and protect the rotator cuffs from injury.\nBecause your shoulders play an important role in so many daily activities—playing musical instruments, typing, driving, not to mention practicing yoga and playing sports—it’s well worth the effort to include inversions and shoulder-strengthening exercises in your daily routine. And in the end, when the burdens of the world prove to be a bit too much for your shoulders, the work of turning upside down can give them some welcome relief. Atlas himself could have used a good headstand."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:52b055e3-4c40-4197-80b2-1fcc63503a26>","<urn:uuid:33d070eb-59ad-4203-bbf6-d805f20fb35f>"],"error":null}
{"question":"How do scientific findings about human brain function challenge common workplace practices, and what role does the default mode network play in task efficiency?","answer":"Scientific findings directly challenge the popular workplace practice of multitasking, as research has proven it to be a modern myth. Studies show that switching between tasks actually reduces productivity and effectiveness - for instance, talking on a phone while driving impairs performance nearly equal to being drunk. The Harvard Business Review found that workers accomplish less when frequently switching between tasks. However, the discovery of the default mode network (DMN) reveals how the brain can work efficiently in certain situations. The DMN functions as an autopilot system that enables quick decision-making during familiar tasks, suggesting that instead of attempting to juggle multiple tasks, workers might be more effective focusing on one task until it becomes routine enough for the DMN to assist in its execution. This aligns with management expert Peter Drucker's advice that effectiveness comes from concentration and doing one thing at a time.","context":["Multitasking: The Modern Myth\nYou are busy. Regardless of your profession, on a daily basis you are inundated with a seemingly endless stream of phone calls, emails, text messages and meetings. Because of the rapid pace and high demands of the modern workplace, everyone is in search of ways to increase their productivity. One popular technique that many utilize in an attempt to execute numerous tasks quickly is multitasking. When a person attempts to multitask, he or she is trying to accomplish two or more tasks at the same time. Many professionals claim that their ability to multitask is one of their most essential skills. These workers contend that multitasking allows them to remain productive even when overwhelmed with activity. Yet, there is one problem with multitasking… it is impossible.\nResearch scientists have made some startling discoveries regarding the capabilities of the human brain. Science has confirmed that though the brain is an extraordinary organ, it has limited cognitive abilities. George Miller, the great cognitive psychologist, wrote about the brain’s limited capacity to be attentive to and process information in his famous article, “The magical number seven, plus or minus two: some limits of our capacity for processing information” which was published in the Psychological Review. Miller demonstrated how the brain can only grasp a small amount of information at one time. This is why phone numbers, excluding area codes, are only seven digits. Scientists maintain that if phone numbers were more than seven numerals they would be forgotten with far greater frequency.\nThe limitations of the brain to concentrate upon multiple stimuli are why multitasking is an impossibility. The brain can only contemplate one idea at a time. As Dr. Pierce Howard, Director of Research for the Center of Applied Cognitive Studies affirmed, “Notwithstanding teenagers’ claims that they can do homework in front of the television set, the brain cannot focus on more than one stimulus at a time.” Neuroscientist John Medina echoes this assertion when he declared, “research shows that we can’t multitask. We are biologically incapable of processing attention-rich inputs simultaneously.” Nobel Prize winning economist Herbert Simon emphasized that human beings consciously “operate largely in serial fashion. The more demanding the task, the more we are single-minded.” Psychologist Edward Hallowell aptly summarizes the mental impossibility of multitasking by comparing it to playing tennis with numerous tennis balls.\nWhat You Are Doing When You Think You Are Multitasking\nWhen many people learn about the human brain’s inability to multitask they become perplexed. This confusion is derived from the experience doing what they deem multitasking. So the question is, if the brain cannot multitask, what is it doing when people believe they are multitasking? Scientists have identified that when the brain attempts to multitask it is actually diverting its focus from one task and giving it to another. As well-known research scientist Mihaly Csikszentmihalyi affirms, “Humans cannot really successfully multitask, but can rather move attention rapidly from one task to another in quick succession, which only makes us feel as if we were actually doing things simultaneously.” Though your brain can maintain a basic awareness of its surroundings, it can only thoughtfully deliberate one idea at a time. So as you read this article and ponder the concepts it is espousing, your brain is unable to simultaneously contemplate what you had for lunch yesterday. Your brain can only think about this article or yesterday’s lunch, but not both.\nThe reality is that those who believe that they are skilled at multitasking simply have good memories that allow them to remember the thoughts they had before they jumped to the other activity. Nevertheless, regardless of the strength of one’s memory, juggling multiple tasks concurrently will hinder productivity. There is a vast amount of scientific research which has verified that bouncing back and forth between tasks lengthens the time needed to complete the tasks and reduces one’s effectiveness in the execution of the tasks.     For example, a research study which examined the effects of talking on a cell phone while driving found that by focusing on a phone call, the driver’s impairment was nearly equal to being drunk. In addition, the Harvard Business Review published the results of a study that analyzed the behaviors of daily workers. The research found that the more the workers moved back and forth from task to task, the less they accomplished. The conclusion of the research was that workers should “stick to one thing at a time.” \nThough the notion of multitasking may seem alluring, science has proven it to be a modern myth. The human brain can only concentrate on one idea at a time. Consequently, by focusing on one task at a time you will improve your productivity. As legendary management expert, Peter Drucker stated, “If there is any one ‘secret’ of effectiveness, it is concentration. Effective executives do first things first, and they do one thing at a time.”\nClick here to download this article in pdf.\n G. A. Miller. “The magical number seven, plus or minus two: some limits of our capacity for processing information.” Psychological Review, 63, 1956. p. 81 – 97.\n Pierce J. Howard. The Owner’s Manual for The Brain. (Austin: Bard Press, 2006). p. 497.\n John Median. Brain Rules. (Seattle: Pear Press, 2008). p. 85.\n Herbert Simon. Administrative Behavior, 4th edition. (New York: Simon & Schuster, 1997). p. 90.\n Edward Hallowell. Crazy Busy: Overstretched, Overbooked, and About to Snap! (New York: Ballantine Books, 2007). p. 19.\n R. Rogers and S. Monsell. “The costs of a predictable switch between simple cognitive tasks.” Journal of Experimental Psychology: General, 124, 1995. p 207-231.\n R. Meuter and A. Allport. “Bilingual language switching in naming: Asymmetrical costs of language selection.” Journal of Memory and Language, 40, 1999. p. 25-40.\n J.S. Rubinstein, D.E. Meyer and J.E. Evans. “Executive Control of Cognitive Processes in Task Switching.” Journal of Experimental Psychology: Human Perception and Performance, 27, 2001. p. 763-797.\n N. Yeung and S. Monsell. “Switching between tasks of unequal familiarity: The role of stimulus-attribute and response-set selection.” Journal of Experimental Psychology-Human Perception and Performance, 29, 2003. p. 455-469.\n Harold Pashler. “Dual-Task Interference in Simple Tasks: Data and Theory.” Psychological Bulletin, vol. 16, no. 2, 1994. p. 241.\n David Strayer, Frank Drews and Dennis Crouch. “A Comparison of the Cell Phone Driver and the Drunk Driver.” Human Factors, vol. 48, no. 2, summer 2006. p. 381 – 391.\n “The Multitasking Paradox.” Harvard Business Review, March, 2013. p. 30.\n Peter Drucker. The Effective Executive. (New York: Harper Collins, 2006). p. 100.","Posted on 24 October 2017\nScientists, including Dr Deniz Vatansever from the University of York’s Department of Psychology, have shown that far from being just ‘background activity’, the so-called ‘default mode network’ may be essential to helping us perform routine tasks.\nWhen we are performing tasks, specific regions of the brain become more active – for example, if we are moving, the motor cortex is engaged, while if we are looking at a picture, the visual cortex will be active. But what happens when we are apparently doing nothing?\nIn 2001, scientists at the Washington University School of Medicine found that a collection of brain regions appeared to be more active during such states of rest. This network was named the ‘default mode network’ (DMN). While it has since been linked to, among other things, daydreaming, thinking about the past, planning for the future, and creativity, its precise function is unclear.\nAbnormal activity in the DMN has been linked to an array of disorders including Alzheimer’s disease, schizophrenia, attention-deficit/hyperactivity disorder (ADHD) and disorders of consciousness. However, scientists have been unable to show a definitive role in human cognition.\nNow, in research published in the Proceedings of the National Academy of Sciences (PNAS), scientists have shown that the DMN plays an important role in allowing us to switch to ‘autopilot’ once we are familiar with a task.\nDr Vatansever said: “Rather than waiting passively for things to happen to us, we are constantly trying to predict the environment around us.\n“Our evidence suggests it is the default mode network that enables us do this. It is essentially like an autopilot that helps us make fast decisions when we know what the rules of the environment are. So for example, when you’re driving to work in the morning along a familiar route, the default mode network will be active, enabling us to perform our task without having to invest lots of time and energy into every decision.”\nDr Vatansever carried out the study as part of his PhD at the University of Cambridge, before joining York as a Research Associate last year.\nSenior author Dr Emmanuel Stamatakis from the Division of Anaesthesia at the University of Cambridge, said: “The old way of interpreting what’s happening in these tasks was that because we know the rules, we can daydream about what we’re going to have for dinner later and the DMN kicks in. In fact, we showed that the DMN is not a bystander in these tasks: it plays an integral role in helping us perform them.”\nThis new study supports an idea expounded upon by Daniel Kahneman, Nobel Memorial Prize in Economics laureate 2002, in his book Thinking, Fast and Slow, that there are two systems that help us make decisions: a rational system that helps us reach calculated decisions, and a fast system that allows us to make intuitive decisions – the new research suggests this latter system may be linked with the DMN.\nThe researchers believe their findings have relevance to brain injury, particularly following traumatic brain injury, where problems with memory and impulsivity can substantially compromise social reintegration. They say the findings may also have relevance for mental health disorders, such as addiction, depression and obsessive compulsive disorder, where particular thought patterns drive repeated behaviours, and the mechanisms of anaesthetic agents and other drugs on the brain.\nThe research was supported by the Yousef Jameel Academic Program, The Stephen Erskine Fellowship from Queens’ College Cambridge, and the NIHR Cambridge Biomedical Resource Centre.\nHis study of the default mode network, published in Proceedings of the National Academy of Sciences (PNAS), was carried out as part of his PhD at the University of Cambridge. He is now working on neural representations of mind-wandering (daydreaming) with Dr Jonathan Smallwood at York."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:660da43e-81da-42e4-b51f-c968c422b6b5>","<urn:uuid:249d25e3-a06f-4d9a-960d-3bc7228176d9>"],"error":null}
{"question":"How do the Royal Naval Dockyard in Bermuda and the National Maritime Museum in London compare in terms of their historical preservation efforts?","answer":"Both institutions preserve maritime history but in different ways. The Royal Naval Dockyard in Bermuda focuses on preserving physical structures and buildings, with ongoing refurbishment efforts for events like the 35th America's Cup in 2017, despite challenges like Hurricane Gonzalo damage and shortage of traditional building materials. The National Maritime Museum in London preserves maritime history through a vast collection of 2.5 million exhibits, including paintings, manuscripts, maps, ship designs, and the world's largest specialized marine library with over 100,000 units of literature.","context":["The Society’s connection with Bermuda began in 2007 when the then Secretary Dr Ann Coats visited the archipelago to carry out research. The Bermuda Dockyard Apprentices Project, commissioned by the West End Development Corporation resulted in a pilot report called ‘Origins of Bermuda Dockyard’. This identified apprentices 1795–1797 and revealed that enslaved Bermudian naval base workers were acquiring their freedom by working at the first naval base in St George’s.\nA meeting was arranged with the current ex-Apprentices in the Dockyard, closed in 1951 and managed since 1982 by the West End Development Corporation (WEDCo). WEDCo Acting General Manager Andrew Dias and Chairman Stanley Lee and the ex-apprentices made me very welcome and showed me many parts of the islands and their communities. The Executive Director of Bermuda Maritime Museum and author of Bermuda Forts, Dr Edward Harris, gave me a tour of the Casemates Barracks, freed from most of its 20th century prison alterations, and joined the Naval Dockyards Society. He has been a critical personality in restoring the dockyard buildings to interpret Bermuda’s history in the National Museum of Bermuda which incorporates Bermuda Maritime Museum.\nBermuda was strategically vital to the Royal Navy after Britain lost the North American colonies, its reefs protecting access apart from the defensible Narrows (Hurd’s) Channel. Later, deep water channels near Ireland Island and St George’s allowed inshore berthing for large ships. Continuous Bermuda naval support facilities began in 1795 when the Royal Navy needed a new base in North America during the French and Napoleonic Wars (1793–1815). In that year ‘His Majesty’s Naval Department’ was at St George’s, the oldest and largest settlement at the eastern end of Bermuda.\nBermuda, with Halifax Yard, became crucial in maintaining and refitting British ships, replacing facilities lost during the War for Independence. Midway between Halifax and the West Indian ports, its islands provided sheltered anchorages, hard limestone for wharves and buildings and a unique and durable native ‘cedar’, a juniper Juniperus Bermudiana which resisted shipworm and was ideal for boat building (in particular Bermuda sloops). After 1809 Bermuda built a careening yard and repair base on Ireland Island. From this date it evolved until in 1814 it resembled a home yard in its officer and physical structures, but lacked a dock until 1870. From 1820–1863 convicts were used to build the dockyard, living on hulks moored in the Camber. To complete its facilities, in 1869 HMS Warrior and other ships towed a floating dock from the River Thames across the Atlantic. In 1870 it was moored against the Great Wharf. Bermuda could then be called a ‘dockyard’.\nSince that visit several articles have been written about Bermuda’s dockyard heritage, particularly after the NDS Conference and Tour held there in 2010, whose papers will be published in Bermuda Dockyard and the War of 1812, Transactions of the Naval Dockyards Society, 10 (2015).\nDespite damage inflicted on the islands by Hurricane Gonzalo in October 2014, and a shortage of traditional building materials, it is not all gloomy news. A number of buildings within the dockyard will be refurbished for the 35th America’s Cup, to be held in Bermuda in 2017. On the community side, Norman Scotland, one of the 49 Bermudian dockyard apprentices sent to Portsmouth Dockyard in 1950 aboard MV Georgic, celebrated his 80th birthday in January 2015, and past dockyard workers are being traced through the Royal Gazette publishing old photographs.\nAll photographs taken by Ann Coats except the stairs group, which was taken by Charlotte Andrews.\nBell, J. (30 October 2014). Quarry sources sought as slate demands run high. The Royal Gazette.\nClark, C. (Nov 2012). Bermuda – A post-colonial challenge. Dockyards, 17(2), 21-7.\nCoad, J. (1989). The Royal Dockyards 1690–1850: Architecture and engineering works of the sailing navy. Aldershot, Scholar.\nCoad, J. (2013). Support for the Fleet. Architecture and engineering of the Royal Navy’s Bases 1700–1914. Swindon: English Heritage.\nCoats, A. (July 2007). The Royal Dockyard at Bermuda. Dockyards, 12(2), 16-19.\nCoats, A. (May 2009). Bermuda Naval Base: Management, Artisans and their Enslaved Workers, 1795–1797 – the Heritage of the 1950 Bermudian Apprentices. Mariner’s Mirror, 95(2), 149-78.\nCoats, A with Harris, E. and Andrews, C. (Nov 2012). Bermuda: Britain’s second permanent settlement in the New World and a major Royal Naval Dockyard. Dockyards, 17(2), 5-12\nCoats, A. with Coad, J., Burrows, S., Gray, J. and Hyde, B. (Nov 2012). Going. Going. Gone! Obliteration of Bermuda Dockyard Heritage: Victoria and Albert Rows and Ship Crest Paintings. Dockyards, 17(2), 12-18.\nCoats, A. and Hyde, B. (May 2014). New solutions for Victoria and Albert Rows, Bermuda Dockyard. Dockyards, 19(1), 11-15.\nHarris, E. C. (2001). Bermuda Forts 1612–1957. Bermuda: Bermuda Maritime Museum Press.\nHyde, B and Coats, A. (Nov 2012). The vanishing dockyard houses of Bermuda. Dockyards, 17(2), 18-20.\nIra, P. (3 Jan 2015). Celebrating one of the fabulous 49. The Royal Gazette.\nJohnston-Barnes, O. (28 Jan 2015). Team Oracle submits plans for base. The Royal Gazette.\nJones, S. (14 Jan 2015). Relatives help name Dockyard workers. The Royal Gazette.\nJones, S. (19 March 2015). Premier happy with AC35 progress at Dockyard. The Royal Gazette.\nMacGrath, L. (20 October 2014). Dockyard suffers significant damage. The Royal Gazette.\nSimpson, L. (27 Jan 2015). Bringing Dockyard back to life. The Royal Gazette.","The national Maritime Museum in London\n- Country: England\n- City: London\n- Address: Romney Rd, London SE10 9NF\nThis Museum simply could not appear in London – the capital of great Britain, once the “Queen of the seas.” The national Maritime Museum was founded according to the official decree of Parliament in 1934 and opened on 27 April 1937 by king George VI. It is located in Greenwich (district of London), is a complex of historical buildings of the XVII century, are world heritage sites. These include the Queen’s House and the famous Royal Observatory Greenwich. Here is the Prime Meridian. Now this Meridian are marked with a green laser beam aimed from the roof of the Observatory directly North.\nThe Museum is the largest in the UK marine projects, and, most likely, one of them is and around the world – in his collection of about 2.5 million exhibits, divided into thematic expositions. The whole Maritime history of England is represented in this rich collection. Pictures of marine painters, not only English but also Dutch painters of the XVII century, ancient manuscripts and maps, which sometimes are a now-vanished land, designs of the ancient ships and astronomical instruments, which once checked the time.\nCollection of portrait painting in Maritime Museum is second only to the National portrait gallery. Here are portraits of the great explorers and discoverers of the past, as well as the great people of England, generals and admirals. Among them are portraits of Vice-Admiral Nelson and Navigator and Explorer James cook. Admiral Nelson as the national hero of England devoted an entire gallery. Here his biography and description of battles, and the uniform in which he took the last fight. His uniform pierced by shrapnel on the shoulder, and this injury during Trafalgarsquare battle in 1805 became fatal for him. Here are his personal weapons and includes memories of his contemporaries and associates. Of them you can even learn that the great Admiral has suffered a… sickness.\nHere assembled a magnificent collection of ancient weapons, daggers, swords, guns, grappling fixtures, and naval artillery. Exposed to and military uniforms of different times, medals, items of clothing. Very interesting collection of caryatids – sculptures installed on the nose of the ships.\nAmong the exhibits there are those who were after the Second world war deported to Germany. Among them are a number of paintings and several models of ships. Here they were in accordance with the terms of the Potsdam conference as the spoils of war, however, the charges in the possession of “stolen art” the Museum is still sometimes sneak into print.\nIn the Museum are over one hundred thousand units of specialized and reference literature related to the marine business, and this library is considered the largest in the world. There are unique copies, published in the XV century. In many subjects the Museum regularly organizes exhibitions and actively exchange with other museums of the world, so its treasures may be available for viewing in various parts of not only England but other countries – it would wish."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2bbf23c4-29fa-4c8d-af48-62e42ce7f123>","<urn:uuid:ed38618a-844b-4d19-a193-c8aa9c677b55>"],"error":null}
{"question":"What approaches did Douglas Trumbull and Bill George take to creating realistic organic environments in their effects work?","answer":"While Trumbull focused on developing specialized techniques like the slit cam machine and painting with light to create realistic space environments like Jupiter in 2001, George took a more practical approach for organic environments in Innerspace by using actual organic materials - green onions for lung villi, gourds and cucumbers for body parts, and diamond dust in water for interstitial fluid effects. Both achieved realism but through different methods - Trumbull through technological innovation and George through creative use of natural materials.","context":["©2012 CreativeCOW.net. All rights reserved.\nDouglas Trumbull is far more than a visual effects artist. Certainly, he played significant roles in three of the most powerful and influential visual effects movies of all time: 2001: A Space Odyssey (1968), along with Close Encounters of the Third Kind (1977) and Blade Runner (1982), Douglas also received an Academy Award nomination for Star Trek: The Motion Picture (1979). But he started his career as an illustrator, and his love of art and sci-fi led to a career that includes countless inventions, 22 patents, simulator rides, as well as writing, producing and directing. His visionary developments include Showscan, a filmmaking and exhibition format -- 65mm negative filmed at 60 frames per second, with 70mm prints projected at 60 frames per second -- that presciently predated today's renewed attention to high-frame rates shooting.\nWhile others reflect on his 45 years of achievement, Douglas Trumbull currently has five movies underway, and some strong ideas about saving the future of filmmaking.\nIn Part 1 of our interview with Douglas in the September/ October issue of Creative COW Magazine, he described the development of his 1983 feature film project Brainstorm, which was intended to be for Showscan what Avatar became for 3D, until the project was stymied by studio politics and the death of its leading actress Natalie Wood. This precipitated Trumbull's move from Hollywood to the Berkshires in Massachusetts and the beginning of his career in simulation rides, first with Back to the Future: The Ride for Steven Spielberg. He went on to discuss the lamentable state of motion picture exhibition, and point the way to a future that not only includes higher framerates, but also brighter screens.\nHere in Part 2, he delves more deeply into his career. Douglas was just in Hollywood to receive the 2011 SMPTE Presidential Proclamation, which recognizes \"individuals of established and outstanding status and reputation in the motion-picture, television, and motion-imaging industries worldwide,\" for his more than 45 years of pioneering work in visual effects photography and groundbreaking innovation in motion-picture technologies.\nHe will be back in Hollywood in February to receive the Visual Effects Society's George Melies Award which honors individuals who have \"pioneered a significant and lasting contribution to the art and/or science of the visual effects industry by a way of artistry, invention and groundbreaking work.\"\nHowever, his career is by no means relegated to the past. Douglas most recently served as the Special Photographic Effects Supervisor for Terence Malick's Tree of Life (2011), and has five movies under contract based on new technology he is developing.\nCreative COW recently spoke to Douglas about his past work, his current work, and the industry's future. All of us at Creative COW are honored to have Douglas join us in the pages of our magazine. Many on the Creative COW Team have our own individual stories of the impact that he has had on our careers and directions.\nI can't actually describe my career, so I don't even try. I'm so diverse, it's kind of a problem for me. I get pigeonholed as a geek or special effects guru, but I say I'm a writer-producer-director-engineer-inventor.\nCinerama takes you \"To the Moon and Beyond\"\nMy career started when I thought I wanted to be an architect and studied illustration. I discovered I didn't want to be an architect, but meanwhile I had gotten into photorealistic airbrush illustration. Because I had a long-standing interest in science fiction, my portfolio quickly filled up with pictures of aliens and spaceships.\nI was also deeply interested in animation, so I thought I'd try to get a job doing that. I went around to different studios, including UPA, the animation studio producing the Mr. Magoo cartoons. They looked at my portfolio and said I was in the wrong place and sent me to Graphic Films, a company that had a specialty contract making films for NASA and the Air Force. I immediately got a job there and started doing animation background illustration for these films. We had a job previsualizing the Apollo program and here I was, really a kid, painting lunar landers and vertical assembly buildings.\nGraphic Films got a contract to do a film for the 1964/65 New York World's Fair for the Travel and Transportation pavilion, To the Moon and Beyond\n, which was projected onto a Cinerama 360 dome. I produced a lot of the artwork for the multi-plane and fisheye photography. The movie was a trip to the moon and it became very abstract at the end.\nStanley Kubrick and Arthur C. Clark both saw this film at the World's Fair and hired me. I got a job working with them to make what was then called Journey Beyond the Stars,\nmoving to London when I was 23. This was the ultimate training experience, working under my mentor Stanley Kubrick and other great professional cinematographers, set builders and artists.\nKubrick was a one of a kind, thinking-outside-the-box kind of person. He was extremely brilliant and wanted to make the first scientifically valid sci-fi movie, based on Clark's short story \"The Sentinel,\" which became 2001: A Space Odyssey\n. It was an incredible opportunity for me, and it turns out I had the right skill set. My mother was a commercial artist and my father was an engineer, and I'm very good at engineering and art.\nThe control panels inside the EVA Pod from 2001: A Space Odyssey are seen reflected on the helmet of Dave Bowman (Keir Dullea) in the scene that comes just prior to the Star Gate sequence.\nThe Star gate sequence from 2001: A Space Odyssey\nIn 2001: Space Odyssey\n, there were some intractable problems that had never been addressed in feature films, such as the Star gate sequence. How do you transit thorough space and time visually? I created what I called the slit cam machine, by which I could make an exposure with the camera lens for a minute or two to create these long time exposures of very controlled light effects, which created patterns that didn't exist in the real world. I also figured out a way to paint miniatures to get a tremendous amount of detail and realism.\nI had another technique to make a realistic Jupiter, a kind of painting with light. I took a painting I made of Jupiter and scanned it onto a spherical object. It took me 8 hours to make one exposure of Jupiter at times. But it also made a realistic and effective image of Jupiter, which the illustrators couldn't do.\nI went back to Los Angeles after that and directed, wrote and produced my own feature film, Silent Running\n. (Michael Cimino, Steven Bochco and Derek Washburn collaborated on the screenplay). Based on an idea from Todd Browning's Freaks, Silent Running\nwas about preserving the world's ecosystem against an environmental catastrophe. We pioneered the use of portable 4x5 plate front projection, and were able to do 15 set-ups a day of process photography in addition to the regular schedule. We shot it all in 32 days for $1.3 million.\nDirecting Bruce Dern on the set of Silent Running, which Douglas also produced and wrote, as well as provided Visual Effects\nWhat I didn't know was that the movie was part of a Universal Studios experiment to see if a movie could succeed on word of mouth alone, without any marketing or advertising. Now you can build an audience with social networking and viral media -- but at the time, word of mouth alone was not enough.\nA VISUAL EFFECTS COMMUNITY\nIn those early days, all of us working in visual effects knew each other. We were working on Close Encounters of the Third Kind\nat the same time that George Lucas was doing Star Wars\n. John Dykstra worked on Silent Running\n, and I knew Bob Abel pretty well. [Ed. note: Among the accomplishments of Robert Abel & Associates was the digital animation for the original Tron\n, using software that they developed themselves.]\nFor Close Encounters,\nwe dabbled with the idea of using digital effects, but it would have taken 15 years of key pounding, so we abandoned it. I partnered with my old friend Richard Yuricich and started Entertainment Effects Group in Marina Del Rey to produce the photographic effects.\nClose Encounters of The Third Kind, for which he was also nominated for an Academy Award for Best Visual Effects\nWe pioneered the first real-time, on-location digital recording of camera motion. My father Donald engineered physical motion control rigs [Ed. note: Donald Trumbull also worked in effects on The Wizard of Oz\n], and Jerry Jeffress built the electronics. EEG was also the first company to composite motion control shots in 65mm.\nThe Enterprise from Star Trek: The Motion Picture, for which Douglas received an Academy Award® nomination for Best Visual Effects\nMINIATURES & VIRTUAL SETS\nI'm back at writing, producing and directing. I absolutely intend to do it my way: to control budgets and to use this technology effectively. The creative approach I'm using is completely out of the box to making live action movies. The idea I'm developing is a way to produce films that look like blockbusters but don't cost the same as big blockbusters.\nImage above and title thumbnail, Douglas Trumbull directs calibration testing for real time compositing of live action and miniatures on the set of his feature UFOTOG, with Joshua Crane shooting.\nI've come up with my own maverick philosophy about digital effects. Even when it comes to the most expensive digital effects produced by the best companies on the planet, the computer graphics often quickly look out-dated, superseded by even better CG.\nA lot of movies made a few years ago don't look good today. They would never pass muster. But movies I've worked on using miniatures look good and don't age. If I did Blade Runner 2\ntoday, I would use miniatures, even though I would use digital compositing and other digital effects far superior to optical printing.\nAbove, Douglas and one of the miniatures he designed for Blade Runner. Courtesy Warner Bros. and Douglas Trumbull.\nBut I think miniatures have been prematurely abandoned, and I don't know why. The fact is that if anyone went into a meeting and proposed miniatures, they'd be thrown out. People think it's antiquated, old school and not as good as digital effects. In fact, I took the virtual set concept to Hollywood 15 years ago and went to the heads of all the studios with demos I'd shot at my studio to show the feasibility. And I didn't get one callback. No one was interested. Now I think everyone will be interested. Audiences have been fractured and we need to make a better product for a lower price.\nRight now, I'm developing films that are going way forward in what's possible with digital technologies. I'm working on a radical new concept that takes advantage of the fact that we can create CG in real-time and put actors into sets and locations in real time.\nDouglas shoots a still of actor Randall Nickerson, in anticipation of live action \"virtual set\" compositing.\nDouglas' assistant Joshua Crane lines up the shot of actor Randall Nickerson on the green cyclorama.\nActor Randall Nickerson composited into early miniature test set.\nIt allows us to produce a real-time live action animatic of the film in the same way that the animation industry can previsualize their films. They film their storyboards, use temp dialogue and music and go through six or eight iterations before they commit to the final render. They de-bug their movies as they make them. Pixar is the leader in the world in doing this, but all the animation companies do the same thing. It allows them to figure out way in advance what's wrong, what's missing.\nI'm trying to bring that creative, iterative process to live action so that we can shoot live action rehearsals -- the whole cast on an empty stage -- very quickly, and then in the final iteration, replace the CG virtual environment with miniatures to get the realism I want to have.\nThe idea is to make a virtual set of any set or location and get it put into the computer as a virtual model that can be performed in real time. The actors on a set can appear to be in this virtual set, and you can shoot in real-time, do rehearsals and mock-ups. Then, by the time you get to the point where you want to shoot the real thing, the virtual set has been modified, texture-mapped, made much more realistic and you shoot that main unit production with your principal cast.\nMeanwhile, you've taken the database and used 3D stereo-lithography to create miniatures that are exactly the same as the virtual set. A miniature looks as good as the real thing, but it's a fraction of the cost. After you've done that, you shoot the miniatures to replace the virtual sets.\nThe RED Epic camera on the Trumbull-designed zeroG jib.\nThe zeroG jib's \"elbow\" is unique to the system, allowing an additional axis of motion, making it possible to move the camera in any direction without friction or mass impeding the move. Also shown on the elbow: some of the encoders used to record motion data and transform it to coordinates for real-time matchmoving with the shooting of miniatures for compositing.\nDouglas' assistant Joshua Crane lines up the RED Epic aboard an encoding Talon head from Sorensen Design, who built the zeroG jib arm.\nI'm working on the virtual set technology with Unreel Pictures, which provides real-time 3D graphics solutions for broadcast and feature film production, and is a leading purveyor of virtual sets for the major networks, cable networks and the Pentagon.\nI'm also working with General Lift, the final frontier of motion control technologies. They have motion control digital camera systems like what I designed 30 years ago, coalesced into one locale. Joe Lewis, who runs General Lift, has been buying up surplus equipment, such as gear from Kerner Digital when they closed. He's the keeper of what some people think of as antiquated mechanical systems, but I think it's the next great step toward integrating miniatures into movies.\nI have five films lined up, and recently built a stage with an 80- foot wide greenscreen at my home in Massachusetts where I can not only experiment and de-bug and perfect virtual live action, but I can do it in a way that doesn't cost an arm and a leg. I can do a proof of concept and demo. We can show an investor that we can shoot 100 set-ups a day. It's a different approach, and I don't see anyone else doing this.\nDouglas on the 60' by 92' stage at his complex in the Berkshire Hills of Western Massachusetts. Originally conceived as an R&D facility to develop and perfect a better way to make sci-fi movies at lower cost, it has now turned into a ready-to-shoot feature film studio that includes production offices, screening rooms, editorial facilities, a machine shop, wood shop, and a metal shop.\nA MAVERICK'S WAY FORWARD\nIt's a period of tumultuous change and extremely high costs in the movie industry. Hollywood still has one foot in the old world of sets, big stages, and expensive locations. And they add to that a gazillion dollars of special effects to make these visual effects-driven summer popcorn movies.\nThe only way the major studios justify themselves is to make these blockbusters. They don't make midlevel movies at all anymore. That's led to an exhibition crisis where the theatres don't have enough different kinds of movies to fill their seats. Regal Cinemas and Open Road Pictures have even formed their own company to address this. It's a reversal of when the studios owned the theatres.\nAs it is now, it's a very dysfunctional business. Even if you have a blockbuster success, the producers may not see a penny for years. Perhaps in the future, the studios will no longer have an exclusive stranglehold on distribution. We'll see tremendously different forms of distribution including possible direct linkages to theatres, so a producer will be able to make a direct deal with a theater chain without making a deal with a studio.\nI think the visual effects industry is being taken advantage of, when they're the epicenter of Hollywood's strength. Right now, the major studios pit the visual effects companies against each other in these vicious bidding wars. The CG companies are providing infrastructure to the movie, making characters, set extensions, wardrobes, props, but they're not participating in the profits. Instead, with these bidding wars, they're bidding against each other and constantly at risk of closing their doors.\nI don't think collective bargaining is the answer, but maybe writers, producers, directors can arise from the ranks of the visual effects industry, and take control of the process and have a piece of the back end. Not everyone is built to be a producer, writer or director, but more and more visual effects artists will emerge who can make the transformation from artist to filmmaker.\nPart one of Creative COW's interview with Douglas Trumbull, Douglas Trumbull Sees a Better Filmgoing Future, is available online and in the September/October 2011 issue of Creative COW Magazine.\nSpecial mention also from Forbes Magazine's John Farrell at http://www.forbes.com/sites/johnfarrell/2012/01/08/trumbull-hollywood-should-go-back-to-using-miniatures/\nPictures courtesy Douglas Trumbull, and MGM and Stanley Kubrick Productions, Paramount Pictures, Sony Pictures Home Entertainment, Universal Pictures, Trumbull/ Gruskoff Productions and MGM, respectively.","“We’re Making Fat Cells, Arteries, and a Heart:” Bill George on Innerspace\nToday teenagers interested in the world of special effects are a few tutorials and some affordable software away from getting their feet wet. In 1977, the requirements were a bit more elaborate. It involved woodshop, sheets of styrene, and maybe a few surreptitious pictures taken at a screening of Star Wars.\nThat’s how a teenaged Bill George got his start – making models from scratch dedicated to George Lucas’s space opera. Four years later, George was working on the crew of Return of the Jedi. Now in his 33rd year at Industrial Light & Magic, George has been a part of the Star Wars, Star Trek, Indiana Jones and Harry Potter series, in addition to taking home an Oscar as a member of the effects team behind Joe Dante’s 1987 sci-fi comedy adventure Innerspace.\nIn that film, burnt-out jet pilot Tuck Pendleton (Dennis Quaid) and his exploratory pod are miniaturized and injected into the body of hypochondriac grocery story clerk Jack Putter (Martin Short) after an experiment gone awry. With Innerspace recently released on Blu-ray via Warner Home Video, George spoke to Filmmaker about his work as model shop supervisor on Dante’s film, his start in the business, and which vegetable looks the most like your lung’s villi.\nFilmmaker: Where did you grow up and how did you fall in love with movies?\nGeorge: I grew up in a small town in Northern California called Gridley and I was just completely enamored with Lost in Space and Star Trek, the fantasy science-fiction worlds which were very, very far from the agricultural community that I lived in. After my parents divorced, my mom, brother and I moved to San Diego in the 1970s. When I was a senior in high school, Star Wars came out and rekindled that passion I had for science fiction as a kid. I was old enough that I had a car and I could drive around so I started going to conventions up in Los Angeles and building my own models. I started showing them at these conventions and before you know it I got a job offer to come work on Star Trek: The Motion Picture.\nFilmmaker: That was your very first feature?\nGeorge: Yeah. 1979.\nFilmmaker: Were you using pre-made Star Wars model kits when you started?\nGeorge: Star Wars was a huge surprise. (20th Century Fox) had no idea it was going to be as popular as it was and they were caught kind of with their pants down as far as the toys and models and such. When I first started making models, they did not have kits, so the only option you had was to build things from scratch. Of course that involved this process of doing undercover research to figure out what (the ships) looked like, because all you had in the film was a brief glimpse of the design.\nFilmmaker: What type of undercover research?\nGeorge: There was some media you could get. There was Cinefantastique magazine and American Cinematographer, but, believe it or not, my brother and I would go into the theater with cameras with high speed film and we would take photographs of the screen. We knew the films well enough that we knew when effects were coming up. They turned out pretty well, surprisingly.\nBack in the ’70s it was kind of the golden age because we had shop class and we had metal shop and wood shop and graphic art. So I had this basic training of how to construct things and that was really helpful. And then I started going to this place called TAP Plastics and they had sheet styrene and solvents and all of that kind of stuff. So I had access to that and was able to emulate what I was seeing in these behind the scenes magazines of how to build models.\nFilmmaker: How old were you when you got the job on Star Trek: The Motion Picture?\nGeorge: I was 20.\nFilmmaker: Were you in college at the time?\nGeorge: I was going to San Diego State and I was in my second year when I got the job offer. I talked to my parents about it and they said, “Well, this is a once in a lifetime opportunity to go work on this movie. You should do it and see how it works out. You can always go back to school.” And of course, I never went back. (laughs)\nFilmmaker: It must have been surreal. In 1977, you start building models because of the impact Star Wars had on you. Then by around 1981 you’re actually working on Return of the Jedi.\nGeorge: It was a dream come true. I worked in LA for a couple of years before I got the job offer up at ILM and of course my dream was to work on a Star Wars film. I was first hired by ILM to work on The Wrath of Khan. ILM was expanding. They had previously done two features (simultaneously) — Dragonslayer and Raiders of the Lost Ark. They were poised to do three films at the same time — Star Trek 2, Poltergeist and E.T. Because they were going to be doing so many projects, there was a rush of hiring and I was one of the people who got hired for that growth.\nFilmmaker: Did you have any input on which films you were assigned to?\nGeorge: Back then you pretty much got assigned to one (at a time), but we all ended up working on different things. The Wrath of Kahn was the main thing I was hired for and I ended up working on the Reliant and some of the Genesis planet stuff.\nFilmmaker: What do you miss most about the days when you were primarily working on practical effects?\nGeorge: There certainly are factions out there (who believe differently as to) which is better. For me, it was kind of time to move on. I love making models — I still do it as a hobby — but as far as for a living, I was ready to do something else. I was a little bit tired of leading the model shop. And the fact that I’ve been at ILM 33 years is because every project is different and over the years I’ve done different (jobs). Otherwise I think I would’ve gotten really bored.\nFilmmaker: What’s an aspect of digital effects work that you prefer?\nGeorge: A lot of the tools are the same, but as far as the types of projects we work on, there’s a huge variation in that. For instance, right now I’m working on a couple of theme park rides for Disney, which I really enjoy. The last film project I did was Unbroken with Angelina Jolie and talk about something different. That was a World War II drama, very different from the space battle-type projects I’ve done a lot of at ILM. That’s one of the things I really like is getting into the mindset of whatever project and director I’m working with to make sure that the work we do at ILM meshes with their (vision).\nFilmmaker: I imagine that the technology required now for your job evolves quickly as well.\nGeorge: That’s a good point. You just can’t sit still or you’ll become obsolete. There is a constant learning process. For me, I also love leading a group of creative people. We’re pulling something from nothing and it’s not an exact science. You really have to think on your feet and you never know who is going to provide the solution. We have a lot of old timers here that have a lot of experience, but sometimes it’s the newbie who’s looking at things from a fresh perspective that gives you the ultimate answer to a question.\nFilmmaker: Let’s backtrack to the practical effects days of Innerspace. Had you worked on any of Joe Dante’s other movies?\nGeorge: I worked on Explorers, which was a very, very fun project. I did a lot of ship construction and prop design. A lot of it was shot up in the (San Francisco) bay area. The scene where the two kids are cornered by that big spider was shot at ILM and I got to puppeteer the robot arms that were pulling things out of their pockets. I’ve always liked Joe Dante’s movies. When I found out Innerspace was happening I was super excited because I was also a big fan of Fantastic Voyage from the ’60s.\nFilmmaker: Did you look at movies like Fantastic Voyage or The Incredible Shrinking Man for reference?\nGeorge: Not really. We didn’t want it to look like Fantastic Voyage. They made their cell walls stained and we didn’t want any of that. We wanted it to look a lot more pleasant. One of our tricks was to use organic (material), like the villi inside the lungs were made from green onions. We went and bought a ton of them and stuck them in clay and made a mold and cast it in latex rubber. Then we shot them underwater so everything was moving all together in these waves. We used gourds, cucumbers, just about anything from the grocery store we could use because they had an organic, alive feeling. Up until that point most everything I’d built was hard-surface — spaceships, landscape miniatures, that kind of stuff. Then all of a sudden we’re making fat cells, arteries, and a heart. Everything was new and different.\nFilmmaker: Was there a process of trial and error in learning which materials to use and what would photograph well?\nGeorge: There was a lot of research and development. (Tuck’s) ship was described in the script as a miniature submarine, so the basic idea was that everything was like it was underwater. There’s interstitial fluid between the cells. There’s fluid in your blood vessels. It was like a submarine movie, but instead of being underwater it was inside the body. I worked with John Fante, who was our lead camera guy, and Harley Jessup, who was the (visual effects) art director, and we’d try anything. Harley found this stuff, diamond dust, that goes into women’s cosmetics. It’s a very, very fine glitter. We put just a little bit of that into the water and then any time the miniature sub’s lights passed through it, you’d get this really amazing sparkle that gave a sense of volume to the water.\nFilmmaker: How many versions of Quaid’s vessel did you construct?\nGeorge: There were two full pods. One was maybe a foot across, and then another one that was 2 1/2 or 3 feet for some of the tighter shots. Then we built some larger sections with things like the grabber arms and thrusters. We ended up with probably a half dozen different bits and pieces of Tuck’s sub to do all the work.\nFilmmaker: What went into building the blood vessels and arteries?\nGeorge: The artery raceway was so many departments coming together and contributing to the success of that sequence. A lot of what drove and still drives what we do is the camera and the engineering department devised a special camera rig with a waterproof housing. Then we had water going through the raceway with blood cells in it and the blood cells had to be transparent and neutrally buoyant so they wouldn’t sink or float, which was a real technical challenge. And because we’re zipping along in the blood stream, there also had to be a current. So the stage guys had to rig a large pump that pushed a huge volume of water through it.\nBeing in the artery raceway, you also have to ask yourself, “Where’s the light coming from?” The trough that it was built in was plexiglass and then we built a clear urethane tube that went inside. So you could light the plexiglass from the outside and you’d get this soft glow. But we didn’t want it to be so bright that it looked like the sun was shining through his body. (laughs)\nFilmmaker: The highlight of the miniature work comes in the finale when a henchmen named Igo is shrunken down and injected into Martin Short’s character in order to take out Quaid’s hero. What were some of the challenges in that sequence?\nGeorge: We used a new system that had been developed called pin-blocking. It was a way of match-moving a live-action element and then taking moving footage and tracking it to that (live-action) movement. We used it for the thrusters and also for putting the footage of Dennis Quaid inside the miniature pod. For that big end sequence we kind of pulled out all the stops. There was a lot of miniature work, blue screen with rod puppets, some of it was shot in the water tank and some of it was shot dry-for-wet, and then the backgrounds all had to be shot. But a lot of it was super, super low tech, like we needed to move this giant esophagus so we just got three stagehands and they grabbed ahold of it and we said, “Okay guys, move rhythmically.”\nFilmmaker: That sequence ends in Martin Short’s acid-filled stomach. How was that set devised?\nGeorge: That was shot in a Doughboy pool that we built a base around. The stage guys rigged up these things that would (make) giant, roiling bubbles. Then we put micro-balloons in the water so it always had this nasty froth on the top. You had to use every part of your brain to make this stuff work. It was disgusting and funny, but very technical.\nFilmmaker: Was the ILM team involved with the practical effects after villain Kevin McCarthy gets shrunken down to child-size? There’s a great scene in a limo which combines forced perspective and puppetry where McCarthy and his mini-henchman grapple with Short and Meg Ryan as their car careens through San Francisco.\nGeorge: Those puppets were all built at ILM in our creature department and that scene was shot on our main stage. We built a double-scale front and back of the car. It was really elaborate, but a very fun sequence, because even though it took a lot of set up, once we started shooting it was all happening live right there in front of the camera. It wasn’t shooting elements and then comping them together.\nFilmmaker: Did you have a video tap for a monitor or any sort of playback?\nGeorge: As I recall I don’t remember any video playback. Certainly we could look through the (viewfinder) and line things up. The background out the window was I think front projected — it was either front or rear projected — and it took up the entire length of our stage. In fact, I think some of the interior of the car went outside the stage because they needed so much throw in order to project the images behind them. Back then, you’d shoot a bunch of (stuff) and then there would be a (usable) little snippet here and a snippet there. When (McCarthy is poking Short in the eyes), it’d just be, “Okay, try it again. A little higher. A little lower.” You just go and you go and you go and you burn through a lot of film and if your editor is good they’re going to be able to pull out little moments that work.\nFilmmaker: Any other Innerspace stories you want to share before we wrap it up?\nGeorge: We had a lot of challenges on the show, and one of them was that we wanted (the miniature sequences set inside Short’s body) to look realistic and we wanted it to look beautiful. We didn’t want it to be gross — and trust me, things get gross pretty quick for people. It was a mandate every day to try to make it look clean and healthy and alive. There’s a scene in the film where Tuck has first been inserted (into Short) and he doesn’t know where he is yet and he uses his laser gun and cuts the artery and he goes inside it. We tried really hard to make sure that it didn’t look too painful. So Harley Jessup and I are at one of the preview screenings up in Sacramento and right when that scene happens there were these two teenage girls in front of us and they turned to each other and they go, “Ew, gross.” We were like, “Well, I guess we failed on that one.” (laughs)\nMatt Mulcahey writes about film on his blog Deep Fried Movies."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:87cff5d0-5519-436e-8b70-9f84c4699372>","<urn:uuid:e198040e-e5bf-4767-b2ee-612ab112166c>"],"error":null}
{"question":"Which process helps create more jobs: recycling or composting?","answer":"Recycling creates more documented jobs. According to the census data, recycling and reuse activities created 757,000 jobs in the United States in 2007, generating $36.6 billion in wages and $6.7 billion in tax revenues. Specifically, for every 1,000 tons of recycled material, 1.57 jobs are created with an average wage of $76,030. The documents don't provide specific employment figures for composting, only mentioning its benefits for gardening and waste reduction.","context":["Recycling is the process of turning old used materials into new ones. Through recycling and reusing waste, energy and raw materials are saved.\nWith the continuously increasing population and use of disposable products and packaging, our finite natural resources are being depleted at an alarming rate.\nCurrently less that 25% of our waste is being recycled, with the remaining being buried or incinerated in landfills. This seems absurd when we could be reusing and/or recycling more than 70% of the waste we produce.\nNearly everything we use in our everyday lives can be recycled including aluminum cans, aluminum foil and bake ware, steel and tin cans used for soup and coffee, cardboard including milk and juice cartons, magazines, office and newspaper, phone books, most glass products, plastics bottles, jars, and jugs, car and household batteries, light bulbs, electronics, and even food.\nIf just half of Americans recycled on a regular basis, it would reduce greenhouse emission by the same factor as taking 25 million cars off the road.\nBy recycling we could reduce exploitation of natural resources, save money, reduce pollution and waste, and create jobs and boost the economy.\nThis article will discuss the many benefits of recycling including environmental and economic benefits.\n1. Preserves Natural Resources & Prevents Habitat Destruction\nMost of the world’s natural resources are finite, meaning they are limited and will run out at some point. Preserving these natural resources is a primary concern for those concerned about the longevity of natural resources available for human use.\nWe can reduce the consumption of natural resources by using recycled materials to make new products and packaging. Preventing waste through source reduction before it is generated can further reduce the need for disposal and save more resources.\nRecycling can result in products better than those made by the virgin materials. For example, after being processed for recycling, the tin in bimetallic cans is more refined and as a result more valuable.\nFor every ton of steel recycled, 40 pounds of limestone, 1000 pounds of coal, and 2,500 pounds of iron ore are saved. By recycling tin we can reduce the need for raw material which reduces mining and its associated pollution.\nAccording to the Pennsylvania Department of Natural Resources, in 2005 the state saved 1.4 million tons of iron ore, 829,786 tons of coal, and 71,124 tons of limestone by recycling over 1.2 million tons of steel.\nBy reducing land disturbances and pollution associated with mining and extraction of new materials we are decreasing the degradation of natural ecosystems and wildlife habitats.\nPaper recycling plays a direct role in the preservation and biodiversity of forests, by lowering the demand for wood.\nThe longleaf pine forest in the southern United States used to cover 90 million acres, but today less than five percent remains due to harvesting mature longleaf pin for the production of wood, paper, and other paper products.\nThe longleaf pine forest is home to more than 20 endangered species. By recycling paper and paper products we are reducing the pressure on the remaining longleaf pine forest and preserving habitat for these endangered species.\n2. Creates Jobs & Benefits the Economy\nRecycling plays an important role in the economy by ensuring waste is re-used and reduced.\nStudies have shown that for every one job in waste management there are four jobs in recycling. After the recycling process, even more jobs are created for making new goods out of the recycled materials.\nIt has been estimated by the Office of the Federal Environmental Executive that recycling and remanufacturing industries create more than $1 billion in revenue and hundreds of thousands of jobs in manufacturing. Jobs range from high quality product manufacturing to materials handling and processing, employing low-, medium-, and highly-skilled workers.\nAccording to the most recent census data, recycling and reuse activities in the United States in 2007 created 757,000 jobs, $36.6 billion in wages, and $6.7 billion in tax revenues.\nTo put these numbers into better perspective, for every 1,000 tons of recycled material 1.57 jobs are created with an average wage of $76,030, and $14,101 in local and state tax revenue is generated.\nRecycling also saves communities money in waste handling, landfill production, and incineration costs associated with burning garbage because waste is being recycled and reused rather than put into the landfills.\nBy buying recycled products and packaging we can create an economic incentive for recyclable materials to be collected, recycled, and manufacture into new products. This creates a closed loop system that reduces the costs of recycling.\n3. Saves Energy, Reduces Pollution, & Preserves Landfill Space\nRecycling reduces pollution because manufactures are reusing materials instead of creating new ones, which also saves energy, and toxic chemicals and greenhouse gases aren’t being released into the atmosphere though incineration in landfills.\nBy recycling hazardous waste, it is prevented from making it to the landfills where it can potentially contaminate water sources through seepage, which has been known to happen.\nThe EPA estimates that 0.1% to 0.4% of surface aquifers are contaminated by landfills and industrial impoundments leaching metals, mineral, explosives, bacteria, viruses, and other toxic substances.\nAnything more than 0% is unacceptable.\nAccording to Stanford University, the amount of energy that is lost by throwing away recyclables such as aluminum cans and newspapers is the equivalent to the annual output of 15 power plants.\nFor every one ton of recycled newsprint 1.7 barrels of oil, 7,000 gallons of water, 4.6 cubic yards of landfill space, 601 kilowatts of energy is saved, and 60 pounds of air pollutants are prevented from being released into the atmosphere.\nOne ton of recycled news print saves 9.0 barrels of oil, 7,000 gallons of water, 3.3 cubic yards of landfill space, 601 kilowatts of energy, and 60 pounds of air pollutants from being released.\nRecycling one ton of plastic saves 16.3 barrels of oil, 5,774 kilowatts of energy, and 30 cubic yards of landfill space.\nFor every one ton of glass 0.12 barrels of oil, 42 kilowatts of energy, and 2 cubic yards of landfill space are saved, and 7.5 pounds of air pollutants are prevented from being released into the atmosphere.\nRecycling one glass bottle saves enough energy to power a light bulb for 4 hours.\nWhen you recycle aluminum, 95% of the energy required to make the same amount of aluminum from its virgin source is saved. Recycling one ton of aluminum saves 40 barrels of oil, 14,000 kilowatts of energy and 10 cubic yards of landfill space.\nWith this type of data, recycling should seem like a no-brainer.\nAs you can see by recycling you are reducing pollution, conserving resources, saving energy, promoting the economy, and creating jobs.\nWhen we are only recycling a quarter of what we produce in waste, not only are we wasting precious natural resources and polluting the environment, but we are also basically throwing away energy and destroying land and habitats just to create more energy to waste.\nThe problem of waste may be the most worrying concern for the environment and overall health of human beings, but there is a way to solve and mitigate this problem. Think about nature and your own family’s well-being as well as the health of future generations and reduce, reuse, and recycle.\nIt is our responsibility to ensure that waste is recycled, and to protect the environment for future generations. By instilling the morals of recycling into our own lives, community, and children we can create an increased awareness on necessity to prevent waste and recycle.\nIf you have ideas on recycling, please continue this conversation in the comments.\nFeatured Image Credit: Homard.net @ Flickr","What is Composting and Why is Composting Important?\nComposting is a natural process by which organic materials decompose. Composting is nature's way of recycling organic material such as leaves, grass clippings, twigs, fruits, and vegetables into a dark, crumbly, earthy-smelling soil conditioner. By concentrating the activity in one place and balancing food, air, and water, compost happens faster.\nRemember, composting is just another form of recycling. When you compost, you are tapping into the natural nutrient cycle. In nature, organic waste from plants and animals is recycled by decomposition. Composting is controlling that decomposition to speed it up and produce a stable and odorless material for plants to use.\nOrganic waste material, like yard clippings and food scraps, can be given \"new life\" through composting. Finished compost is a wonderful soil amendment that improves texture and adds important nutrients into the soil in your garden, creating healthy, thriving plants. From tomatoes to tulips, compost keeps your garden growing strong! If residents compost their yard and kitchen waste, we can go a long way toward satisfying California's challenging goal of a 75% total reduction in waste.\nBenefits of Composting\nOrganic wastes, such as food waste and yard waste, make up 25 to 50% of what people throw away. While you may not be able to compost all of the organic waste you generate, composting can significantly cut down on your overall trash.\nWhen we throw away yard and food waste, it decomposes in a landfill and releases methane gas, a potent greenhouse gas. While most landfills have technology to capture much of this methane, eliminating the gas at its source is even better.\nOther benefits of composting include:\n- Saves you money by replacing store-bought soil conditioners\n- Helps garden and house plants by improving the fertility and health of your soil\n- Saves water by helping the soil hold moisture and reducing water runoff\n- Benefits the environment by recycling valuable organic resources, reducing transport and processing of materials, and reducing waste to our landfills\nWhat can I compost?\nYard waste, such as fallen leaves, grass clippings, weeds, garden plants remnants, and kitchen scraps make excellent compost. However, care must be taken when composting kitchen scraps. Meats, bones, and fatty foods (such as cheese, salad dressing, and leftover cooking oil) do not belong in the bin. Place those items in the garbage.\nWhat Goes In the Compost Pile?\nBrowns are dried or dead organic materials that serve as sources of carbon. Browns are useful for retaining moisture, creating small air pockets, and supporting a more diverse community of decomposers in the pile. Browns include woody materials, dead or dried yard debris, chopped branches and twigs, bark, straw, sawdust, coffee filters, tea bags, shredded paper and paper products.\nGreens are fresh organic materials that serve as sources of nitrogen. Greens are the primary energy source of the active microorganisms, and are useful as a supplementary source of moisture in the pile. Greens include fresh yard trimmings, fresh grass clippings, fresh or moldy fruit and vegetable scraps, coffee grinds, tealeaves, breads, certain types of manure.\nWater helps ensure efficient processing of organics. Ideally, the pile is kept as moist as a wrung out sponge. Too little moisture will inhibit decomposition, but too much water can produce smelly, anaerobic conditions.\nAir is essential for a sweet, earthy-smelling compost pile. Turning your compost pile regularly will help to inhibit the growth of odor-causing anaerobic bacteria, and will result in faster decomposition.\nComposting can be practiced in any home, apartment, or townhouse. Identify a place in your yard that is out of the way but accessible to deposit yard and food waste. Make sure you can reach this location with a garden hose. There are many ways to construct a compost bin starting from the very simple - a pile on the ground to the fancy store bought composting bin with aeration holes and turning mechanism.\nMost people choose to construct a simple box form out of old lumber or pallets with slats spaced apart for adequate ventilation. This is cheap and keeps with the reuse theme.\nNow start creating a pile of vegetation. Try for a 2:1 ratio of brown materials (dried leaves, woody material, etc.) and green materials (food scraps, grass, etc.). Add enough water so that the pile is evenly moistened, like a damp sponge. Turn the pile weekly or when you notice the top layer start to dry out. A good pitchfork is recommended, as it is easier to use than a shovel.\nDepending on variables such a temperature, moisture content, and how often you turn the pile, you could get a rich compost material as soon as a month or as long as a year.\nAvoid the Usual Pitfalls\nThere are times when you may experience an odor emanating from your compost or many flies surrounding it. This is usually due to not immediately covering food waste deposited into the pile. When food waste is added, be sure to turn the pile immediately, or add some fresh leaves or grass on top of it. This will keep the odors and flies away.\nRemember, smaller bits of greenwaste decompose faster than large pieces. If you continually have large quantities of greenwaste to compost, you might consider the purchase of a chipper/shredder to grind your greenwaste into finer particles.\nThere are varieties of problems that you may experience while composting. Follow the guide below to solve your problem!\n- Compost pile smells very bad / offensive\n- Pile isn't heating up / getting warm\n- Material isn’t breaking down quickly\n- Pile is attracting ants\n- Excessive flies and/or rodent foraging\n- Anaerobic conditions (not enough air)\n- Pile too small, too dry and/or not enough greens\n- Not enough moisture and/or large material size\n- Pile too dry, food scraps not buried\n- Food scraps exposed, wrong ingredients\n- Turn the pile, add more browns to the pile\n- Add more greens to the pile, add water while turning\n- Add water, chop materials into smaller pieces\n- Add water, bury all food scraps in core of pile\n- Bury food scraps in core, do not add meat, dairy or oils\nAre Worms Ok?\nAfter a while, you may notice that worms are starting to populate your compost pile. Good job! That is an indicator that you have a good compost mix going on and the worms are happy to be there. Redworms are great at recycling decomposing organic matter into rich humus. They generate nutrient rich worm castings, which improve soil fertility and structure. You can jump-start your worm populations by purchasing redworms at most local nurseries, bait shops or over the internet.\nWhat is Grasscycling?\nGrasscycling is the natural recycling of grass by leaving clippings on the lawn when mowing. They decompose quickly and release valuable nutrients back into the soil.\nAdditional Resources and Information\nThe following websites, videos, and books offer additional information that will be helpful in your composting adventures. Many other composting videos and websites are available so do your research so that you can become the ultimate backyard composter.\nCalifornia Department of Resources Recycling and Recovery (CalRecycle)\nCalRecycle brings together the state’s recycling and waste management programs and continues a tradition of environmental stewardship. Please be sure to check out CalRecycle’s tips on household composting, Home Gardening, and composting bins. CalRecycle also has a great guidance document on building your own composting bin.\nEnvironmental Protection Agency (EPA)\nThe EPA’s mission is to protect human health and the environment. They offer a range of helpful tips including composting at home. Be sure to visit the EPA’s Composting at Home website as it provides guidance, information, and troubleshooting on composting.\nWhether you are composting in a composting bin or simply starting a compost pile, many tutorial videos online can assist you. We have provided a few for you here.\nHow to Make Compost: Presented by Lowe's (2:24 min)\nComposting 101: Presented by Grow Organic (3:54 min)\nThe Master Composter – http://www.mastercomposter.com/\nGuide to Composting at Home – http://www.improvenet.com/a/guide-to-composting-at-home\nOrganic Gardening Guru – http://www.organicgardeningguru.com/\nCompost Mania – http://compostmania.com/blog/\nSearch in the App Store or Google Play if you want to use an app for composting!\n- Home Composting for Organic Composters\n- Compost Works\nThe Rodale Book of Composting: Easy Methods for Every Gardener – The essential guide to composting for all gardeners and environmentally conscious people. By Grace Gershuny, Deborah L. Martin\nThe Complete Compost Gardening Guide – The authors’ bountiful, compost-rich gardens require less digging, weeding, mulching, and even less planting. By Deborah L. Martin, Barbara Pleasant\nComposting: A Practical Step by Step Guide – The guide explains the benefits of composting – to the environment and to your own garden – and describes exactly how composting works. By Victoria Heywood\nHome Composting Made Easy – Fully illustrated with step-by-step, no-nonsense instructions and state-of-the-art advice by gardening and compost experts. By C. Forrest McDowell, PhD, Tricia Clark-McDowell\nComposting: An Easy Household Guide – A full-color guide for both beginners and experienced composters, with an A–Z reference section. By Nicky Scott"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b495e812-e00b-4cff-91fe-df63d0adae3a>","<urn:uuid:1b41579e-a015-49d9-a2a2-ec7776552fcb>"],"error":null}
{"question":"What are the legal protections against greenwashing in consumer markets, and how are fashion brands responding to increased scrutiny of their environmental claims?","answer":"Legal protections against greenwashing include general consumer protection laws like Ireland's Consumer Protection Act 2007, which prohibits false claims about goods or services. The EU Directive on Unfair Commercial Practices considers misleading practices that distort buying decisions to be unfair. Penalties can reach €60,000 or 18 months imprisonment for serious violations. In the fashion industry, brands are becoming more sophisticated in their sustainability communications, though a study found that while 63% of brands communicate about environmental credentials, only 20% publish sustainability reports. To improve credibility, fashion brands are advised to substantiate environmental claims through externally verified sources and develop a pro-environmental culture throughout their organizations to avoid accusations of greenwashing.","context":["How to tell if an ‘eco-friendly’ firm is greenwashing\nWe’re seeing more ‘eco-friendly’ and ‘all natural’ products on our shelves, as consumers demand more sustainable choices.\nBut as companies come under increased pressure to meet their Environmental, Social, and Governance (ESG) goals, how can we tell if some are twisting the truth?\nWhen their sustainable claims are untrue or exaggerated they can be accused of ‘greenwashing’.\nThis basically means they are misleading consumers to make them believe their products or services – or the business itself is sustainable, when that is not the case.\nAre there laws to protect consumers from greenwashing?\nThere are no laws in Ireland that specifically challenge greenwashing.\nHowever, there are more general consumer protections, such as the Consumer Protection Act 2007 (CPA), as Victor Timon, Partner at law firm Lewis Silkin explained.\n“Under the CPA, a seller must not make false claims about any goods or services they are selling,” he said.\nThe CPA now also incorporates the EU Directive on Unfair Commercial Practices as part of that legislation.\n“Under the directive, a commercial practice is unfair if it is misleading and is likely to distort your buying decision,” Mr Timon noted.\nThe Sale of Goods and Supply of Services Act 1980 also includes an implied term that goods will correspond with their description.\nThe Advertising Standards Authority in Ireland (ASAI) also sets rules and issues decisions in respect of false or misleading advertising.\nHowever, Mr Timon pointed out that unlike its UK counterpart, it has no power to enforce those decisions.\nWhat are the penalties for firms that do engage in greenwashing?\nThe Competition and Consumer Protection Commission (CCPC) is charged with policing consumer protection law in Ireland.\nMr Timon said the CCPC can issue compliance notices, apply to court for prohibition orders against traders to prevent certain practices, or ultimately prosecute a trader in court and ask the judge to apply a fine or even imprisonment in certain cases.\nHowever, the CCPC does not get involved in individual cases between traders and consumers, which Mr Timon said must be pursued by the consumer through the courts.\n“Penalties under the CPA for summary judgements (lower courts) are up to €3,000 or a prison term of six months, or both,” Mr Timon said.\n“Convictions on indictment (higher courts) are up to €60,000 or a prison term of 18 months, or both,” he added.\nWhat firms have been accused of greenwashing?\nOne of the most talked about cases is the Volkswagen emissions scandal.\nIn 2015, the US Environmental Protection Agency found that many Volkswagen cars being sold in America had been fitted with a ‘defeat device’.\nMr Timon explained that this was basically a piece of software in diesel engines that could detect when they were being tested, and then changed the performance to make the results look better.\n“Volkswagen was making a big push to sell diesel cars in the US at the time.\n“It subsequently admitted the offence which spilt over into car sales in Europe also – and paid billions of dollars in compensation and fines,” he said.\nSeparately, in 2020, the UK’s Advertising Standards Authority (ASA) upheld a complaint about a Ryanair advertisement, which claimed that the airline had the lowest carbon emissions in Europe.\n“Ryanair has the lowest carbon emissions of any major airline – 66g CO2 for every passenger kilometre flown,” the advert stated.\nHowever, Mr Timon explained that the comparison to other “major airlines” only included four others – and there is no industry definition of what is considered to be a “major airline”.\n“Perhaps the nail in the coffin, was that the figures were based on efficiency rankings from 2011 – of little value for a substantiation claim in 2019,” he said.\nMore generally at the end of 2020, the European Commission and national consumer authorities conducted a sweep of websites across the member states specifically targeting greenwashing.\nMr Timon said in more than half of the cases, the trader did not provide sufficient information for consumers to judge the claim’s accuracy.\n“In 37% of cases, the claim included vague and general statements such as ‘conscious’, ‘eco-friendly’ and ‘sustainable’, which aimed to convey the unsubstantiated impression to consumers that a product had no negative impact on the environment,” Mr Timon explained.\nHe said in 59% of cases the trader had not provided easily accessible evidence to support its claim.\nIn their overall assessment, they believed that in 42% of cases the claims may be false or deceptive – and could therefore potentially amount to an unfair commercial practice under the Unfair Commercial Practices Directive.\nWere any Irish companies fined after the EU sweep last year?\nThree traders in Ireland were approached by the Competition and Consumer Protection Agency in relation to their green claims following the EU wide sweep last year.\nHowever, no penalties were issued.\nA spokesperson for the CCPC said in all three cases, the companies engaged with the CCPC and provided the additional information sought.\nAs a result, the firms carried out a number of actions.\nThese included amendments to their websites and the removal of errors relating to product details.\nThey also amended their websites to provide clarifications and ‘additional substantiation’ regarding environmental claims.\nA spokesperson for the CCPC said they were satisfied with the actions taken by the traders and did not deem that there was a need for enforcement actions to be taken.\nThe CCPC did not provide the name of the companies in question.\n“Due to the nature of the CCPC’s enforcement role and ongoing market surveillance in this area, we are unable to provide any further details relating to the traders in question, at this time,” the spokesperson said.\nThe spokesperson added that the CCPC “regularly” participates in European ‘Consumer Protection Cooperation Network’ (CPC Network) sweeps, which are a set of market surveillance checks that are carried out by the CPC Network across Europe.\nHow can I spot companies that are greenwashing?\nIf you are unsure as to the authenticity of a business or brand’s ‘green claims’, a spokesperson for the Competition and Consumer Protection Commission suggests you follow a number of steps before you buy.\n1 Look for proof of claims\nIf you are unsure about a brand’s ‘green claims’, do some quick research online or via social media to find out more.\nBe wary of businesses with vague or superficial references to sustainability.\nMost genuine brands will want to share details of their ethical and sustainable approach to doing business, so, instead, look for brands with sections of their website or social media pages, dedicated to outlining their ‘green credentials’.\n2 Research the parent company\nAlthough one particular brand or product may prove genuine in terms of their ‘green claims’, it’s important to examine the bigger picture, including any parent company it may be owned by or affiliated with.\nResearch the parent company’s green credentials and look to the overall sustainability lifecycle.\nFor example, a particular brand may use 90% recycled packaging materials, but what about the other processes of the wider business group – such as manufacturing, logistics or sourcing materials?\n3 Don’t judge a brand by its packaging\nWatch out for vague language such as ‘eco-friendly’ or ‘all-natural’, as well as ‘green’ imagery which can be used to convey the product or brand as being sustainable or ethical.\nDon’t judge a brand by its packaging alone.\nBe sure to read the list of ingredients and check the labels thoroughly before you buy to ensure you are satisfied that the claims are genuine.\n4 False certifications\nLook out for fake ‘approved by’ seals or standards marks by institutions that may not even exist.\nIf you’re unsure about a certification seal, do some quick research to find out more before you buy.\nInstead, look out for approved seals such as EU Ecolabel, EU Organic Logo, Fair Trade Certified, and Rainforest Alliance Certified.\nWhat should I do if I suspect a company is greenwashing?\nThe CCPC monitors compliance with consumer protection law on an ongoing basis.\nTheir enforcement work is based on information they obtain through a range of channels including; contacts to their consumer helpline, research, and market surveillance.\n“If a consumer believes that they have been misled about ‘green claims’, we would ask that they contact us so that we can assist them in addressing their issue,” a spokesperson for the CCPC said.\nYou can call the helpline on 01 402 5555 or log on to ccpc.ie for more information.\nAre stronger laws on the way to protect consumers?\nWith the focus growing on sustainability, it is inevitable that specific legislation dealing with green labeling is on the way.\nIn December 2019, the European Commission published the European Green Deal to tackle environmental challenges.\nMr Timon said this will result in an EU regulation requiring manufacturers to be able to prove any statements they make about the sustainability, eco-friendliness, or other “green” attributes of their products.\n“The new regulation has been expected this year – probably to come into force two years after that,” he said.\nIt is expected that this will be followed by specific legislation to empower consumers to pursue greenwashing claims.\nIn the meantime, in respect of a consumers’ general rights, Mr Timon said the ‘New Deal for Consumers’ published by the EU in 2018 contains GDPR type fines for breaches of consumer law.\n“Regulators like the CCPC will have the right to make test purchases and evaluate products against a manufacturer’s or seller’s claims about their performance – including any green credentials,” he said.","The importance of the environmental credentials of fashion brands has grown and, increasingly, marketing functions recognise the need for a convincing and trustworthy approach to the way they conduct business and communicate this with consumers (Goldsmith el al., 2000: 44-45; Ginsberg and Bloom, 2004: 83-84). Coppolechia (2009: 1356) states that over half of the United States ‘baby boomers’ consider themselves to be environmentally conscious consumers and businesses undoubtedly feel the pressure to fulfil their customers’ expectations. In relation to the fashion industry, the green segment of this market is willing to pay more for ecologically responsible products (Laroche et al., 2001: 514; Meyer, 2001: 327-328). The increase in consumer knowledge on environmental sustainability and the preference for strong environmental credentials of products, services and companies has led to the increase in fashion organisations communicating their environmental credentials through marketing activities which has contributed to the creation of the term ‘greenwashing’. Greenwashing is clearly defined by Delmas and Burbano (2011: 65) as ‘the intersection of two firm behaviors: poor environmental performance and positive communication about environmental performance’. Delmas and Burbano (2011) state that three of the key drivers behind firms’ greenwashing is the regulatory context, consumer and investor demand, in addition to limited or imperfect information on a company’s environmental performance, as well as uncertainty about the regulatory punishment for greenwashing in marketing campaigns.\nThe brand comparison website ‘Rank a Brand’ found that fashion and clothing brands are becoming more adept at developing their sustainability communications highlighting an issue that many brand consumer communications regarding green credentials do not match up with efforts to improve their environmental sustainability. The study found that out of the 368 brands investigated, 63% communicate about their environmental credentials on their website however, only 20% publish a sustainability report (Sweet Skins, 2016). To help support marketers with effectively communicating an organisation’s environmental approach and achievements, the Federal Trade Commission updated their Green Guide in 2012; however, as Fernandez (2016) highlights, the major limitation is that they are guides not ‘rules or regulations’ which supports Delmas and Burbano’s claim that uncertainty about regulatory punishment is a key factor which allows greenwashing to continue to proliferate. As the number of companies using green marketing campaigns in the fashion industry continue to grow, this will mean that, firstly, green brands’ messages will become diluted amongst competitors green messages, secondly, it will be increasingly difficult for marketers to achieve significant environmental differentiation, if this is the brand’s aim and, thirdly, consumers will start to becoming wary as the green marketing overload means consumers are unsure which messages to believe.\nThe frustration for fashion brands such as Patagonia and Esprit, who take their sustainability approach seriously and have invested significant resources to achieve this, is that direct competitors can still make environmental marketing claims potentially without having to support these through verified evidence. The Green Washing Index (2017) claims that ‘When properly trained, consumers see right through this “green screen”. Then greenwashing backfires, damaging the company’s reputation and, ultimately, their sales’. Therefore, a recommendation for clothing companies, such as Patagonia and Esprit, is to develop marketing and educational campaigns to educate current and potential consumers within their target market on environmental sustainability and on how to recognise potential greenwashing messages and organisations. This could be achieved through collaborating with industry-specific organisations such as the Ethical Fashion Forum or the Green Washing Index and would help consumers make more informed choices.\nIn relation to marketers attempting to improve fashion brand recognition and preference through green marketing, it is important for a pro-environmental culture and brand identity to be developed. Also, importantly, the environmental claims need to be substantiated through externally verified sources which will improve the credibility of the claims and avoid accusations of greenwashing. Goldsmith et al. (2000: 43-44) and Kim and Damhorst (1999: 20) state that the credibility of the advertiser significantly influences the extent to which the consumer perceives the claims to be made as ‘truthful’ or ‘believable’. Therefore, from a marketing perspective, if significant improvements are desired from green marketing campaigns, it is equally important to focus on internal marketing in collaboration with other functions, for example, sustainability, supply chain management and human resource departments, to embed an environmental sustainability approach into operational decision making. Without a cultural focus (or shift, if this is not the current managerial paradigm) towards a holistic approach to improving environmental sustainability across the organisation, there will be an increased risk that the external green messages do not match, crucially, the consumers’ experience or the internal organisational behaviour.\nCoppolecchia, E. K. (2009) The greenwashing deluge: Who will rise above the waters of deceptive advertising? In: (ed.) University of Miami Law Review, 1353-1407. HeinOnline.\nDelmas, M. A. and Burbano, V. C. (2011) The rivers of greenwashing, California Management Review, 54 (1): 64-87.\nFernandez, C. (2016) Greenwashing in fashion: will sustainable marketing messages ever become easier to navigate? Fashionista. Available at: http://fashionista.com/2016/08/greenwashing-fashion-marketing; accessed 17 January 2017.\nGinsberg, J. L. and Bloom, P. N. (2004) Choosing the right green marketing strategy: green marketing has not fulfilled its initial promise, but companies can take a more effective approach if they realize that a one-size-fits-all strategy does not exist, MIT Sloan Management Review, 46 (1): 79-84.\nGoldsmith, R. E., Lafferty, B. A., Newell, S. J. (2000) The impact of corporate credibility and celebrity credibility on consumer reaction to advertising and brands, Journal of Advertising, 43 (3): 43-54.\nGreen Washing Index (2017) About greenwashing. Green Washing Index. Available at: http://greenwashingindex.com/about-greenwashing/; accessed 17 January 2017.\nKim, H. and Damhorst, M. L. (1999) Environmental attitude and commitment in relation to ad message credibility, Journal of Fashion Marketing and Management, 3 (1): 18-30.\nLaroche, M., Bergeron, J. and Barbaro-Forleo, G. (2001) Targeting consumers who are willing to pay more for environmentally friendly products, Journal of Consumer Marketing, 18 (6): 503-21.\nMeyer, A. (2001) What’s in it for the customers? Successfully marketing green clothes, Business Strategy and the Environment, 10 (5) 317-330.\nSweet Skins (2016) Greenwashing in fashion. Sweet Skins. Available at: http://sweetskins.com/greenwashing-in-fashion/; accessed 17 January 2017."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:d3ba5008-ee5e-4a63-96d7-23bb17a42143>","<urn:uuid:3809d40c-4ea1-421a-a5ad-3ec5cde2b7b0>"],"error":null}
{"question":"Hey climate nerds! 🌍 Which is more uncertain in climate science - the impact of aerosols on global warming or the potential 'tipping points' in the climate system?","answer":"Both factors have significant uncertainties, but they are characterized differently. The impact of aerosols is widely acknowledged as one of the most significant and uncertain aspects of climate change projections, with observed global warming being considerably less than expected due to aerosol effects. Meanwhile, rapid transitions or 'tipping points' in the climate system cannot yet be predicted with confidence, though they could make climate change either less or more severe than current estimates.","context":["Type of news:\nType of programme:\nFilter by wmo strategic priority:\nFilter by regions:\n132 contents match your search.\nPublish Date: 7 May 2021\nA Global Methane Assessment released by the Climate and Clean Air Coalition (CCAC) and the United Nations Environment Programme (UNEP) shows that human-caused methane emissions can be reduced by up to 45 per cent this decade.\nThe impact of aerosols on the atmosphere is widely acknowledged as one of the most significant and uncertain aspects of climate change projections. The observed global warming trend is considerably less than expected from the increase in greenhouse gases, and much of the difference can be explained by aerosol effects. Aerosols impact climate through direct scattering and absorption of incoming solar radiation and trapping of outgoing long-wave radiation as well as through alteration of cloud optical properties and the formation of clouds and precipitation.\nPublish Date: 11 March 2021\nThe tenth anniversary of the earthquake, Tsunami and nuclear accident in Japan is being marked by ceremonies to mourn the victims and reflection on lessons learned, including stronger multi-hazard early warning systems and environmental emergency response coordination, to prevent a future tragedy.\nPublish Date: 11 January 2021\nWMO is hosting a webinar as part of a technical review process of the draft statement on meteorological and air quality factors affecting the COVID-19 pandemic. The webinar takes place on 12 January and the statement is open to scientific review until January 22 nd .\nPublish Date: 11 December 2020\nThe World Meteorological Organization is supporting the First World Virtual High Mountain Summit, which is spearheaded by Colombia. It brings together more than 50 participants from four continents from the public and private sectors, academia, and civil society, to discuss biodiversity and ecosystem services as well as climate variability and change.\nPublish Date: 9 December 2020\nA green pandemic recovery could cut up to 25 per cent off predicted 2030 greenhouse gas emissions and bring the world closer to meeting the 2°C goal of the Paris Agreement on Climate Change, a new UN Environment Programme (UNEP) report finds.\nPublish Date: 3 December 2020\nUN Secretary-General António Guterres delivered a landmark speech on the state of the plane t at Columbia University in New York on 2 December, setting the stage for dramatically scaled-up ambition on climate change over the coming year. He drew heavily from WMO’s provisional report on the State of the Global Climate\nPublish Date: 13 November 2020\nA framework for research linking weather, climate and COVID-19 has been published in Nature Communication. The paper follows the WMO co-sponsored Virtual Symposium on Climatological, Meteorological and Environmental Factors in the COVID-19 Pandemic held in early August.\nBulletin nº Vol 63 (2) - 2014\nPublish Date: 3 November 2014\nCities – particularly megacities – are becoming focal points for climate change impacts. Rapid urbanization, accelerating demand for housing, resource supplies and social and health services, place pressure on already stretched physical, social and regulatory infrastructure, heightening risks and vulnerability. In South America, internal migration flows – as well as immigration – are mostly to cities.","What is climate change mitigation?\nClimate change refers to any change in climate over time, whether due to natural variability or as a result of human activity. The science supporting climate change mitigation seeks to understand these changes and their drivers to improve the knowledge underpinning mitigation policy.\nClimate change mitigation includes actions we take globally, nationally and individually to limit changes caused in the global climate by human activities. Mitigation activities are designed to reduce greenhouse emissions and/or increase the amounts of greenhouse gases removed from the atmosphere by greenhouse sinks.\nGlobal goal for mitigation\n‘During the past million years, the average temperature of the Earth’s surface has risen and fallen by about 5°C, through 10 major ice age cycles. The last 8,000 years, however, have been relatively stable at the warmer end of this temperature range’ (Australian Academy of Science 2010).\nHuman civilisations have flourished in this relatively stable climate. We depend on water supplies that rely on a certain type of climate, and our agriculture depends on certain patterns of rainfall and temperature. A shift in these stable weather patterns will affect all aspects of our lives, including our capacity to grow food, the availability of water, where we live, the infectious diseases and pests that thrive in our region and affect our health, and our ability to keep our homes and families safe from extreme weather.\n‘Dangerous’ climate change\nThe goal for global mitigation was set in 1992 by the (United Nations Framework Convention on Climate Change-UNFCCC) as: ‘stabilisation of greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic interference with the climate system’ (Article 2, UNFCCC).\nAlthough the UNFCCC mentions ‘dangerous anthropogenic interference’, it does not define it. In 2009, a formal definition for ‘dangerous’ climate change was adopted as part of the Copenhagen Accord. Under this agreement, global efforts would focus on limiting the rise in global average temperatures to 2°C above pre-industrial levels.\nOur understanding of climate sensitivity (see Our climate system: how it works and changes) and the consequences of global warming (see Climate change impacts in NSW) continues to improve. In 2010 governments agreed to review the level of agreed global warming and consider whether it should be limited further to 1.5°C above pre-industrial levels (Cancun Agreement).\nUncertainties in projections\nAlthough a precise value for the likely warming cannot be calculated because of uncertainties about how the climate system will respond to small disturbances and to mitigation activities, climate models and evidence from past climate change are able to provide a plausible range of values.\nThe international scientific community is working hard to understand and narrow down the uncertainties in future climate projections, through modelling, observations and synthesis of results.\nIt is possible that there will be rapid transitions in the climate or ‘tipping points’ associated with global warming, but they ‘… cannot yet be predicted with confidence. These uncertainties work in both directions: There is a chance that climate change will be less severe than the current estimates of climate science, but there is also a chance that it will be more severe’ (Australian Academy of Science 2010).\nEffect of greenhouse gas levels on global temperatures\nTwo different approaches—climate system modelling and studying past climates—both tell us that global warming will continue as the concentration of greenhouse gases increases in the atmosphere. Both methods project a warming of around 3°C (within a range of 2°C to 4.5°C due to uncertainty) in response to a doubling of the concentration of carbon dioxide in the atmosphere (Australian Academy of Science 2010).\n'Continued “business as usual” reliance on fossil fuels is expected to lead to a doubling of pre-industrial levels of carbon dioxide by about 2050, and possibly a tripling by about 2100. This emission pathway for carbon dioxide, coupled with rises in the other greenhouse gases, would be expected to produce a warming of around 4.5°C by 2100, but possibly as low as 3°C or as high as 7°C’ (Australian Academy of Science 2010).\nResearch into mitigation\nClimate change mitigation actions are designed to reduce greenhouse emissions and increase the amounts of greenhouse gases removed from the atmosphere by greenhouse ‘sinks’. The oceans, plants and soils are carbon dioxide sinks because they absorb more carbon than they emit (for more information, see Causes of climate change).\nGlobally, scientific and technological research is under way into how to reduce the amount of greenhouse gases that we emit. This includes research into:\n- cleaner energy supplies such as electricity from renewable sources and automotive biofuels\n- using energy more efficiently to avoid burning fossil fuels\n- developing more efficient transport\n- improved cropland and livestock management\n- better waste management\n- reducing land clearing and deforestation.\nAs carbon dioxide accumulates in the atmosphere, natural sinks of carbon dioxide become even more important as a means of absorbing additional concentrations.\n- IPCC Fifth Assessment Report, Climate Change 2014: Mitigation of Climate Change\n- IPCC Fifth Assessment Report, Climate Change 2013: The Physical Science Basis\n- IPCC Special Report: Renewable Energy Sources and Climate Change Mitigation\n- Moss RH, Edmonds JA et al. 2010, 'The next generation of scenarios for climate change research and assessment\n- IPCC 2007: Expert Meeting Report: Towards New Scenarios – Technical Summary\n- IPCC AR5: Synthesis Report"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b034007a-beda-47fd-8df4-76740d34631e>","<urn:uuid:30a940b9-c6f4-4e95-a420-ff1676c115f7>"],"error":null}
{"question":"wat was special bout vinyl records used in SIGSALY encryption system?","answer":"The SIGSALY system used special vinyl records produced by Muzak Corporation that contained random white noise generated by mercury-rectifier tubes. Only three copies of each key segment were made, and the records were designed to be used just once and then destroyed. Operators were instructed to burn the records after playing, reducing them to a 'Vinylite' plastic biscuit. These records served as audio crypto-keys necessary for deciphering encrypted communications.","context":["This edition of the Music of Radio continues to explore developments around electronically generated speech. Homer Dudley, an engineer and acoustics researcher who worked for Bell Telephone Laboratories (BTL), made significant contributions to this field beginning with his invention of the Vocoder and Voder. The development of these two instruments was detailed in last month’s column. Now I will turn my attention to how the Vocoder was employed in encrypting the transmissions of high ranking officials during WWII for the SIGSALY program. SIGSALY, by-the-way, is simply a cover name for the system and is not an acronym.\nIn 1931 BTL had developed the A-3 scrambler that was used by Roosevelt and Churchill, but the security of this device was eventually compromised by German’s at a radio post in South Holland who had been intercepting the Prime Ministers telephone calls. The A-3 had worked with the Trans-Atlantic Telephone by splitting speech up into different bands, but it wasn’t difficult to reassemble as the Germans proved in 1941, making the situation surrounding communications security to become intolerable to the Allies.\nIn 1942 the Army contracted BTL to assist with the communication problem and create “indestructible speech” or speech that could withstand attempts at code breaking. From this effort the revolutionary 12-channel SIGSALY system was born. To create SIGSALY workers sifted through over 80 patents in the general area of voice security. None of these fit the needs of the allies, but Homer Dudley’s Vocoder did and formed the basis of the system. For SIGSALY a twelve-channel Vocoder system was used. Ten of the channels measured the power of the voice signal in a portion of voice frequency spectrum (generally 250-3000 Hz). Two channels were devoted to “pitch” information and whether or not unvoiced (hiss) energy was present. The Vocoder enciphered the speech as it went out over phone or radio. In order to be deciphered at each end of the conversation an audio crypto-key was needed. This came in the form of vinyl records.\nFrom the standpoint of music history it is interesting to note, as Dave Tompkins did in his book How to Wreck A Nice Beach: The Vocoder fromWWII to Hip-hop, that the SIGSALY system employed two-turntables alongside the microphone/telephone. The classified name for this vinyl part of the operation was SIGGRUV. The turntables were used to solve the problem of needing a cryptographic key. They played vinyl records produced by the Muzak Corporation, a company famous for the creation of elevator music. The sounds on these records weren’t aimed at soothing weekend shoppers or people sitting in waiting rooms. Muzak had been contracted into pressing vinyl that contained random white noise, like channel 3 on an old television set. The noise was created by the output of very large mercury-rectifier tubes that were four inches in diameter, and over a foot high. These generated wide band thermal noise that was sampled every twenty milliseconds. The samples were then quantized into six levels of equal probability. The level information was converted into channels of a Frequency Shift Keyed audio tone signal recorded onto a vinyl master. From the master only three copies of a key segment were made. If these platters had been commercial entertainment masters thousands would have been pressed from its blueprint. If any SIGGRUV vinyl still exists, and for security reasons they shouldn’t have, those grooves are critically rare.\nIt had to be insured that no pattern could be detected so the records had to be random noise. If the equipment had somehow been duplicated by the Axis powers, the communications would still be uncompromised as the they required the crypto key of the matching vinyl, required at each terminal. This made the transportation of these records, via armored truck, the most secure since Edison invented the Phonograph. Just as the masters were destroyed after making three keys, each vinyl key was only ever to be played once, as operators were instructed to burn after playing. The official instruction read, “The used project record should be cut-up and placed in an oven and reduced to a plastic biscuit of ‘Vinylite'”. As another precaution against the grooves falling into enemy hands the turntables themselves had a self-destruct mechanism built into them that could be activated in case a terminal was compromised. Thinking of all this sheds new light on the idea of a DJ-Battle.\nKeeping the turntables at two different terminals across the globe synchronized was another technical hurdle that BTL overcame. If a needle jumped or the system went out of synch only garbled speech was heard. At the agreed upon time, say 1200 GMT, operators listened for the click of the phonograph being cued to the first groove. The turntables were started by releasing a clutch for the synchronous motor that kept the turntable running at a precise speed. Fine adjustments were made using 50-Hertz phase shifters (Helmholtz coils) to account for delays in transmission time. The operators would listen for full quieting of the audio as synchronization was established. Oscilliscopes and HF receivers were also used to keep systems locked to international time.\nA complete SIGSALY system contained about forty racks of heavy equipment composed of vacuum tubes, relays, synchronous motors, turntables, and custom made electromechanical equipment. In the pre-transistor era all of this gear required a heavy load of power so cooling systems were also required to keep it all from getting fried. The average weight of a set up was about 55 tons.\nThe system passed Alan Turing’s inspection (if not his test) as he had been briefly involved with the project on the British side. On July 15, 1943 the inaugural connection was established between the Pentagon and a room in the basement below Selfridges Department Store in London. Eventually a total of twelve SIGSALY encipherment terminals were established, including some in Paris, Algiers, Manila, Guam, Australia and one on a barge that ended up in the Tokyo Bay. In the year 1945 alone the system trafficked millions of words between the Allies.\nTo keep all of this operational a special division of the Army Signal Corp was set up, the 805th Signal Service Company. Training commenced in a school set up by BTL and members were sent to various locations. Their tasks required security clearances and a firm grasp on cutting edge technology which they were tasked to operate and maintain. For every eight hours of operation the SIGSALY systems required 16 hours of maintenance.\nIn putting the system together eight remarkable engineering “firsts” were achieved. A review conducted by The Institute of Electronic and Electrical Engineers in 1983 lists them as follows:\n1. The first realization of enciphered telephony\n2.The first quantized speech transmission\n3.The first transmission of speech by Pulse Code Modulation (PCM)\n4.The first use of companded PCM\n5.The first examples of multilevel Frequency Shift Keying (FSK)\n6.The first useful realization of speech bandwidth compression\n7.The first use of FSK – FDM (Frequency Shift Keying-Frequency Division Multiplex) as a viable transmission method over a fading medium\n8.The first use of a multilevel “eye pattern” to adjust the sampling intervals (a new, and important, instrumentation technique)\nTo do all these things required precision and refinement in new technology. SIGSALY has left the world with a rich inheritance that spans developments in cryptology, digital communications, and even left its mark on music.\nHow to Wreck A Nice Beach: The Vocoder from WWII to Hip-hop: The Machine Seaks by Dave Tompkins, Melville House, 2010\nSIGSALY: The Start of the Digital Revolution by J.V. Boone and R.R. Peterson, retrieved at:\n[This article originally appeared in the January 2017 issue of the Q-Fiver.]"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1b2191c0-16b1-46c8-a544-423470f71c03>"],"error":null}
{"question":"As someone interested in cultural symbols, what's the significance of animals in both the Year of the Ox celebration and traditional Valentine's Day gifts?","answer":"In Chinese culture, the ox symbolizes strength, selflessness, and steady progress - it's viewed as a creature that works tirelessly without complaint or need for praise. According to legend, the ox's placement as second in the zodiac came from carrying the rat across a river. Meanwhile, in Valentine's Day tradition, animals appear differently - through symbolic meanings in gifts like flowers, where specific varieties carry romantic messages (for example, roses signifying love). While the ox represents virtues of hard work and dedication in Chinese culture, Valentine's Day animal symbolism is more focused on romance and courtship, such as birds being associated with the mating season which was believed to begin in mid-February during medieval times.","context":["It’s Valentine’s Day today which, after Christmas, is the biggest card sending day of the year.\nLike other celebrations, Valentine’s Day has very long history; however no-one seems to be sure of its origin.\nThere were many early Christian martyrs named Valentine three of whom were remembered on 14 February:\n- St Valentine of Rome who died around 270AD in a Roman prison. One legend says he continued to perform marriage ceremonies after they were banned by the emperor (who wanted to increase the number of bachelors, as they were supposed to make better soldiers than married men). Another legend says he died for refusing to give up his faith. A third legend says he gave a note to a young woman (a jailers daughter in some versions), signed “Your Valentine”.\n- St Valentine of Terni, who is said to have been killed during the persecution of Emperor Aurelian. A bishop who performed miracles, healed people, and was persecuted and beheaded around 170AD.\n- A third St Valentine was martyred in Africa with a number of companions.\nAnother possible source of inspiration is an ancient Roman fertility festival Lupercalia, which used to be celebrated on 15 February. In medieval times people believed that the middle of February was when birds started their mating season.\nWhatever its origins, the oldest known valentine still in existence today is a poem written by Charles, Duke of Orleans to his wife while he was imprisoned in the Tower of London following his capture at the Battle of Agincourt.\nIn Great Britain, Valentine’s Day began to be celebrated widely around the seventeenth century. By the middle of the eighteenth century, it was common for friends and lovers in all social classes to exchange small tokens of affection or handwritten notes – so much so that in 1835 the secretary of the Post office in England was complaining about an additional 50-60,000 letters being sent.\nThe introduction of the Penny Post in 1840 made it possible for more people to afford to send items through the post. Valentine card production became big business and cards sent during this period were often beautifully made and decorated.\nIn the United States the first commercial valentines, made of embossed paper and lace, were sold in the 1840s by Esther Howland of Worcester, Massachusetts whose father operated a large book and stationery shop.\nSome local traditions have grown up in Britain:\n- Jack Valentine disappears into thin air after knocking at the door and leaving gifts in Norfolk. Children are as likely as adults to receive a visit.\n- According to Welsh folklore, ornately carved lovespoons were traditionally made from a single piece of wood by a young man as a token for his sweetheart to show his affection and intentions. Each segment of a lovespoon can have different symbols, and each symbol has a different meaning. Sailors would often carve love spoons during their long journeys, which is why anchors were often incorporated.\nIf you’ve ever wondered why some gifts are supposed to be romantic here are a few pointers:\n- Lace is apparently romantic because women’s handkerchiefs used to have lace around the edges. If a woman was interested in a man, she’d drop her handkerchief, allowing him to pick it up and giving them an excuse for formal introductions.\n- The romance of flowers has its roots in their use as a language, a fashion popular in the 18th and 19th centuries. Each flower would have a meaning: violets – modesty, faithfulness; yellow tulip – hopeless love; snowdrop – hope; red rose – I love you; daisy – innocence, loyal love.\n- Chocolate is supposed to have various effects, ranging from inducing happiness to acting as an aphrodisiac. This is an exaggeration. Chocolate does contain some compounds that are linked to feelings of love, but the digestive process hugely reduces their effectiveness!\nTo find out more or see examples of antique valentines try:\nAnd if all of this is a bit too sickly, The National Archives has a piece about a crime of love at Huntingdonshire archives.","Happy Year of the Ox\nThe Lunar New Year is approaching—New Year’s day is Friday, February 12, in fact. At CAIS we welcome the Lunar New Year with our school’s biggest and most festive community celebration: Mass Greeting or tuán baì 团拜. This year’s Mass Greeting will, of course, be virtual.\nThere are many countries around the world that celebrate the Lunar New Year. One of the unique ways it is celebrated in Chinese culture—and at CAIS—is by associating each year with a symbolic animal. We are coming to the end of the Year of the Rat and welcoming the Year of the Ox. Why is it, in China, that years are symbolized by animals? What is the origin of this tradition? What meanings are associated with the various animals? And will the Year of the Ox be better than the Year of the Rat—please?\nThere are many different theories about the origin of the twelve symbolic animals that form a repeating twelve year cycle. The twelve animals or shíèr gè shēngxiào 十二个生肖 are so woven into Chinese culture that people take them for granted and don’t really know how this cultural tradition originated. Scholars have postulated theories linking the origin to astronomy, astrology, animal worship, meteorologic cycles, and totemism. A modern ethnologist named Liu Xiaohan has posited that the twelve symbolic animals actually originated in the Yí 彝 culture, an ethnic minority in the mountains of China’s southern and southwestern provinces. The fact is that no one really knows for sure—the first written reference to the twelve symbolic animals in a form that resembles their modern configuration was in the Eastern Han 东汉 (25-220 CE) by the philosopher Wang Chong 王充 (c. 20-97 CE) in an eclectic collection of his essays called Lùnhéng 论衡 (not on my list of book recommendations). Yet there have been even earlier archaeological discoveries of references—written on bamboo strips—to a cycle of symbolic animals associated with years that date back to the Warring States or Zhànguó 战国 period (475-221 BCE). This is all to say that this stuff has been around for a long time.\nCultural phenomena that have persisted for so many years in China tend to be the subject of much analysis—both folk and scholarly, and to accumulate a lot of imputed meanings and significance. For instance, it is common to associate the qualities of people with the qualities of the animal of their birth year. If you subscribe to this folk belief it would explain why the current class of CAIS fourth grade girls are destined to rule the world (i.e., a lot of dragons). FYI, I’m a rat—unsurprising, no doubt, to some of you. Some families are reluctant to allow their children to marry if the couple’s birth year animals are considered to be an inauspicious match. (I do not recommend you look into this with your current spouse—marriage is challenging enough as it is!) Some animals are domesticated and helpful (horses, oxen, roosters) while others are wild, unpredictable and may wreak havoc on the human world (tiger, snake, rat). Accordingly, both kinds of people (and years) need to be handled with the appropriate care.\nNumerology has always played a big role in Chinese culture, ancient and modern; think of the five elements (wǔ xíng 五行), double happiness (shuāngxǐ喜喜), eight honors and eight shames(bā róng bā chǐ八荣八耻), and of course the 888 Campus for our Middle School. The 12 year cycle of symbolic animals corresponds to the 12 Terrestrial Branches (dì zhī 地支), a hopelessly complicated (for me, anyway) system used in times past to designate days, months, and years. Each of the 12 symbolic animals corresponds to one of the Terrestrial Branches. If you have any Chinese calligraphy hanging in your home, the artist probably signed it with the date on which it was painted, but you are unlikely to to see anything like “October 1, 1985” or “December 27, 2019”—instead you’ll see a combination of the 12 Terrestrial Branches and 10 Heavenly Stems and you’ll need to visit Google (or Baidu) to figure out the corresponding dates on the Gregorian calendar. You can imagine the market for fortune tellers based on birth date, symbolic animal, potential marriage partner, etc, and most open air markets in China have a few fortune tellers who will happily predict your fate or choose your wedding day based on a combination of these factors, including your birth year animal. I suppose that nowadays you can probably get this service via WeChat, but I’m not that sophisticated of a user.\nSo, is there anything significant about the Year of the Ox? In fact, there is. There is an often repeated saying in Chinese, “may your child become a dragon” (wàng zǐ chéng lóng 望子成龙). As a parent, I would actually prefer that my child become an ox. Strong and quiet, the ox lowers its head, focuses on what is in front of it, and powers ahead, often pulling something heavy, like a plow through the muck. It labors selflessly, all day, on behalf of others, without complaint and requiring no praise, no reward, no special treatment. Legend has it that the twelve animals raced across a river to determine their order in the 12 year cycle. The ox was the strongest swimmer, and rat the weakest. So the ox carried the rat on its back, and reaching the other bank of the river, the rat jumped to dry ground. Consequently the rat is the first animal in the 12 year cycle, the ox second. 2020, the Year of the Rat has presented us with enormous challenges. It is precisely virtues of the ox that will get us through 2021 and the pandemic. So I hope that we can celebrate the Lunar New Year by taking a lesson from the ox.\nFInally, I’d like to give a special New Year shout out to a friend and colleague who exhibits all the virtues of the ox—Kevin Chang. This is Kevin’s final Mass Greeting with CAIS, and he has selflessly dedicated his career—without complaint and requiring no reward—to the forward progress of our Chinese immersion program. Kevin is a huge part of our school’s history, and we wish him all the best as he completes the school year with us before heading off to Hong Kong for his next twelve year cycle.\nHappy New Year, CAIS."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9fae1763-e29e-44f2-a527-cc8f9d9a8caf>","<urn:uuid:410c18d6-cbb5-4a8a-b8f5-e562c69d9893>"],"error":null}
{"question":"How do Cushing's Disease in dogs and thyroid problems in women similarly impact their respective reproductive systems?","answer":"Both conditions affect reproduction, but with different manifestations. In dogs with Cushing's Disease, there is a loss of fertility as one of the symptoms. In women, thyroid disorders can have multiple reproductive impacts including affecting ability to become pregnant, interfering with ovulation, causing abnormal timing of puberty and menstruation, creating very light or heavy menstrual periods, and potentially leading to early onset menopause.","context":["What do you think is the most common hormonal disease of dogs? Diabetes? Thyroid?\nIn our Adelaide clinic it’s easily Cushings Disease, also called Cushings Syndrome or Hyperadrenocorticism.\nWhat Is Cushings Disease?\nSimply, it’s caused by an excess in the body of the hormone cortisol, or artificial substitutes like prednisolone.\nCortisol is a hormone with many functions. It:\n- increases blood sugar\n- suppresses the immune system\n- aids in the metabolism of carbohydrates, protein and fat\n- decreases bone formation\nWhich Dogs Get Cushings Disease?\nAll dogs can get Cushings disease, but the following breeds have an increased risk:\n- Miniature Poodle\n- Bichon Frisé\n- Yorkshire Terrier\n- Toy breeds in general\nThere is no increased risk in desexed dogs, and males and females are at equal risk.\nObesity may increase the risk. If your dog is gaining weight, this page is here to help.\nWhat Does Cushings Disease Look Like In Dogs?\nExcess cortisol causes many symptoms. They include:\n- Drinking a lot of water\n- Increased appetite or hunger\n- Thin hair and hair loss on the back, sides and rump\n- Pot belly\n- Less interest in play or exercise\n- Increased weight\n- Panting even in cool conditions\n- Urinary infections\n- Hard skin patches or blackhead\n- Loss of fertility\n- Thin skin & slow wound healing\nThese days, with the increased use of annual screening blood tests we find a raised Alkaline Phosphatase enzyme and diagnose the disease before anyone is aware it’s there.\nWhat Causes Cushings Disease?\nThere are three sources of the excess hormone:\n- Pituitary Dependent Hyperadrenocorticism (PDH)\n- Adrenal Tumour (AT)\n- Iatrogenic Hyperadrenocorticism (IH)\nPDH is caused by a small, usually benign, tumour in the pituitary gland, which is located at the base of the brain. The tumour produces too much of the hormone ACTH which in turn tells the adrenal glands to make too much cortisol.\nAT is a simple tumour in the adrenal gland itself. This time it’s the tumour cells themselves that produce too much cortisol. Later you’ll see why this matters.\nIH is a bit of an ‘oops’ moment. This form used to be very common when cortisone, prednisolone, triamcinolone or dexamethasone were used a lot to treat canine skin disorders. The overuse of these drugs mimics natural cushings disease symptoms. Nowadays there are many better ways to treat skin problems in dogs but sometimes IH can’t be avoided when we treat an autoimmune disease.\nHow Do We Diagnose Cushings Disease?\nVets mostly use either the ACTH Stimulation Test or the Low Dose Dexamethasone Suppression Test (LDDST).\nWe mostly use the ACTH Stim test for diagnosis as it’s the same one we then use to monitor your dog’s progress. The LDDST is a more sensitive test we may use when the situation is unclear, though it can sometimes cause false positive results from other illnesses. Cushings tests require at least half a day in hospital and a series of injections.\nHow To Treat Cushings Disease In Dogs\nTreating the IH form is easy: just withdraw the drug. Notice we didn’t say stop. If a dog is on cortisone drugs for a long time, it causes the body’s own cortisol production to stop and the adrenal gland to shrink via a feedback loop.\nIf you stop the drug cold turkey, you will cause a severe and usually fatal adrenal crisis called Addison’s Disease. Talk to your vet about slowly tapering the drug in a controlled manner.\nTo treat PDH or AT you really need to know which form it is. To do this, all we need to do is schedule an abdominal ultrasound examination. Any trained ultrasonographer should be able to easily locate and carefully examine the adrenal glands.\nIf the adrenal glands are both evenly enlarged, it’s PDH. If one gland is irregularly enlarged, and the other one shrunken, it’s AT.\nTreating AT can be easy or hard. If it’s nicely isolated then it’s a reasonably simple surgery to remove an adrenal tumour. The problem is that the adrenal gland nestles next to the aorta and vena cava, the two major blood vessels in the body. If the tumour has spread into these (usually already detected on ultrasound) we can’t operate.\nDogs with inoperable AT, or those awaiting surgery, often still respond well to trilostane…\nDogs with PDH have a good prognosis. The pituitary tumour is not yet able to be removed in Australia, but it shouldn’t cause any other problems.\nWe use drugs to either suppress the production of cortisol or block its action in the body.\nThe only drug registered for the treatment of cushings disease in dogs in Australia is trilostane. It’s also the safest. For these reasons, we’ll always recommend using this drug first.\nIt works by blocking the action of cortisol at the receptor sites in the body.\nAdvantages are the high safety index, the reversibility when stopped and good results (and the legislative protection for us vets!). It’s also a very good drug to use when planning AT surgery.\nThe main disadvantage is cost, when compared with the older treatment. There is also a reported occurrence of acute adrenal necrosis when using trilostane, which we have seen twice without serious consequences.\nFor decades this was the treatment of choice until the availability of trilostane. It is a form of specific chemotherapy which targets the adrenal gland. By giving just the right amount, we selectively destroy or deactivate a percentage of the adrenal gland until it produces the right amount of cortisol.\nYou can see why this drug is more dangerous; too much can cause irreversible damage. It’s also not licensed in dogs, meaning vets have to obtain informed consent for its use. Owners need to take full responsibility for any unforeseen .or harmful effects resulting from its use, and release the vets from any blame.\nWe still use mitotane in three situations, when:\n- Trilostane treatment produces unsatisfactory results\n- The cost of trilostane is unacceptable to a dog owner\n- Giving tablets is very hard or unsafe\nLong term therapy with mitotane is possible with only one or two doses per week, instead of daily doses with trilostane.\nWith both treatments, frequent monitoring is essential. Cortisol is a vital hormone in the body, and overdosing with either drug can be fatal.\nCost Of Treatment\nThe following comparision is at 2016 prices and applies to a 6.5kg dog on average doses. This is a guide only, individuals will vary greatly.\nMitotane costs $13.40 per week\nTrilostane costs $29.20 per week\nHow Successful Is Cushings Treatment?\nVery. We like diagnosing and treating Cushings Disease in dogs because we know how much most dogs will improve.\nThe onset of symptoms is slow and often thought to just be the effects of ageing.\nInside most dogs with cushings syndrome is a playful, active & healthy puppy just waiting for you.","Thyroid Issues In Women\nWhen many think of endocrine disorders, they think of estrogen and testosterone problems.\nHowever, a more common endocrine disorder is abnormal levels of the Thyroid Stimulating Hormone, and the side effects of thyroid disorders can be confusing and easily misdiagnosed.\nWhat is the Thyroid?\nThe thyroid is a small gland in the neck behind the larynx. Its main functions are to regulate energy production and metabolism throughout the body—everywhere from the heart to the skin. Too much or too little of this hormone can have serious consequences.\nMany patients with thyroid disorders experience excessive fatigue, depression, hair loss, unexplained weight gain and sleep problems. However, routine blood test often fail to detect insufficient levels of this hormone in some of these patients. Also, symptoms can vary widely from person to person, making it harder to detect the issue.\nThyroid Disorders in Women\nThyroid disorders can affect both adults and children, but women are more likely to have thyroid disorders than men. This is because the thyroid gland has much to do with a woman’s reproductive system. Insufficient levels of this hormone can greatly affect a woman’s body.\nResearch shows that a slightly underactive thyroid may affect a woman’s ability to become pregnant. According to a study published in The Journal of Clinical Endocrinology & Metabolism, women who have unexplained infertility were nearly twice as likely to have higher levels of this hormone that stimulates the thyroid gland than women who did not conceive due to known issues with their partner’s sperm count.\nA reason for this might be that an overactive thyroid can affect ovulation. Thyroid disorders have the potential to prevent an egg from dropping for fertilization. For women with an underactive thyroid, the ovaries are at an increased risk for cysts, which can also prevent ovulation.\nThyroid disorders can also cause puberty and menstruation to occur abnormally early or late and can cause very light or very heavy menstrual periods. It can even cause absent menstrual periods and early onset menopause (near the early 40’s or earlier).\nWho’s at Risk?\nWomen with goiter, anemia or type 1 diabetes are at a greatest risk for developing thyroid issues. Also, women with thyroid problems in the past or who have had radiotherapy affecting the thyroid gland are more likely to develop these issues.\nSymptoms of Hypo/Hyperthyroidism\nHypothyroidism, when the thyroid isn’t making enough hormones, can cause one to feel very cold, constipation, muscle weakness, weight gain, pale skin and a slow heart rate, among other symptoms.\nHyperthyroidism, when the thyroid is making too many hormones, can cause weight loss, feeling nervous or anxious, increased sweating, trembling, trouble sleeping and feeling hot, among other symptoms.\nIf you believe you are at risk for either, visit an endocrinologist to get an examination.\nWeekly Health Tips are brought to you by UCF Health, the College of Medicine’s physician practice. Offering primary and specialty care under one roof, UCF Health treats patients age 16 and up in primary care and age 18 and up for specialty care. Most major insurance plans are accepted. Two locations are now open: the original in East Orlando at Quadrangle and University boulevards just blocks from the main UCF campus, and the newest one in Medical City at Narcoossee Road and Tavistock Lakes Boulevard. Information for both facilities can be found at UCFHealth.com, or call (407) 266-DOCS to schedule an appointment.\nSubscribe to Weekly Health Tips\nGet Health Tips from UCF Health in your email each week! Subscribe here.\nWeekly Health Tips are brought to you by UCF Health, the College of Medicine’s physician practice. Offering primary and specialty care under one roof, UCF Health treats patients age 16 and up in primary care and age 18 and up for specialty care. Most major insurance plans are accepted. Two locations are now open: the original in East Orlando at Quadrangle and University boulevards just blocks from the main UCF campus and in Medical City at Narcoossee Road and Tavistock Lakes Boulevard. Schedule an appointment online today."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:1ac95a5f-71bb-4d84-89a5-a3115e8d14e8>","<urn:uuid:7e135f20-2763-4261-b8d0-f3315c6f4890>"],"error":null}
{"question":"How can nail salons meet the International Mechanical Code ventilation requirements?","answer":"According to the 2012 International Mechanical Code (IMC), nail stations must be equipped with a source-capture system capable of exhausting not less than 50 cubic-feet-per-minute. Some jurisdictions also require source capture ventilation at pedicure spas. For example, Regal Nails complies by using vented tables, and some manufacturers like Alfalfa Nail Supply have released pedicure spas with two vents to meet IMC requirements.","context":["The invisible beachball-sized sphere of air that surrounds your face in all directions is a great place to start implementing efficient salon ventilation. NAILS walks you through all of the things you need to know, from what to look for in a ventilation system to how the components work to a few tweaks you can make in your day-to-day habits for extra protection. Get ready to breathe easy.\nIllustration by Luisa Montalto\nWe know there’s a lot to think about when managing a nail salon. From product ordering to fixing a leaky faucet to the crucial art of client booking, sometimes it seems that everything is urgent and that anything that doesn’t require immediate attention is a low priority, no matter how important it may be in the long-run. So we know that it’s easy to ignore the comments of a client or two who complain about the lingering odors in the salon or the dust that is covering your clothing by mid-afternoon. Maybe you even made an attempt at addressing these issues, such as buying a home or office air purifier from the local hardware store. But even with the air purifier, you notice the smell lingers and the dust remains.\nIt’s difficult to make ventilation a priority, especially when you don’t even smell the salon odors anymore and you feel perfectly healthy. Maybe you will remain perfectly healthy for the next 50 years (we certainly hope so!), but here’s a message from your future self: It would have been so easy to prevent the headaches, the sore throats, the watery eyes, the constant sneezing, and the shortness of breath. All it would have taken would have been a few days of research of contacting a few nail salon ventilation companies (there’s even a list in this article), ordering some equipment and dealing with a one-time hassle of figuring out where to place it in the salon, then doing some simple maintenance every few months (the equivalent of changing a light bulb) to keep the system working. Isn’t that worth a lifetime of breathing easy?\n“The nail salon is a tough environment,” says Jeff Cardarella, president of salon ventilation manufacturer Aerovex Systems. “The vapors and the dust are similar to a printing press room. It’s strong industrial solvents you’re working with, and they’re stubborn to remove.” (That’s why a home or office air purifier won’t work in a nail salon.) The vapors come from the products themselves, which you’ve probably noticed when twisting the top off of a new bottle of acetone or acrylic monomer, and the dust comes from filing off enhancements. Even if you can’t see the dust, that doesn’t mean it’s not there. What you can’t see can really hurt you: “When it comes to some of the UV gel products, the dust is so fine that it’s insidious,” says Len Roulier, president of salon ventilation manufacturer Air Impurities Removal Systems Inc. “The UV gel dust just kind of floats up and gets into everything.” It can lodge itself inside your lungs, causing a slew of respiratory and other ailments.\nSo if a salon is such an unpleasant work environment, why aren’t there laws protecting nail techs requiring ventilation systems to be installed in every salon?\nThat’s a complicated question without a great answer. Many different government agencies hold some claim to enforcing clean air requirements in nail salons. However, in many cases, the laws are outdated or simply unenforced. The Occupational Safety and Health Administration (OSHA) requires engineering controls when chemicals in the workplace exceed permissible exposure limits (PELs). However, in nail salons PELs are almost never exceeded. Unfortunately, that doesn’t mean there aren’t potential health effects at concentrations below OSHA PELs. Plus, OSHA doesn’t have established PELs for some of the chemicals used in nail products.\nThe International Mechanical Code (IMC) also addresses ventilation and is the basis for most state-specific mechanical codes (generally enforced by local building departments), so in many places salons must comply with its requirements for construction permits and/or certificates of occupancy. Updated every three years, the IMC has addressed nail salon ventilation since 2006. But, as it was only mentioned in a footnote, many inspectors overlooked it.\nThe 2012 IMC raises the bar on salon ventilation, stating that nail stations in nail salons must now each be provided with a source-capture system capable of exhausting not less than 50 cubic-feet-per-minute. Of course, it remains to be seen which jurisdictions will enforce the code. Benjamin Bell, principal of the architectural division of CESO Inc., which handles the design of nail salon chain Regal Nails, says Regal Nails chooses to be proactive in complying with the IMC: “Because of the sheer number of salons we have designed in locations all over the country, we have been made aware of the code requirements by multiple jurisdictions. As we ran into the concern more frequently, we did some thorough research into the issues, including discussing them with the International Code Council (the group that issues the IMC), and devised a series of stepped responses that address the concerns.” He adds that all jurisdictions that opt to enforce the 2012 code are likely to require the use of vented tables. “We have also begun to see some jurisdictions that require source capture at the pedicure spas as well,” Bell says. (Alfalfa Nail Supply even released a pedicure spa, the Katai I Pedispa V Series, that features two vents to meet the IMC requirements.)\nState cosmetology boards also hold some jurisdiction over salon air quality. However, many boards use vague undefined terms like “adequate” when describing the requirements for a ventilation system, which isn’t really enforceable.\nFinally, some agencies like public health departments and state boards focus more on the safety of salon clients, not of workers. But clients are only in the salon for about an hour. You’re in the salon the entire day.\nThough it’s unlikely your salon will face any legal repercussions from insufficient ventilation, there are compelling health reasons to protect yourself and your staff. The vapors and dust of nail products are generated close to your breathing zone and can cause a variety of acute and long-term diseases. When ventilation isn’t sufficient, airborne chemicals build up, increasing your exposure. And simply having the heating-ventilation-air-conditioning (HVAC) system running doesn’t lessen your exposure. “Chemical vapors are usually heavier than regular air, so they settle downward. That means that by the time the vapors get to the ceiling vent, they’ve already gone through your breathing zone,” Roulier says.\nSeveral local governments recently started offering incentives for salons to improve their air quality. In November 2010 (and effective from February 2012), San Francisco adopted legislation to create a Healthy Nail Salon Recognition Program by which its Department of the Environment recognizes salons that meet certain parameters including ones that “install mechanical ventilation unit(s) within one year of entering recognition program” and that “allow SFE program staff to monitor air quality within the salon.” It will also give away ventilation systems to the first 10 businesses that agree to become “healthy” salons.\nIn King County (home to Seattle), Wash., the Local Hazardous Waste Management Program (LHWMP) instituted a voucher program that reimburses nail salons up to 50% of the purchase price (up to $500 lifetime) for items that lower nail tech exposure to dust and vapors, such as nitrile gloves, N95 masks, and metal cans with tight-fitting lids. “It was determined from studying some incentive programs in the country that financial assistance provided businesses with the incentive they needed to change procedures,” says Laurie Foster, nail salon project coordinator for the LHWMP. Fifty-three nail salons have used the voucher program since 2008. (It is funded by residential and commercial solid waste and utility ratepayers and by residents and businesses that take their solid waste to a transfer station or landfill.) “People from all over the country have contacted me regarding the healthy nail salon program,” Foster says.\nThe biggest challenge to be overcome may be that many people, including many in the nail industry, think it’s inevitable that a nail salon will smell, well, like a stereotypical nail salon. However, with the latest in ventilation technology, a nail salon can have the same smell as the outside air. “Today, we can do that,” Cardarella says. “There should be no discernible difference in the clean air outside the salon and inside the salon.” Indeed, on average, a nail salon exhaust system has the potential to reduce exposures by at least 50%, according to an in-depth survey report released in September 2012 by the National Institute of Occupational Safety and Health (NIOSH). (The report shows results from tests of several different configurations of salon ventilation systems and is available at\nConvinced yet? If so, we’ve done quite a bit of your research for you so you know exactly what to look for in a ventilation system and how to beat some common obstacles that keep nail techs from installing proper ventilation.\nNext page: The Ideal Ventilation System and Inside a Source-Capture Ventilation System"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:31caa475-0bd4-45bc-8250-44cdcfd3501e>"],"error":null}
{"question":"Does social anxiety disorder affect daily functioning, and what behavioral experiments are used to address it?","answer":"Social anxiety disorder significantly impairs daily functioning - sufferers can't participate in class, eat alone, have trouble with job interviews, and experience physical symptoms like sweating and racing hearts. Behavioral experiments are used as treatment where clients identify their fears about specific situations (like saying 'no' to others) and then test these fears in real situations to gather evidence about their actual ability to cope. The therapist and client analyze the results together to help challenge and modify exaggerated fears.","context":["W. McCall, (2000, January 17). For some shyness is a crippling disease. (Associated Press). The Boston Globe, p. A7.\nShyness stems from a natural survival instinct that protects human beings from any threatening creature…from our days in the wild to urban congestion. Shyness, as commonly understood, describes extreme cautiousness in social situations. In its extreme, shyness becomes a phobia called social anxiety disorder.\nExperts rank the social anxiety disorder as the “third most widespread mental health problem” in the United States-following depression and alcohol or drug abuse. Specialists in this affliction say “3 to 5 percent of Americans are so shy they need treatment.”\nWhen Marissa Turner of Portland, Oregon was growing up she felt herself weird because she always wanted to be alone-even avoiding opportunities to play with her cousins. She wouldn’t think of raising her hand in class out of fear that she might embarrass herself and have classmates make fun of her. If a teacher called on her, “she would sweat and tremble, and her throat would tighten.”\nI knew I was afraid of a lot of people. I didn’t have a lot of friends. I thought I was different.\nShyness followed her well into her twenties as she sought employment in Washington, DC:\nThe average reaction for me walking into a room full of people would be a rapid heart rate, sweaty hands, that kind of racing feeling in my head and a sort of fuzziness. I found it hard to focus mentally on anything.\nDr. Michael Resnick of Oregon Health Sciences University in Portland, who has studied and treated shyness, comments: “It’s an awful experience for the people who suffer a severe form of it.”\nDr. Robert Reichler of Seattle’s Pacific Institute of Mental Health has treated children and young people afflicted with shyness.\nI see kids in my practice who can’t function in class, who can’t participate in class. They don’t have friends, they eat alone, they spend time in libraries alone. (Later on) they have trouble getting a job, they have trouble handling job interviews, their hearts pound, they sweat, they can’t talk. It’s a major impairment, not just shyness.\nMichael Resnick traces the social anxiety disorder back into our biological history.\nAncient humans evolved a certain level of anxiety to cope with danger and survive against wild animals. (Today) some people have more of this (social caution) than others, and it gets to a point that it’s not helpful. It goes haywire, and instead of a protective mechanism, it actually harmful.\nHaving conferred with experts, the writer of this article concludes:\nAlthough it is widespread, social anxiety disorder is considered mild compared to less common but more profound mental diseases, such as schizophrenia. Still, it can ruin lives and disrupt families, businesses, and schools.\nQuestions for Reflection and Discussion\n- Who do you know who has a problem with shyness?\n- Did you know that extreme shyness is often labeled and treated as social anxiety disorder?\n- Do you know anyone afflicted with shyness to that extent?\n- What most impressed you about this article, and what further questions does it raise?\n- How would you intervene in the life of someone afflicted with extreme shyness?\n- How do you think shyness and then the more extreme social anxiety disorder can be treated?\n- Where will you turn for more information and help?\n- Most people have experienced at least some tinges of shyness.\n- For many it has been a problem at one time or other.\n- Extreme shyness and social anxiety attacks are painful and injurious.\n- We need to help children and young people who have tendencies toward shyness.\n- Be aware of further resources available.\n© 2017 CYS","Exposure Therapy and Behavioral Experiments\nCognitive Behavioral Therapy is the most validated form of therapy for anxiety disorders. In a huge review of studies exploring the effectiveness of CBT, the researchers found “The efficacy of CBT for anxiety disorders was consistently strong” (click here for a link to a free copy of this study). In another comprehensive review of studies exploring CBT for generalized anxiety, the authors concluded “when compared to waiting list control groups, these treatments have large effects on worrying, anxiety and depression, regardless of whether effects were measured with self-report measures or with clinician-rated instruments” (click here for a copy of this study). Many people would prefer their anxiety just went away but research suggests few people who do nothing, reduce their anxiety. As the studies above suggest, one of the things people can do to reduce their anxiety is cognitive-behavioral therapy and at the core of CBT for anxiety is exposure therapy and behavioral experiments.\nSimply put, exposure therapy involves exposing the client to their feared situations in a gradual, repeated, and prolonged manner until the client becomes desensitized. For example, if a client is terrified of public speaking they may work with their therapist to create a list of anxiety provoking situations (speaking in class, watching videos about public speaking, taking a class on public speaking, imagining speaking in public, speaking in a meeting at work, talking to a cashier at a store, doing a speech at Toastmasters, etc.). The client then ranks these situations from least anxiety provoking to most anxiety provoking, this is called an anxiety hierarchy. Then starting with a situation that does evoke some anxiety but is not overwhelming, the client would repeatedly expose themselves to these situations, over and over, until they become desensitized to this situation. A useful example of this most people can relate to is learning to swim as children. Many children are afraid or at least tentative about the water, but their parents encourage them to become more comfortable over time, often enrolling them in swim classes. As the child is gradually exposed to the water over time, they become less afraid. Alternatively, the children that are terrified of the water, and refuse to ever go in the water continue fear the water.\n*For more information about exposure therapy see my article “Overcoming Anxiety and Avoidance” by clicking here, or download our free anxiety workbook by clicking here.\nBehavioral experiments can take a number of different forms. Similar to exposure therapy, the client and therapist may work together to create a list of anxiety provoking situations. Then the client and therapist will work together to identify the client’s fears about what could go wrong in these situations. Often these fears are exaggerated and extreme but the client sees them as perfectly reasonable. So then we create a little experiment in which the client enters the feared situation and observes to see if their fears come true. Then the client runs the experiment several times in an attempt to gather more information. Once the client has run the experiment several times, they reconvene with the therapist to discuss what they have learned about their fears and their abilities to cope.\nFor example, perhaps one of the situations on the client’s anxiety hierarchy is saying “no” to other people. Perhaps the fear in this situation is something like “If I say no to my mother when she asks me to do something, she will call me selfish, the rest of my family will hate me, and I won’t be able to handle that.” So then the client and therapist run an experiment in which the client talks with their mother, explains they are trying to be more assertive and wants to work on saying “no” sometimes. Then the next time their mother asks them to give them a ride to the store, the client explains they cannot help that day due to other commitments, then observes what happens. In this situation, the client’s mother may very well call the client “selfish” and their family may complain, but the client also learns they can survive being called selfish and occasionally listening to complaints. Or perhaps the client’s mother understands the client has other commitments and they schedule a plan in which the client gives their mother a ride another time, or the client’s mother finds another way to the store. See the below table for an example how I would write-up a behavioral experiment assignment with a client.\nAnother type of behavioral experiment involves the client identifying what they suspect other people believe then conducting a survey to assess the accuracy of the client’s assumptions. For example, the client may believe that people believe that women over a particular weight are “unattractive.” Clients may start by simply ask some trusted friends or family about this assumption. They may take photos of themselves or others to friends or family and ask people about their impressions about the people in the photos. In this example, the client ideally learns that attractiveness is not directly related to something as arbitrary and simplistic as weight.\nHow it works:\nIn my experience, exposure therapy and behavioral experiments have been immensely helpful for clients. Clients usually report dramatic shifts in their anxiety in only a short period of time. In CBT we assume the client becomes less anxious in situations they are exposed to because they learn that their initial assumptions about the dangerousness of the situation is exaggerated and their beliefs about their abilities to cope with the danger posed by those situations is minimized. In other words, their cognitions change. After exposing themselves to a variety of different situations, the client learns they have a habit of exaggerating danger and minimizing their abilities to cope, and so they become less anxious in other situations they have not exposed themselves too.\nFor example if you are anxious about travelling but you muster the courage to go to Mexico, you might learn that Mexico isn’t as dangerous as you expected. Then if you go to Germany you might learn that Mexico and Germany are safe. Then if you go to India you might learn that Mexico, Germany, and India can be travelled to safely. Then you might learn that travelling in general can be done safely, not only to those countries you have been to in the past.\nOne challenge in using exposure and behavioral experiments is explaining the importance of actually facing fears to the client. Some clients have spent decades avoiding situations that make them anxious and the thought of deliberately exposing themselves to these situations is terrifying. People tend to want to avoid situations that make them anxious. Unfortunately, it is this very avoidance which perpetuates the anxiety indefinitely.\nAnother barrier to effective exposure and behavioral experiments is called “safety behaviors.” Safety behaviors can sometimes resemble the compulsions of someone who struggles with OCD. Safety behaviors are unnecessary, excessive, or unhelpful activities or strategies people use in anxiety provoking situations to protect them from “something going wrong.” When the client is anxious about going to a new restaurant they may excessively research the restaurant online to create a plan to prevent something “bad” from happening. In this example the excessive research is the safety behavior the client uses to keep themselves “safe.” Then when they go to the new restaurant and nothing “bad” happens they convince themselves it is because they researched prior to going. In this situation, the client has not learned that they can safely go to a variety of different restaurants, but instead they have learned they can go to a restaurant when they excessively research it in advance. Other examples include holding glasses very tightly when you’re afraid of spilling, distracting yourself with your phone when afraid of standing in lines, and never disagreeing with people when you are afraid of conflict. Research suggests it is imperative safety behaviors are identified so the client can either avoid using them or gradually reduce the use of safety behaviors over time.\nSometimes a client’s anxiety is about situations to which they cannot be exposed. For example, the client might be terrified of earth quakes, someone dying, their son being in a car accident, getting fired, or being homeless. When the client cannot be directly exposed to their feared situations, we have got to get creative. We can watch videos of these situations, we can do research, we can read stories, we can write then recite our own stories in which the client is exposed to these situations, etc. This is based on the idea that thinking about the feared situation can actually desensitize the client. Some of these interventions are called “imaginal exposure.”\nResearch suggests CBT is an effective treatment for anxiety disorders. Exposure therapy and behavioral experiments are core elements of CBT for anxiety. By facing fears the client learns their feared situations are not as catastrophic as originally predicted and their ability to cope is better than expected."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:dfaba31f-4132-42e9-8745-8577a900f216>","<urn:uuid:9592eadd-6db6-42e7-a33c-12a57ef1c170>"],"error":null}
{"question":"As a student of ancient administration systems, I'm wondering what were the key differences between documentary practices in Persian and Egyptian bureaucracies?","answer":"In Egypt, documentation existed in multiple languages and scripts, with evidence of bilingual archives and accounts, as shown by the existence of Demotic and Greek ostraca (pottery fragments with writing) and bilingual administrative texts. The Persian system, meanwhile, established a more centralized bureaucratic structure where provincial treasury officials and secretaries reported directly to the king. The Persians also developed an sophisticated communication system with relay riders (similar to the Pony Express) who could carry messages across the empire via the King's Highway, covering the 1677 miles from Susa to Sardis in just seven days. The Persians particularly relied on Syrian and Babylonian scribes' administrative skills to keep their government running smoothly.","context":["Life in a Multi-Cultural Society: Egypt from Cambyses to Constantine and Beyond\nEgypt from its incorporation into the Persian empire in 525 B.C. was home to a multi-cultural society with several strong cultural traditions. An Egyptian majority population lived in contact (sometimes closer, sometimes more distant; sometimes more frequent, sometimes less frequent) with non-Egyptian populations who had come to reside in Egypt. By the hellenistic period these non-Egyptian populations included large numbers of Greeks and Macedonians who had settled in Egypt after its conquest by Alexander the Great, as well as Jews and other semitic-speaking people from Syria-Palestine; after Egypt's incorporation into the Roman empire, Roman citizens and Roman soldiers from around the empire appear more and more frequently. Integrated study of the period from Cambyses to Constantine (a period of almost 1,000 years), therefore, requires knowledge and appreciation of the material remains (documentary, artistic, architectural, and archaeological) and cultural antecendents of several cultures. The rich documentary and \"archaeological\" resources preserved from this period would allow it to serve as a model for analysis of other multi-cultural societies.\n- Studies in Ancient Oriental Civilization 51\n- Chicago: The Oriental Institute, 1992\n- ISBN: 0-918986-84-2\n- Pp. xxvii + 514; 10 figures; 32 plates; 5 tables\n- Paperbound 9 x 11.75 inches\nContributors and Contributions\nObservations on Civil Jurisdiction in Late Byzantine and Early Arabic Egypt\nAndrews, Carol A. R.\nUnpublished Demotic Texts in the British Museum\nBianchi, Robert Steven\nThe Cultural Transformation of Egypt as Suggested by a Group of Enthroned Male Figures from the Faiyum\nEgyptians and Greeks in an Early Laographia Account (P. Berol. 25161)\nBurstein, Stanley M.\nHecataeus of Abdera's History of Egypt\nSome Greeks in Egypt\nCorcoran, Lorelei H.\nA Cult Function for the So-Called Faiyum Mummy Portraits?\nThe Lake of Moeris: A Reprise\nDarnell, John Coleman\nThe Kbn.wt Vessels of the Late Period\nAbout the Origins of Early Demotic in Lower Egypt\nThree Mirrors with Demotic Inscriptions\nThe Wandering Personnel of the Temple of Narmuthis in the Faiyum and Some Toponyms of the Meris of Polemon\nHanson, Ann Ellis\nEgyptians, Greeks, Romans, Arabes, and Ioudaioi, in the First Century A.D. Tax Archive from Philadelphia: P. Mich. inv. 880 recto and P. Princ. III 152 revised\nHeidorn, Lisa A.\nThe Persian Claim to Kush in Light of Evidence from Lower Nubia\nArchives bilingues de Nomarques dans les Papyrus de Ghôran\nSome Thoughts on the Subject \"'State' and 'Church' in Ptolemaic Egypt\"\nDie Medinet Habu Ostraca: Excavation of The Oriental Institute of The University of Chicago 1928/29\nGagos, Traianos; Ludwig Koenen; and Brad E. McNellen\nA First Century Archive from Oxyrhynchos or Oxyrhynchite Loan Contracts and Egyptian Marriage.\nDie ägyptischen Priester des ptolemäischen Königskultes (Zusammenfassung)\nVisitors to Elephantine: Who Were They?\nMartin, Cary J.\nDemotic Contracts as Evidence in a Court Case?\nMcCleary, Roger V.\nAncestor Cults at Terenouthis in Lower Egypt: A Case for Greco-Egyptian Oecumenism\nBibliography and Description of Demotic Literary Texts: A Progress Report\nMidgley, James H.\nA Bilingual Account of Monies Received\nDemotische Termini zur Landesgliederung Ägyptens\nDemotic and Greek Ostraca in the Third Century B.C.\nReport on New Demotic Texts from Tuna-el-Gebel\nOates, John F.\nThe Basilikos Grammateus\nAramaic-Demotic Equivalents: Who Is the Borrower and Who the Lender?\nGreco-Egyptian Double Names as a Feature of a Bi-Cultural Society\nRay, J. D.\nJews and Other Immigrants in Late Period Egypt\nThe Administration of Late Ptolemaic Egypt\nRitner, Robert K.\nImplicit Models of Cross-Cultural Interaction: A Question of Noses, Soap, and Prejudice\nWriting Egyptian: Scripts and Speeches at the End of Pharaonic Civilization\nSmith, H. S.\nForeigners in the Documents from the Sacred Animal Necropolis, Saqqara\nTait, W. J.\nDemotic Literature and Egyptian Society\nGreek and Demotic School-Exercises\nThomas, Thelma K.\nGreeks or Copts?: Documentary and Other Evidence for Artistic Patronage During the Late Roman and Early Byzantine Periods at Herakleopolis Magna and Oxyrhynchos, Egypt\nThompson, Dorothy J.\nLiteracy and the Administration in Early Ptolemaic Egypt\nVan't Dack, E.\nL'Armée de Terre Lagide: Reflet d'un monde multiculturel?\nVleeming, S. P.\nThe Tithe of the Scribes (and) Representatives\nWinnicki, J. K.\nDemotische Stelen aus Terenuthis\nEin Zug nach N","Few people today can boast a longer and prouder history than the Iranians, descendants of the ancient Persians. Not only did they build the greatest empire of the ancient Near East, but they also absorbed the ancient civilizations they ruled, in particular that of Mesopotamia. They then added their own distinctive touches and passed them on to Islamic civilization, still one of the main cultural traditions of modern times. Therefore, this remarkable people who have survived and flourished from antiquity to the present have been a major connecting link with our past.\nWe first encounter the Persians around 2000 B.C.E. emerging from the grassy steppes of Central Asia in the north. At that point, they were closely associated with two other peoples: the Medes and Aryans. The latter of these turned eastward, crossed the Hindu Kush Mountains, and overthrew the Indus River civilization. Eventually these nomads would settle down and build Indian civilization upon the foundations laid by the Indus culture. Meanwhile the Persians and Medes were turning westward where they encountered the Elamites, a people whose extended contact with Mesopotamia had influenced them to absorb the culture of the \"Cradle of Civilization\".\nThe Medes and Persians in turn started absorbing Elamite culture. One need only look at the relief sculptures of the Persians, with their curly beards and stiff formal poses, to see the connection with Mesopotamia. However, the process of becoming civilized was a long one for these people, since they were still on the northeastern fringes of the older Near Eastern cultures. When they emerge fully into the light of history in the pages of the Greek historian Herodotus, they are still very nomadic in their customs and values. According to Herodotus, the nomadic Persians had only three simple goals in educating their sons: \"to ride a horse, to draw a bow, and to speak the truth.\" What more did nomads need? The Medes were actually the first of these nomadic peoples to establish an empire when they joined forces with Babylon to overthrow the Assyrian Empire in 6l2 B.C.E. In the aftermath, Babylon took the richer civilized lands of the Fertile Crescent, while the Medes took the more extensive but wilder lands to the north. Among their subjects were their compatriots, the Persians. It is here that we encounter the founder of the Persian Empire.\nHerodotus gives us a detailed and somewhat fanciful account of Cyrus the Great's rise to power. As in the stories of so many great men and legendary figures in history, from Sargon of Kish and Moses to Oedipus and Romulus and Remus, Cyrus barely survived infancy due to a royal death sentence from a king nervous about the child's destiny. In each story, someone saves the baby, who grows up and comes back to overthrow the king who tried to do him in. What does seem clear is that Cyrus led the Persians in revolt against the Medes and overthrew them around 550 B.C.E.\nAlthough the Medes' old neighbors were certainly glad to see the powerful Median state overthrown, they soon found their new neighbor, Persia, was an even more dangerous foe. Cyrus first turned westward against Croesus, king of Lydia in Asia Minor, a land renown for its wealth, as seen in the old saying \"rich as Croesus\" to denote how wealthy someone is. In order to deal with the tough Lydian cavalry, Cyrus placed camels in front of his lines. The Lydian horses, unused to the camels' strange smell, panicked and bolted, giving Cyrus the victory and Lydia. Cyrus next turned south against Babylon, whose empire was seething with revolt. Herodotus claims that Cyrus had his troops divert the course of the Euphrates so they could march into the city's unguarded river gates. However true that may be, Babylon's empire collapsed like a house of cards, leaving Cyrus the master of a huge empire. Still, he pressed onward, this time into the vast and wild expanses of Central Asia. His intentions here were probably defensive, to protect the frontiers of civilization from the swarms of nomadic horsemen to the northeast. It was here in 530 B.C.E. that Cyrus died in battle against a tribe known as the Massagetae. In his twenty-nine year reign, he had built the largest empire in history up to that time.\nCyrus' son and successor, Cambyses (530-522 B.C.E.), is mainly remembered for his conquest of Egypt in 525 B.C.E. His attempts to conquer the Nile further south and the desert oases of the Sahara met with less success. Supposedly, one of Cambyses' armies was swallowed up by a desert sandstorm. Cambyses was especially unpopular with the Egyptians, who claimed he committed various atrocities, including the slaying of the sacred bull of Apis. Since our main source for his life is Herodotus, who relied heavily on Egyptian sources for his book, we have a picture of Cambyses as a drunken lunatic,. Cambyses died in 522 B.C.E on his way to Babylon to crush a revolt led by his cousin, Darius, who then succeeded him as the next Great King of Persia.\nAlthough Cyrus had founded the Persian Empire, Darius I (522-486 B.C.E.) gave it the internal organization and structure that allowed it to last for 200 years. His accomplishment is all the more impressive when we consider the empire's enormous size, the scale of which no one had ever dealt with before. Darius dealt especially with three areas: organization of the empire's provinces, keeping the provincial governors under control, and maintaining communications with his far flung empire.\nOrganizing the provincial government presented two options. Darius could either create small provinces with governors too weak to rebel, but also too weak to defend their provinces against invasion. Or he could create large provinces able to defend themselves, but also more capable of defying his authority. He created about twenty large provinces, called satrapies. These ensured that he would not have to race from one end of his empire to the other defending it against every little tribe that decided to attack. Each such campaign might involve years of preparation, marching and fighting. Meanwhile, other frontiers would be vulnerable to attack, involving more years of campaigning and leaving the king with little time for other duties.\nSince larger provinces gave the governors, known as satraps, a lot of power, Darius took several precautions to keep his satraps from rebelling. For one thing, he had the provincial treasury officials, secretaries, and garrisons answer directly to him, not to the satraps, except in emergencies. This generally deprived the satraps of the money and troops they needed to revolt while ensuring the defense of the satrapies. There were also officials known as the \"King's Ears\". These personal agents of the king would travel to the various satraps' courts to check up on their behavior and official records. The King's Ears commanded a great deal of fear and respect, sometimes showing up with no armed escort, but still being able to put down rebellious satraps before the revolts went beyond the planning stages.\nCommunications in such a far-flung realm was another major problem. Here the Persians adopted the Assyrian practice of setting up a system of relay riders, much like the old Pony Express in American history. Each horse and rider would carry a message for a day and then pass it on to the next horse and rider. In order to speed things along, the Persians established a road system to tie the empire together. The most famous of these was the King's Highway, which stretched 1677 miles from the Persian capital of Susa to Sardis in Asia Minor. It had patrols against bandits, relay stations with fresh horses for the royal messengers, and 111 inns for travelers, placed about one day's journey apart from each other. Another road going through the desert to Egypt had underground cisterns with water for travelers. Although these roads helped trade and travel, their main priority was for the relay riders who could carry a message from Sardis to the king in Susa within seven days, an amazing speed for back then. As Herodotus described these riders: \"Nothing stops these couriers from covering their allotted stops in the quickest possible time--neither snow, rain, heat, nor darkness.\"\nIn general, Darius took existing practices and institutions and adopted them on a larger scale. However, in one respect, he differed quite markedly from previous Mesopotamian rulers. That was in his treatment of Persia's subjects. Darius realized that there was no way his far-flung empire could survive constant revolts such as had plagued the Assyrians. Therefore, he followed a policy of tolerance toward his subjects' customs and religions. For example, the Jews were allowed to return to Israel from their Babylonian captivity, causing them to sing the Persians’ praises in the Bible.\nDarius and other Persian kings also adopted local titles, such as pharaoh in Egypt, to win popular support. Sometimes they also kept local rulers in power as Persian vassals, such as in the Greek cities in Asia Minor. This hopefully would ensure them more loyalty, although it could backfire if those rulers were unpopular to begin with. While Persian rule may not have been wildly popular, most people tolerated it as an improvement over the harsher rule of the Assyrians and Babylonians. Keeping their subjects happy went a long way toward keeping the Persian Empire intact. It also ensured the cooperation of the Syrians and Babylonians, whose scribes and administrative skills were badly needed to keep the government running smoothly.\nThe Persians also worked hard to promote economic prosperity. Their roads, strong government, and stable coinage encouraged trade. They also promoted agriculture with irrigation projects and the introduction of new crops to different areas, such as sesame to Egypt and rice to Mesopotamia. Of course, increased prosperity also generated more taxes. The Persians also kept their subjects happy by charging moderate tax rates, about twenty per cent of a person's income. Despite this modest tax rate, the Persian kings were fabulously wealthy. By the time Alexander the Great took over the Persian Empire in 330 B.C.E., the Persian kings had reportedly amassed a treasury of 5500 tons of silver.\nDarius and other Persian kings further enhanced their authority by assuming divine or semi-divine status to overawe their subjects. In certain provinces, such as Egypt, they took the titles of local rulers who were often seen as gods. They also built a fabulous capital, Persepolis, in the middle of the desert, and adorned it with magnificent government buildings. The Persians also adopted the elaborate court ritual of their subjects. One had to go through a virtual army of officials before getting an audience with the king. When one approached the king, he performed a rite known as proskynesis, which involved throwing oneself at the king's feet. It was a great honor just to be allowed to kiss the hem of his garment and a serious offence for anyone outside the king's closest friends and advisors to look him in the eye. Such elaborate ritual could enhance the king's authority, but it could also cut him off from the day-to-day realities of empire.\nThe Persians, like most ancient peoples, started out with a polytheistic religion to account for the forces of nature. However, around 600 B.C.E., a new religion emerged, called Zoroastrianism after its founder, Zoroaster. This was a dualistic religion, which meant it saw life as a constant struggle between the forces of good and evil. In the end people would all be held accountable for their deeds in a judgment day when they would go to heaven as a reward for good deeds or suffer eternal punishment for their sins. Zoroastrianism seems to have had some influence on Judaism. In the book of Daniel, which takes place at the Persian court, the ideas of Heaven and Hell and of Satan as a force always opposed to God first appear in the Bible. Both of these ideas have become central to Christianity and Islam as well as Judaism.\nAny state needs a strong ruler to keep things running smoothly. After the death of Xerxes (486-464 B.C.E.), the Persian Empire lacked that strong hand. As a result, various problems developed that fed back upon one another and led to Persia's decline and fall. For one thing, weak rulers led to numerous provincial revolts, especially in Egypt, which always had detested Persian rule. Secondly, the provincial satraps also became more independent, ruling their satrapies more as kings than as the king's loyal subjects. They even carried on their own foreign policies and waged war on each other, which only added to Persia's problems.\nRevolts and unruly satraps caused serious economic problems for the empire. Persian taxes became heavier and more oppressive, which led to economic depression and revolts, which in turn led to more repression, heavier taxes and so on. The Persian kings also started hoarding gold and silver rather than re-circulating it. This created economic turmoil without enough gold and silver for doing business. As a result of this economic turmoil, the Persian kings got weaker still, which fed back into the problem of revolts and powerful satraps and so on.\nAround 400 B.C.E., Cyrus the Younger, a royal prince, rebelled against his brother and king, Artaxerxes. Although Cyrus was killed in battle, his force of 10, 000 Greek mercenaries survived only to find themselves stranded in the heart of Persia. In order to get home, they marched and fought their way through a good part of the Persian Empire. This exploit, known as the March of the Ten Thousand, exposed the weakness of the Persian Empire. This encouraged Alexander the Great to invade Persia, which he conquered in a remarkably short time and with a remarkably small army.\nNevertheless, the Persians survived and reestablished their empire under the Sassanid dynasty around 200 C.E. Around 650 C.E., they fell once again, this time to the Arabs inspired by their new religion, Islam. Still, Persia survived, passing its culture on to the Arabs. Thus the Islamic culture which emerged was very much Persian, and ultimately Mesopotamian, in origin. The Persian Empire revived once again around 1500 under the Safavid dynasty, and its culture and traditions live on today in modern Iran."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a5f682a6-85f1-4c4c-bbf7-d3d3c92e47fb>","<urn:uuid:2bdacbaf-ffed-4009-ae7e-0ff152e63797>"],"error":null}
{"question":"How do the water resistance capabilities compare between the Panerai Radiomir 1940 Tourbillon GMT and the Rolex Sea-Dweller 4000?","answer":"The Panerai Radiomir 1940 Tourbillon GMT has a water resistance of 5 bar (approximately 50 metres), while the Rolex Sea-Dweller 4000 is significantly more water-resistant, being guaranteed waterproof to a depth of 1,220 metres (4,000 feet).","context":["Last week, we’ve introduced you the new Panerai Radiomir 1940 3-Days Automatic with Panerai’s First Micro-Rotor Movement. During Watch and Wonders 2014, Panerai also came with something less mainstream, more complicated, more limited. But something that some of us can dream of: a Radiomir 1940 equipped with the P2005 movement, meaning a tourbillon with a second time-zone and 6 days of power reserve. Here is the Panerai Radiomir 1940 Tourbillon GMT Oro Rosso PAM558.\nPanerai is know for its rugged, large, massive and now iconic dive watches. Inspired by 1940s and 1950s editions, all of them share the same DNA. What especially differentiates the watches from the Officine are the materials used for the cases and the movements. Some of them relies on simple ETA 6497 but most are using in-house calibres now. And some are using superb manufacture movements. Take a look at the Panerai Radiomir Platino and Oro Rosso 47mm Special Edition or the Panerai Radiomir 1940 Chronograph and you’ll understand our definition of superb. OK, both are using some of our favorite movements, some Minervas, known for their glamorous curves. With the Radiomir 1940 Tourbillon GMT Oro Rosso PAM558, Panerai prefers to keep the game modern – at least for the movement – and encloses a P2005 into its new limited edition.\nThe P2005 that equipped the Panerai Radiomir 1940 Tourbillon GMT Oro Rosso PAM558 is not new – which doesn’t mean it is not a piece of interest. We already covered it when Panerai introduced the Luminor 1950 Tourbillon GMT Ceramica. It comes with an unusual 30-second tourbillon that rotates perpendicular to the balance, when a classical tourbillon does its rotation on its axis and (usually) in one minute. It also features Panerai’s usual ¾ style (even if it is composed of 3 separated bridges), with a straight graining, that hides 3 barrels, for 6 days of power reserve – indicated on the movement side by a hand. All of this is of course visible through a sapphire caseback, that will allow you to enjoy the polish beveled angles of the bridges and the circular graining of the main plate.\nTalking about mechanics, the front shows also some interesting features, with a GMT hand located on the central axis, a second time-zone at 3 (with AM / PM indication) and a small second at 9 (inside this subdial is a rotating tourbillon indicator). The GMT hand can be adjusted by one-hour increments directly by the screwed crown, without disturbing the seconds or the minutes. The dial comes with Panerai’s iconic sandwich structure. The Panerai Panerai Radiomir 1940 Tourbillon GMT Oro Rosso PAM558 has a nicely grained face, with a brown satin sunburst finish.\nMoving to the case, collectors won’t be disturbed as it presents the classical Radiomir 1940 shape – meaning large lugs (not the wire loop strap attachments of the classical Radiomir) and a case without the crown protection of the Luminor models. This the transitional shape between the two iconic models of the brand, the old Radiomir and the more modern 1950’s Luminor. It is here made of 18k red gold (Oro Rosso in Panerai’s nomenclature) and has a quite impressive 48mm diameter. Not the most discreet combination. However, the brown dial and strap with this gold case are highly elegant and warm.\nThe Panerai Radiomir 1940 Tourbillon GMT Oro Rosso PAM558 will be limited at 3à pieces only, for a price that is not yet defined. More about it on Panerai’s Official Website.\nSpecifications of The Panerai Radiomir 1940 Tourbillon GMT Oro Rosso PAM558:\n- Movement: hand-wound mechanical, Panerai P.2005 calibre, executed entirely by Panerai, 16¼ lignes, 9.1 mm thick, 31 jewels, Glucydur® balance, 28,800 alternations/hour. KIF Parechoc® anti-shock device. Power reserve 6 days, three barrels. 239 components.\n- Functions: Hours, minutes, small seconds, second time zone, 24h indicator, power reserve indicator on the back, tourbillon.\n- Case: Diameter 48 mm, 18 ct. polished red gold. Screw-down winding crown personalized OP.\n- Bezel: 18 ct. polished red gold.\n- Back: See-through sapphire crystal.\n- Dial: Brown, with luminous Arabic numerals and hour markers. 24h indicator at 3 o’clock, seconds and tourbillon indicator at 9 o’clock.\n- Crystal: Sapphire, formed of corundum, 1,5 mm thick. Anti-reflective coating.\n- Water-resistance: 5 bar (~ 50 metres).\n- Strap: PANERAI personalised alligator strap and polished 18 ct. red gold adjustable buckle.\n- Reference: PAM00558.","Rolex introduces a modern version of the legendary Oyster Perpetual Sea-Dweller, a hot favourite among diving professional since its introduction 1n 1967.\nThis 40 mm technical divers’ watch, waterproof to a depth of 1,220 metres (4,000 feet), features the latest in Rolex innovation: Cerachrom bezel insert in ceramic, long-lasting Chromalight luminescence, paramagnetic blue Parachrom hairspring, Oysterlock safety clasp and Rolex Glidelock bracelet extension system.\nOriginally designed for the pioneers of professional deep-sea diving, the Sea-Dweller 4000 is equipped with one of the inventions that contributed to its stature: the helium escape valve, patented by Rolex in 1967. This ingenious safety valve releases helium from the watch case as the gas expands during the decompression phases of deepwater saturation dives, while preserving the water-proofness of the watch.\nThe Sea-Dweller 4000’s 60-minute graduated, unidirectional, rotatable bezel enables divers to precisely and safely monitor their dive and decompression times. It is equipped with a patented black Cerachrom bezel insert manufactured by Rolex in a virtually scratchproof ceramic whose colour is unaffected by ultraviolet rays.\nThe graduation is coated via PVD (Physical Vapour Deposition) with a thin layer of platinum. The sleek black dial features large Chromalight hour markers and hands filled with luminescent material that emits a long-lasting blue glow for excellent legibility in dark conditions.\nThe Sea-Dweller 4000’s Oyster case, guaranteed water proof to a depth of 1,220 metres (4,000 feet), is a paragon of robustness and reliability. The characteristically shaped middle case is crafted from a solid block of particularly corrosion-resistant 904L steel.\nThe fluted case back is hermetically screwed down with a special tool exclusive to Rolex watchmakers. The winding crown, fitted with the patented Triplock triple water-proofness system, screws down securely against the case, offering watertight security akin to a submarine’s hatch. It is protected by a crown guard that is an integral part of the middle case.\nThe crystal is made of virtually scratchproof sapphire. The waterproof case of the Sea-Dweller 4000, housing its high-precision movement, ensures optimal protection from water, dust, pressure and shocks.\nThe new Sea-Dweller 4000 is equipped with calibre 3135, a self-winding mechanical movement with a date display entirely developed and manufactured by Rolex. Like all Rolex Perpetual movements, the 3135 is a certified Swiss chronometer, a designation reserved for high-precision watches that have successfully passed the Swiss Official Chronometer Testing Institute (COSC) tests. Its architecture, like that of all Oyster watch movements, makes it singularly reliable. The oscillator, the true heart of the watch, has a blue Parachrom hairspring patented and manufactured by Rolex in an exclusive alloy of niobium and zirconium. Insensitive to magnetic fields, the Parachrom hairspring offers great stability when exposed to temperature variations and remains up to 10 times more precise in case of shocks than a traditional hairspring.\nThe Sea-Dweller 4000’s Oyster bracelet is equipped with an Oysterlock safety clasp that prevents accidental opening, and with a double extension system that allows the watch to be worn comfortably over a diving suit up to 7 mm thick. The Fliplock extension link extends the bracelet by 26 mm, while the Rolex Glidelock system allows fine adjustments of the bracelet length in 2 mm increments for a total of approximately 20 mm.\nOyster (monobloc middle case, screw-down case back and winding crown)\nDiameter: 40 mm\n904L stainless steel super-alloy, satin finish\nCase Back: Screw-down with Rolex fl uting\nBezel: Unidirectional rotatable 60-minute graduated bezel with black Cerachrom insert in ceramic, engraved numerals and graduations coated with platinum via magnetron sputtering (PVD)\nWinding Crown: Screw-down, Triplock triple water-proofness system\nCrown Guard: Integral part of the middle case\nCrystal: Scratch-resistant sapphire\nWater-proofness: 1,220 metres (4,000 feet)\nCalibre 3135, Manufacture Rolex\nMechanical movement with bidirectional self-winding via\nOfficially certified Swiss chronometer (COSC)\nFrequency: 28,800 beats/hour (4Hz)\nParamagnetic blue Parachrom hairspring\nLarge balance wheel with variable inertia\nHigh-precision regulating via four gold Microstella nuts\nTraversing Balance Bridge\nJewelling: 31 rubies\nPower Reserve: Approximately 48 hours\nCentre hour, minute and seconds hands\nInstantaneous date at 3 o’clock\nStop-seconds for precise time setting\nBlack, satin finish\nHour markers: Highly legible Chromalight appliques (long-lasting luminescence) in 18 ct white gold\nHands: Chromalight hands in 18 ct white gold\nType: Oyster three-piece solid links\nMaterial: 904L stainless steel superalloy, satin finished, polished edges\nClasp: Oysterlock folding safety clasp\nRolex Glidelock extension system (20 mm in increments of 2 mm)\nFliplock extension link (26 mm)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:88916479-57c3-4844-8fdd-9003b6886b58>","<urn:uuid:a5b9a306-a203-4181-9609-e372d0982069>"],"error":null}
{"question":"Which is more risky: competitive bidding or sole sourcing in IT outsourcing?","answer":"Competitive bidding is generally less risky than sole sourcing. While competitive bidding is more resource-intensive, it provides better cost savings potential, encourages competitively priced bids, offers multiple viewpoints that can identify potential pitfalls, and provides fallback options if negotiations fail. In contrast, sole sourcing has significant negatives including loss of leverage, inability to compare alternatives, less aggressive pricing, and could impact the legitimacy of the deal. Additionally, sole sourcing may actually take longer as there is no time pressure that comes from a competitive environment.","context":["When planning to outsource all or part of an organization’s IT functions, it\\’s important to perform active risk management throughout all stages of the outsourcing lifecycle. As a quick recap of Part 1 of this series, managing project risk is a process of identifying potential failure points in a plan, determining the probability of occurrence, and then estimating the impact of each. With that information in hand, an organization can move to the next step of actively managing risks by deciding which risks are tolerable and which ones need mitigation.\nAfter determining your overall strategy, it\\’s time to seek out potential providers of the outsourced IT services your company needs. While you can always just call Lou, the second cousin of your sister-in-law who works for a big acronym company based in D.C, (don’t laugh too hard — this happens all too often); the best method for accomplishing this selection is to run a competitive bid process and issue a Request for Proposal (RFP).\nBy reaching out to multiple providers experienced in the outsource services that your firm needs, you will encourage the bids to be competitively priced. In addition, you will likely get a disparity of viewpoints that will point out potential pitfalls that may have otherwise been missed. Finally, in deference to the adage about keeping all of one’s eggs in one basket, having multiple vendors bid (and in some case jointly win) an outsourcing opportunity provides your organization with a fallback strategy if negotiations or delivery later go awry.\nA simple but frequently forgotten risk management tenet to remember is that you want to provide every opportunity for your vendor to be a success. While this seems obvious, often executives can be heard saying things like, “I don’t worry about the details. That’s what we pay them for…” or “Our contract is ironclad. One misstep on their part and the penalties are so bad we’ll practically own their company.” While your trains sit idle as engineers try to figure out why the two tunnels didn’t meet under the channel or your order screens remain dark while you sit in court arguing with your call center provider over liquidated damages, your clients are flocking elsewhere.\nThe more clarity and detail you can provide in your RFP, the better your potential partners can determine if they can provide a solution and what it might entail. This does not mean that you have to tell them exactly what the solution is; one of the great benefits of a competitive bid process is the opportunity to have input from a variety of very knowledgeable organizations on how best to meet your goal. It does mean, however, that you should provide as much specificity as possible in regard to your current state. If you lack a complete inventory of your company’s IT architecture/infrastructure including existing (legacy) application portfolio, IT support structure and current projects, costs, and service levels, then a high level assessment of components and their suitability for outsourcing (and/or insourcing) needs to be accomplished first.\nWithin the RFP process there are other ways of reducing risk. The more time that you can provide your potential partners to perform discovery and develop their responses, the better. If for some reason your schedule is constrained, then provide as much access to internal subject matter experts for question and answers, follow-up meetings, etc. as possible.\nAnother easy tactic for reducing risk: Throughout the RFP, try to use quantitative descriptions instead of qualitative ones wherever possible. A personal favorite is when I see a client request or a vendor promise “best-in-class.Ó Firsthand experience has shown that when it comes to IT, “best in class” means something completely different to an aerospace firm than it does to a highway-paving company.\nCutting to your bottom line, the more unknowns your service provider faces, the greater the risk. Risk costs money; the more risk that can be driven out of an IT outsourcing solution, the less a vendor will charge you and the greater the chance you have for a successful outsourcing endeavor.\nReprinted with permission from Alsbridge.\nRead Part 1, doing risk assessment in your outsourcing endeavors:\nRead Part 3, evaluating service provider proposals:\nRead Part 4, covering disputes in the contract\nRead Part 5, a guide to setting SLAs\nRead Part 6, managing outsourcing after the contract is signed","How to start off a successful outsource project\n1.0 Know what you want\nThere must be clear scoping of the demand and what is being put to the market. The objectives for the outsourcing must be consistent and reasonable – cost reduction, as an aim together with increase service may be inconsistent. Sign off internally why you are doing this and agree what is driving the whole process – this is important from the vendors perspective as well. If the vendor knows that cost reduction or technology refresh are key objectives the response can be tailored to your precise needs. Furthermore, objectives can change over time and the original case for an Outsource can be undermined by events. Revisiting the rationale you agreed internally is an important task during the process – don’t be driven by the running train take a time out to check you still need to do this.\n2.0 Put in place a clear process.\nDecide whether you are asking for a sole source versus competitive bid from the market. Sole sourcing is usually suggested (particularly by the vendor) if there is a history with the supplier and there is a time constraint – but there are significant negatives. Loss of leverage, not being able to compare alternatives, less aggressive pricing to name but three – and a sole source could have high impacts such as the legitimacy of the deal. Last but not least, the process may actually take longer as there is no time pressure that comes from a competitive environment.\nIn a competitive bid position cost savings have a better chance of being realised and new suppliers can come with more innovative proposals than the in-house incumbent – at least in principle. The process can actually be quicker as the client can drive the competitive process – by a strict time based approach to the process for example. But on the other hand competitive bidding is more resource intensive, for the supplier as well as the client, so make sure you resource well.\nBe precise, not prescriptive, comprehensive but concise in the layout – focus on key objectives. We need the ‘what’ not the how – avoid laying down all sorts of preconditions about how the service is to be delivered – that’s the suppliers job in the proposal. I have seen in several RFP’ s detailed specifications of what packages to use and how precisely the service is to be delivered – effectively closing off all innovative solutions that may have been available from the vendor. Also specific demands will drive up the cost – the vendor may be able to offer off-the-shelf solutions that will work just as well as your specific demands but at very favourable rates.\nA request for informartion (RFI) is a high-level document inviting a general response and can be used as a test for possible solutions and to pre-select candidates for the bid. Usually there is no bid price given by the suppliers – nor should we expect too much detail here. An request for proposal (RFP) invites a formal response and takes longer for the vendor and the customer to evaluate. In a large bid this cost can come to millions of dollars so make sure before you issue a RFP you really mean to go ahead. Ensure you are being realistic in your demands and take care that the quality and clarity in the RFP promotes conformance in the proposals received.\n3.0 Manage the Communication Channels\nIn negotiation avoid shortcuts and set specific goals – and ensure they are delivered. Evaluate, clarify and frame negotiations to keep competition alive. Document all discussions and carry out frequent self-assessment. Use a term sheet as this helps drive and track the discussion and allows apples to apples comparison – over time the term sheet can evolve into a contract so it is well worth the effort to create one.\nManage the up and down communication channels carefully. Make sure no seniors speak to vendors and control vendor access to senior management carefully. Some vendors are good at getting around the formal process to the senior management and exploiting this access to short-circuit the tender process. We all know of ‘golf course’ deals that cut through a bid process and enable vendors to return to the customer team informing them they ‘know’ the requirements of senior management. Most golf course deals end in disaster so should be avoided like the plague.\nKeep talking to vendors and meet frequently to discuss the proposals – the more open and interactive the better the eventual outcome will be.\n4.0 Cover the Details\nFirst of all vendors to this for a living – often the vendor sales team have been doing this for years and when this is done will move onto the next. The customer side on the other hand may have not done this before or at least the team carrying out the supplier proposal evaluation may be completely new compared to the last time the outsource process was done. Also some of the customer team will have a day job to contend with – don’t forget this (or holidays etc.) and plan capacities well. Plan well, resource well and set realistic time scales – time pressure can act in the vendor’s favour and allow skipping of important details.\nNever let issues that should be solved at negotiation drift into ‘we will solve this later’ discussions. They never are and these can be a source of major conflict later. A trade union official some time ago told me: ‘It is better for the negotiation to break down rather than the agreement’. All-important details must be cleared before signing a contract.\nPartnership rhetoric will appear at some stage in the discussions from the vendor side. Partnership usually means giving all the risks to the vendor from the customer side or closing out competition from the vendor side (sole sourcing). Partnership can be invoked to get over tricky points and put them off until later stages or to close out competition. Partnership should be based on performance and strict business principles not waffle. I know it is often said we can handle the things we forgot later in a change process – I have personally never found this to be free of major problems and cost – so beware of this.\nFinal point maximum gain minimum vendor pain during the proposal stage – and remember to ask yourself what you are looking for from outsourcing until you know what it is!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:c9e3617e-ca92-4d76-b859-c88e06deeb75>","<urn:uuid:4355ce6f-4612-4322-bf01-c83aeccc0225>"],"error":null}
{"question":"What's the link between radioactive decay rates and cancer risks from nuclear testing?","answer":"Radioactive decay rates are proportional to the amount of radioactive nuclides present, and these rates are unaffected by chemical or physical states. When nuclear tests released radioactive materials into the environment, this led to radiation exposure that has been scientifically proven to cause various types of cancer. Studies have linked radiation exposure to most forms of leukemia, as well as thyroid, lung, and breast cancer, with cancer development occurring 10-40 years after exposure. According to some studies, including an assessment by International Physicians for the Prevention of Nuclear War, atmospheric nuclear testing could cause approximately 2.4 million cancer deaths.","context":["Radioactive Decays – transmutations of nuclides\nRadioactivity means the emission of alpha (a) particles, beta (b) particles, or gamma photons (g) from atomic nuclei. Radioactive decay is a process by which the nuclei of a nuclide emit a, b or g rays. In the radioactive process, the nuclide undergoes a transmutation, converting to another nuclide.\nThe variation of radioactivity over time is called decay kinetics. The characteristics of kinetics are expressed in decay constant and half life. Variations of radioactivity in mixtures of radioactive nuclides and consecutive decays are often considered, and decay kinetics serves science and technology in many applications.\nIn radioactive decay processes, some of the things are conserved, meaning they do not change. The number of nucleons before and after the decay is the same (conserved). So are electric charges and energy (including mass). The relationship between nuclides is best seen in a chart based on the number of neutrons and the number of protons. Such a chart clearly shows relationships among isobars, isotopes, isotones, and isomers.\nThe mass number of a nuclide does not change in b and g decays, but it decreases by 4 in a decay due to the emission of a helium nucleus. Thus, there are four families of radioactive series based on mass numbers starting with 232Th, 237Np, 238U, and 235U respectively. Masses of their family members are in the 4n, 4n +1, 4n +2, 4n +3 categories, where n is an integer. Sources of natural radioactive materials such as radium, radon, and polonium, came from the natural occurring radioactive nuclides 235U, 238U, and 232Th, whereas 237Np is a man made nuclide, because this nuclide no longer exists in the planet Earth.\nStudies of radioactive decays led to theories of nuclear stability and nuclear structure. Some of these theories will be examined as we take a closer look at atomic nuclei. Concepts such as energy states of nucleons, angular momentum, parity of nuclear energy state etc. will be introduced. These concepts and theories provide the tools for the discussion of energy in radioactive decays.\nWe will look at radioactivity and decay kinetics, look at transmutation of nuclides in radioactive decay, the nuclide chart, which is used to discuss the four families of radioactive decay, look at atomic nuclei closely, and look at the energy aspect in radioactive decays.\nRadioactivity and Decay Kinetics\nThe emission of alpha (a), beta (b) or gamma (g) rays by a sample of substance is called Radioactivity. A sample may emit one or more types of radioactive ray. The number of a, b or g rays emitted per unit time is called the decay rate. The study of radioactive material requires the identification of types of rays emitted, decay rates, changes in decay rates, and the nuclides in the sample that emit the rays. The variation of decay rates over time from a fixed amount of nuclide is called decay kinetics, which is an important topic in the study of radioactivity.\nRadioactivity Units, Decay Constants and Half Lives\nThe decay rate is measured in decays per unit time, and the SI unit for radioactivity is becquerel (Bq) which is 1 decay or disintegration per second (dps)*. The widely used unit curie (Ci) defined as the decay rate of 1.0 g of radium earlier is 3.700 x 1010 Bq.\nA Summary of Decay Kinetics\nNo is number of nuclei at t = 0,\nln N = ln No – l t\n- How are decay rates related to the amounts of radioactive nuclide?\nHow do decay rates vary over time?\n- Do chemical states of a radioactive nuclide (element) affect its decay rate?\nSamples containing the same amount of uranium and thorium have very different decay rates. A radioactive source contains one or more radioactive nuclides. The decay rate of a sample is proportional to the amount of radioactive nuclide present. Decay rate of a nuclide is unaffected by its chemical or physical state, studies have shown.\nThe amount of radioactive nuclide can be expressed in unit g, mole, or number of nuclei (N). The disintegration rate of N nuclei (called activity, A, or in mathematical notation ‑ dN/dt) at any given time is proportional to N. The proportional constant is called the decay constant, l. A summary of decay kinetics is given in the text box.\n* Long before SI units were established, radioactivity was compared to a quantity called curie (Ci), which was originally the radioactivity of 1.0 g of radium. One Ci is now defined as the quantity of any radioactive material that gives 3.700 x 1010 dps or Bq. Thus, Ci is considered a cgs (cm, gram & second) unit, and 1 Ci = 3.700 x 1010 Bq.","General overview of the\neffects of nuclear testing\nThe material contained in this chapter is based on official government sources as well as information provided by research institutions, policy organizations, peer-reviewed journals and eye witness accounts.\nThe CTBTO remains neutral in any ongoing disputes related to compensation for veterans of the nuclear test programmes.\nNuclear weapons have been tested in all environments since 1945: in the atmosphere, underground and underwater. Tests have been carried out onboard barges, on top of towers, suspended from balloons, on the Earth’s surface, more than 600 metres underwater and over 200 metres underground. Nuclear test bombs have also been dropped by aircraft and fired by rockets up to 320 km into the atmosphere.\nThe National Resources Defense Council estimated the total yield of all nuclear tests conducted between 1945 and 1980 at 510 megatons (Mt). Atmospheric tests alone accounted for 428 mt, equivalent to over 29,000 Hiroshima size bombs.\nThe first nuclear test was carried out by the United States in July 1945, followed by the Soviet Union in 1949, the United Kingdom in 1952, France in 1960, and China in 1964. The National Resources Defense Council estimated the total yield of all nuclear tests conducted between 1945 and 1980 at 510 megatons (Mt). Atmospheric tests alone accounted for 428 mt, equivalent to over 29,000 Hiroshima size bombs.\nThe amount of radioactivity generated by a nuclear explosion can vary considerably depending upon a number of factors. These include the size of the weapon and the location of the burst. An explosion at ground level may be expected to generate more dust and other radioactive particulate matters than an air burst. The dispersion of radioactive material is also dependent upon weather conditions.\nLarge amounts of radionuclides dispersed into the atmosphere\nThe 2000 Report of the United Nations Scientific Committee on the Effects of Atomic Radiation to the General Assembly states that:\n“The main man-made contribution to the exposure of the world's population [to radiation] has come from the testing of nuclear weapons in the atmosphere, from 1945 to 1980. Each nuclear test resulted in unrestrained release into the environment of substantial quantities of radioactive materials, which were widely dispersed in the atmosphere and deposited everywhere on the Earth’s surface.”\nThe first nuclear test was carried out by the United States in July 1945, followed by the Soviet Union in 1949, the United Kingdom in 1952, France in 1960, and China in 1964.\nConcern over bone-seeking radionuclides and the first mitigating steps\nPrior to 1950, only limited consideration was given to the health impacts of worldwide dispersion of radioactivity from nuclear testing. Public protests in the 1950s and concerns about the radionuclide strontium-90 (see Chart 1) and its effect on mother’s milk and babies’ teeth were instrumental in the conclusion of the Partial Test Ban Treaty (PTBT) in 1963. The PTBT banned nuclear testing in the atmosphere, outer space and under water, but not underground, and was signed by the United States, the Soviet Union and the United Kingdom. However, France and China did not sign and conducted atmospheric tests until 1974 and 1980 respectively.\nAlthough underground testing mitigated the problem of radiation doses from short-lived radionuclides such as iodine-131, large amounts of plutonium, iodine-129 and caesium-135 (See Chart 1) were released underground. In addition, exposure occurred beyond the test site if radioactive gases leaked or were vented.\nGradual increase in knowledge about dangers of radiation exposure\nOver the past century, there has been a gradual accumulation of knowledge about the hazards of radioactivity. It was recognized early on that exposure to a sufficient radiation dosage could cause injuries to internal organs, as well as to the skin and the eyes.\nAccording to the 2000 Report of the United Nations Scientific Committee on the Effects of Atomic Radiation to the UN General Assembly, radiation exposure can damage living cells, killing some and modifying others. The destruction of a sufficient number of cells will inflict noticeable harm on organs which may result in death. If altered cells are not repaired, the resulting modification will be passed on to further cells and may eventually lead to cancer. Modified cells that transmit hereditary information to the offspring of the exposed individual might cause hereditary disorders. Vegetation can also be contaminated when fallout is directly deposited on external surfaces of plants and absorbed through the roots. Furthermore, people can be exposed when they eat meat and milk from animals grazing on contaminated vegetation.\nRadiation exposure has been associated with most forms of leukaemia, as well as cancer of the thyroid, lung and breast.\nStudies reveal link between nuclear weapon testing and cancer\nThe American Cancer Society’s website explains how ionizing radiation, which refers to several types of particles and rays given off by radioactive materials, is one of the few scientifically proven carcinogens in human beings. Radiation exposure has been associated with most forms of leukaemia, as well as cancer of the thyroid, lung and breast. The time that may elapse between radiation exposure and cancer development can be anything between 10 and 40 years. Degrees of exposure regarded as tolerable in the 1950s are now recognized internationally as unsafe.\nAn article featured in Volume 94 of American Scientist on Fallout from Nuclear Weapons Tests and Cancer Risks states that a number of studies of biological samples (including bone, thyroid glands and other tissues) have provided increasing proof that specific radionuclides in fallout are implicated in fallout-related cancers.\nIt is difficult to assess the number of deaths that might be attributed to radiation exposure from nuclear testing. Some studies and evaluations, including an assessment by Arjun Makhijani on the health effects of nuclear weapon complexes, estimate that cancer fatalities due to the global radiation doses from the atmospheric nuclear testing programmes of the five nuclear-weapon States amount to hundreds of thousands. A 1991 study by the International Physicians for the Prevention of Nuclear War (IPPNW) estimated that the radiation and radioactive materials from atmospheric testing taken in by people up until the year 2000 would cause 430,000 cancer deaths, some of which had already occurred by the time the results were published. The study predicted that roughly 2.4 million people could eventually die from cancer as a result of atmospheric testing.\nCHART 1 – EFFECTS OF RADIONUCLIDES\n|6.7 hours||Inhalation in excessive concentrations can result in dizziness, nausea, vomiting, loss of consciousness, and death. At low oxygen concentrations, unconsciousness and death may occur in seconds without warning.|\n|432 years||Moves rapidly through the body after ingestion and is concentrated within the bones for a long period of time. During this storage americium will slowly decay and release radioactive particles and rays. These rays can cause alteration of genetic materials and bone cancer.|\n|8 days||When present in high levels in the environment from radioactive fallout, I-131 can be absorbed through contaminated food. It also accumulates in the thyroid gland, where it can destroy all or part of the thyroid. May cause damage to the thyroid as it decays. Thyroid cancer may occur.|\n|30 years||After entering the body, caesium is distributed fairly uniformly through the body, with higher concentration in muscle tissue and lower concentration in bones. Can cause gonadal irradiation and genetic damage.|\n|10.76 years||Inhalation in excessive concentrations can result in dizziness, nausea, vomiting, loss of consciousness, and death.|\n|28 years||A small amount of strontium 90 is deposited in bones and bone marrow, blood and soft tissues when ingested. Can cause bone cancer, cancer of nearby tissues, and leukaemia.|\n|24,400 years||Released when a plutonium weapon is exploded. Ingestion of even a miniscule quantity is a serious health hazard and can cause lung, bone, and liver cancer. The highest doses are to the lungs, the bone marrow, bone surfaces, and liver.|\n|12 years||Easily ingested. Can be inhaled as a gas in the air or absorbed through the skin. Enters soft tissues and organs. Exposure to tritium increases the risk of developing cancer. Beta radiation emitted by tritium can cause lung cancer.|\n* ( i.e. amount of time it takes for half of the quantity of a radioactive material to decay)\nMeasuring radiation doses and biological risks\nScientists use different terms when measuring radiation. The terms can either refer to radiation from a radioactive source, the radiation dose absorbed by a person, or the risk that a person will suffer health effects from exposure to radiation. When a person is exposed to radiation, energy is deposited in the body’s tissues. The amount of energy deposited per unit of weight of human tissue is called the absorbed dose. This is measured using the rad or the SI Gy. The rad, which stands for radiation absorbed dose, has largely been replaced by the Gy. One Gy is equal to 100 rad.\nThe curie (symbol Ci) is a unit of radioactivity. It has largely been replaced by the Becquerel, which is the unit of radioactivity. One Becquerel is defined as the number of atoms which decay per second in a sample. The curie unit is named after Marie and Pierre Curie, who conducted pioneering research on radiation.\nA person's biological risk (i.e. the risk that a person will suffer health effects from an exposure to radiation) is measured using the conventional unit rem or the SI unit Sv.\nCHART 2. EFFECTS OF DIFFERENT LEVELS OF RADIATION\n|Radiation dose in rems||Health impact\n||Possible chromosomal damage.|\n|20-100||Temporary reduction in number of white blood cells. Mild nausea and vomiting. Loss of appetite. Fatigue, which may last up to four weeks. Greater susceptibility to infection. Greater long-term risk of leukaemia and lymphoma is possible.|\n|100-200||Mild radiation sickness within a few hours: vomiting, diarrhea, fatigue; reduced resistance to infection. Hair loss. In sufficient amounts, I-131 can destroy all or part of the thyroid gland, leading to thyroid abnormalities or cancer. Temporary male sterility.|\n|200-300||Serious radiation sickness effects as in 100-200 rem. Body cells that divide rapidly can also be destroyed. These include blood cells, gastrointestinal tract cells, reproductive cells, and hair cells. DNA of surviving cells is also damaged.|\n|300-400||Serious radiation sickness. Bone marrow and intestine destruction. Haemorraging of the mouth.|\n|400-1000||Acute illness, possible heart failure. Bone marrow almost completely destroyed. Permanent female sterility probable.|\n|1000-5000||Acute illness, nerve cells and small blood vessels are destroyed. Death can occur in days.|\nThe United States' Nuclear Testing Programme"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:ca72e686-3faa-4f2f-80a4-6e3102252cfc>","<urn:uuid:39ba22f3-4a96-46f5-91cf-dae49ef008d4>"],"error":null}
{"question":"What percentage of nonprofit organizations have succession plans, and how does this relate to leadership diversity in the Baltimore-Washington region?","answer":"Only 17% of nonprofit organizations indicate having a succession plan according to the Daring to Lead study. This is particularly concerning for the Baltimore-Washington region's nonprofit sector, where about a quarter of organizations are led by baby boomer chief executives who will soon retire, necessitating a transition to a new generation of leaders. This leadership transition is happening amid significant demographic changes in the region, which has evolved from a primarily white-black society to one with residents from all parts of the globe.","context":["The nonprofit sector in the Baltimore–Washington region is undergoing a profound, albeit quiet, revolution driven by demographic change. The people and communities that nonprofits serve increasingly reflect a multiracial and multi–ethnic world, and a new generation of leaders will soon emerge as baby boomer executives retire. This report examines whether the Baltimore–Washington region's nonprofit sector reflects the new demographic realities. The report, based on a representative sample of 501(c)(3) organizations, documents the extent to which the region's nonprofit boards, staff, and executive leadership are racially and ethnically diverse. It also analyzes diversity by the organization's size, type, and geographic location, and examines how the sector has been affected by the current economic downturn.\nThe text below is an excerpt from the complete document. Read the full report in PDF format.\nThe nonprofit sector in the Baltimore–Washington region is undergoing a profound, albeit quiet,\nrevolution driven by demographic change. First, the people and communities that nonprofit\norganizations serve increasingly reflect a multiracial and multi–ethnic world. No longer is the region\nprimarily a white–black society. It now has residents from all parts of the globe that bring different\ncultures, languages, and values to the region.\nSecond, nonprofit–sector leadership is beginning to move to the next generation. About a quarter of\nall nonprofits in the Baltimore–Washington region are led by baby boomer chief executives. As\nthese leaders retire and phase out of active roles, a new cadre of leaders will take up the sector's\nreins and direct its work. How ready is the sector to address this important transition?\nTo learn whether the Baltimore–Washington region's nonprofit sector reflects current demographic\nrealities, the Baltimore–Washington Regional Nonprofit Racial Diversity Collaborative (the\nCollaborative) commissioned the Urban Institute's Center on Nonprofits and Philanthropy to survey\nnonprofit organizations in the region and study the diversity of nonprofit executive directors,\ngoverning boards, and paid staff.\nThe Study's Questions\nThis report addresses four questions:\n- What percentage of executive directors, board members, and paid staff in the sector is people of\ncolor, and what percentage is members of specific racial–ethnic communities?\n- Is there a gender difference in the leadership of organizations led by people of color?\n- How does the diversity of nonprofit leadership vary by the size of the organization, field of\nactivity, or geographic location within the region?\n- What effects, if any, is the current economy having on nonprofit organizations in terms of\ndemand for services and funding, and are the effects correlated with the racial and ethnic\ndiversity of organizational leadership?\nThe findings of this study provide a valuable baseline for understanding how racially and ethnically\ndiverse the nonprofit sector in the Baltimore–Washington region is. The findings also give an\nimportant context for the Collaborative's efforts to promote diversity and strengthen the skills of\nAbout the Survey\nThe survey is based on a representative sample of 501(c)(3) organizations in the Baltimore–Washington region. The sample was stratified by counties within the region, type of nonprofit, and\nsize of organization to ensure good representation of nonprofits across the region. Hospitals and\nhigher education are excluded from the sample. Data were collected using a mixed–mode technique\n(e.g., by mail, web, and telephone). The survey resulted in 283 usable responses, yielding a\nresponse rate of 32.6 percent. Further detail on regional definitions and the types of nonprofits in\nthe study can be found in the appendices.\n(End of excerpt. The full report is available in PDF format.)\nMeasuring Racial-Ethnic Diversity in California's Nonprofit Sector\nUsage and reprints: Most publications may be downloaded free of charge from the web site and may be used and copies made for research, academic, policy or other non-commercial purposes. Proper attribution is required. Posting UI research papers on other websites is permitted subject to prior approval from the Urban Institute—contact email@example.com.\nIf you are unable to access or print the PDF document please contact us or call the Publications Office at (202) 261-5687.\nDisclaimer: The nonpartisan Urban Institute publishes studies, reports, and books on timely topics worthy of public consideration. The views expressed are those of the authors and should not be attributed to the Urban Institute, its trustees, or its funders. Copyright of the written materials contained within the Urban Institute website is owned or controlled by the Urban Institute.","As published in The Long Beach Business Journal, Third Sector Report Columnist\nIt’s too bad that whenever the word, “succession,” finds its way into conversations at most nonprofit organizations, most view its utterance as code for someone needs to go.\nWhen an inevitable transition does occur, these same organizations soon discover that having kept succession in the closet ultimately costs their worthy causes significant amounts of time, money and angst.\nWhy Succession Plans Are Crucial\nThe third sector is no different than the other sectors when it comes to human resource matters and leadership. Most transitions in key positions aren’t planned. Messy exits can be a public relations challenge. And, transitions that should have happened earlier, but for personal or political reasons were avoided, creates a domino-effect of other’s choosing to vacate their positions instead.\nWhile nonprofits face other common workforce realities such as a Baby Boomer generation retiring en masse and the difficulty of attracting and keeping up-and-comers, the sector has a couple of quirks that make leadership continuity slightly more complicated: The first is that each of these realities can be applied not only to paid employees, but also to volunteers. The second is that without term limits and a commitment to succession, great causes can easily evolve into family-run businesses disguised as nonprofits.\nAll of these factors sum up to a subject that is emotionally charged and filled with deeply-rooted confusion over what constitutes prudent succession planning for a nonprofit organization.\nReplacing key positions, managing with a leadership void, and curbing high attrition are expensive propositions for any enterprise. For organizations that depend on people’s generosity to survive, however, these are situations that demand pro-active and visible attention.\nMost Nonprofits Lack Adequate Succession Plans\nThe Daring to Lead study released by CompassPoint Nonprofit Services in Oakland dramatically illustrates the current state of the sector when it comes to succession:\n- 17 percent of 3,000 nonprofit organizations indicate having a succession plan\n- 24 percent of nonprofit executives report planning to vacate their positions within two years\n- 45 percent have no executive performance review process to annually discuss succession\nBased on these numbers, there is a significant quantity of important nonprofit organizations whose futures are at risk. Sadly, most of them have little idea how large the risk is because succession has been considered an unmentionable in their cultures.\nFor those who have believed that investing time and money in leadership development, retention and succession will hurt a nonprofit’s overhead calculations, the Daring to Lead numbers suggest that the day in court is nigh. The costs of sloppy human resource practices, poorly managed executive exits, replacing avoidable vacancies, and a lack of competent future board leadership creates a tab that is going to difficult to prove as circumstantial. The sum of these costs will be even more difficult defend as responsible fiduciary oversight in the name of charity.\nSuccession, like fundraising, is an integral piece of a successful nonprofit’s organizational culture. Both subjects are forms of resource development that are equally vital for fueling its future.\nSix Crucial Attributes of a Successful Succession Plan\nCultures that value succession have six distinct attributes. The first is operating with a common understanding that succession is about ongoing efforts to assure a continuity of good leadership is in place for the organization. It is not about naming an heir-apparent or planning one person’s transition.\nThe second attribute is completing a contingency operations plan every year and creating a warehouse of vital organizational information should a key position become vacant for any reason. The third attribute is a series of board-approved succession policies which mandates such things as performance reviews, term limits, human resource procedures, and nominations.\nDemonstrated competencies in basic human resources is the critical fourth attribute. The fifth is completing and managing from a strategic plan that pays as much attention to developing human capital as it does to developing financial capital for creating its future. And, the final attribute is assigning the task of succession to key people as part of their job responsibilities and performance objectives.\nFor me, nonprofit leadership succession has become an obsession. Perhaps I’ve stood on the curb for too many years watching people confuse or avoid the subject and declining to get on the succession planning bus. It’s painful to witness so many great executives, boards and organizations now at significant risk of being hit by it.\nImage courtesy of pexels.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:18c4daf4-8c6d-4a6a-bec7-0ac2f6a95ce4>","<urn:uuid:17b4a9a1-a659-4558-9695-74b1c5279415>"],"error":null}
{"question":"Please explain how the use of confirmation bias differs between a business merger decision and the Wason's number experiment?","answer":"In a business merger decision, confirmation bias manifests as considering only information that supports the merger while discounting negative indicators, which could lead to devastating financial consequences. In contrast, Wason's number experiment (2,4,6) demonstrates confirmation bias through participants primarily testing even number sequences that fit their hypothesis about the rule, rather than trying to disprove it by testing other number combinations. The actual rule was simply ascending numbers, showing how confirmation bias in both cases leads to incomplete analysis and potentially wrong conclusions, though with different stakes - theoretical in the experiment versus significant business impact in merger decisions.","context":["Behavioral Biases and Their Effect on Investors’ Decisions\nMost important decisions we make in life involve two elements: the objective, measurable facts, and our subjective views on the desirability of what is there to gain and what is there to lose by our decision. There is no formula. Both elements are important and neither is sufficient by itself. It is no different when we make decisions about risk and investments. The challenge all investors face is that while objective elements can be tracked and measured, subjective elements are sometimes influenced by personal biases and can be made with irrational thinking.\nTraditional finance theories assume that investors are rational and make optimal investment decisions to maximize their wealth. However, behavioral finance theories indicate that when making investment decisions, investors fail to remain fully rational due to various psychological biases.\nIn their groundbreaking paper, “Prospect Theory: An Analysis of Decision under Risk” (1979), Daniel Kahneman and Amos Tversky convincingly demonstrate that even when all information is available, individuals are highly susceptible to cognitive errors. They concluded that human beings are by and large irrational decision-makers.\nAs human beings, we all have certain built-in biases. We will focus on the three most prevalent: confirmation bias, loss aversion, and herd mentality.\nPeople mostly will look for information or ideas that will validate their own beliefs. For example, when watching a TV channel, viewers will choose a channel that represents their political views, and avoid those with opposed opinions (CNN and Fox News). Many times even if the person reads the two sides of an argument/opinion, the confirmation bias will cause the person to only remember the information which confirms his thoughts about the topic. In investments as well, investors tend to stick to one view for a long time, validating it by reading only supporting research.\nSource: Billy Ireland\nIn their research, psychologists Amos Tversky and Daniel Kahneman found that people’s attitudes toward the probability of gains (“upside risk”) may be very different from their attitudes toward the probability of losses (“downside risk”). Most people feel the pain of loss much more than the joy of gains.\nOne example is the fear of flying. We all know that the chance of an airplane accident is significantly lower than the chance of an automobile accident, yet most people are still more afraid of flying than driving. Why is that? The answer is very simple, passengers give more weight to the magnitude of an accident than to the probability of an accident. In an airplane accident, the outcome will most likely be fatal.\nWe are all social animals. It is known that people do not like to be left out of a trend or a movement. The herd mentality can be seen in the financial markets. Just think about the recent “hot” investments in bitcoin and marijuana stocks, many investors joined the party late and suffered significant losses.\nHistorical examples of herd driven bubbles and busts:\nIs there a way to avoid falling into the biases trap?\nThe answer is no; We are all human and for that we cannot eliminate 100% of the influence of these emotional biases. We can, however, minimize their effect on our investment decisions by having a well thought out methodology that takes into account the objective facts as well as our subjective views.\nHere are some suggestions:\n- Having the appropriate tools to objectively determine potential risks and returns on an investment idea/asset are extremely important. CNBC and Bloomberg TV are not objective tools.\n- Define what risk is for you. Is it an asset price volatility, low return, or a full loss of principal? In the case of institutional investors, the risk may come from a decline in reputation or loss of job security.\n- Define in advance what risks you are willing to take and for what level of return. The best investment ideas are asymmetrical in terms of potential gain and loss, in your favor.\n- Remember you can always be, and many times will be, wrong. But be able to assess what the implications are of you being wrong and manage that risk accordingly.\n- Have a colleague with an opposing opinion “break” your investment thesis to test the strength of your idea.\n- When the fear and discomfort, or enthusiasm, you feel about an investment idea is shared by almost everyone else, go back to the drawing board and retest your theses.\nJust by following these simple guidelines for decision making, you’ll be doing better than the majority of investors in the market place. Underlying fear and greed are the propellers that drive the engine – so what investors have to remember is that going against the herd is often difficult, but the best decision. We all know to buy low, and sell high – but putting that into practice is more difficult than we think.","“The real voyage of discovery consists not in seeking new landscapes but in having new eyes.” – Marcel Proust\nWhat is Confirmation Bias?\nConfirmation bias is our unintentional tendency to seek or interpret information in ways that confirm what we already think or believe. Confirmation bias is sometimes called my-side bias or confirmatory bias.\nConfirmation biases are common. We all have them.\nIn some ways, confirmation biases are useful because they help us to quickly filter and categorize the information flood we face every day. But confirmation bias can also reinforce misperceptions that lead to poor decision making based on incomplete, misleading or wrong information.\nIn some cases, confirmation bias can be harmless, such as for the runner who believes that eating a bagel for breakfast will help her run better. She will remember speedy post-bagel workouts and forget those slower runs.\nBut confirmation bias can also be harmful or destructive. The man who considers his unemployed brother’s television viewing habits as evidence that all unemployed people are lazy is damaging to their relationships as well as unfair to the unemployed people he encounters.\nAnd confirmation bias can be devastating to consequential decisions. Considering information that supports the idea of a business merger and discounting information that suggests that the merger is a bad idea could lead to a ruinous decision.\nGive This a Try\nIn 1960, Peter Wason, a cognitive psychologist at University College, London, devised a series of experiments in which he asked participants to identify a rule about three numbers.\nGive it a try yourself. The numbers are 2, 4, 6. Now, what do you think the rule is?\nParticipants in the study were allowed to give examples of their idea of the rule, which the researchers would tell them was correct or not.\nWhat numbers did you use to test your theory about the number rule?\n8, 10, 12? Yes, those numbers conform to the rule.\n34, 36, 38? Yes, those numbers also conform to the rule.\nIn fact, most Wason’s experiment participants thought they had the problem solved after just a few examples that “proved” their idea that the rule was even numbers. Very few gave any examples of other numbers to disprove their hypothesis.\nIf your theory is that the rule is even numbers, you would be wrong. The rule is actually ascending numbers. The series 1, 2, 3 would also follow the rule, as would 723, 900, 4,000. If you had asked if 10, 8, 6 followed the rule, the researchers would have told you that no, it did not.\nThis experiment is a classic example that demonstrates how confirmation bias can lead to wrong conclusions.\nConfirmation Bias in Marketing Research\nConfirmation bias can be a problem in interpreting quantitative data but is a particular bias to watch out for in qualitative research. Focus groups, interviews and other forms of qualitative research is interpreted differently than the numbers from a survey. The researcher must filter unstructured data, sort and classify to identify trends and themes. The qualitative researcher must question and verify conclusions, ensuring that their own beliefs are not influencing the research outcome.\nThe fact that everyone is prone to confirmation bias is one reason that qualitative research, in particular, is best conducted by an outside researcher, rather than by an in-house researcher. Being a part of an association or company, familiar with the thinking, preferences and hopes of that organization’s leadership as well as part of the organization’s culture, creates a filter through which the researcher will search for information and interpret information once it is found.\nExamples of Confirmation Bias\nThe Wrong Pricing Conclusion\nI had a client who had conducted in-house research to determine the problem with slow sales of a bundle of digital product downloads. They believed that pricing for the product was too high and, indeed, their survey research using a standard price sensitivity meter confirmed that their target customers thought that the product was too pricey. They adjusted their offer, creating an even more generous bundle of downloads. But they were baffled when rather than solving their sluggish sales problem, sales decreased.\nI conducted interviews with target customers about the product and pricing. Indeed, the problem was not the price of the product, but the large number of downloads included in the price. The target customers said they would never use so many downloads before they expired and so didn’t want to pay for something they wouldn’t use. As I explained it to the client, it would be like offering a terrific sale on milk as long as you purchased 50 gallons. Who would buy 50 gallons of milk since it would go sour before you could drink it all?\nBarbara is obsessively punctual. In fact, it drives her crazy when people are late and this is putting a strain on her marriage because she believes that her husband Mark is always late. “He is always at least five minutes late. Always.”\nBarbara’s therapist asked Barbara to keep a written record for a week of when Mark was supposed to leave, arrive or do something he promised to do. Barbara thought it would be a fun game and provide written proof the next time she confronted Mark about his tardiness.\nShe was surprised when at the end of a week she had recorded 16 situations when Mark was, in fact, on time or even early leaving for work, picking up their daughter from daycare or meeting her at the car repair place to pick her up. He was only late once during the week. Barbara realized that she had been stewing on the few times that Mark was late and not even noticing when he was on time or early.\nGroupthink can contribute to confirmation bias. When everyone on a work team agrees with everyone else, playing follow-the-thought-leader or avoiding conflict by just keeping quiet when you have a conflicting opinion, you’re contributing to others’ confirmation bias.\nI often saw this at the advertising agencies I worked with, particularly when it came to creative campaigns. Account teams would pile on and agree with a direction\nHow to Avoid Confirmation Bias\nAlthough self-confidence is a valuable quality in the workplace, humility and a healthy dose of self-doubt could save you from the pitfalls that confirmation bias can cause. Don’t always be so sure that you are right and the other side is wrong. Think about Wason’s study and try to prove your ideas or your theory are wrong.\nWhen facing a decision, be honest with yourself about your motives to take a particular action. For example, if you’re anxious to jump on a recommended stock buy and see all the signs pointing to the idea that it’s a good investment, indulge in some self-reflecting before making the buy. Is your desire to get some quick returns clouding your ability to judge fairly the advisability of the purchase? This is the time to have a structured analysis plan in place to avoid making bad decisions.\nHere are some questions to ask when it’s time to make a decision on your beliefs.\n- How do I know I am correct?\n- Have I thoroughly looked at sources and information that are contrary to what I believe?\n- Have I evaluated this information with an open mind?\n- How do my personal views influence how I am viewing this information?\n- How would someone with a different view on this issue interpret this information?\n- How can I broaden my pool of knowledge to include diverse points of view?\nAnd finally, listen to the skeptics of your precious ideas. They provide a counterbalance to your ideas on the issue you are facing. Ask them their views and what shaped those views."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:6560f05a-d6fb-4585-b77f-5158f93815dd>","<urn:uuid:3183bbac-ef70-42e2-9e3d-dd11c6f3ec19>"],"error":null}
{"question":"What are the key differences between designing windows for a sustainable riverside home and a zero energy home in terms of their energy efficiency considerations?","answer":"In a sustainable riverside home, windows are designed primarily with deep verandahs on South and West sides to provide sufficient shade, focusing on air circulation and cross-ventilation to eliminate hot air build-up. In contrast, zero energy homes require more precise window planning, with a recommended window-to-floor ratio of about 14% and specific requirements for U-values (U 0.14 to U 0.23) to reduce heat loss. For zero energy homes in northern climates, south-facing windows should have a Solar Heat Gain Coefficient (SHGC) of 0.5 or greater, combined with climate-appropriate overhangs. Additionally, zero energy homes prefer fewer larger windows over more smaller ones due to better glass-to-frame ratios, and casement and fixed windows are favored over sliders or hung windows due to reduced leakage.","context":["Many people dream of leaving the city to lead a slower, more meaningful life outside it. Few, however, are able to live that dream. Just a stone’s throw from the [RaBV] bungalow here is a sustainable weekday home of a couple who come into the city on weekends.\nMuch of this one-acre site is between 1.5 and 3m above the average water level of River Pej, so during the monsoon, the lower section of the plot is often flooded once or twice for a few hours at a time. The initial plan was to build almost touching the river but that would mean building on stilts. Instead—taking into consideration the high water mark of 2005 which saw the worst flood in living memory—we decided to build on a small rise at the other end of the plot. Thanks to climate change, such freak events as the cloudburst of 26th July 2005 are likely to happen with increasing frequency and we must understand and prepare for them instead of brushing these facts under the carpet.\nThe little rise is next to the access road so the approach to the car parking area is a little steep but, other than that, there are no disadvantages. There used to be a shed on this mound so there were no trees that needed to be designed around.\nAs the building is on a slope, the extra height at the bottom has been used to create two basements. One stores gardening and filtration equipment along with the rainwater harvesting tanks while the other has batteries, inverters and other electrical equipment for the photovoltaic solar panels.\nDesign Considerations for Sustainability\nAs with the [RaBV] bungalow, the climatic conditions to be considered were hot days and pleasant nights with a strong monsoon. We needed sufficient shade on the South and West sides and this was taken care of with deep verandahs. Air circulation and cross-ventilation were important to eliminate the build-up of hot air. For the most part, roofs are sloping with only a small fraction of flat terrace where the solar hot-water systems are placed.\nInstead of a typical compact layout, this house was designed as a series of spaces with clear zoning of public and private. When seen from a distance–and a height–it looks like three houses in a cluster rather than just one. Central to all three spaces is the court and the open tank. This is not some amoeboid pool for people to float around with a cold beer but a straight 15m strip for exercise. Oh, and it’s a tank because, well, it’s a tank. Water from here goes to the vegetable garden and to many of the trees on this plot.\nThe clients, currently in their early 50s, want to spend the bulk of their time here exploring their creative side. Accordingly, one of the major spaces in the house is a workshop to be used for painting, stained-glass making and sliver-smiting. There is also a small study, two bedrooms, a utility room, a living/dining room and a very large kitchen.\nMaterials & Systems\nThe foundations were constructed from local basalt and the superstructure from local bricks.We discussed the possibility of using fly-ash bricks but the clients had reservations because of the debate over fly-ash being carcinogenic.\nMany internal walls were left un-plastered and the roofs had a steel structure with Mangalore tiles on battens without any under-layer.\nDoors and windows were either beautiful old ones that were salvaged from demolished homes or were made anew from reclaimed old Burma teak. The credit for sourcing them all goes completely to the clients. They also purchased five lovely old wooden pillars during their travels, which were incorporated into the design. Since these were only 2.5m tall, we made a tapered concrete base which was then clad with the same grey granite as was used for the adjoining parapet walls.\nAll the lights are low-energy, mostly LEDs, while the fans and refrigerator are inverter-type so their energy consumption is also lower than average. As these fans are a relatively new product, it remains to be seen if they stand the test of time.\nBath and kitchen water is heated using one solar panel on each of the terraces. There are also twelve photovoltaic solar panels on the roof of the workshop which provide enough electricity to run all the lights and fans as well as some of the appliances.\nA good amount of the rainwater is harvested. Some of it is collected in tanks for drinking water throughout the year. This is necessary as the river, though perennial, sometimes contains urea washed in from fields upstream; even though the water is clean enough for bathing and washing, it is not advisable to use it for drinking or cooking. The remaining harvested rainwater is used for recharging a bore-well that is an emergency backup water-source. Whatever rainwater is not harvested either seeps into the ground or flows directly into the river.\nEmbedded dual cisterns flush the low-flow WCs and kitchen waste water is sent directly into a soak-pit from where it percolates into the ground.\n|Structural & Waterproofing||Mr. Ratnakar Chaudhari|\n|Overall | Civil, Plumbing, Roofing, Painting||Mr. Rajesh Phatak|\n|Electrical||Mr. Rafeek Shaikh|\n|Carpentry & Joinery||Mr. Ramashankar Mistri|\n|Solar Hot Water||Solar World|\n|Solar Photovoltaic | Panels, Batteries, Inverters||Sunlit Future|\n|Swimming Tank Filtration||Oceanic Enviro Pvt. Ltd.|","Zero Energy Home Design\nBy Joe Emerson and Jason Offut\n“The most powerful instrument of change on the planet is the stroke of a designer’s pen.” Edward Mazria, Architecture 2030\nIt Begins with the Design\nAn affordable Zero Energy Home begins with an integrated Zero Energy Home (ZEH) design that is grounded in a detailed understanding of affordable Zero Energy Home Construction techniques. It is best to develop a ZEH design in conjunction with a project team – owner, builder, energy consultant, the landscaper, and designer. Getting to the level of insulation, air tightness, energy efficiency, and solar exposure needed to create a cost-effective Zero Energy Home requires that a wide variety of small issues be effectively addressed in the design phase, including exploring the most cost effective options for reaching Net Zero and verifying them through Energy Modeling.\nFor the most cost-effective ZEH design, the selection of the site and the siting of the home must consider climate, weather patterns, wind, sun exposure, shade, temperature, heating/cooling degree-days and topography. It is important to choose a site with a sufficiently wide east-west lot line to allow for the placement of the home on the site so that there is adequate south facing roof for solar collectors and south facing windows and doors for passive solar gain. The site should be free of immoveable obstructions, such as trees, neighboring homes, and land-forms that could interfere with adequate solar access. The site should not be excessively exposed to prevailing winds or have other topography features that might unnecessarily increase the costs of a Zero Energy Home.\nA thorough solar evaluation of the site, using a handheld shade analysis tool, such as a Solmetric SunEye is recommended. The SunEye measures shade and solar access for the home and offers digital and graphical displays of the amount of solar energy available at the site. Many solar companies can visit the site and offer this solar analysis at no cost to the homeowner or builder. The solar analysis is important for designing passive solar, solar photovoltaic (PV) and solar thermal (hot water) systems. It impacts the site selection, location and orientation of the home and roof, roof design, and roof and window area needed for solar PV and passive solar.. This data may also indicate which trees need to be cleared or trimmed to increase solar exposure. Taking full advantage of the solar potential for each site is one of the lowest cost strategies for achieving a successful Zero Net Energy Home Design.\nThe Basis of Design\nAfter the site analysis has been concluded, the next step is for the project team to assist with identifying and defining the project parameters and specifications and for the designer or architect to complete the Basis of Design (B.O.D.). This document acts as the “road map” for the project. The B.O.D. identifies key project elements such as homeowners’ requirements and preferences; building type, scope, and key design details; specific goals, strategies, and specifications for reaching Zero Net Energy; and the sustainable and renewable resources to be included.\nDesign elements specified in a Zero Energy Home B.O.D. may include: raised heel trusses; double walls with offset studs or other energy conserving wall assembly; simple, appealing lines without sprawl or unnecessary corners; placement and sizing of windows; placing and sizing of south window overhangs, strategies for reducing heat loss through conduction; plans for ducts and wiring to be inside the conditioned space; and other features that will make the home easy to insulate and easy to make air tight. At different stages during the design process, the energy consultant should conduct energy modeling using the energy parameters being discussed to contribute approximate energy usage data to the B.O.D. The project team contributes to the B.O.D.throughout the design process..\nSize and Shape Matter\nWhen contemplating the design and the construction of an affordable Zero Energy Home, size and shape do matter. Smaller homes use less energy for space heating and cooling. Limiting the size of the home will have a direct impact on overall energy required on site, helping to reduce costs. The savings from building a 10% smaller home can help pay for the additional cost of a Zero Energy Home without sacrificing quality or livability. Smaller homes can be designed to look, feel, and live larger while also having ample storage space, and can be more convenient and livable than larger homes.\nShape is also important. The shape of the home should be kept simple and in scale to the user and the site. Rather than a sprawling design, a building with a low exterior surface to volume ratio, with simple clean lines and minimal “corners” will save energy and construction costs. It is useful to think of the home as an insulated six-sided box with the insulated floor, four walls, and ceiling making up the “box”. Each additional corner or cantilever increases the amount of framing required and makes air sealing and insulating more challenging.\nThe excellent book, The Not So Big House, describes how a smaller home can be designed to look large, spacious and comfortable. Features, such as extra storage spaces within otherwise poorly utilized spaces in a home, or the addition of extra storage space or a flex room in the garage area outside of the more costly conditioned space, can contribute to making a smaller home function as well as a larger home.\nDesign to Use the Sun\nZero Energy Homes should be designed to use the sun’s energy as much as possible, for: generating electricity, heating hot water, and utilizing passive solar space heating. After a site analysis and an understanding of what solar resources are available on site, passive solar design concepts should be incorporated. If the home can be located and appropriately exposed to the sun for the site and local climate, energy savings are possible from the start. These savings can be achieved by enhancing solar gain in the winter and by reducing heat loads in the summer. In climates where heating is more important than cooling, the building should have its long side facing the sun, providing more opportunity for the sun’s warmth to heat the home. In climates where cooling is more important, the long axis of the home could be perpendicular to the sun.\nDepending on the specific climate and location, the total area of southern facing windows in the home should be determined to optimize passive heating of the home. The home’s overhangs, window header heights, window sill heights, and door heights should be coordinated and designed to optimize the use of the solar energy available for the specific site and climate. The width and height of roof overhangs will depend on site-specific solar exposure and climate and are important factors in determining the amount of passive solar heating possible. They should be designed to avoid excessive heat gain in the summer and optimize solar heat gain in the winter.\nOnce passive solar design considerations have been taken into account, the home design should optimize the active solar components, such as the solar hot water and solar photovoltaic systems. Adequate roof area should be planned for the solar panels, solar installation codes, and the associated fire department access codes. Since roof pitch affects the efficiency of the solar PV and hot water panels, it should be optimized within the design constraints of the home. The more effectively passive solar features, air sealing, insulation, and other energy efficiency measures can be designed into the home, the less energy the home will need to receive from the more costly active solar systems, such as large PV systems.\nDesign for Added Insulation\nThink of the home as a six-sided box in which all six sides need to have the most cost-effective insulation possible. The R-values on each side of the box, as determined by energy modeling, must be sufficient to reach the zero net energy goals. Thicker walls, deeper floor assembly and raised heel trusses for the roof may need to be included in the design to accommodate sufficient insulation depth depending on the insulation materials and methods selected. Depending on the local climate, the use of dense-pack fiberglass or cellulose insulation may require, 8″ to 12″ thick, double walls with off-set studs to provide adequate space in the walls for the insulation. The R-value for each side of the six sides should be detailed on the plans. Moisture related issues should be considered in the design of highly insulated and airtight building assemblies such as these. Designing assemblies that are both breathable and airtight prevents moisture related challenges.\nDesign with a Continuous Air Barrier\nThe house should be designed with a continuous air barrier. All the cracks, holes, and exterior envelope penetrations of the home’s six-sided box must be systematically sealed. All penetrations, for electrical, ERV/HRV venting, gas, and water should be minimized and included in the design. For example, recessed can lighting can be eliminated or the cans can be recessed in soffits. Similarly ERV/HRV vents can be used in place of less efficient, leaky bathroom and kitchen venting systems, further reducing penetrations. The air barrier should be drawn on the building plans and carefully detailed, indicating which contractor is responsible for each air sealing detail and what material they should use. This information could later be included in the scope of work for each subcontractor.\nOne option to consider is to move the thermal boundary from the ceiling to the roof by insulating the rafters (a.k.a. a “hot roof”) by applying spray foam to the underside of the roof sheathing, creating an exterior air barrier. Drywall on the interior face of the roof rafters would serve as the interior air barrier. This alignment of the thermal boundary with the roof can simplify air-sealing details and create interior spaces for ventilation ducts and other mechanical equipment. The cost effectiveness of this approach and the environmental issues of spray foam should be carefully evaluated\nUsing a Thermal ByPass checklist during the design phase will help ensure that all aspects of designing a continuous airtight thermal barrier are considered. An in-depth Thermal ByPass Checklist is available from Energy Star or from Green Building Advisor.. The thermal barrier should be explicitly detailed on the home design.\nKeep Ducts Inside\nAll duct-work could be designed to be within the conditioned space to optimize the integrity of the air barrier and insulation. According to Green Energy Advisor, ducts can be kept inside by locating them in conditioned areas such as a basement or crawl space, unvented conditioned attic, open-web floor trusses (especially in a two story home), soffits, dropped ceilings, or a chase designed into special roof trusses.\nMinimizing Thermal Bridging\nDuring the design phase, thermal bridging should be eliminated as much as possible at foundations, edges, corners, soffits, eaves, connections, decks and penetrations. Door and window installation details and second story floor interfaces are other areas of concern. Double walls with offset studs and with separate top and bottom plates will greatly reduce thermal bridging in the walls, as will a thick layer of exterior foam on the studs. Fewer exterior wall offsets will also reduce the potential for thermal bridging. Decks and porches can be designed so that they are actually separate from the house, completely eliminating thermal bridging. The design should specify for the builder and subcontractor, how thermal bridging can be avoided.\nWindows and Doors\nThe orientation of doors and windows must take climate, wind, sun and shade into account. Since the home is a well-insulated, highly airtight, “six-sided box,” windows and doors are relatively poorly insulated “holes” in that box, and are more expensive than the wall assembly that make up the “box”. Therefore minimizing window area is a very important design strategy for achieving an affordable Zero Net Energy Home. Minimizing window area, while providing plenty of light, optimal passive solar heating to living areas, and adequate ventilation in the summer presents a unique design challenge.\nAn overall window to floor ratio of about 14% and, in northern climates, a south facing window to floor ratio of 6%, are often recommended for a Zero Energy Home, but that ratio will depend on the climate, as well as site and design considerations. Low U-value windows are important (from U 0.14 to U 0.23 as needed to reduce heat loss and maintain optimum comfort), but are less effective as the window area increases. South-facing windows should have a Solar Heat Gain Coefficient (SHGC) of o.5 or greater, in conjunction with climate appropriate overhangs, to allow for passive solar heat gain in the winter months, but not in the summer months. Fewer larger windows are more energy efficient than more, smaller windows, because there is a higher glass-to-frame ratio with larger windows, and the frames are where the leakage occurs because the frames have a lower u-value than the glass area of the window. Casement and fixed windows are less leaky than sliders or hung windows. Windows and doorways should be located to reduce exposure to prevailing winds, reducing heating load requirements and weather infiltration. If doors are exposed to prevailing winds, they should have three point door latches or be part of an air-lock entryway with double entry doors.\nAdvanced Framing Techniques\nAdvanced framing techniques should be included in the design as much as possible, because they use less lumber, allow for more insulation, and reduce opportunities for conduction. Members are sized based on calculated loads and continuous load paths are used from the roof to the foundation with studs 24 inches on center. Headers are sized for actual loads so that unnecessary framing members are eliminated, saving cost and resources. Using engineered lumber and recycled lumber production bi-products and allowing for longer spans compared to standard lumber products is recommended.\nHeating, Cooling, Ventilation and Hot Water\nIf heat-pump ductless mini-splits are used for heating and cooling, airflow to all the rooms needs to be considered during the design phase. Utilizing door cuts under the bedroom doors, transoms over some of the doors, or sound insulated air vents in the walls could be part of the design. Preferably, energy efficient ventilation systems (ERVs and HRVs) may be used to optimize airflow in the home. Specifying models of ERV/HRVs that have a circulation mode that actively circulates air from the heat source in the living area to the bedrooms can bring both heat and cool air to all rooms without needing any additional venting, and ensures that the heating requirements of the building code are met. In cold climates, heat-pump mini-splits that provide heat down to minus 18 degrees below zero should be specified, such as the Mitsubishi “Hyper Heat” model. Care should be taken that the heating system is sized properly for the local climate and home dimensions and characteristics. For greater efficiency the HRV/ERV needs to be centrally located so that it has short runs for supplying air to the living area and bedrooms and for extracting air from the kitchen and bathrooms. The location of the HRV/ERV should be part of the design and should be designed to accommodate the specific model chosen, as size varies considerably.\nThe hot water tank needs to be centrally placed to bathrooms and kitchen to minimize hot water runs, or consider a half-loop hot water recirculation system. Energy modeling should be conducted to determine which hot water system is most cost effective: Solar Thermal, Heat Pump Hot Water Tanks, or a well-insulated standard electric water heater with electricity supplied with additional solar panels. Once the most cost effective hot water system has been determined, the design needs to include adequate space for the system selected.\nDesigning For Builders\nWhile affordable Zero Energy Homes should be designed using as many standard building techniques as possible, some aspects of building Zero Energy Homes, such as double offset walls, may be new to builders and subcontractors. All such building strategies for a Zero Energy Home should be clearly outlined and specified on the design and in accompanying notes. When an architect or designer clearly outlines all the details required to make a Zero Energy Home, he or she makes it possible for any experienced builder and subcontractor to successfully build an affordable Zero Net Energy home. To ensure the efficacy of a Zero Net Energy Home design, the designer should be familiar with all 12 steps of the Zero Energy Home building process.\n|Sign Up for Our Newsletter Photo Gallery Find a Builder Find a Designer Find Zero Homes for Sale Find a Green Realtor@ Design Guidelines Building Guidelines How To Videos|\n- Design Build\n- Buy Sell\n- About Us"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:e2ae0fbb-f657-467f-8dde-b9f3644bbf7b>","<urn:uuid:6d632c66-5ccb-4415-9c9d-a3960ff71f09>"],"error":null}
{"question":"How does weaving serve as both an artistic expression and a therapeutic practice in different cultural contexts?","answer":"Weaving serves as artistic expression through elaborate works like Sonya Clark's Hair Craft Project, where hairstylists create complex sculptural forms and wall weavings that explore cultural identity and community. As a therapeutic practice, weavers like Della Cheney describe it as 'a healing therapy' and a 'powerful way of life,' while others like Shgen George find it soothing, noting 'it soothes me to just go to my loom and weave.' The practice connects generations and communities, with artists passing techniques down to their children and creating ceremonial items that move forward through time.","context":["We sometimes visit the hair salon for change, almost as if we are asking for a new identity. Hair not only comprises our physical appearance but it marks out our lives with various styles and length. Molecularly, hair follicles contain our actual DNA, but also, somehow, our spiritual selves. Using hair as subject matter, contemporary fiber artist Sonya Clark weaves a cultural tapestry of the very fabric of our community. Physically, she allows for a mindful build-up of elaborate textures that, in many ways, represent a landscape of places and people. Inherently, a history is formed—a tactile topography where a wealth of cultural wisdom emerges from Clark’s mind and through the collaborative hands of other artisans.\nTrained as a fiber artist, Sonya Clark draws from her heritage. Specifically—materially, through her interest in African American hair. Her series “The Hair Craft Project” encompasses photography and fiber arts where Clark introduces something very beautiful, selfless, and unexpected. In the spirit of collaboration and with craftswomen in mind, Clark digs deep into her personal communities to invite hairstylists to use her head…as a canvas.\n“Hairdressers are my heroes. The poetry and politics of Black hair care specialists are central to my work as an artist and educator. Rooted in a rich legacy, their hands embody an ability to map a head with a comb and manipulate the fiber we grow into complex form. These artists have mastered a craft impossible for me to take for granted.”\nHairstylists/barbers are important, prominent occupations in our local communities. Like any serious discipline, styling and grooming hair is a complicated and time-enveloping craft. Close relationships inevitably form through ritual and vanity. These bonding friendships come across beautifully in this exhibit. The works comprising “The Hair Craft Project” manifest in an intimate narrow gallery space at the heart of The Fed Galleries, Kendall College of Art and Design. One side of the long gallery space is lined with ten large-scale photographs, the opposite side contains the paired wall weavings on canvas of similar size. The photographs themselves, with their saturated backgrounds, are forces. In each image, Clark stands with her back to the viewer, prominently modeling every unique hairstyle design as if she were a walking sculpture. The design is foremost, close, tight-weaved and revealing the sculptural contours of Clark’s head. To the left in the the photographs, the individual hairdresser proudly addresses the viewer. Featured in this series are Kamala Bhagat, Dionne James Eggleston, Marsha Johnson, Chaunda King, Anita Hill Moses, Nasirah Muhammad, Jameika and Jasmine Pollard, Ingrid Riley, Ife Robinson, Natasha Superville, and Jamilah Williams.\nOn the opposite side of the narrow gallery, so that they are paired with each photograph, the wall weavings stand as interpretive works executed by each of the hairdressers represented in the photograph. These works are slightly less traditional and yet magnificent personal abstract art. Each wall weaving consists of a stretched canvas substrate, square and minimal, clearing the stage for each complex sculptural weaving. Clark’s materials of threads or cords have a linear, pliable element and the softness of the crocheted, knotted, sculptural forms are textures that invite the hand. Because their scale is human, if we imagine we are able to act upon our impulses, it would be an intimate stroke. The crowns of our heads have spiritual connotations. A life force flows from the top of our heads. This is a place of thought, conjuring love, where a halo might be symbolized, and brings up vivid storytelling, such as the tale of Samson and Delilah, and Rapunzel.\nThe dark silk thread representing hair, particularly, African American hair, is inventive, playful, and very powerful. Close up, their varied patterns are keenly manipulated into buttresses; coils of twisting braids curve like fingers that seek to identify and enhance points of connection and tension. It’s as if, through Clark’s methodologies, she expresses that even as we try to impose our will on nature, nature imposes its anarchy back on us. Here Clark engages us with these works as abstract art but also these are contemporary cultural artifacts that reference history in multi-facets of intersecting threads and braids.\nEvery skillful hair tapestry adeptly explores certain personal symbolic interpretations of each craftswomen. And yet, we are all part of a vast tapestry. Through materials and processes Clark’s art contains wonderful accounts of the ways in which the artists and craftspeople of our immediate community come together to form our cultural landscape. And because the thread takes us adeptly to the larger metaphors: warp and weft becomes identity, and family, community, the workplace, and the world. Here is where life inevitably tangles. And “The Hair Craft Project” does it so elegantly. Exploring the importance of urban place for identity and individuals, Clark touches human lives, our lives. On the tangle-braiding of community, cultural value is expressed appropriately as “The Hair Craft Project” is centered in the context of the larger exhibition series, “I AM: Money Matters” focusing on currency, consumption, and value.*\nViewing photographs of actual sculpted hair and then physically exploring the wall weavings in “The Hair Craft Project” transforms the familiar into a metaphorical construct. Crossing through the images and weavings I imagine a space that follows a meticulous and yet open conceptual map. Here I envision a community that contains all human cultures, addressing class, race, and identity. It is through this acknowledgement that Clark highlights the talents embedded within the landscape of her community and culture. Or rather, she surrenders to the beautiful hand of community. “The Hair Craft Project” exemplifies a dedication to craft, especially in the fiber arts, where weaving, stitching, and adorning with feminine sensibility shows us that an arduous craft is imbued with a sense of time and stirred with storytelling. In doing so, these embroidered threads contain the pattern of time.\nThere are many threads that run throughout our lives. The very fabric of our culture is a complex tapestry of these threads crossing and weaving. Everything around us is in constant flux. In life, nothing is ever really fixed in place but rather everything interacts in dynamic relationships, especially as we settle for a while in the communities in which we reside. We all touch, all walks of life; these threads snag and tangle—ignorance, self interest, cultural indifference, dogmatic and outdated belief systems that are not our own to begin with. But as we take hold of the thread of life, we should be careful because we touch others. By holding onto these threads of life, we leave ourselves the chance to find our own way, though all the while we should caution ourselves that we touch others. When this happens, we weave tapestries.\n* Clark was co-winner of the 2014 ArtPrize Grand Juried Prize. This was the first year the Juried Grand Prize panel actually decided on a split of the $200,000 prize. (Her co-winner was Anila Quayyum Agha’s excellent meditative installation, “Intersections”).","Whenever Juneau weaver Shgen George travels, her loom goes with her. She transports the large portable loom in a ski bag, advice she got from Clarissa Rizal.\n“This loom and this ski bag have literally traveled around the world with me and I weave every day, pretty much no matter what, even if it’s two minutes,” George said.\nShe spoke during the Chilkat and Ravenstail Weaving Symposium to a packed clan house audience inside the Walter Soboleff Building Wednesday. It was one of the first official events of Sealaska Heritage Institute’s Celebration 2016.\nChilkat and Ravenstail are traditional weaving practices of the Tlingit, Haida, Tsimshian and other Northwest Coast people of Southeast Alaska, the Yukon and British Columbia. It’s traditionally used to make ceremonial regalia, like robes, dance tunics, aprons, leggings, vests, bags and hats. Artists spend years weaving a robe or ceremonial blanket.\nDuring the symposium, George dragged the ski bag in front of the audience and assembled the wood pieces with nuts and bolts in front of everyone.\n“I pulled this bag through Narita [International] Airport in Japan,” said George, who is a teacher for the Juneau School District and did a teaching exchange in Japan.\nThere she weaved a headband, apron and robe in various shades of pink for her daughter, Gabby Kay. When they returned to Juneau, Sealaska Heritage Institute purchased the whole ensemble, and now George and her daughter are working on another robe, a blue one, to replace the sold one.\nAs George finished putting the loom together at the clan house, the symposium audience let out “oohs” and “aahs” and clapped when her work-in-progress was revealed.\n“It soothes me to just go to my loom and weave,” George said. “My daughter and I stand at my loom and weave together.”\nShe said it’s her hope that her children will continue to pass on weaving to future generations.\nWeaver Marsha Hotch described a practice that’s been taking place for centuries — how she processes mountain goat hair for Chilkat weaving. She learned from Agnes Bellinger.\nDuring the symposium, Hotch shared slideshow photos of bloody mountain goats in the back of a pickup truck or slung around the shoulders of a hunter.\n“I’m sorry if this offends you, but this is how mountain goat is processed,” Hotch said. “It takes collaboration with the community to be able to weave. It’s wonderful to weave, but the preparation and how a weaver gets to weave, there’s a process there and that process wasn’t always mentioned.”\nOne of her photos showed her son, Stanley Hotch, packing a goat off a mountain behind Klukwan.\n“These can weigh anywhere from 180 to 200 pounds and some billy goats over 300 pounds,” Hotch said.\nOnce the goat is skinned, she said, “the cleaning part is a chore.” The hide has to be rolled up and left for some time in a cool place.\nShe described the first she time she unrolled a hide.\n“After leaving it, I started unrolling my hide to slip the wool off and when that smell started getting up to me, I started having memories tumbling out and I just started crying. I actually had my hands in mountain goat wool as a little girl,” Hotch said.\n“I had forgotten that,” she said, but was reminded by “just smelling that mountain goat and having my hands in it and pushing it again.”\n“Spinning was another chore. The earlier you get your mountain, the shorter the fibers,” Hotch said. “I had to be very patient.”\nWhen it comes to dying the wool, Hotch showed pictures of her “accidents” when a mistake changed a teal color to a gray.\nShe recalled once bringing her own wool to the state museum where she viewed archived weaving items.\n“I just wanted to old to meet the new and let them know we’re still trying to do the best we can,” Hotch said.\nHaida and Tlingit weaver Della Cheney said every piece of art and regalia that’s displayed in museums around the world are “part of us and our history.”\n“When I look into the room and see the weavers, I see how we’re connected. Each of us is a warp and the things we create are the weft,” Cheney said, using weaving terminology.\nShe said weavers give woven items to family members and children.\n“Each robe, each hat, each apron will move forward with our children and grandchildren through time. Today as I weave and create new regalia, the techniques I practice are from the time before us,” she said.\n“This process of weaving is a healing therapy for all of us, and it’s just a very powerful way of life.”\n• Contact reporter Lisa Phu at 523-2246 or email@example.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:89bb19a9-97cc-4cfb-af80-9e5b5e35109c>","<urn:uuid:487b3957-ec82-49d1-a744-05ace9ce6ab0>"],"error":null}
{"question":"I'm trying to create an allergy-free bedroom environment. What measures should I take regarding bedding and furniture choices, and how should I maintain these items to minimize allergens?","answer":"For an allergy-free bedroom, cover mattresses, pillows, and quilts with dust mite resistant covers and wash them every two months. Use synthetic rather than feather pillows and doonas, as they tolerate regular washing. For furniture, avoid upholstered pieces and opt for leather, vinyl, plastic, or wood. Wash sheets and pillowcases weekly in hot water (above 55°C), or if using cold water, add essential oils like eucalyptus or tea tree oil. For additional allergen control, maintain humidity levels below 50% and consider removing carpets in favor of bare boards or tiled floors that can be damp mopped.","context":["The house dust mite gets its name from its habitat – household dust. The main component of dust is shed skin flakes, which is the mite’s preferred food source. Areas around the home that are heavily used, such as beds and upholstered furniture, will have much higher mite populations than the rest of the house.\nThe most common type of dust mite found in Australian homes is Dermatophagoides pteronyssinus, which tends to prefer coastal rather than inland areas. This mite has been associated with dermatological and respiratory allergies in humans, such as eczema and asthma. However, there is no single, definitive sign that house dust mites trigger a person’s allergy symptoms. Asthma, for example, can be triggered by a range of other indoor allergens such as fungi (moulds) or animal dander (fluff from hair, fur or feathers).\nSymptoms of allergic reaction to dust mites\nHouse dust mites can trigger respiratory or dermatological conditions including asthma and eczema. Symptoms can include:\n- a tight feeling in the chest\n- runny nose\n- itchy nose\n- itchy eyes\n- itchy skin\n- skin rashes.\nPhysical characteristics of the house dust mite\nThe characteristics of a house dust mite include:\n- less than half a millimetre in length, which makes it hard to see with the naked eye\n- oval-shaped body\n- light coloured body with fine stripes\n- life span of around two months or so, depending on the conditions.\nAllergic reaction to dust mites\nUnlike other common household bugs (fleas, for example), dust mites don’t bite. Their bodies, secretions and faeces contain particular proteins that can trigger allergic symptoms in susceptible people.\nCommon hiding spots for dust mites\nThe diet of the house dust mite includes shed skin flakes, pollen and fungal spores. It prefers warm, humid and dark environments. Common hiding spots around the home include:\n- mattresses and bed linen\n- upholstered furniture\n- shag-pile or long-fibred carpets\n- soft toys.\nDiagnosis and treatment for dust mite allergies\nAllergy testing can determine whether house dust mites trigger your respiratory or dermatological symptoms. See your doctor for further information and advice.\nIf tests show that you are allergic to house dust mites, there are ways to reduce your immune system response. For example, you could undergo allergen immunotherapy, which involves deliberately exposing you to dust mite extracts to ‘train’ your immune system not to overreact.\nMeasures designed to reduce your household’s dust mite population may also be helpful.\nHow to reduce the dust mites in your home\nIt is impossible to destroy your entire dust mite population, but you can reduce their numbers. Allergic reactions are dose-related, so the fewer dust mites you have in your home, the less you may be troubled by respiratory or dermatological symptoms.\nIt is important to remember that the droppings of dead dust mites continue to provoke allergic reactions. You must not only reduce your dust mite population, but also take steps to remove their dead bodies and faeces from your home.\nTips to reduce dust mites in your home\n- Cover mattresses, pillows and quilts with dust mite resistant covers. The covers must be washed every two months. Some health funds may provide a rebate for these items.\n- Wash sheets and pillowcases weekly in water hotter than 55°C. Alternatively, if washing in cold water, use a commercial product containing essential oils, such as eucalyptus or tea tree oil.\n- Hot tumble dry (for half an hour after dry) or dry clean household items – this will kill house dust mites, but not the allergen they produce.\n- Wash blankets and non-encased doonas every two months.\n- Use synthetic rather than feather pillows and doonas, as these tolerate regular washing.\n- Remove sheepskin or woollen underlays and any other sheepskin products.\n- Remove all soft toys from the bedroom and replace with wooden or plastic toys, which can be washed. Or, if a soft toy is allowed, it should be washed weekly using the same method used for sheets. (Freezing soft toys overnight doesn’t work, because it doesn’t remove the allergen.)\n- Damp dust or use electrostatic cloths to clean hard surfaces weekly, rather than a feather duster.\n- Reduce humidity – have a dry and well-ventilated house. Have adequate floor and wall insulation and avoid evaporative coolers.\n- Avoid upholstered furniture – leather, vinyl, plastic and wood are best.\n- Avoid heavy curtains – Venetian blinds or flat blinds are better. Washable curtains or external shutters are other options.\n- Wash clothing before use if it has been stored for a long time.\n- Remove carpets, rugs and mats (where practical and affordable) – bare boards and tiled floors are preferable as they can be damp mopped or cleaned with electrostatic cloths.\n- Wash rugs and mats regularly and dry them outside in full sunshine (if possible).\n- Vacuum weekly, including the seams of mattresses and upholstered furniture. Vacuuming causes house dust mite allergens to become airborne for up to 20 minutes, so if you are allergic to dust mites, you should wear a mask or ask someone else to vacuum. You may air the house for an hour or so after vacuum cleaning to help clear the air.\nDubious dust mite treatments\nSome treatments that claim to reduce dust mite populations have little or no benefit, including:\n- chemical sprays\n- air filters\n- electric blankets\n- negative ion generators\n- allergen-free products.\nWhere to get help\nThis page has been produced in consultation with and approved by:\nAustralasian Society of Clinical Immunology and Allergy (ASCIA)\nContent on this website is provided for information purposes only. Information about a therapy, service, product or treatment does not in any way endorse or support such therapy, service, product or treatment and is not intended to replace advice from your doctor or other registered health professional. The information and materials contained on this website are not intended to constitute a comprehensive guide concerning all aspects of the therapy, product or treatment described on the website. All users are urged to always seek advice from a registered health care professional for diagnosis and answers to their medical questions and to ascertain whether the particular therapy, service, product or treatment described on the website is suitable in their circumstances. The State of Victoria and the Department of Health & Human Services shall not bear any liability for reliance by any user on the materials contained on this website.","Tips For Proactive Allergy Relief\noth over-the-counter and prescription medications can alleviate allergy symptoms, but if you aren’t getting the relief you need, think prevention. By taking a pro-active approach to allergens in your home, you can often stop symptoms before they start. Some solutions are easy and offer instant relief, some will require a change in habits or take a bit more time. All are worth the misery they save by keeping allergies under control.\nA great deal of pollen and outdoor contaminants travel into your home on people’s shoes. Get family members into the practice of removing their shoes at the door, and store everyday shoes in a mudroom or breezeway if you have one. Bonus: You’ll save cleaning time and wear and tear on carpets and floors.\nYour home should be smoke-free. The dangers of second-hand smoke are well established, and allergy sufferers are particularly sensitive to smoke. If someone in your household still hasn’t kicked the habit, insist they smoke outside, or at the very least, confine smoking to a single room of the home with an air purifier, with doors and registers of that room closed off from the rest of the home.\nDust with a damp cloth or special dusting cloth that magnetizes and traps dust, dander and pollen that have settled on household surfaces. Dry dusting simply blows pollen and dust around in the air. Avoid feather dusters.\nKeep pets restricted to certain areas of the house, preferably in rooms without carpeting. Make bedrooms off-limits for pets. Wash and brush pets often, especially if they spend much time outdoors. If you are getting a new pet, research low-allergen breeds, as well as those that shed minimally.\nChoose your vacuum system wisely. Vacuum often, and consider a system that transports dust, dirt and allergens to a central source. Central vacuum systems like Vacuflo and Dirt Devil don’t recirculate particles back into the air.\nKeep mold in check by maintaining the relative humidity of your home below 50%. Use a dehumidifier in the basement and don’t forget to empty and clean the units often. Bathrooms should be equipped with fans that vent to the outside; always run fans while showering. Periodically check pipes, faucets, appliances, ductwork, ceilings and roofs for leaks that can cause moisture buildup in hidden places and create a breeding ground for mold.\nChoose cleaning products carefully to avoid contact with Volatile Organic Compounds (VOC) such as formaldehyde and ammonia. While a good air purifier will offset some of the harmful fumes of cleaning products, choose natural cleaners when possible, as well as products that do not contain artificial fragrances that can irritate allergies. Use proper ventilation and wear a mask when using paint, floor polishes or other substances containing chemicals.\nDust mites can be your downfall, though it’s not dust mites themselves that trigger allergies, but their waste products. Starve dust mites by reducing their food sources: dead skin cells and perspiration. Your prime battleground in the war against dust mites? The bedroom, though dust mites live everywhere in the home, especially in upholstered or carpeted areas. Learn winning strategies in the war against dust mites here.\nA few simple changes in your everyday way of life can help relieve common allergy symptoms for you and your family.https://www.clevescene.com/cleveland/skincell-pro-reviews-2021-scam-complaints-and-side-effects-list/Content?oid=36157922"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:e9b9ba5a-9be2-4d83-bf9b-28af62310b5b>","<urn:uuid:168b118c-38df-4829-9aa7-b14979ef9197>"],"error":null}
{"question":"I'm a physical therapist working with stroke patients. How effective is NDT compared to other treatments, and what are the common depression symptoms we should monitor?","answer":"Studies show NDT (Neuro-Developmental Treatment) has no clear superiority over other treatments for stroke rehabilitation. While NDT can improve patient outcomes, research indicates it is generally as effective as conventional therapy, with no significant advantages for sensorimotor control, dexterity, mobility, or daily activities. Only limited evidence supports its superiority for balance. Regarding depression symptoms, clinicians should monitor: lack of attention, memory problems, difficulty planning and organizing, abstract thinking challenges, lack of self-awareness, emotional changes, and decreased processing speed. These symptoms warrant attention especially if they persist beyond six weeks post-stroke, as post-stroke depression affects nearly one-third of survivors.","context":["Dealing with post-stroke depression\nTreatment starts with recognising the signs of depression. This is what you need to keep in mind if you think you or someone you know may be suffering from post-stroke depression.\nPost-stroke depression (PSD) is very common and affects nearly one-third of stroke survivors at any one time after stroke. According to an American Heart Association journal article, individuals with depression after stroke are at a higher risk of delayed recovery, poor quality of life and mortality.\nKnowing when it’s not normal\n‘It is completely normal to experience sadness, a low mood, a sense of helplessness, anxiety, uncertainty, excessive worry, irritability or anger after a stroke. These symptoms would be normal occurring within a period of six weeks after suffering a stroke, but if they persist beyond this period, it’s possible the symptoms have escalated to depression,’ says Mfimfi Mngomezulu, a clinical psychologist at the Life Mental Health Unit at Life Glynnview.\n‘The patient might find it difficult to enjoy the things they did previously, have trouble sleeping and lack energy and appetite. There might also be a sense of hopelessness and frustration, and a general feeling of being undeservingly punished.’ Anxiety may or may not be linked to this kind of depression, he explains, but its existence may be incapacitating, rendering a stroke survivor less able to cope with everyday life.\nA life-changing event\nAccording to Sandra Laten, a medical social worker at the Life Rehabilitation Unit at Life Pasteur Hospital, a stroke can cause enormous changes in personality, mood and emotions, which in turn impacts the patient’s ability to reintegrate into society. ‘Some patients are highly functioning after a stroke. However, caution is advised. It is imperative to assess the patient’s mental health after a stroke, as well as signs of anxiety – and get the help they need,’ she says. ‘Assessments take place at:\n- six weeks (in-patient, while they are in rehabilitation),\n- three months,\n- six months, and\n‘These are to identify problems along the road to well-being. The follow-up assessments and interventions are done on an out-patient basis via referral from the rehabilitation unit. Thankfully, these conditions are highly treatable and full recovery, in most cases, is possible.’\nPost-stroke depression treatment\nHelp comes in the form of sessions with a clinical psychologist or a psychiatrist, where medication is prescribed as part of the recovery process.\n‘Antidepressant medication and psychological treatments work well together as part of the most effective treatment plan for stroke patients with depression and anxiety,’ explains Dr Anersha Pillay, a psychiatrist at the Life Rehabilitation Unit at Life Riverfield Lodge. ‘Psychological treatment may include interpersonal therapy, cognitive behaviour therapy, behaviour therapy and mindfulness-based cognitive therapy. Lifestyle is also discussed as a means to help patients feel good – eating well and adequate exercise can also positively impact well-being,’ she adds.\nGetting help may also come in the form of a caregiver and family support, as well as possible necessary modifications to the home to enable the patient to be able to move about safely, should that be a challenge.\nCommon post-stroke depression symptoms\nDr Pillay shares the most noteworthy signs and symptoms:\n- Lack of attention\n- Apparent clumsiness\n- Memory problems\n- Difficulty planning and organising\n- Abstract thinking and judgment is a challenge\n- Lack of self-awareness\n- Emotional changes\n- Decreased processing speed\nAlternative forms of therapy\nMfimfi indicates that other forms of adjustment disorder treatment therapies for recovery may be mentally and physically beneficial:\n- Acupuncture: known to help with pain relief\n- Yoga and t’ai chi: good for spatial awareness and balance\n- Regular massage: can help to improve fine motor skills and circulation\n- Aromatherapy: relaxing and reduces stress\n- Neurocognitive rehabilitation and music therapy: useful if the problem is of a cognitive nature\n- Stroke support groups: instrumental in educating and affording a sharing space for survivors\nTalking to someone who has had a stroke\nDepending on the location of the stroke in the brain, different abilities in the patient will be affected. Dr Pillay explains that:\nThe right hemisphere of the brain controls emotions, thinking and spatial orientation, and the left controls language and comprehension skills. ‘It is therefore very helpful as a caregiver to know where the stroke occurred in the brain in order to best communicate with the stroke patient and to try to avoid frustration,’ she says.\nDr Pillay shares these tips:\n- Keep the home environment as safe as possible and get rid of clutter and distractions.\n- Create a calming environment to help the patient focus and reduce confusion.\n- Be patient with the stroke survivor, keeping a normal tone of voice and giving the person time to think about their responses.\nLife Mental Health has nine mental health facilities across South Africa. You can get help by contacting a facility near you:\nLife Hunterscraig Hospital\n041 586 2664\nLife St Mark's Clinic\n043 707 4400\n010 009 6200\n011 655 5519\n011 741 5460\n087 352 2100\nLife Riverfield Lodge\n087 352 3765\nLife St. Joseph’s\n031 204 1470\nLife St. Vincent’s\n021 506 5111\nThe information is shared on condition that readers will make their own determination, including seeking advice from a healthcare professional. E&OE. Life Healthcare Group Ltd does not accept any responsibility for any loss or damage suffered by the reader as a result of the information provided.","Presentation on theme: \"Neuro-Developmental Treatment & Stroke\"— Presentation transcript:\n1Neuro-Developmental Treatment & Stroke Luke AdanLo SaechaoLyle SilverthornMikki ConnorChris LovelaceMichelle Smith\n2Learning ObjectivesAt the completion of this presentation, the learner will be able to:Describe the main principles of NDTDescribe early NDT vs. recent NDTDescribe the effectiveness of WSTT vs. NDT for improving gaitDescribe how NDT compares to other conventional therapy approaches.List common problems with reviews of NDT\n3NDT BackgroundNDT approach began in the early 1940’s from the work of Mrs. Berta Bobath (Physical therapist) and pediatric neurologist Dr. Karel Bobath (Psychiatrist/Neurophysiologist).Based on their experience of working with children with CP and adults with hemiplegiaObservations were based on the Reflex/Hierarchical modelReflex Hierarchical model: Movement is determined by a rigid heirarchy of reflexes.Abnormal movement is the result of de-inhibition of reflexes. Removal of CNS.\n4NDT and Adult Hemiplegia Main problems in patients with UMN lesions:Abnormal coordinationAbnormal postural toneThus, aims should be:Introduction of more selective movement patterns in preparation for functional skillsReduction of spasticityBobath, 1990\n5Early NDTBobath originally believed in reflex inhibiting postures (RIPs)Placed and held patients in RIPs to break up the abnormal postural and movement patterns.Believed this would change the activity of the whole body due to the “normalization” of postural tone.No spontaneous carry over into movement and function occurred.Treatment was too static and was not continued in this wayBobath, 1990\n6Revised NDTTheory: Dynamic “autoinhibition” by using reflex inhibiting movementsAs patient moves, PT prevents the unwanted parts of the abnormal movement by using “key points of control”Particularly proximal jointsPT should gradually withdraw control as the movement continuesBobath, 1990\n7NDT Main PrinciplesIt is impossible to superimpose normal movement patterns on abnormal ones, so abnormal patterns need to be inhibitedMovement is a sensory-motor experience: We do not learn a movement but the “sensation of a movement”By moving the proximal part of the body it is possible to influence and change movements of the distal parts“Shunting”: Position of the periphery sends sensory information to the CNS, causing the CNS to mirror the movement pattern in its output of excitatory signals to the periphery.Magnus: Studies on catsBobath, 1990\n8Evolution of NDT Principles NDT in North America is currently based on an interactive complex systems modelProblems in tone, posture, balance, and movement are equally important in producing atypical synergies that interfere with functional activities.NDT recognizes that it is essential to evaluate measurable changes in functions as well as changes in motor and body systems that support those functions.Neuro-Developmental Treatment Association, 2007\n9Evolution of NDT Principles Original Core Concepts Still ApplicableBobath’s therapeutic handling techniques make normal posture/movements more easy/likely to occurBobath’s focus on the interaction of impairments, function, and life participation (expanded to ICF)Bobath’s focus on taking a “holistic” approach to treating patientsNeuro-Developmental Treatment Association, 2007\n10NDT in the Clinic Therapeutic handling allows the therapist to: Feel the client’s response to changes in posture or movementFascilitate postural control and movement synergies that broaden the client’s options for selecting successful actionsProvide boundries for movements that distract from the goalInhibit or constrain those motor patterns that, if practiced, lead to secondary deformities, further disability, or decreased participation in societyHowle , 2002\n11Weight Supported Treadmill Training vs. NDT Treadmill Training With Partial Body Weight SupportCompared With Physiotherapy in NonambulatoryHemiparetic PatientsHeese, S. et al.Stroke. 1995;26:Who here thinks NDT works better than Weight supported treadmill training?\n12PurposeCompare the efficiency of PT based on NDT vs. WSTT in gait training for post stroke chronic hemi paretic patients.- Compare the efficiency of PT based on NDT vs. WSTT in gait training for post stroke chronic hemi paretic patients.Heese et al. 1995\n13Participants 7 nonambulatory hemiparetic patients 52 to 72 years old The study had 7 nonambulatory hemiparetic patients (52 to 72 years old). They had a very small sample group, but lets see what happens.Heese et al. 1995\n14Methods A-B-A single case study design 3 phases were administered to the participants1st phase= WSTT2nd phase= NDT3rd phase= WSTT- The pts were treated with an A – B – A single case study design. Which means they are given treatment A 1st, then treatment B, and then treatment A again.The 1st phase was WSTT, 2rd phase was NDT, and the 3rd phase was again WSTT “follow the pattern?” =)Heese et al. 1995\n15Results-Functional Ambulation Category - One of the measurements they used for results was the Functional Ambulation Category.- As you can see FAC levels only improved during the TM phases* Treadmill training was superior to NDT with regard to improvement of gait ability tested by the FAC (P < .05)Heese et al. 1995\n16Results-Rivermead Motor Assessment - Another measurement they used was the Rivermead Motor Assessment- According to the study no therapy proved to be superior because there was not a significant difference between the two.Heese et al. 1995\n17Results-gait velocity The last measurement they used was Gait VelocityTreadmill training was more effective than NDT (P<.05)A phase – patients increased their gait velocity with a mean of 150.4%B phase – walking speed did not change consistentlyA phase – patients increase their gait velocity with a mean of 43.5%Heese et al. 1995\n18Conclusion (Big Picture) WSTT is superior to NDT because WSTT is…Task oriented exerciseMore independentHigher dosageWSTT is a better approach to gait training because according to this study it is a task oriented exercise,it teaches independency from the PT, and has better dosage ( more reps ). (MIKE)Heese et al. 1995\n19Thaut, Leins et al. Rhythmic Auditory Stimulation Improves Gait More Than NDT/Bobath Training in Near-Ambulatory Patients Early Poststroke: A single-Blind, Randomized Trial. Neurorehabil Neural Repair 2007;21:455Purpose: to examine the clinical efficacy of RAS for post stroke gait training by comparing it to NDT.\n20Subjects155 hemiparetic patients were randomly selected to (RAS group or NDT group).Age: 69 ± 11155 hemiparetic patients were randomly selected and randomly assigned to experimental group (RAS group) or the control group (NDT group).the ave. age for both groups was around 69 plus or minus 11Thaut et al. 2007\n21Methods RAS - metronome and music tapes NDT – Bobath principles Major gait parameters measured: velocity, stride length, cadence, and swing symmetry.The RAS group followed established protocols using a metronome and specifically prepared music tapes.The NDT group practiced similar instructions about gait parameters.The major gait parameters measured were 1. velocity 2. stride length 3. cadence 4. swing symmetryHeese et al. 1995\n22ResultsBoth groups improved in all gait parameters , but more significant differences were found in favor of the RAS in all 4 gait parameters.as you can see the SL and cadence in the RAS group similarly increased the same amount. This is important because these parameters when coupled suggests a more functional recovery of gait mechanics.Heese et al. 1995\n23Conclusion (Big Picture) According to this study RAS is superior to NDT because…RAS gives the pt. an external cue to regulate parameters of gait.It only works when its on. When off only a few minutes will transfer.It only works when its on. When its turned off only a few minutes will transfer because of the high dosage given. This is what Mike and I discussed.Heese et al. 1995\n24Paci, M. PHYSIOTHERAPY BASED ON THE BOBATH CONCEPT FOR ADULTS WITH POST-STROKE HEMIPLEGIA: A REVIEW OF EFFECTIVENESS STUDIES. J Rehabil Med 2003; 35: 2–7Systematic Review of 15 trials out of 7266 RCTs, 6 CTs, 3 Case SeriesNo level 1 studies due to small sample size or weak evidence from P-valueAge range years“NDT is the most widely used approach in the rehabilitation of hemiparetic subjects in Europe, and it is well known and frequently used in many countries, including the USA, Canada, Japan, Australia and Israel”Purpose:Is there evidence that NDT is effective?Is NDT more effective than other treatments for adults with hemiplegia?\n25NDT Vs. EMG Feedback No difference found in all outcome measures Upper LimbEMG activityUpper Extremity Function TestFinger Oscillation TestHealth Belief SurveyMood and Affect TestsBasmajian et al, 2003Lower LimbROMGait analysisMulder et al., 1986\n26NDT Vs. Traditional Functional Retraining General Rx NDT group improved more on Barthel Index than TFRNo significant difference in all measuresFunctional Independence Measure (FIM)Box & Block TestNine-hole Peg TestSalter et al., Gelber et al., Lewis, 2003\n27NDT vs. Brunnstrom General Rx No significant difference in all outcome measuresAction Reach Arm TestBarthel IndexGait speedWagenaar et al., 2003\n28NDT Vs. Motor Relearning Programme General Rx MRP group improved more in:Barthel IndexMotor Assessment ScaleSodring Motor Evaluation ScaleNo difference found inNottingham Health ProfileLanghammer et al., 2003\n29NDT Vs. Forced Use Upper Limb Forced Use group had more improvements than NDT in Action Reach Arm Test (dexterity)No difference in all other outcome measuresRehabilitation Activities ProfileFugl-MeyerMotor Activity LogVan der Lee et al., 2003\n30ConclusionNo evidence supporting NDT as the optimal type of treatment.Important to note:So even though NDT may NOT be superior, it does positively effect recoveryThere was a significant improvement in most of the measured parameters for the NDT groups, but the improvements weren’t significantly different than other treatmentsPaci, 2003\n31Hiraoka, K. Rehabilitation Effort to Improve Upper Extremity Function in Post-Stroke Patients: A Meta-Analysis. J Phys Ther Sci (13), 5-9.Studies ranged between14 trials reviewedAll RCTsInterval Since Stroke0 days to 8 yearsLength of Treatment2 to 50 weeksSample Size20 to 282 people\n32Methods Interventions Assessed NDT vs. Conventional PT Conventional PT vs. No RxEMG biofeedback vs. Conventional PTEMG biofeedback vs. No RxUpper extremity function assessed by:Rivermead Motor Assessment Arm Scale,Action Reach Arm Test,Fugl-Meyer Assessment,Upper Extremity Functional Test,Frenchay Arm Test.Hiraoka, 2001\n33Results Used Cohen’s criteria to determine effect size - Large effect (significant difference) = ≥0.8- Medium effect (difference) = 0.5 – 0.8- Small effect (no difference) = 0.2 – 0.5Interventions AssessedNDT vs. Conventional PT: effect size = (0.01)Conventional PT vs. No Rx: effect size = 0.51EMG biofeedback vs. Conventional PT = 0.75EMG biofeedback vs. No Rx = 0.85\n34ConclusionThe effects of NDT and conventional treatment are almost identicalEMG Feedback had a larger effect on improving UE function in post stroke patients than NDT or conventional PTHiraoka, 2001\n35Yelnik, A. et al. Rehabilitation of Balance After Stroke With Multisensorial Training: A Single-Blind Randomized Controlled Study. Neurorehabil Neural Repair 2008; 22: 468Objective:Compare 2 physical rehabilitation approaches to restore balance after recent stroke: NDT vs Multisensorial TrainingMethods:68 patients who were able to walk without human assistance3 to 15 months post first strokeReceived NDT or Mulitisensorial Rx for 20 sessions in 4 weeksSample SizeNDT = 35 patientsMultisensorial = 33 patients\n36Outcome Measures Standing balance Berg Balance Scale Dynamic balance Assessed during walking by percentage of double-limb stance timeDaily IndependenceFunctional Independence Measurement (FIM)Quality of LifeNottingham Health ProfileYelnik et al., 2008\n37Assessment Differences between groups on Day 30 No difference between groupsDifferences between groups on Day 90Both the NDT and Multisensorial approach showed significant improvements in all outcome measures compared to baseline measures, but the Multisensorial approach showed more improvement.However, the differences between-groups were of no statistical significanceYelnik et al., 2008\n38ConclusionNo significant differences between NDT and Multisensorial TrainingNo evidence that one approach is superior to the otherYelnik, A. et al\n39Kollen, B.J. et al. The Effectiveness of the Bobath Concept in Stroke Rehabilibation: What is the Evidence? Stroke (Journal of the American Heart Association). 2009(40), e89-e97.16 trials reviewedSample size: 813 patients total ( in individual studies)Inclusion criteria:Involvement of adult patients with a cerebrovascular accidentThe effects of the Bobath Concept were compared with those of an alternative methodRandomized, controlled clinical trial (RCT)Only English or Dutch publications were considered for inclusion.\n40Inclusion Criteria (Cont.) Rehab outcomes were measured in one or more of the following:Sensorimotor function of the upper and/or lower extremityBalance controlMobility (The ability to (re)position the body by transfer or gait)Dexterity (Reaching, grasping, fine hand use)Activities of Daily living (ADLs)Health-Related Quality of Life (HRQOL)Cost effectivenessBoudewijn et al. 2009\n41ResultsThere was no evidence of the superiority of NDT for sensorimotor control of the upper and lower limb, dexterity, mobility, ADLs, HRQOL, and cost-effectivenessOnly limited evidence was found to support the superiority of NDT for balanceBoudewijn et al. 2009\n42Common Problems with reviews of NDT Little homogeneity between studiesStage of stroke recoveryTreatment intervalAge of patientsOutcome measuresTreatment comparisonFailure to clarify exact methods used\n43Why Do We Use NDT? Personal Experience of the Therapist Authority Evidence Based PracticeNDT works, but not better or worse than other methodsIf you are going to put your hands on a patient NDT is a good intervention to use\n44HOWEVER, today there is good evidence to support other interventions: CIMTBWSTTTask-Specific TrainingMental Imagery WE NEED TO BE EDUCATORS IN THE CLINIC!\n45Learning ObjectivesAt the completion of this presentation, the learner will be able to:Describe the main principles of NDTDescribe early NDT vs. recent NDTDescribe the effectiveness of WSTT vs. NDT for improving gaitDescribe how NDT compares to other conventional therapy approaches.List common problems with reviews of NDT\n46Works CitedBobath, B. (1990). Adult Hemiplegia: Evaluation and Treatment, 3rd Edition. Oxford: Heinemann Medical Books. Foley, N. et Al. Upper Extremity Interventions. Evidence-Based Review of Stroke Rehabilitation. 2009; Hesse, S. et. al. (1995). Treadmill Training with Partial Body Weight Support Compraed With Physiotherapy in Nonambulatory Hemiparetic Patients. Stroke. 26: Hiraoka, K. Rehabilitation Effort to Improve Upper Extremity Function in Post-Stroke Patients: A Meta-Analysis. J Phys Ther Sci. 2001(13), 5-9. Howle, J.M. (2007). NDT in the United States: Changes in Theory Advance Clinical Practice. Retrieved April 2009 from Howle, J.M. (2002). Neuro-Developmental Treatment Approach: Theoretical Foundations and Principles of Clinical Practice. Neuro-Developmental Treatment Association. Kollen, B.J. et al. (2009). The Effectiveness of the Bobath Concept in Stroke Rehabilibation: What is the Evidence? Stroke (Journal of the American Heart Association);40:e89-e97.\n47Works CitedLennon, S. & Ashburn, A. (2000). The Bobath concept in stroke rehabilitation: a focus group study of the experienced physiotherapists’ perspective. Disability and Rehabilitation, 22 (5):Paci, M. Physiotherapy based on the bobath concept for adults with post-stroke hemiplegia: a review of effectiveness studies. J Rehabil Med 2003; 35: 2–7.Thaut, M.H. et al, (2007). Rhythmic Auditory Stimulation Improved Gait More that NDT/Bobath Training in Near-Ambulatory Patients Early Poststroke: A Single-Blind, Randomized Trial. MeurorehabilNeuralRepair; 21:Yelnik, A. et al, (2008). Rehabilitation of Balance After Stroke With Multisensorial Training: A Single-Blind Randomized Controlled Study. Neurorehabil Neural Repair; 22: 468"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:517ff258-f57d-4e95-88af-f604c6b37d19>","<urn:uuid:9df9376b-f212-46a2-913f-f846205f761f>"],"error":null}
{"question":"I'm taking web design classes - how do file organization methods compare between a regular HTML website and a Blogger template?","answer":"In a regular HTML website, files are organized through traditional file pathing where you reference files relative to the current page location - for example using '../default.htm' to reference the folder above or '/myPics/myPics.htm' for subfolders. In contrast, Blogger templates use a more structured approach with predefined sections (Header, Content, Footer, Sidebar) that contain widgets to display content. The content organization in Blogger is handled through the template's XML structure and widget system rather than direct file management, with sections requiring specific IDs and classes to organize the content properly.","context":["Learn HTML and how to publish on the internet\nThis course is designed to provide the student with a starting point for understanding the world of Web Development. It will provide sufficient training for you to start producing your own HTML pages and publish them to the Internet.\n- Learn to create web pages.\n- Explore different methods of laying out an HTML page.\n- Investigate some of the techniques employed by web developers to Navigate between web pages.\n- Experiment with the use of images and background images on web pages.\n- Use style sheets to change the look and feel of a web page.\n- Build a web site based on a client design specification.\n- Start this excellent course at any time.\n- Study in your own home and at your own pace.\n- Learn from our helpful and highly experienced tutors.\nCOURSE STRUCTURE AND CONTENT\nCourse Duration: 100 hours.\nStart Date: Start at any time - study at a pace that suits you, and with full tutor support for the duration of your studies.\nContent: There are 8 lessons as follows:\n1. Getting Started\n- What is HTML.\n- What is a tag.\n- Create an html page.\n- Write a file.\n- File Naming.\n- View your First Page.\n- Your First Page Explained.\n- Structure of an HTML page.\n- Structure of Tags.\n- Tag Attributes.\n- Adding More Detail to a Page.\n- Laying out Text.\n- Adding Colour.\n- External Style Sheet.\n2. Page Layout\n- What is CSS\n- Defining a Style\n- Using CSS for Web Page Layout\n- Importing CSS Styles into HTML\n- Styles used for page content and layout\n- The Box Model\n- Maximum Heights and Widths\n- Using the Display Properly\n- CSS Units\n- Common Layouts\n- Text Layout\n- What is a Hyperlink?\n- External Links.\n- Text Links.\n- Naming Links.\n- Image Links.\n- The Target Attribute.\n- Email Links.\n- Internal Links.\n- Navigation Bar.\n- Navigation and Usability.\n4. Images and Page Weights\n- Image Format.\n- Selecting Image Type.\n- Sourcing Images.\n- Viewing Images on a Web Page.\n- Background Images.\n- Tricks with Background Images.\n- Page Weight.\n- Optimal Page Weight.\n- Image Optimisation.\n5. Colour and Style\n- Designing with Colours and Styles.\n- Understanding Hexadecimal.\n- Named Colours.\n- Named Colours vs Hexadecimal.\n- Web Safe colours.\n- Tags that Support Colour.\n- CSS - Cascading Style Sheets.\n- Cascading Styles.\n- Font Matching.\n- Inline Styles Using an ID.\n- CSS Classes.\n- IDs and Classes.\n- External Style Sheets (ESS).\n- Linking a Page to ESS.\n6. Designing a Web Site\n- Planning your Design.\n- Interviewing a Client.\n- Design Review.\n- Designing a Home Page.\n7. Building and Testing a Web Site\n- Planning a Site Before you Build.\n- Prototype Design.\n- Navigation and Build.\n- Test during and after Build.\n- HTML Standards.\n- Well Formatted HTML.\n- Usability Checks.\n- What is FTP.\n- Anonymous FTP.\n- FTP Client.\n- Registering your Domain.\n- Web Hosting.\n- Affording the Overheads.\n- Understand the basics of HTML and create your first HTML Page.\n- Explore the use of HTML tables to layout a web page.\n- Recognise the many different types of HTML links used to navigate a web page and web site.\n- Understand the importance of navigation in relation to people browsing your site.\n- Learn how to add images to a web page and understand the importance of page weights and download speeds.\n- Understand the web safe palette and the use of style sheets to control the look of a web page.\n- To be capable of designing and planning a basic web site that satisfies a client requirement.\n- Understand the importance of interpreting web site specifications in the planning and constructing of a web site.\n- Make your web site visible to the outside world.\nWhat is a hyperlink?\n“An element in an electronic document that links to another place in the same document or to an entirely different document.” www.webopedia.com\nThe HTML tag for a hyperlink is the <a> tag closed with an ending tag </a>. The hyperlink tag must contain an attribute which specifies the file or location to be linked to, that is, href=”…”, with the URL of the page or location placed between the quotes. For example:\n<a href=”http://www.mySite.com/myPics.html”>My Pictures</a>\nAn external hyperlink is one that when clicked will take the user to a new page within your site or to a new site. By default the new page will be displayed within the same browser window. Later we’ll look at how to specify a different or new window.\nThe hyperlink tag surrounds text which is displayed highlighted and is the link that the user may click. As displayed to the user, the hyperlink looks like this:\nThis is a text hyperlink\n… and the HTML code looks like this:\n<a href=“http://www.somewhere.com”>This is a text hyperlink</a>\nWhen the user clicks the link they will be taken to the web site www.somewhere.com. The destination is specified using the href attribute. In this example, the link is to another website and not another page within the current site, and therefore includes http://www. The link points to a domain name and not a specific page within that site. If a particular page is not specified, then the hyperlink will link to the default page for the site which should be named default.htm (an internet-wide standard).\nTo link to a page within your site you would simply specify the file name of the page you wish to link to, e.g. <a href=”myotherpage.htm”>…</a>. Notice that you do not need to include http://www or any folder information for a page that is saved in the same location as the current page.\nWithin your own site, if the page you wish to link to is located in a different folder to the current page, you will need to include the folder as well, for example:\n<a href=”/myPics/myPics.htm”>My Pictures</a>\nThe above hyperlink will link to a page called myPics.htm in a sub-folder of the current one, called myPics.\nThe location of files is called pathing, and just like locating files on your computer, files on websites use the same conventions. To reference the folder above the one you are currently at, use two dots before the slash, e.g. <a href=”../default.htm”>\nHOW THE COURSE WORKS\nYou can start the course at any time.\nIt is studied by distance learning, so you can study in the comfort of your own home. But this doesn't mean you are all alone in your studies. Our highly qualified and friendly tutors are there to help you every step of the way. If you have any questions at all, they are always happy to help.\nTHE ADVANTAGES OF STUDYING WITH ACS\n- You can start the course at any time and study at your own pace.\n- Fit your studies around your own busy lifestyle - we provide full tutor support for all the time you are studying.\n- Study where you want to - online studies offer the flexibility for you to determine where and when you study.\nWHY SHOULD YOU STUDY THIS COURSE?\n- Learn about HTML and publishing on the internet.\n- Learn to create web pages.\n- Learn about web design and page navigation.\n- Learn about building a web site for your business.\nRegister to Study - Go to “It’s Easy to Enrol” box at the top of the page and you can enrol now.\nGet Advice – Email us at email@example.com OR\nUse our FREE COUNSELLING SERVICE to contact a tutor","Are you an active blog reader and want to share your views, thoughts, experiences that you earned in past years? Is it your dream to have your own blog to communicate globally? Then, you are not far away – a single step and learn how to create Blogger template.\nHow to Create Blogger Template?\nHaving your own unique blogger template is something sumptuous. But the question arises – from where to start? The simple answer is, there are two reliable methods for designing a template as follows\nThere are two ways to Create Blogger Template\n- Create Blogger Template with Manual Method\n- Create Blogger Template with TemplateToaster\nIf you have good knowledge of coding, you will go for manual method to create blogger template. But if you are a beginner and don’t know how to code – then TemplateToaster is the best to choose to create blogger template. You require no coding with this blogger template creator. It will give you an easy drag & drop interface to design your template. You just select what you want, rest of the things it will handle. Let’s delve deeper to see the detailed procedure that how to design blogger template with both the methods\nCreate Blogger Template with manual method\nA Blogger template consists of XHTML and blogger elements. To design a basic layout of the template, you will use namespaces. A namespace (xmlns) is pre-defined to use with the Blogger by Google. 3 types of namespaces are basically used as follows\n- xmlns:b – ‘b’ specifies that this namespace is used to access the blogger elements.\n- xmlns:data – It is used to specify that from where the data of the blog comes.\n- xmlns:expr – Calculates the expression for attributes.\nYou will write all the code in main.xml file of Blogger.\nSyntax for Basic Layout\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\"> <html xmlns='http://www.w3.org/1999/xhtml' xmlns:b='http://www.google.com/2005/gml/b' xmlns:data='http://www.google.com/2005/gml/data' xmlns:expr='http://www.google.com/2005/gml/expr'> <head> <title><data:blog.pageTitle/></title> </head> <body> <!-- BODY CONTENTS --> </body> </html>\nA blogger template is divided into sections. The basic sections are: Header, Content, Footer, Sidebar. You will use the Widget element to define the content in a section. Note that you can’t use HTML within a section. But around a section, it’s permissible to use HTML.\nThe correct format of section will be\n<b:section id=’ ‘ class=' ' maxwidgets=' ' showaddelement=' '> <b:widget……../> </b:section> While the below format, will be considered Wrong: <b:section id=' ' class=' ' maxwidgets=' ' showaddelement=' '> <h1>Content heading</h> <div>Content</div> </b:section>\nYou need to specify following attributes in a section. id is the only required attribute while others are optional.\n- id – It is the unique name of section specified in letters and numbers only.\n- class – Consists of common classes such as ‘navbar,’ ‘main,’ ”header,’ ‘footer, and ‘sidebar,’. If you change templates later, these will let you to decide whether transfer your content or not. You can also use other class names if you wish.\n- maxwidgets – It limits the maximum number of widgets that can be added in a section.\n- showaddelement – Consists of ‘yes’ or ‘no’ value. ‘Yes’ is the default. This establishes whether the Page Elements tab displays the ‘Add a Page Element’ link or not.\n- growth – It can be ‘horizontal’ or ‘vertical’. ‘vertical’ is the default. This determines whether widgets are arranged side-by-side or stacked within a section.\nSyntax to Add Sections\n<b:section id='header' class='header' maxwidgets=\"1\" showaddelement=\"no\"> <!-- Section contents --> </b:section> <b:section id=\"sidebar\" class=\"sidebar\" maxwidgets=\"\" showaddelement=\"yes\"> </b:section> <b:section id='main' class='main' maxwidgets=\"1\" showaddelement=\"no\"> <!-- Section contents --> </b:section> <b:section id='footer' class='footer' showaddelement=\"no\"> <!-- Section contents --> </b:section>\nA widget is the main part which displays real data for section. It works as a placeholder. Section only defines the layout elements. Some default widgets are available in blogger. However, you can also create your own custom widget.\nWidget can have many attributes. Out of which, only id and type are required and others are optional.\n- id – It can have letters and numbers only. Each widget ID is unique. A widget’s ID can only be changed by deleting the widget or creating a new widget.\n- type – It indicates the type of widget and can have one of the valid widget types listed below\n- locked – It can have a ‘yes’ or ‘no’ value. The default value is ‘no’. You cannot move or delete a locked widget from the Page Elements tab.\n- title – Displays the title of the widget. If not specified, a default title such as ‘List1’ is used.\n- pageType – It can be ‘all,’ ‘archive,’ ‘main,’ or ‘item’. ‘All’ is the default. A widget will display only on the designated pages.\n- mobile – It can be ‘yes’, ‘no,’ or ‘default’. It tells a widget will display on mobile or not. Only Header, Blog, Profile, PageList, AdSense, Attribution will be displayed on mobile if it is set to ‘default.’\nWidgets are included within a section. The syntax to add a widget in a section will be somewhat like this\n<b:section id=\"sidebar\" class=\"sidebar\" maxwidgets=\"\" showaddelement=\"yes\"> <b:widget id='CustomSearch1' title='Search' type='CustomSearch' locked='false'/> <b:widget id='FollowByEmail1' title='Follow By Email' type='FollowByEmail' locked='false' /> <b:widget id='PopularPosts1' locked='false' title='Popular On Relatemein' type='PopularPosts'/> <b:widget id='Label1' type='Label' locked='false' /> </b:section>\nCreate blogger template with TemplateToaster blogger template creator\nIn this method, no coding is involved. Just a simple drag & drop interface to create Blogger template. More interestingly, it is a complete solution to the frequently asked questions by readers that how to make responsive Blogger template as all the templates designed with TemplateToaster web design software are responsive by default.\nTemplateToaster website builder is the most convenient method to create Blogger template. Just download it by visiting the official site. Its trial version is free. Follow the easy steps to install. The first screen you see after installation will look like\nFrom here, you will make a CMS selection. As TemplateToaster supports many CMSs, you will see many options. But you will select Blogger.\nNow, you will get the screen to choose a sample template to customize or create blogger template from scratch. Select “Start From Scratch” and click “Modify”.\nFrom next screen, you can choose a color scheme and font typography for your template. You can change it anytime during designing.\nThis is the Main Interface of the software. From here, you can design your template as per need. It has many self-depicting tabs like Header, Menu, Footer, Content, Slideshow, Sidebar etc. It facilitates you hundreds of latest option to design a unique template.\nDesigning the Header\nSelect the Header tab from the Top menu of the main screen.\nNow, set the width by selecting Width option. Here, a Full Width header is placed.\nSelect the Background option. It will help you to set a background color, gradient or image. Here, a Background Image is selected. It gives you an image gallery but you can also browse your own custom image.\nSet the Height of header as shown below.\nDraw a Textarea with the help of Text Areas option. Write the appropriate text here.\nDesigning The Menu\nSpecify the Menu Position with respect to the header.\nHere, the Menu is placed Inside Header by selecting Inside Header option.\nSet the Height of Menu. You can also specify a custom menu height.\nSet a Background color for the menu. You can also set a gradient or image. Even, you can make a custom color by specifying different brightness and opacity with More Color.\nSet the Typography of menu items. Here, you have different options available as Font family, size, color, alignment etc. as shown below\nYou can also set a different color on different states of a menu button. As below, with Menu Button Properties option, a background color is specified for Home menu button.\nFinally, set the alignment of menu items by Menu Button Properties → Alignment → Horizontal → Left to Page.\nDesigning the Sidebar\nTemplateToaster gives you many options to apply left, right or both sidebars.\nHere, a left sidebar for search is placed from sidebar tab.\nSet a background color for it. Rest of the work for adding widgets will be done after export.\nDesigning the Content (Main Area)\nNow for designing the content part, select the Content tab.\nIf you want to display metadata i.e information about the post like title, posted date, author, category etc., click Metadata → Show Metadata.\nYou can set the number of columns in the content area. For that, move to Columns option and select the suitable option. A two-column display is selected below.\nDesigning the Footer\nFinally, design the footer with Footer tab.\nSet the width as Full width.\nFrom Background option, apply a background color as shown below\nPlace the social media icons from Social Icons option and link them to appropriate sites. You can select any icon from icon gallery that fits your design.\nNow, double-click the copyright text to customize it. It will open a Format tab. From here, you can set alignment, angle or typography of text.\nExport the Template\nOnce you are ready with your template, export it. For that, select export option from Quick Toolbar. It is the last option in the rightmost place. You can also select Export option from File menu.\nOnce you click export, you will see an Export Blogger Theme dialog box. Write the folder name and path. Click Export button.\nNow, a Google accounts dialog will open. Here, choose an existing account from the drop-down menu or add a new account name and click Save.\nNow, it will redirect you to Google sign-in page. Here, give your email and click Next.\nEnter the password and click Next.\nGrant the permissions to TemplateToaster by clicking ALLOW.\nYou will be redirected to Blogger dashboard.\nNow from left pane of the screen, go to Theme → Backup/Restore. Upload your exported template file here by browsing.\nSelect the template file by clicking Open.\nUpload the template file.\nNow in the Theme section, you can see your own theme.\nGo to Posts from the left pane of the screen as shown above.\nNavigate to New Post option to publish a new post.\nOnce you draft your post in the blogger post editor, you can Publish it. Similarly, you can add more posts.\nNow you will click the View Blog option to preview the blog page.\nAdding Widgets in Sidebar\nAs you have added the posts, the only task remained is of adding widgets to the Sidebar.\nFor that from left pane select Layout. It will open a layout window.\nClick Add a Gadget in sidebar-left-1.\nIt will open a gadget list, you will add a Featured post from here.\nNow, you will configure featured post by adding different Post snippet. Check the snippets you want as shown above. Click Save.\nNow if you View Blog, it will print output with the featured post as shown.\nThis is your final Blogger Template. It’s really exciting as you create Blogger template of your own. Isn’t it?\nWhich way you use to create blogger template ?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:32cad695-6d42-445f-ab78-ebfdcd500294>","<urn:uuid:3d23c12f-3997-4e6a-900d-e58b7023b5ee>"],"error":null}
{"question":"What types of real estate transactions fall under RERA supervision, and which loans are exempt from RESPA coverage?","answer":"RERA encompasses both private and commercial real estate projects, including ongoing projects without occupancy certificates, with mandatory registration for projects over 500 square meters or eight flats. In contrast, RESPA does not cover commercial or business loans, all-cash sales, rental property transactions (except 1-4 residential units sold to individuals), and secondary market transactions. Both regulations have specific scopes aimed at different aspects of real estate transactions.","context":["The introduction and enactment of Real Estate Regulation and Development Act (RERA) was made on May 1st 2016. The same has been made applicable on May 1st 2017. This article will tell you all you need to know about RERA. Readers/Investors can decide whether RERA works for them or against them.\nWe are all mindful of the way that the real estate in India is exceedingly chaotic and to a great extent unregulated. The usage of a decent real estate bill has been long looming. The requirement for controls and uniform rules was additionally being felt by the business which kept on being seen contrarily by purchasers as a result of deceitful exercises of a couple.\nThe Real Estate Development and Regulation Bill 2013, was first drafted and affirmed by the then UPA, the United Progressive Alliance bureau in 2013. On 7th April 2015, the new Union Bureau (NDA) gave its endorsement to the alterations to the Real Estate Development and Regulation Bill.\nThe Bill was been passed in Rajya Sabha on the 10th of March 2016. It turned into an Act after the President of India gave his consent to it on 25th of March, 2016.\nThe fundamental goal of RERA is to secure the enthusiasm of purchasers (property purchasers), to advance reasonable play in land exchanges and to guarantee auspicious execution of ventures.\nKey features of Central Real Estate Regulations Act (RERA)\nBelow are the main features of Real Estate Act – 2016:\nScope of Real estate regulation act\nThe Act encompasses both private and business land ventures of the real estate industry.\nIt is required for the real estate engineer to enlist the venture with the concerned RERA (State particular) and get a legitimate enrollment number before proceeding with the venture. In the event that the venture, which meets the RERA criteria, does not get enlisted then the specialist can impose a punishment of up to 10% of the venture cost.\nThe enlistment is substantial for a predefined period as said by the manufacturer in the form of application. The real estate engineer is responsible for holding the timely submission of the project; else he/she chances enduring misfortunes/punishments.\nThe inclusion of any currently ongoing project is mandatory under this ACT which is into play since May 1st 2017. Any project which does not have an OC certificate will be considered an ongoing project under this act. OC ensures that the building has consented to all the required building guidelines, neighborhood laws and it is protected to involve. Inhabitance authentication is issued by nearby metropolitan experts or building proposition office that gives no protest to involve the working under reference for its predetermined utilize. The OC is issued just once the building has been finished in all regards and can be possessed.\nThis introduction of real estate development act has made it mandatory to register the project with the regulatory authority board for usage of 500 square meters of area or eight flats.\nEstablishment of Real Estate Regulatory Authority\nThe act has appointed multiple regulatory authority bodies over different states and union territories. These authorities will further appoint adjucating officers who will be responsible to resolve any disputes, impose penalty is applicable or even compensation if required.\nRegistration & Public Disclosure of Real Estate Projects\nThe bill makes it compulsory to enlist all the land ventures with the separate State Administrative Specialists. The manufacturers or real estate organizations should compulsorily reveal the points of interest of all the enrolled activities, for example, subtle elements of promoters, venture, format arrange, plan of improvement works, arrive status, status of statutory endorsements and divulgence of assertion, names and addresses of land operators, contractual workers, planner, basic designer and so on., For wrong exposure of data or for not conforming to the revelations and prerequisites, the authority can levy a punishment. The Administrative authority can even drop the venture endorsement.\nThe act places a liability clause on the builder for all maintenance arising out of the building after hand over to the customer within the first 5 years. Further in case of delay or default in handing over the possession the builder will be liable to pay similar rate of interest as the home buyers default or delay.\nHome buyers and Exit Clause\nHome purchasers can now leave the land to extend at any phase of development. The developer is additionally bound by law to restore the cash gathered from purchasers inside 45 days, subsequent to deducting the booking sum. On the off chance that the developer neglects to give ownership of the flat or finish the venture according to the stipulated time, the home purchaser can now end the agreement and is qualified for a discount of the sum paid with interest within 45 days of such end.\nDefinition of carpet area\nThe RERA Act plainly characterizes what is ‘cover zone’. Usable spaces like kitchen and toilets have been incorporated into the meaning of ‘cover territory’.\nCarpet area is a measure of the net usable region of the unit and does exclude regular territories, galleries, verandahs and so on; while, the super developed region could be an expansion of both.\nIn this way, to guarantee that the shopper realizes what he is paying for, it has been made required for the land engineer to indicate carpet area. Basically, carpet region is the territory inside the dividers of a unit where a buyer can live or have his office. Notwithstanding when the galleries, verandahs or porches are only accessible inside a unit these can’t be added to the Cover Zone as endorsed in the act.\nOne of the greatest agony focuses for Level purchasers has been ‘venture delays’. The majority of the developers utilize the accumulations of one anticipate to purchase another real estate parcel or for the business extension, rather than utilizing the assets to finish the on-going venture.\nIn this manner to ensure buyer of a venture the Act orders that of all accumulations 70% assets be saved in an escrow account kept up with a planned business bank.\nThese assets can be accessed by a land designer exclusively for the motivation behind the development of the venture to which it has a place. The land designer can pull back assets from this record in the extent to the phase of work.\nThe request for the withdrawal of assets is to be confirmed by a designer, engineer and a contracted bookkeeper so that land engineer’s cases are justifiable and legitimized.\nAdherence to Project Plans\nTo alter a project plan, structural design & specifications of the plot, apartment or a building, the Promoter has to get the consent of minimum two-third allottees (buyers) after the necessary disclosures.\nPhase wise Projects\nWhere the real estate project is to be developed in phases, every such phase shall be considered a standalone real estate project, and the promoter / builder shall obtain registration under this Act for each phase separately.\nResidents Welfare Association\nIf majority of the flats in the apartment have been allotted / sold, ‘welfare association’ has to be formed within 3 months.\nReal estate Agents\nThe real estate operators who mean to offer any plot or level of an enrolled extend need to enlist with the separate State real estate Administrative authority. It is currently obligatory for the land designer to give insights about the real estate specialists that are managing the venture, the subtle elements must be made accessible on the web. Insights from different experts like Designers, Specialists and so on., ought to be kept on the web.\nMarketing and advertising of project\nIn the majority of the cases, we watch that there is no co-connection between the points of interest given in showcasing materials and venture handouts and the real and the finished result.\nFrom now on, the act doesn’t allow such exercises as anything appeared in the advertising material should be in accordance with the last item or else land designer will be at risk to punishments under the Act.\nThe ad or outline issued or distributed by the engineer should say conspicuously the site address of the Authority, wherein all points of interest of the enlisted extend have been entered and incorporate the enrollment number acquired from the Expert and such different matters coincidental thereto.\nGuidelines for home buyers\nIt is obligatory for a shopper to make convenient installments to the real estate engineer.\nOnce the inhabitance Testament is issued by the land designer, the shopper is required to take ownership within two months time.\nIt is necessary for a shopper to display dynamic support in the arrangement of an affiliation, an agreeable society or any league of buyers.\nAnalysis of Real Estate Regulation Act\nA real estate segment is an exceptionally un-composed division. In this way, authorization of Land Act is certainly an appreciated move, for the home purchasers as well as for the business.\nYet, is this act truly advantageous? Will the purchasers truly pick up from it?\nEverything comes down to whether State governments will actualize the RERA as it is or not. In spite of the fact that RERA is a focal law (pertinent to all States and Union Regions with the exception of J&K), its execution will rely on upon State Governments, as land is a state subject. The individual State Governments can adjust the RERA and affirm it. In this way, any weakening of the original act can be a reason for concern.\nNotable example of the difference is as follow after the implementation of the ACT on May 1st 2017.\nAs per Central Real estate Regulations Act, all on-going projects where OC is not issued are also included under the scope of the Act w.e.f 1st May, 2017.\nBut, state government of Uttar Pradesh has modified this rule and added few exclusions in their respective State RERA U.P. government approved State RERA on 27th Oct, 2016. Same is the case with the Gujarat state, on-going projects have been excluded.\nIn this way, we can see two unique variants of the relevance of a similar rule run the show. On the off chance that you want to purchase a Level, you are encouraged to experience that State particular RERA principles and rules.\nUp until this point, a portion of the states like UP, Gujarat, Andhra Pradesh, Madhya Pradesh, Orissa and so on., have settled and advised their standards. A portion of the rest of the states has confined their draft administers and are yet to tell them.\nThe focal RERA is pertinent as it is in all the Union Regions. How about we trust every one of the States outlines their tenets as per the act in letter and soul and not attempt to keep the purchasers off guard. The entire reason for ordering the RERA will go futile in the event that it is not followed completely by every one of the partners.\nDo you believe that Real estate Regulations Act is beneficial to a home buyer? Will this Act keep a check on unscrupulous activities that happen in Real estate sector?\nThese are some of the query that will be answered soon in near future.","A “federally related mortgage loan” is any loan which is secured by a lien on residential real property designed principally for the occupancy of from one to four families and made in whole or part by any lender insured by an agency of the federal government or regulated by the federal government. 12 USC § 2602(1).\n- 1 Which of the following would most likely not be considered a federally related mortgage loan as defined by respa?\n- 2 Is a Heloc a federally related mortgage loan?\n- 3 How do I know if my loan is a federally backed loan?\n- 4 What are the 6 RESPA triggers?\n- 5 What types of loans does RESPA apply to?\n- 6 What type of loan is not covered by RESPA?\n- 7 What loans are not covered by RESPA?\n- 8 What is prohibited by RESPA?\n- 9 Are bank loans federally backed?\n- 10 How do I know who my mortgage is backed by?\n- 11 Is my mortgage covered by the cares act?\n- 12 What is the 3 7 3 rule in mortgage terms?\n- 13 What is a TILA violation?\n- 14 What is the TILA RESPA rule?\nA private mortgage loan would not be considered a federally-related mortgage loan.\nThe basic coverage of RESPA is ” any federally related mortgage loan.” loans for property improvement; HELOC, home equity lines of credit; and. reverse mortgages.\nHow do I know if my loan is a federally backed loan?\nIf you want to find out whether your loan is federally back, you can use the Freddie Mac or Fannie Mae lookup tools. You can also call your loan servicer to ask (they are required by law to tell you). If you have questions about whether you can get a federally-backed loan, talk to Integrity First Lending today.\nWhat are the 6 RESPA triggers?\nThe six items are the consumer’s name, income and social security number (to obtain a credit report), the property’s address, an estimate of property’s value and the loan amount sought.\nWhat types of loans does RESPA apply to?\nRESPA applies to the majority of purchase loans, refinances, property improvement loans, and equity lines of credit.\nWhat type of loan is not covered by RESPA?\nCommercial or Business Loans Normally, loans secured by real estate for a business or agricultural purpose are not covered by RESPA. However, if the loan is made to an individual entity to purchase or improve a rental property of 1 to 4 residential units, then it is regulated by RESPA.\nWhat loans are not covered by RESPA?\nTransactions generally not covered under RESPA include: “ an all cash sale, a sale where the individual home seller takes back the mortgage, a rental property transaction or other business purpose transaction.” “The sale of a loan after the original funding of the loan at settlement is a secondary market transaction.\nWhat is prohibited by RESPA?\nSection 8 of RESPA prohibits anyone from giving or accepting a fee, kickback or anything of value in exchange for referrals of settlement service business involving a federally related mortgage loan. In addition, RESPA prohibits fee splitting and receiving unearned fees for services not actually performed.\nAre bank loans federally backed?\nGovernment loans are insured or backed by the U.S. federal government. There are many types of government loans, including loans for college education, mortgages, disaster relief, opening a business and loans to support veterans.\nHow do I know who my mortgage is backed by?\nYou can look up who owns your mortgage online, call, or send a written request to your servicer asking who owns your mortgage. The servicer has an obligation to provide you, to the best of its knowledge, the name, address, and telephone number of who owns your loan. It’s not always easy to tell who owns your mortgage.\nIs my mortgage covered by the cares act?\nWhat Types Of Loans Are Covered Under The CARES Act? Under the act, mortgage forbearance relief must be offered to anyone experiencing a financial hardship due to COVID-19 for all federally backed mortgages. This includes loans guaranteed by the FHA, USDA and VA, among others.\nWhat is the 3 7 3 rule in mortgage terms?\nTiming Requirements – The “3/7/3 Rule” The initial Truth in Lending Statement must be delivered to the consumer within 3 business days of the receipt of the loan application by the lender. The TILA statement is presumed to be delivered to the consumer 3 business days after it is mailed.\nWhat is a TILA violation?\nSome examples of violations are the improper disclosure of the amount financed, finance charge, payment schedule, total of payments, annual percentage rate, and security interest disclosures. Under TILA, a creditor can be strictly liable for any violations, meaning that the creditor’s intent is not relevant.\nWhat is the TILA RESPA rule?\nThe TILA-RESPA rule consolidates four existing disclosures required under TILA and RESPA for closed-end credit transactions secured by real property into two forms: a Loan Estimate that must be delivered or placed in the mail no later than the third business day after receiving the consumer’s application, and a Closing"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:11359f20-5fe3-440f-932c-cbf4b2d18ef1>","<urn:uuid:b160e3d8-f3bf-4a1d-93a9-cc664a44d9cb>"],"error":null}
{"question":"Which development has more housing units: Roots in Utrecht or Dalston Works in London?","answer":"Roots in Utrecht has 220 homes scheduled for completion in 2024, while Dalston Works in London contains 121 apartments, making Roots the larger development in terms of housing units.","context":["Commissioned by project developer VORM, in collaboration with WE architects, EVA architects, and ZUS, we designed a sustainable residential neighborhood in Leidsche Rijn Centrum Oost in Utrecht; Roots. Roots consists of a wide variety of housing typologies for different target groups: from studios and single-family homes to urban villa apartments built with a wooden shell. Completion of the 220 homes is scheduled for 2024.\nA rich residential program\nThe development pays great attention to the quality of life of the future residents. Roots' buildings are characterized by their robust character and detailed finishes with bay windows, loggias, and porches. This gives the streets an urban, lively atmosphere through which one directly experiences the proximity to the center of Utrecht. The new residential buildings will have a balanced mix of target groups, thanks to a wide variety of housing typologies.\nFuture residents can meet in characterful, collective inner gardens that will also be provided with natural play elements for children. All apartment buildings have covered bicycle racks and two compact underground garages have been created for parking cars, which are completely hidden from sight. The routing through the collective inner garden and the informal transitions between private and collective spaces contribute to the special role that the outdoor space plays in this plan and contributes to the residents' connection with their surroundings.\nOptimal sustainable design\nThe homes in Roots are being developed in a circular fashion and are healthy, comfortable, functional, always easily changeable, and, above all, affordable. Building materials are given a second life. This is expressed in the reuse of wood and plastics from wheelie bins and crates from Utrecht for furnishing elements of the inner garden. Pergolas are made from scrap metal, paths are made from semi-paved olivine for the direct binding of CO2. Circular concrete from New Horizon is also used. In addition, the realization process will be as CO2-neutral as possible, with a wooden frame that will maximize the environmental benefits of construction in several ways. The energy-neutral homes will be delivered with an average GPR score of 8.3. The entire plan will receive a materials passport in Madaster.\nClimate adaptation & biodiversity\nIn Roots, landscape, and architecture are harmoniously brought together. Together with the gardens, green roofs, and facades form a balanced system of water management that anticipates climate change. With various nesting opportunities in the facades, Roots also contributes to improving biodiversity in the environment. The interior gardens, with their native plantings, also attract butterflies, bees, and other insects, which in turn attract birds. Here, animals find a natural habitat within the city. The green environment has a cooling effect, the plants capture particulate matter and muffle noise: all effects that contribute to a pleasant living environment. All homes are carefully designed with the most sustainable materials and equipped with a collective climate system, which cleverly anticipates the use of solar energy and uses a neighborhood battery. This smart system ensures that the homes provide more energy than they consume.\nCharacterful architecture connected to nature and history\nThe area of over 9,700 m² literally forms the connection between the old and new Utrecht. In addition, the connection with nature and connection with the history of Utrecht is at the basis of the Roots design. The collaborating design firms have interpreted these themes in their own way. This results in a varied and lively streetscape of nature-inclusive buildings that subtly refer to the recognizable Utrecht architecture. Various elements of the original Utrecht river and production landscapes have also been brought back in the lush courtyard gardens with native plantings and water features.","Dalston Works Mixed-Use Development, Hackney Architecture, Homes, Offices, Retail Spaces, Building Images\nDalston Works Mixed-Use Development in Hackney\nHousing, Retail and Industry Buildings in London, UK – design by Waugh Thistleton Architects\n22 Oct 2017\nDalston Works Mixed-Use Development\nDesign: Waugh Thistleton Architects\nLocation: Dalston, Hackney, North East London, England, UK\nHackney-based architecture practice completes £24m Dalston Works project for Regal London\nWaugh Thistleton Architects has completed Dalston Works, the world’s largest cross-laminated timber building for London property developer Regal London. The mixed use development, located in the heart of Dalston, showcases how the innovative use of sustainable materials like CLT can help deliver highquality, high density housing to the capital without compromising the environment.\nDalston Works comprises 121 apartments for rent alongside two ground level courtyards flanked by 1500 sqm of retail and restaurant space. To the south of the site a 3500 sqm flexible workspace hub caters to the growing creative community in Dalston.\nSimon De Friend, CEO at Regal Homes said:\n“As a responsible and innovative developer, Regal London is proud to deliver high quality homes that will not only visually enhance areas and contribute to ongoing regeneration, but which also champion and lead the way in the development of sustainable homes.\n“This is a pivotal moment in our history as Dalston Works, our first project in the private rented sector and a beacon for how sustainable materials like CLT can deliver high-quality, high-density housing to the capital without compromising the environment, is declared open. We are passionate about a development that offers a genuine long-term rental and environmental solution for Londoners, whether it be local families or London professionals.”\nThe building is broken into several distinct volumes with varying roof heights, designed to maximise daylight to the courtyards and living spaces. Dalston Works’ brickwork references the surrounding Victorian and Edwardian housing and detailing of local warehouses, while providing a contemporary addition to the local streetscape.\nLocated on a prominent corner site previously occupied by industrial buildings, the building’s ten storeys reach higher than was thought possible, given the structural restrictions posed by the proximity to a planned Crossrail route. Through the use of timber for the external walls, party walls, floors, ceilings, stairs and lift cores, Dalston Works weighs merely a fifth of a comparable concrete structure, warranting the viability of the scheme in the site context.\nAndrew Waugh, of Waugh Thistleton Architects said: “Dalston Works reveals the future of low carbon construction. Our building offers an exemplar solution to a demand which will only increase: the construction of high density, affordable and environmentally sustainable homes.\nDalston Works demonstrates the possibilities of engineered, cross-laminated timber as a cost efficient, and desperately necessary, viable alternative to the polluting technologies of concrete and steel. The collaborative effort between our progressive client and passionately innovative design team has resulted in something truly special: Dalston Works sets a seminal precedent for the continuous innovation of engineered timber construction.”\nThe sustainability credentials of the building are significant with approximately 50 percent reduction in the embodied carbon of the structure compared to a traditional concrete frame building. The timber also acts as carbon storage with over 2,600 tonnes of C02 locked into the material. This effectively makes the building carbon negative for the first years of its usage.\nUsing offsite construction shortened the construction programme allowing the frame to be completed in 374 days; while 80% fewer site deliveries reduced disruption, lowering the impact of the development on local residents and the environment.\nGavin White, Director and CLT expert at Ramboll, said:\n“Dalston Works is a real landmark project, and a testament to the versatility of CLT. It showcases what can be achieved when a forward thinking client enables you to demonstrate the benefits of offsite construction methods.\nIt has been exciting to work with a team who are as passionate about the benefits of offsite construction as we are, and we look forward to seeing many more such schemes emerge across the UK. The height and size of Dalston Works, and its excellent performance in terms of sustainability and efficiency, demonstrate what can be achieved with this dynamic material.”\nDalston Works Mixed-Use Development – Building Information\nClient: Regal London\nArchitect: Waugh Thistleton Architects\nStructural Engineers: Ramboll (CLT), PJCE (Concrete)\nContractor: Regal London\nDeveloper: Regal London\nPhotography: Daniel Shearing\nDalston Works Mixed-Use Development in Hackney images / information received 221017\nLocation: Dalston, Hackney, London, England, UK\nLondon Building Designs\nContemporary London Architectural Designs\nLondon Architecture Links – chronological list\nLondon Architecture Tours – bespoke UK capital city walks by e-architect\nHackney Housing : Adelaide Wharf\nNorth East London Buildings\nKings Crescent Estate Phases 1 and 2, Hackney\nArchitects: Karakusevic Carson Architects and Henley Halebrown\nphoto © Peter Landers\nKings Crescent Estate Phases 1 and 2\nFrampton Park Baptist Church\nDesign: Matthew Lloyd Architects LLP\nphotos : Benedict Luxmoore, Patricia Woodward\nFrampton Park Baptist Church Building in Hackney\nComments / photos for the Dalston Works Mixed-Use Development in Hackney page welcome\nWebsite: Waugh Thistleton Architects"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:15acfe70-a044-4402-bdef-1c2e7a969bf8>","<urn:uuid:ca2de926-07f7-4cad-81b4-b561b255a18e>"],"error":null}
{"question":"What tools are being used for personal monitoring in modern relationships, and what legislative measures protect social media privacy at work?","answer":"Personal monitoring tools range from basic internet searches to sophisticated technologies including keystroke monitoring software, surveillance cameras, and portable lie detectors. These tools are commonly used to monitor romantic interests, family members, and friends, with some technologies like caller ID becoming standard features of modern communication. Regarding workplace privacy, legislation has been enacted in nearly half of U.S. states to protect employees' social media privacy. These laws, first implemented in Maryland in 2012, prohibit employers from requiring access to private social media accounts or passwords from current and prospective employees. States like California, Virginia, and Illinois have adopted similar protective measures to prevent employer intrusion into personal social media accounts.","context":["Lateral Surveillance, Risk and Governance – Mark Andrejevic\n“This article focuses on emerging strategies for what might be described as lateral surveillance: not the top-down monitoring of employees by employers, citizens by the state, but rather the peer-to-peer surveillance of spouses, friends, and relatives.”\n“Lateral surveillance, or peer-to-peer monitoring, understood as the use of surveillance tools by individuals, rather than by agents of institutions public or private, to keep track of one another, covers (but is not limited to) three main categories: romantic interests, family, and friends or acquaintances. It also comprises several levels of monitoring, ranging from casually Googling a new acquaintance to purchasing keystroke monitoring software, surveillance cameras, or even portable lie detectors. Rather than providing an exhaustive taxonomy of surveillance technologies and practices, this section explores examples of monitoring strategies in an effort to elaborate the logic of peer-to-peer surveillance, one that it is hoped might prove of some use in illuminating a constellation of practices ranging from the use of lie detectors in reality TV formats to the growing market for home surveillance products, and the commonplace practices of peer monitoring via cell phone, IM, or the Internet. While some of the practices described below might seem absurd, such as submitting children to a portable lie detector test, others have become so commonplace that they have passed into unreflective use, such as caller ID, once a technology paid for by those with security concerns, now a service as ubiquitous as cell phones. The following sections explore three inter-related forms of lateral surveillance: the use of the Internet, the development of do-it-yourself information gathering technologies, and that of offline investigative tools. In each case the goal is to use representative examples of the technologies to provide some concrete examples of the argument developed in previous section as well as to illustrate developments in lateral monitoring that don’t receive the kind of attention –academic or otherwise –that more top-down forms of surveillance have generated. Of central interest to this paper is the constellation of monitoring practices that emerge from a consideration of the available technologies and techniques. They are, I would argue, worthy of consideration in their own right not as a unique phenomenon but as part of the monitoring assemblage associated with the deployment of new information and communication technologies.”\nThis article explores a range of technologies for ‘lateral surveillance’or peer monitoring arguing that in a climate of perceived risk and savvy skepticism individuals are increasingly adopting practices associated with marketing and law enforcement to gain information about friends, family members, and prospective love interests. The article argues that the adoption of such technologies corresponds with an ideology of ‘responsibilization’associated with the risk society: that consumers need training in the consumption of services and the development of expertise to monitor one another. Rather than displacing ‘top-down’ forms of monitoring, such practices emulate and amplify them, fostering the internalization of government strategies and their deployment in the private sphere. In an age in which everyone is to be considered potentially suspect, all are simultaneously urged to become spies.","Has a potential employer asked you to provide your password to a social media account? Or asked you to bring up your social media page during an interview? Perhaps your employer has asked you to plug company products or services on your private account or insisted that you accept a “friend” request from your manager.\nIf you’ve faced any of these requests in the workplace, you’re not alone. In recent years, employers have become increasingly concerned about what employees and potential employees are posting on social media sites. Some employers want to weed out unprofessional or unqualified job applicants. Others want to make sure that their workers aren’t posting offensive messages or negative statements about the company. And some may just want just want to keep tabs on employees, whether they are on duty or off.\nA growing number of state legislatures have stepped in to ban this practice. If you work in one of these states, you might be protected if your employer tries to get access to your social media accounts.\nThe first widely publicized case of an employer asking an employee to hand over his social media password happened in Maryland. In 2011, state Department of Corrections employee Robert Collins went to his recertification interview, where he was asked to provide his Facebook password. According to news reports, Collins then had to sit there while his interviewer scrolled through online posts by his friends and family, apparently looking for anything that might conflict with his duties as a corrections officer. Collins went to the ACLU, which publicized the case. Shortly after, Maryland became the first state to ban employers from asking employees or job applicants for access to their private social media pages.\nSince Maryland passed its law in 2012, many other states have followed suit. Nearly half of the states (including California, Virginia, and Illinois) now protect applicants and employees from having to provide social media passwords to employers or otherwise give employers access to their accounts. More states consider passing these laws each year.\nThese laws differ in the details, including what employers may do and what remedies employees have if their rights are violated. In addition to prohibiting employers from asking for or requiring prospective and current employees from providing their passwords, some state laws prohibit employers from:\nIf a prospective or current employer has asked you to compromise the privacy of your social media accounts, talk to an experienced local employment lawyer right away. Because this area of the law is rapidly evolving, you’ll need to consult with a lawyer who is up to date on legal developments in your state—and on how courts have interpreted existing laws. Especially if your employer retaliates against you for refusing to hand over your social media password (for example, by taking you out of the running for a promotion), you may have a strong legal claim.\nThe above rules apply when an employer is trying to gain access to an employee's private social media account. However, under federal labor laws, employees also have certain rights when it comes to the content of their social media posts. To find out what kind of posts are protected, see Do Labor Laws Protect Employee Posts on Social Media?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:61db08e9-3816-4830-94e3-4a3cba4969eb>","<urn:uuid:7a4249cb-d688-42a4-8946-3f3739bd3c68>"],"error":null}
{"question":"How do Banded Iron Formations from the Precambrian compare to ocean conditions in the Mesoproterozoic era?","answer":"Banded Iron Formations (BIFs) appeared around 2600 mya and continued until about 1800 mya, representing precipitation of dissolved iron from seawater as dissolved oxygen content increased. The ocean at the start of the Mesoproterozoic (1600-1550 mya) maintained similar conditions, being dominantly anoxic and ferruginous (iron-rich). However, while the Precambrian BIFs ceased around 1800 mya marking a rise in atmospheric oxygen, the Mesoproterozoic showed further evolution with a progressive oxygenation event starting at approximately 1570 mya.","context":["The time between the formation of the earth and the beginning of the Cambrian(about 570mya) is a 4000 my long period known as the Precambrian, this includes approximately 90% of geological time of which we know very little about as pre-Cambrian rocks are poorly exposed, many have been eroded or metamorphosed and fossils are seldom found.\nThe Precambrian has been divided into 3 Eons: 1. Hadean (4600-3800 mya of which there is no rock record) 2. Archean 3800-2500 mya) 3. Proterozoic 2500-570 mya. The present atmosphere is greatly depleted in Ne, Xe and Kr which are inert gases that should be preserved in the atmosphere.\nThis suggests that the earth’s initial atmosphere was lost early on either by boiling away during the magma ocean event or by being carried away by intense solar wind in the early solar system. At the end of the Hadean the present atmosphere and hydrosphere began to develop from volcanic emissions. It was during the proterozoic that a critical change occurred in the atmosphere, when it changed from a trace oxygen content of the Archean atmosphere to above 15% oxygen by 1800 mya.\nIt is widely believed that this change was brought about by the emergence of cyanobacteria which had adapted to create energy from the sun by photosynthesis(probably due to a shortage of raw materials for energy), as a result they had began to poison the earlier anaerobic bacteria or archea with their waste product; oxygen. This essay will focus on the evolution of the atmosphere and its relation to the banded iron formations of the late Precambrian. Banded Iron Formations\nCloud (1968) calls Banded Iron Formations, rhythmically banded chemical sediments of large, open water bodies that take different aspects but most characteristically consists of alternating layers of iron- rich and iron-poor silica. It is present in some of the oldest volcanic sequences (greenstone belts ca 2800 mya) and is a common sediment type formed until approximately 1800 mya. Although some younger formations with similar structure can be found there is great distinctions between them and the BIF’s of the Precambrian.\nThe BIF’s can occur in sequences which range from 15 ferric iron. Some people believe the iron to be of volcanic origin, weathered and transported into the oceans or exhaled from fumeroles Relation to the atmosphere The link between Banded Iron Formations (or BIF’s) and this change in oxygen levels is a close one, as BIF’s appeared about 2600 mya and continued until about 1800 mya. These deposits of marine hematite and quartz represent a precipitation of dissolved iron from sea water as the dissolved oxygen content of the water increased.\nAfter 1800 mya, Banded iron formations are rare, but terrestrial red beds are common for the first time suggesting that iron is being oxidised and precipitated in soils and rocks on land in the source area of sediments instead of being dissolved and carried into the oceans in its unoxidised form. Thus, it has been reasoned that the BIF-red bed transition marks the rise of atmospheric oxygen. Complimentary information comes from detrital uraninite in Archean and earliest Proterozoic alluvial rocks.\nBecause this uranium mineral can survive prolonged transport only in media containing little or no oxygen, the lack of detrital uraninite deposits younger than 2300 ma also points towards a significant environmental transition (Roscoe,1969). Not all scientists have accepted the validity of these observations or of their interpretation, Dimroth and Kimberley(1976) argued that at least some red beds antedate the end of BIF deposition, that Archean granites have paleoweathering profiles indicative of oxic environments, and that oxidised sulphur minerals (sulphates) occur in some of the oldest known sedimentary successions.\nAll of these observations are correct, and we must ask whether they preclude the interpretation of Archean and earliest Proterozoic environments as oxygen poor. Holland (1984) believed the answer to be no. As the formation of red beds and oxidised weathering profiles on granitic substrates requires oxygen, but only in minute quantities- considerably less than is needed for aerobic metabolism.\nAlso, marine sulphate does not require free oxygen as all as H2S can be photooxidized anaerobically to SO42- by photosynthetic bacteria, while the photochemical oxidation of volcanogenic S and SO2 to sulphate was probably a steady source of oxidised sulphur in the Archean oceans(Walker, 1983). Towe (1990) has specifically argued for the development of aerobic respiration early in the Archean and, therefor, for the presence of 1 to 2% PAL (present atmospheric level) O2 in the atmosphere since that time.\nThe possibility that oxygen levels reached this physiologically important threshold so early is not contradicted by the sparse geochemical data available for early Archean rocks, Towe’s model however suffers from the absence of Archean O2 sinks other than Fe2+. Some believe that the neglect of volcanic gases in his model casts significant doubts on the validity of his analysis. Knoll(1979) believed increases in atmospheric oxygen were probably occasioned by increases in primary productivity and/or decreased rates of oxygen consumption.\nHe believes the increase from very low O2 levels to 1 to 2% PAL may have been related to productivity increases associated with rapid growth and stabilising of the continents during the late Archean and earliest Proterozoic, in contrast, Cameron(1983) stated that ‘the later increase to 15% PAL does not seem to be related to a major tectonic event. The high oxygen level in today’s atmosphere must be related to the role of PO2 in the maintaining of redox balance of the atmosphere-biosphere-ocean-lithosphere system’. The nature of the connection is still in dispute.\nIt is believed that atmospheric PO2 determines the concentration of O2 in surface ocean water, but the influence of the O2 concentration in seawater on the burial efficiency of organic matter within marine sediments seems to be slight. Nutrients are a more likely link between PO2 and the burial rate of organic matter, and hence between PO2 and rates of long term O2 generation(Betts and Holland,1991). Holland constructed a plausible argument that links the marine geochemistry of PO43- to that of iron and hence to the O2 content of the atmosphere today.\nHe believes that if this argument is true then the history of atmospheric O2 may have been controlled by a complicated feedback system involving the marine geochemistry of iron and phosphorus. This would explain the rapid increase in PO2 around 2100 mya as marking the passage of the system across a threshold from one steady state to another. Conclusion The complexities and conflicting arguments involved in relation to the evolution of the atmosphere and its links to banded iron formation are hard to over emphasise.\nIt is a topic which still has a long way to go and one which may never be conclusively understood due to the lack of evidence from the Precambrian rocks (or lack of rock record altogether). However the general consensus seems to be that for a period of over a 1000 my until 1800 mya, conditions where favourable for chemical deposition of iron, there was distinct changes occurring in the atmosphere with rapid increase in free O2 coupled with a fundamental change in the evolution of early life from anaerobic bacteria to aerobic cyanobacteria.","The Mesoproterozoic era (1,600–1,000 million years ago (Ma)) has long been considered a period of relative environmental stasis, with persistently low levels of atmospheric oxygen. There remains much uncertainty, however, over the evolution of ocean chemistry during this period, which may have been of profound significance for the early evolution of eukaryotic life. Here we present rare earth element, iron-speciation and inorganic carbon isotope data to investigate the redox evolution of the 1,600–1,550 Ma Yanliao Basin, North China Craton. These data confirm that the ocean at the start of the Mesoproterozoic was dominantly anoxic and ferruginous. Significantly, however, we find evidence for a progressive oxygenation event starting at ~1,570 Ma, immediately prior to the occurrence of complex multicellular eukaryotes in shelf areas of the Yanliao Basin. Our study thus demonstrates that oxygenation of the Mesoproterozoic environment was far more dynamic and intense than previously envisaged, and establishes an important link between rising oxygen and the emerging record of diverse, multicellular eukaryotic life in the early Mesoproterozoic.\nSubscribe to Journal\nGet full journal access for 1 year\nonly $8.25 per issue\nAll prices are NET prices.\nVAT will be added later in the checkout.\nTax calculation will be finalised during checkout.\nRent or Buy article\nGet time limited or full article access on ReadCube.\nAll prices are NET prices.\nRasmussen, B., Fletcher, I. R., Brocks, J. J. & Kilburn, M. R. Reassessing the first appearance of eukaryotes and cyanobacteria. Nature 455, 1101–1104 (2008).\nKnoll, A. H., Javaux, E. J., Hewitt, D. & Cohen, P. Eukaryotic organisms in Proterozoic oceans. Philos. Trans. R. Soc. Lond. B 361, 1023–1038 (2006).\nAgić, H., Moczydłowska, M. & Yin, L. Diversity of organic-walled microfossils from the early Mesoproterozoic Ruyang Group, North China Craton—a window into the early eukaryote evolution. Precambrian Res. 297, 101–130 (2017).\nJavaux, E. J., Knoll, A. H. & Walter, M. R. Morphological and ecological complexity in early eukaryotic ecosystems. Nature 412, 66–69 (2001).\nVorob’eva, N. G., Sergeev, V. N. & Petrov, P. Y. Kotuikan Formation assemblage: a diverse organic-walled microbiota in the Mesoproterozoic Anabar succession, northern Siberia. Precambrian Res. 256, 201–222 (2015).\nZhu, S. et al. Decimetre-scale multicellular eukaryotes from the 1.56-billion-year-old Gaoyuzhuang Formation in North China. Nat. Commun. 7, 11500 (2016).\nSummons, R. E., Bradley, A. S., Jahnke, L. L. & Waldbauer, J. R. Steroids, triterpenoids and molecular oxygen. Philos. Trans. R. Soc. Lond. B 361, 951–968 (2006).\nLyons, T. W., Reinhard, C. T. & Planavsky, N. J. The rise of oxygen in Earth’s early ocean and atmosphere. Nature 506, 307–315 (2014).\nPlanavsky, N. J. et al. Low Mid-Proterozoic atmospheric oxygen levels and the delayed rise of animals. Science 346, 635–638 (2014).\nZhang, S. et al. Sufficient oxygen for animal respiration 1,400 million years ago. Proc. Natl Acad. Sci. USA 113, 1731–1736 (2016).\nDaines, S. J., Mills, B. J. & Lenton, T. M. Atmospheric oxygen regulation at low Proterozoic levels by incomplete oxidative weathering of sedimentary organic carbon. Nat. Commun. 8, 14379 (2017).\nPoulton, S. W., Fralick, P. W. & Canfield, D. E. Spatial variability in oceanic redox structure 1.8 billion years ago. Nat. Geosci. 3, 486–490 (2010).\nPlanavsky, N. J. et al. Widespread iron-rich conditions in the mid-Proterozoic ocean. Nature 477, 448–451 (2011).\nPoulton, S. W. & Canfield, D. E. Ferruginous conditions: a dominant feature of the ocean through Earth’s history. Elements 7, 107–112 (2011).\nWang, X. et al. Oxygen, climate and the chemical evolution of a 1400 million year old tropical marine setting. Am. J. Sci. 317, 861–900 (2017).\nSperling, E. A. et al. Redox heterogeneity of subsurface waters in the Mesoproterozoic ocean. Geobiology 12, 373–386 (2014).\nLuo, G. et al. Shallow stratification prevailed for ~1700 to ~1300 Ma ocean: evidence from organic carbon isotopes in the North China Craton. Earth Planet. Sci. Lett. 400, 219–232 (2014).\nTang, D., Shi, X., Wang, X. & Jiang, G. Extremely low oxygen concentration in mid-Proterozoic shallow seawaters. Precambrian Res. 276, 145–157 (2016).\nGuo, H. et al. Sulfur isotope composition of carbonate-associated sulfate from the Mesoproterozoic Jixian Group, North China: implications for the marine sulfur cycle. Precambrian Res. 266, 319–336 (2015).\nMei, M. Preliminary study on sequence-stratigraphic position and origin for molar-tooth structure of the Gaoyuzhuang Formation of Mesoproterozoic at Jixian section in Tianjin. J. Palaeogeogr. 7, 437–447 (2005).\nTian, H. et al. Zircon LA-MC-ICPMS U–Pb dating of tuff from Mesoproterozoic Gaoyuzhuang Formation in Jixian Country of North China and its geological significance. Acta Geosci. Sin. 36, 647–658 (2015).\nLi, H. et al. Further constraints on the new subdivision of the Mesoproterozoic stratigraphy in the northern North China Craton. Acta Petrol. Sin. 26, 2131–2140 (2010).\nMichard, A., Albarède, F., Michard, G., Minster, J. F. & Charlou, J. L. Rare-earth elements and uranium in high-temperature solutions from East Pacific Rise hydrothermal vent field (13 °N). Nature 303, 795–797 (1983).\nSholkovitz, E. R., Landing, W. M. & Lewis, B. L. Ocean particle chemistry: the fractionation of rare earth elements between suspended particles and seawater. Geochim. Cosmochim. Acta. 58, 1567–1579 (1994).\nCantrell, K. J. & Byrne, R. H. Rare earth element complexation by carbonate and oxalate ions. Geochim. Cosmochim. Acta. 51, 597–605 (1987).\nBau, M. Controls on the fractionation of isovalent trace elements in magmatic and aqueous systems: evidence from Y/Ho, Zr/Hf, and lanthanide tetrad effect. Contrib. Mineral. Petrol. 123, 323–333 (1996).\nNozaki, Y., Zhang, J. & Amakawa, H. The fractionation between Y and Ho in marine environment. Earth Planet. Sci. Lett. 148, 329–340 (1997).\nBau, M. & Koschinsky, A. Oxidative scavenging of cerium on hydrous Fe oxides: evidence from the distribution of rare earth elements and yttrium between Fe oxides and Mn oxides in hydrogenetic ferromanganese crusts. Geochem. J. 43, 37–47 (2009).\nGerman, C. R., Holliday, B. P. & Elderfield, H. Redox cycling of rare earth elements in the suboxic zone of the Black Sea. Geochim. Cosmochim. Acta. 55, 3553–3558 (1991).\nBau, M., Moller, P. & Dulski, P. Yttrium and lanthanides in eastern Mediterranean seawater and their fractionation during redox-cycling. Mar. Chem. 56, 123–131 (1997).\nTostevin, R. et al. Low-oxygen waters limited habitable space for early animals. Nat. Commun. 7, 12818 (2016).\nNothdurft, L. D., Webb, G. E. & Kamber, B. S. Rare earth element geochemistry of Late Devonian reefal carbonates, Canning Basin, Western Australia: confirmation of a seawater REE proxy in ancient limestones. Geochim. Cosmochim. Acta. 68, 263–283 (2004).\nBanner, J. L., Hanson, G. N. & Meyers, W. J. Rare earth elements and Nd isotopic variations in regionally extensive dolomites from the Burlington–Keokuk Formation (Mississippian): implications for REE mobility during carbonate diagenesis. J. Sediment. Petrol. 58, 415–432 (1988).\nZhang, K., Zhu, X. & Yan, B. A refined dissolution method for rare earth element studies of bulk carbonate rocks. Chem. Geol. 412, 82–91 (2015).\nPoulton, S. W., Frallck, P. W. & Canfield, D. E. The transition to a sulphidic ocean ~1.84 billion years ago. Nature 431, 173–177 (2004).\nClarkson, M. O. et al. Dynamic anoxic ferruginous conditions during the end-Permian mass extinction and recovery. Nat. Commun. 7, 12236 (2016).\nWood, R. A. et al. Dynamic redox conditions control late Ediacaran metazoan ecosystems in the Nama Group, Namibia. Precambrian Res. 261, 252–271 (2015).\nClarkson, M. O., Poulton, S. W., Guilbaud, R. & Wood, R. A. Assessing the utility of Fe/Al and Fe-speciation to record water column redox conditions in carbonate-rich sediments. Chem. Geol. 382, 111–122 (2014).\nPoulton, S. W. & Canfield, D. E. Development of a sequential extraction procedure for iron: implications for iron partitioning in continentally derived particulates. Chem. Geol. 214, 209–221 (2005).\nPoulton, S. W. & Raiswell, R. The low-temperature geochemical cycle of iron: from continental fluxes to marine sediment deposition. Am. J. Sci. 302, 774–805 (2002).\nRaiswell, R. & Canfield, D. E. Sources of iron for pyrite formation in marine sediments. Am. J. Sci. 298, 219–245 (1998).\nLing, H. et al. Cerium anomaly variations in Ediacaran–earliest Cambrian carbonates from the Yangtze Gorges area, South China: implications for oxygenation of coeval shallow seawater. Precambrian Res. 225, 110–127 (2013).\nLi, R., Chen, J., Zang, S. & Chen, Z. Secular variations in carbon isotopic compositions of carbonates from Proterozoic successions in the Ming Tombs Section of the North China Platform. J. Asian Earth Sci. 22, 329–341 (2003).\nGuo, H. et al. Isotopic composition of organic and inorganic carbon from the Mesoproterozoic Jixian Group, North China: implications for biological and oceanic evolution. Precambrian Res. 224, 169–183 (2013).\nCanfield, D. E., Raiswell, R., Westrich, J. T., Reaves, C. M. & Berner, R. A. The use of chromium reduction in the analysis of reduced inorganic sulfur in sediments and shales. Chem. Geol. 54, 149–155 (1986).\nThis work was supported by NSFC Grant 41430104 and CAGS Research Fund YYWF201603 to X.K.Z., a China Scholarship Council award to K.Z. and a China Geological Survey Grant DD20160120-04 to B. Yan. S.W.P. acknowledges support from a Royal Society Wolfson Research Merit Award. We thank L. Gao and P. Liu for field guidance, and F. Shi, C. Tang, X. Peng, C. Pan, N. Zhao, C. Bao, Z. Zhou, F. Zhang and Y. Guo for field-work assistance. We acknowledge F. Xu and M. Lv for assistance in the elemental analysis, Y. Xiong for help with the Fe-speciation experiments, Y. Shen, K. Chen and W. Huang for carbon isotope analyses and F. Bowyer for assistance with cathodoluminescence. We also express our thanks to J. Li, D. Li, Y. He, J. Ma, X. Zou and K. Du for logistical support.\nThe authors declare no competing interests.\nPublisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nAbout this article\nCite this article\nZhang, K., Zhu, X., Wood, R.A. et al. Oxygenation of the Mesoproterozoic ocean and the evolution of complex eukaryotes. Nature Geosci 11, 345–350 (2018). https://doi.org/10.1038/s41561-018-0111-y\nThe eukaryotic MEP-pathway genes are evolutionarily conserved and originated from Chlaymidia and cyanobacteria\nBMC Genomics (2021)\nScientific Reports (2021)\nProceedings of the Indian National Science Academy (2021)\nPhosphorus-limited conditions in the early Neoproterozoic ocean maintained low levels of atmospheric oxygen\nNature Geoscience (2020)\nMineralium Deposita (2019)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:122a711b-4236-48e4-a0f6-4c141b62b100>","<urn:uuid:ec9e0b60-3215-4c7c-a2e6-e4eff2069e08>"],"error":null}
{"question":"How to position players in youth baseball defense for ages 10 and above?","answer":"The key defensive positions are up the middle, including pitcher, second base, shortstop, and center field. The left side of the field should have strong arms, while the right side can have weaker arms. First baseman should be tall and able to catch well. Second baseman needs to be quick and field well. Shortstop should have good range and above-average arm. Third baseman needs a great arm and quick reflexes for the 'hot corner'. In the outfield, center field should be the fastest with a good arm, left field sees more action, and right field is suitable for developing players. The catcher needs a strong, accurate arm and shouldn't fear being behind the plate.","context":["It’s been said that crime wins games, yet protection wins champions. This is particularly real in baseball– when the various other team doesn’t rate, they desperate.\nGamers, particularly children, are usually just as sensitive to where they show up in the field as where they appear in the batting schedule. In the formative years of T-ball and also 8-and-underplay it is essential to give gamers a chance to play throughout the area, evaluating their arm toughness, catching capacity as well as knowledge of the game.\nBy the time players get to the age of 10 and older, it’s a lot easier to identify just what sort of protective player they are. Some youngsters will excel in the infield while others will have a knack for catching a fly round in the outfield.\nBegin crafting your defensive strategy with this guide, specially developed for youth players. Bear in mind that your approach will alter significantly for 14 and older age.\nYour Total Strategy\nYour crucial defensive placements are up the center. That means that your finest fielders should be a bottle, second base, shortstop as well as center field. The left side of the area needs to have solid arms, while the right side of the field could have weaker arms. If you comply with these general rules you’ll be positioned to win a fair quantity of video games this season.\nThe major high quality you’ll look for in an initial baseman is the ability to capture the sphere. They need to be active and also not be afraid to get in front of the sphere. This player is not worried to try anything to catch the baseball. A taller player is usually ideal, as they’ll provide a bigger target to the fielders as well as have the ability to extend to capture unreliable throws.\nEvery second baseman should fast on his feet. You’ll want to find a gamer that could field the round well by getting in front of it, as well as that, reveals some ability to backhand a ground ball. This player does not have to have the strongest arm, as most of his tosses will simply be to very first base or second base, both of which will be much less compared to 30 feet away if the gamer is effectively placed. This gamer shouldn’t be afraid of the ball.\nArguably, this needs to be just one of your ideal defensive players. The shortstop must reveal great array and also the capacity to field sharply struck baseballs. Choose somebody who has an above-average arm, as much of their tosses will certainly be a great distance.\nBecause this player has the lengthiest toss to first base, pick someone who has a wonderful arm– a player capable of making solid, accurate tosses that will certainly defeat runners to the bag. 3rd base is called the ‘warm corner’ for a factor. Third basemen cannot hesitate to get in front of the round, and also they need to be quick due to the fact that they’re responsible for fielding bunts a lot of the moment.\nAt this age, you really just need a player that can capture a fly around. In an initial couple of methods of the period, you’ll promptly uncover who has the ability to catch a fly ball as well as that does not. Capturing a fly ball needs speed and also excellent judgment. Occasionally at this age, it is difficult to come by both. If you have a team of gamers that you believe are outfielders, the best one must be placed in center field. This is a player that is extremely rapid and also has a good arm. They’ll be accountable for backing up spheres hit to left and right area, so they’ll get over there rapidly.\nThe left area tends to see more baseballs than the appropriate area in young age groups, so if you have a gamer that is having a hard time to develop, appropriate field is an area where they will certainly obtain the least action. Use techniques as a time to place them in a much more challenging outfield placement in order to earn that setting throughout game time.\nAll outfielders must have a strong arm, as they’ll be tossing to a cutoff man the majority of the time. But while a strong arm is essential, quickness is the most beneficial property in the outfield as they’ll obtain the sphere back to the middle of the area as quickly as feasible.\nIn the later years, the catcher rules the field. He supervises of every little thing from calling the pitch to readjusting the defensive positioning. At the younger ages, this need to be a gamer that has a strong and accurate arm, and also isn’t really scared to be behind the plate. Primarily, catchers supervise of keeping the baseball before them and also throwing away baserunners\nFor ages 10 and under they commonly aren’t most likely to be calling pitches, yet that does not detract from a should be baseball-wise. They need to acknowledge when the infield needs to be playing “in” and when the defense needs to be in a specialized placement, such as playing “no increases.\nThere’s a reason that the pitcher setting obtains a Gold Handwear cover honor every MLB season. It’s due to the fact that their job isn’t done after tossing the ball to the catcher. They’re expected to field the round easily and also make a great toss, also. If they’re pitching, it most likely implies they already have an exact as well as harder-than-average throw, so let’s skip that factor. Several baseballs will be struck to or around the bottle. If they can cut them off before they support him to an infielder, you’ll win several games. Since at this age the catcher isn’t quite mobile yet, the bottle will should area spheres hit delicately before them, as well as ones struck dramatically up the middle."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a6a4b7b4-5cf4-4596-b1a1-824bc9bf52e7>"],"error":null}
{"question":"What are the key differences between the early textile dyeing evidence from Israel's Arava desert and the blue pigments of Ancient Egypt in terms of their preservation and stability?","answer":"The textile dyes found in Israel's Arava desert, including red from madder plant and blue from woad plant, were wash-resistant but required specific preservation conditions. In contrast, Egyptian blue pigment proved exceptionally stable to environmental elements and has survived so well that it can still be detected today through its unique photoluminescence properties when illuminated with red light. Natural blue pigments like azurite were unstable in air and would transform into green malachite, while indigo dyes could fade quickly in sunlight. The superior stability of Egyptian blue led to its widespread use beyond Egypt, while textile dyes required more careful preservation.","context":["Tel Aviv University archaeologists have revealed that cloth samples found in the Israeli desert present the earliest evidence of plant-based textile dyeing in the region. They were found at a large-scale copper smelting site and a nearby temple in the copper ore district of Timna in Israel’s Arava desert and are estimated to date from the 13th-10th centuries BCE.\nThe wool and linen pieces shed light on a sophisticated textile industry and reveal details about a deeply hierarchical society dependent on long-distance trade to support its infrastructure in the unforgiving desert.\nThe study was published in PLOS ONE. It was led by Dr. Erez Ben-Yosef of TAU’s Department of Archaeology and Near Eastern Cultures and Dr. Naama Sukenik of the Israel Antiquities Authority; and conducted in collaboration with Vanessa Workman of TAU’s Department of Archaeology, Dr. Orit Shamir of the Israel Antiquities Authority and Dr. Zohar Amar, Dr. Alexander Varvak and Dr. David Iluz of Bar-Ilan University.\nTextiles suggest significant social stratification\n“This was clearly a formative period, with local kingdoms emerging and replacing Egyptian hegemony in Canaan,” Dr. Ben-Yosef said. “These beautiful masterpieces of weaving and dyeing — the first evidence of industrial dyeing at the time, of wash-resistant color on textile — support the idea of a strong, hierarchical Edomite Kingdom in Timna at the time.\n“It is apparent that there was a dominant elite in this society that took pains to dress according to their ‘class,’ and had the means to engage in long-distance trade to transport these textiles — and other materials and resources — to the desert.”\nThe research suggests a sophisticated dyeing process involving cooking colorful plants in water, then adding fleece fixed with alum to create a chemical bond between fabrics and dye. The result is a wash-resistant colorful fabric.\nThe researchers radiocarbon-dated the textile pieces and harnessed gas chromatography to identify the cloth’s organic molecules. They found “red” molecules produced from the madder plant and “blue” molecules from the woad plant.\n“Both plants were known in antiquity as sources of organic dyes,” said Dr. Ben-Yosef. “We know that these plants were used to create elaborate costumes during the Roman period, more than a thousand years later. Now we have evidence in the region of an Edomite society wearing textiles produced the same way, versus an earlier ‘primitive’ smearing of color on fabric.”\n“We can make many inferences according to this discovery,” Dr. Ben-Yosef continued. “To force a large group of people to work in dangerous mines in the desert, you need a strong ruling party — an elite that probably wore exquisite clothes to further distinguish themselves. The smelters, working in furnaces, were considered ‘magicians’ or even priests, and they probably wore fine clothing too. They represented the highest level of society, managing a sensitive and complex process to produce copper from rock.”\nEvidence of long-distance trade\nThe textile dye presents evidence of long-distance trade, Dr. Ben-Yosef noted. “Clearly this is not local. These plants require a lot of water and probably hail from the Mediterranean regions. The dyeing required special craftspeople, an entire industry that could not have subsisted in the desert. If Jerusalem was indeed opulent in the time of King Solomon, and the Temple covered in copper, we can assume a link to that kingdom.”\nThe textiles are currently being stored in special facilities at the Israel Antiquities Authority and will one day be presented in museums in Israel and elsewhere.","Colours of Ancient Egypt – Blue\nBy Anna Pokorska, on 16 October 2018\nThis is the second in the Colours of Ancient Egypt series; if you want to start at the beginning, click here.\nThe colour blue has already featured in a couple of posts in this blog (e.g. check out Cerys Jones’ post on why the Common Kingfisher looks blue) but it seems impossible to me to discuss colour, especially in Ancient Egypt, and not start with blue. Arguably, blue has the most interesting history of all the colours, which can be attributed to the fact that it is not a colour that appears much in nature – that is, if you exclude large bodies of water and the sky, obviously. Naturally occurring materials which can be made into blue colourants are rare and the process of production is often very time-consuming. In Ancient Egypt, pigments for painting and ceramics were ground from precious minerals such as azurite and lapis lazuli; indigo, a textile dye now famous for its use in colouring jeans, was extracted from plants.\nLeft: two pieces of azurite (Petrie Museum, UC43790); Right: lapis lazuli (Image: Hannes Grobe)\nHowever, all the above-mentioned colourants presented issues which limited their use. Azurite pigment is unstable in air and would eventually be transformed into its green counterpart, malachite. Lapis lazuli had to be imported from north-east Afghanistan (still the major source of the precious stone) and the extraction process would produce only small amounts of the purest colourant powder called ultramarine. Finally, indigo dyes can fade quickly when exposed to sunlight.\nAnd yet it seems that the Ancient Egyptians attributed important meaning to the colour blue and it was used in many amulets and jewellery pieces such as the blue faience ring, lapis lazuli and gold bracelet or the serpent amulet from the Petrie Museum collection (below).\nFrom left to right: blue faience ring with openwork bezel in form of uadjat eye (Petrie Museum, UC24520); lapis lazuli serpent amulet (UC38655); fragment of bracelet with alternative zig-zag lapis lazuli and gold beads (UC25970).\nTherefore, the race to artificially produce a stable blue colourant began rather early. In fact, the earliest evidence of the first-known synthetic pigment, Egyptian blue, has been dated to the pre-dynastic period (ca. 3250 BC). It was a calcium copper silicate (or cuprorivaite) and – although the exact method of manufacture has been lost since the fall of the Roman Empire – we now know that it was made by heating a mixture of quartz sand, a copper compound, calcium carbonate and a small amount of an alkali such as natron, to temperatures over 800°C.\nFragment of fused Egyptian blue (Petrie Museum, UC25037).\nThis resulted in a bright blue pigment that proved very stable to the elements and was thus widely used well beyond Egypt. In fact, its presence has recently been discovered on the Parthenon Marbles in the British Museum due to its unusually strong photoluminescence, i.e. when the pigment is illuminated with red light (wavelengths around 630 nm) it emits near infrared radiation (with a max emission at 910 nm).\nAfter its disappearance, artists and artisans had to make do with natural pigments and, being the most stable and brilliant, ultramarine became the coveted colourant once again. In fact, during the Renaissance, it is reputed to have been more expensive than gold and, as a result, often reserved for the pictorial representations of the Madonna and Christ. And so, the search for another replacement was back on. But it wasn’t until the early 1700s that another synthetic blue pigment was discovered, this time accidentally, by a paint maker from Berlin who, while attempting to make a red dye, unintentionally used blood-tainted potash in his recipe. The iron from the blood reacted with the other ingredients creating a distinctly blue compound, iron ferrocyanide, which would later be named Prussian blue. Naturally, other man-made blue pigments and dyes followed, including artificial ultramarine, indigo and phthalocyanine blues.\nHowever, it wasn’t quite the end of the line for Egyptian blue, which was rediscovered and extensively studied in the 19th century by such great people as Sir Humphry Davy. And not only are we now able to reproduce the compound for artistic purposes, scientists are finding more and more surprising applications for its luminescence properties, such as biomedical analysis, telecommunications and (my personal favourite) security and crime detection.\n Lorelei H. Corcoran, “The Color Blue as an ‘Animator’ in Ancient Egyptian Art,” in Rachael B.Goldman, (Ed.), Essays in Global Color History, Interpreting the Ancient Spectrum (NJ, Gorgias Press, 2016), pp. 59-82.\n Benjamin Errington, Glen Lawson, Simon W. Lewis, Gregory D. Smith, ‘Micronised Egyptian blue pigment: A novel near-infrared luminescent fingerprint dusting powder’, Dyes and Pigments, vol 132, (2016), pp 310-315.\n2 Responses to “Colours of Ancient Egypt – Blue”\nColours of Ancient Egypt – Introduction | UCL Researchers in Museums wrote on 11 January 2019:\n[…] can now read about the colours blue and […]\n[…] Colours of Ancient Egypt – Blue […]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4eaea9fb-3ad8-4c35-ac24-8c3cac27fa79>","<urn:uuid:2cabec81-3fc7-423b-b6f6-6f7652a65d85>"],"error":null}
{"question":"What is the main difference between a simile and personification in poetry?","answer":"The main difference is that a simile directly compares one thing to another using words like 'like' or 'as' (for example 'life was like a box of chocolates'), while personification gives human attributes to non-human things or ideas. Personification can be viewed as a special kind of simile or metaphor, where the comparison specifically involves giving human characteristics to inanimate objects.","context":["Two common figures of speech in English are the simile and the metaphor. Sometimes their functions are confused or mistaken. In this discussion we’ll further explore what a metaphor means and how it can enhance our writing when properly used.\nWhat Does Metaphor Mean?\nBefore we focus on the meaning and function of a metaphor, let’s first establish its difference from a simile. Both similes and metaphors help to further understanding and expand imagination concerning thoughts and ideas. They achieve this by creating figurative rather than literal associations.\nA simile directly compares one thing and another, mostly commonly by linking them with the word like or as. For example, a popular line in the movie Forrest Gump was “My momma always said life was like a box of chocolates; you never know what you’re going to get.” The comparison between life and a box of chocolates is direct and immediate.\nBy definition, a metaphor is an expression of nonliteral resemblance in a way that is implied rather than stated, as it is in a simile. In making its comparative relation, the metaphor will have a primary, concrete term or concept and a secondary one to provide figurative depth and clarity. Unlike a simile, a metaphor will not compare things by using the words like or as.\nBecause of the looser comparison it makes, a metaphor allows more room to evoke feeling and imagery. By creating a more-abstract association between things, it prompts the reader to think further about the logic and truth of the relation, which can heighten emotion or intellectual awareness concerning it.\nA metaphor states that one thing is another thing not because they are the same, but because they share traits. Returning to our Forrest Gump example, if a simile directly compares life to a box of chocolates, a metaphor might express life more suggestively in the following way:\nLife is the fallen branch moved along by the rush of the river, always pushed and turned and spun, sometimes gently, sometimes furiously, unsure of where it will land but certain to get there.\nThe same example could be stated as simply as life is the fallen branch moved along by the river. Clearly, the comparison in this model is neither direct nor exact.\nLet’s look at a few more examples of metaphorical expressions:\n|black sheep||angel in disguise||My desk is an industrial spill of paper.|\n|heart of stone||the elephant in the room||Her smile gives us light.|\n|music to my ears||They left us high and dry.||You are the peace in my storm.|\nAvoid Mixed Metaphor\nAs a good writer, you will want to avoid mixed metaphors, which combine two or more incompatible images that become nonsensical.\nIn other words, we don’t want to start by comparing life to a fallen branch and end the comparison with an aluminum can.\nLife is the fallen branch moved along by the river, pushed and turned and spun, a soda can on its wandering way to the recycling plant.\nWe have now created two unrelated metaphors to illustrate the primary concept, thereby sapping the strength and clarity of the comparison.\nSimilarly, we would refrain from mixed-metaphor mash-ups such as:\nA pillar of courage, he will blaze a trail of glory through fields of gold where diamonds might be found among the coal.\nBy understanding what a metaphor means, how it functions, and what it can achieve when used with taste and restraint, we can make our writing even richer with eloquence.\nSimiles and Metaphors\nNow that you’re more familiar with what a metaphor means, identify if each expression includes a metaphor or not.\n1. I think Robby is turning into a couch potato.\n2. That bodybuilder might as well be made of steel.\n3. That shirt is as American as apple pie and baseball.\n4. The company decision-makers need to stop living in a silo.\n5. I’m so tired I’m going to sleep like a log tonight.\nPop Quiz Answers\n1. I think Robby is turning into a couch potato. metaphor\n2. That bodybuilder might as well be made of steel. metaphor\n3. That shirt is as American as apple pie and baseball. no metaphor\n4. The company decision-makers need to stop living in a silo. metaphor\n5. I’m so tired I’m going to sleep like a log tonight. no metaphor\nIf the article or the existing discussions do not address a thought or question you have on the subject, please use the “Comment” box at the bottom of this page.","Personification, as you already know, is a figure of speech in which a thing, an idea or an animal is given human attributes. The non-human objects are portrayed in such a way that we feel they have the ability to act like human beings. 25 Examples Of Poems With Personification Personification can be used to emphasize a point in your writing. It can also be used to make a reader understand something you're trying to say. Personification is a way to add more description to your poem by giving human characteristics to inanimate objects. Personification through Poetry - SAS - pdesas.org This allows the students to examine how personification can be used to write poetry and make poetry more effective for the reader. The students will also be provided with the experience of writing poetry based on their understanding to make the real-world connection of how personification can be used in poetry. Writing 101: What Is Personification? Learn About ...\nPersonification - Simple English Wikipedia, the free encyclopedia\nWriting a poem is an exciting adventure. Even if you think that you are not naturally creative, you can still write a great poem. It is a challenging but fun process, which is a personal and unique way to express yourself. How to Write an I Remember Poem | Pen and the Pad Memories make rich subject matter for poetry. Whether you write about nostalgia for a carefree summer vacation or the sting of a painful loss, I Remember poems bring to mind not only the emotions you felt at the time, but numerous sensory… How to Write a Free Verse Poem (with Sample Poems) - wikiHow How to Write a Free Verse Poem. Let's say you're working on homework or an assignment for school and you're all set to start. There's only one problem: you don't know how to write a free verse poem! How to Write a Poem (with 3 Sample Poems) - wikiHow How to Write a Poem. Writing a poem is about observing the world within or around you. A poem can be about anything, from love to loss to the rusty gate at the old farm. Writing poetry can seem daunting, especially if you do not feel you.\nA collection of downloadable worksheets, exercises and activities to teach Personification, shared by English language teachers. Welcome to ESL Printables , the website where English Language teachers exchange resources: worksheets, lesson plans, activities, etc.\n5 Ways: How to Write a Poem - How to Write Poetry\nBecause personification involves making a comparison, it can be viewed as a special kind of simile (a direct or explicit comparison) or metaphor (an implicit comparison). In Robert Frost's poem \"Birches,\" for example, the personification of the trees as girls (introduced by the word \"like\") is a type of simile:\nBy using personification, you are able to paint an image to the minds of the people you are trying to convey an image or expanded to greater lengths the vision and imagination of the receiver of your message. You can use personification for when you are writing a poem, a song, or a story because this can help you describe and explain things more. Creative writing personification - palgroup.org Printable writing club shared with personification is a personification m. Techniques are fifty examples of the table and. Invite kids and phrases for outstretched fingers to personify a mini lesson was inspired by a winter. Persona is and teach them to writing one of personification poem or personification is given human characteristics. Lesson: Writing Personification Poetry - BetterLesson This lesson attempts to engage children in writing free-verse poetry, using their powers of observation, imagination, and personification. Children are introduced to the idea of personification by asking them to do what many children do so easily, imagine. This lesson can stand alone, or be taught as part of a unit on poetry. Creative writing personification | Clore Automotive\nPersonification - Definition and Examples | LitCharts\nDirections for Writing a Personification Poem Identifying the Subject's Attributes. As with any example of personification,... Selecting Strong Words. Using your subject’s attributes as a starting point,... Choosing a Poetic Form. From the highly structured sonnet to the more structurally ... Personification Poetry Lesson Plan - Kenn Nesbitt's ... Then, they will create their own poetic sentences and short poems using personification. Personification means using human qualities or actions to describe an object or an animal. The word “personification” actually contains the word “person,” and to personify an object means to describe it as if it were a person. Write a Poem Using Personification - VisiHow Now it's your turn. Try and write your own poems using personification, and then share them with us in the section below. In this section, you can also leave any questions or queries. You'll also find links to the other videos in this poetry series. You've been watching a video on how to use personification in poetry.\nPoetry Writing: 10 Tips on How to Write a Poem | Jerz's ... Jerz > Writing > General Creative Writing Tips [ Poetry | Fiction ]If you are writing a poem because you want to capture a feeling that you experienced, then you don't need these tips. Just write whatever feels right. Only you experienced the feeling that you want to express, so only you w Highhill Homeschool: Personification Writing Activities for Kids We wrote personification poetry and short stories. When animals or objects take on human traits they are being personified. Children's books and poetry are full of personification as there are often talking animals. Personification Poetry We began by reading several poems with personification such as The Table and the Chair by Edward Lear. Then ... Personification, the Ocean | Teen Ink The ocean tickled my feet. The waves ran across the shore. It wrapped around me and gave me a hug. The ocean sang while it crashed on the shore. It danced through the rocks."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:36b737e4-23fe-4db2-be13-6904fadd7b3a>","<urn:uuid:d766c9a0-66ca-4d68-863d-b4fcc16f3e6f>"],"error":null}
{"question":"Do both the public budgeting process and Kwanzaa celebrations incorporate symbolic physical objects in their practices?","answer":"Yes, both incorporate symbolic physical objects. Public budgeting utilizes online tools and publicly accessible documents for budget dissemination and discussion. Kwanzaa employs several physical symbols including a kinara (seven-candle holder), decorative mat, communal cup (Kikombe cha Umoja), corn and other crops, posters of the seven principles, and a black, red, and green flag. These objects are used to facilitate participation and represent core principles in both practices.","context":["- Pattern Languages\n- Liberating Voices (English)\n- Liberating Voices (other languages)\n- Liberating Voices (Arabic)\n- Liberating Voices (Chinese)\n- Liberating Voices (French)\n- Liberating Voices (German)\n- Liberating Voices (Greek)\n- Liberating Voices (Hebrew)\n- Liberating Voices (Italian)\n- Liberating Voices (Korean)\n- Liberating Voices (Portuguese)\n- Liberating Voices (Russian)\n- Liberating Voices (Serbian)\n- Liberating Voices (Spanish)\n- Liberating Voices (Swahili)\n- LIBERATING VOICES (VIETNAMESE)\n- Civic Ignorance (English)\n- Digital Resources\nPattern number within this pattern set:395\nUniversity of Washington Evans School\nDeveloping a budget is a task often left to financial \"experts\" even though the decisions that result from the budget-making process impact everyone, and the ideas that inform budget decisions often are improved by the experience and insights of a wide range of individuals. Budget development is in fact a \"political\" act, with \"winners\" and \"losers\" most of whom never participate in the process.\nProperly understood, budgets and the budget development process are tools through which social values are expressed and manifested in useful public activity. This pattern explains the importance of budgeting and encourages participation in all stages of budget development. Public budgeting connects to several other patterns. For example: participating in the creation of budgets is an ideal way to foster Civic Intelligence (pattern 1); joint budget development helps create Shared Vision (pattern 9); public budgeting via online tools is an example of Using Collaborative Technologies for Civic Accountability (pattern 26); and understanding budgets is one aspect of Power Research (pattern 293).\nA fundamental step in the life of any organization is the design of a budget. The decisions which are made early in the process (e.g., What is to be budgeted for? What are the sources of income? Who is to be paid? What are the categories of effort which are highly compensated and what effort is to be considered voluntary?) often set core parameters for the future, and impact not only the ways in which time and money are spent, but also the values and reputation of the organization, and even its soul. But budgeting is often treated as a \"technical\" process which should be handled by experts rather than as a political activity in which many people should be invited and encouraged to participate. One way in which budgets can be more easily discussed publicly is to use online tools to disseminate budget information, host public discussions, and create sample budget variations -- though from our experience, we believe this should be coupled with face-to-face discussions whenever possible. The best-known example of participatory budgeting is found in Porto Alegre, Brasil, where community residents (now numbering in the thousands) have cooperated since 1989 in annual deliberations about the allocation of a portion of the municipal budget. Poor citizens are vastly more engaged in this process than is typical in budgeting processes, and increasing proportions of the city's revenue have been directed towards improving the most impoverished parts of Porto Alegre. While there is some disagreement over how much of this outcome to attribute to the participatory budgeting process, there is no doubt about the increased sensitivity of all citizens to the importance of budgeting decisions. In June, 1996, the United Nations declared the \"popular administration\" of Porto Alegre as one of forth urban innovations at the Second Conference on Human Settlements. Various related experiments in participatory budgeting have taken place on several continents since Porto Alegre, typically fine-tuned to local circumstances, with an evolving set of principles promoting conditions that enhance the effectiveness of the process. Several important attempts at involving typically excluded citizens in the budget allocation process have occurred in the U.S. -- often during progressive periods. Two of the most significant were the Affirmative Neighborhood Information Program during Mayor Harold Washington's tenure in Chicago (Kretzmann, 1992), which failed to survive successor administrations; and the Seattle Public Schools multi-year experiment in decentralized, \"school-based\" budgeting, supported by an online budgeting tool (Halaska, 2000). In the Seattle experiment, a vastly increased proportion of district resources was redistributed from the central administrative offices to individual schools. School principals were encouraged to engage in a public budgeting process where trade-offs (e.g. reduced class size vs after-school music programs) were actively debated -- both in public meetings and online. The process was messy because \"democracy is messy\" and was controversial at every stage, in part because it surfaced hidden assumptions about core values in public education. Some participants believed that this process had the potential to provoke a fundamental rethinking of the purposes of the education process itself. Key findings from the Chicago and Seattle experiments align with the principles of Porto Alegre and elsewhere. For example, it is important that significantly different approaches to budgeting such as these become so embedded that they cannot readily be set aside by later regimes. Equally critical is that traditional budget staff be convinced about the importance of participatory budgeting. While philosophical and political discussions about larger scale budget issues can be done without technical assistance, detailed information about current costs and funding formulas typically reside with budget staff. Without their support, key budget information can be difficult to obtain. Moreover, while the ideology of participatory budgeting has wide appeal, critical studies should be undertaken to determine under what circumstances participatory strategies have lasting effects and whether, in the case of participatory budgeting for example, systemic changes such as in the labor market must occur for poorer citizens to benefit from these new strategies in the long run.\nBudgets for organizations in the public sphere should be developed openly and inclusively, in public meetings and using publicly accessible online tools. Budget assumptions should be discussed, and rethinking of assumptions, priorities, and allocations should be encouraged, no matter how far they depart from current practice. At every stage, the results of the process should be made public for feedback and refinement. Attention should be paid to what has been learned from experience (for example, about the wisdom of convincing traditional budget staff of the utility of public budgeting), and studies of the long-range impact of participatory budgeting are essential.","Kwanzaa celebration with its founder, Maulana Karenga, and others\n|Observed by||African Americans|\n|Type||Cultural and ethnic|\n|Significance||Celebrates Black heritage, unity and culture.|\n|Date||December 26 until January 1|\nCollective Work and Responsibility\n|Related to||Black History Month|\nKwanzaa is a week long celebration held in the United States to honor universal African heritage and culture. People light a kinara (candle holder with seven candles) and give each other gifts. It takes place from December 26 to January 1 every year. It was created by Maulana Karenga and was first celebrated in 1966 - 1967. Non-African Americans also celebrate Kwanzaa. The holiday greeting is \"Joyous Kwanzaa\".\nHistory and naming of the holiday[change | edit source]\nKwanzaa is a celebration that started in the black nationalist movement of the 1960s. It was created as a way to help African Americans reconnect with their African cultural and historical heritage.\nDuring the early years of Kwanzaa, Karenga said that it was meant to be an alternative to Christmas, that Jesus was psychotic, and that Christianity was a white religion that black people should shun. However, as Kwanzaa became more popular, Karenga changed his position so that practicing Christians could also feel included. He stated in the 1997 Kwanzaa: A Celebration of Family, Community, and Culture, \"Kwanzaa was not created to give people an alternative to their own religion or religious holiday.\"\nMany Christian African Americans who celebrate Kwanzaa do so in addition to observing Christmas.\nPrinciples and symbols[change | edit source]\nKwanzaa celebrates what its founder called the seven principles of Kwanzaa, or Nguzo Saba (originally Nguzu Saba—the seven principles of blackness). Karenga said that this \"is a communitarian African philosophy,\" . It consists of what Karenga called \"the best of African thought and practice in constant exchange with the world.\" These seven principles comprise Kawaida, a Swahili term for tradition and reason. Each of the seven days of Kwanzaa is dedicated to one of the following principles, as follows:\n- Umoja (Unity): To strive for and to maintain unity in the family, community, nation, and race.\n- Kujichagulia (Self-Determination): To define ourselves, name ourselves, create for ourselves, and speak for ourselves.\n- Ujima (Collective Work and Responsibility): To build and maintain our community together. To make our brothers' and sisters' problems our problems, and to solve them together.\n- Ujamaa (Cooperative Economics): To build and maintain our own stores, shops, and other businesses and to profit from them together.\n- Nia (Purpose): To make our collective goal the building and developing of our community. This in order to restore our people to their traditional greatness.\n- Kuumba (Creativity): To do always as much as we can, in the way we can. This so that we can leave our community more beautiful and better than we inherited it.\n- Imani (Faith): To believe with all our heart in our people, our parents, our teachers, our leaders, and the righteousness and victory of our struggle.\nKwanzaa symbols include a decorative mat on which other symbols are placed, corn and other crops, a candle holder with seven candles, called a kinara, a communal cup for pouring libations, gifts, a poster of the seven principles, and a black, red, and green flag. The symbols were designed to convey the seven principles.\nDuring Kwanzaa, families also decorate their households with objects of art. They use colorful African cloth such as kente, especially the wearing of kaftans by women. Fresh fruits that represent African idealism are also used. It is normal to include children in Kwanzaa ceremonies and to give respect and gratitude to ancestors. Libations are shared, generally with a common chalice (a shared cup), Kikombe cha Umoja, passed around to all people present. A Kwanzaa ceremony may include drumming and musical selections, libations, a reading of the African Pledge and the Principles of Blackness, reflection on the Pan-African colors, a discussion of the African principle of the day or a chapter in African history, a candle-lighting ritual, artistic performance, and, finally, a feast (Karamu). The greeting for each day of Kwanzaa is Habari Gani? which is Swahili for \"What's the News?\"\nCultural exhibitions include the Spirit of Kwanzaa, an annual celebration held at the John F. Kennedy Center for the Performing Arts featuring interpretive dance, African dance, song and poetry.\nRelated pages[change | edit source]\n- The Black Candle (2009) – a documentary film about Kwanzaa, narrated by Maya Angelou\n- Dashiki – A shirt or suit worn during Kwanzaa celebrations\n- Kaftan (boubou) – A dress worn by women during Kwanzaa celebrations\n- Kufi – A cap worn during Kwanzaa celebrations\nFurther reading[change | edit source]\n- A program to raise the faith level in African-American children through Scripture, Kwanzaa principles, and culture, Janette Elizabeth Chandler Kotey, DMin, Oral Roberts University,1999\n- The US Organization: African American cultural nationalism in the era of Black Power, 1965 to the 1970s, Scot D. Brown, PhD, Cornell University, 1999\n- Rituals of race, ceremonies of culture: Kwanzaa and the making of a Black Power holiday in the United States,1966—2000, Keith Alexander Mayes, PhD, Princeton University, 2002\n- Interview: Kwanzaa creator Ron Karenga discusses the evolution of the holiday and its meaning in 2004, conducted by Tony Cox. Tavis Smiley (NPR), 26 December 2003\n- Tolerance in the News: Kwanzaa: A threat to Christmas? By Camille Jackson | Staff Writer, Tolerance.org, 22 December 2005\nReferences[change | edit source]\n- \"\"Why Kwanzaa Video\"\". \"Ron Karenga\". http://www.africanholocaust.net/news_ah/kwanzaa.html.\n- Keith Mayes, cited by Megan K. Scott, \"Kwanzaa celebrations continue, but boom is over\", Buffalo News, 17 December 2009. Accessed 25 December 2009.\n- Bush, George W. (2004-12-23). \"Presidential Kwanzaa Message, 2004\". Office of the Press Secretary. http://georgewbush-whitehouse.archives.gov/news/releases/2004/12/20041223-2.html. Retrieved 2007-12-24.\n- \"Clinton offers holiday messages\". CNN. 1997-12-23. http://www.cnn.com/ALLPOLITICS/1997/12/23/message/. Retrieved 2007-12-24.\n- Gale, Elaine (1998-12-26). \"Appeal of Kwanzaa continues to grow; holidays: today marks start of the seven-day celebration of African culture, which began in Watts 32 years ago and is now observed by millions.\". Los Angeles Times. http://pqasb.pqarchiver.com/latimes/access/37610058.html?dids=37610058:37610058&FMT=ABS&FMTS=ABS:FT&date=Dec+26%2C+1998&author=ELAINE+GALE&pub=Los+Angeles+Times&desc=Appeal+of+Kwanzaa+Continues+to+Grow%3B+Holidays%3A+Today+marks+start+of+the+seven-day+celebration+of+African+culture%2C+which+began+in+Watts+32+years+ago+and+is+now+observed+by+millions.&pqatl=google. Retrieved 2007-12-24.\n- Megan K. Scott, \"Kwanzaa celebrations continue, but boom is over\", Buffalo News, 17 December 2009. Accessed 25 December 2009.\n- Karenga, Maulana (1967). \"Religion\". In Clyde Halisi, James Mtume. The quotable Karenga. Los Angeles: University of Sankore Press. pp. 25. 23769.8. http://www.piratepundit.com/karenga6.html.\n- J. Lawrence Scholer, \"The story of Kwanzaa\", Dartmouth Review, 15 January 2001.\n- Williams, Lena (1990-12-20). \"In Blacks' Homes, the Christmas and Kwanzaa Spirits Meet\". The New York Times. http://www.nytimes.com/1990/12/20/garden/in-blacks-homes-the-christmas-and-kwanzaa-spirits-meet.html?pagewanted=1. Retrieved 2010-05-07.\n- \"The Symbols of Kwanzaa\". http://www.officialkwanzaawebsite.org/symbols.shtml. Retrieved 2010-12-24.\n- Kwanzaa Greeting\n- A Model Kwanzaa Ceremony\n- The Spirit of Kwanzaa\n- The Dance Institute of Washington\nOther websites[change | edit source]\n- The Official Kwanzaa Web Site\n- The Black Candle: a Kwanzaa Celebration by Maya Angelou\n- Why Kwanzaa was created by Karenga\n- The History Channel: Kwanzaa"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:0c82d7e6-06ed-424c-8f4d-43520d16eaf8>","<urn:uuid:401ac1a4-e8a9-4eac-839e-4ebd15ba34fc>"],"error":null}
{"question":"¿Cuáles son las diferencias fundamentales entre la configuración MIDI del Alesis SR18 y el M-Audio Hammer 88 en cuanto a sus conexiones y requisitos de alimentación?","answer":"The Alesis SR18 and M-Audio Hammer 88 have several key differences in their MIDI setup and power requirements. The Alesis SR18 requires a USB MIDI cable interface (like the Cakewalk UM-1G) for connection and needs specific driver installation and MIDI settings configuration (CLOCK IN: OFF, DRUM OUT: ON, DRUM IN: ONV1, MIDI CH: 12). In contrast, the M-Audio Hammer 88 is USB-powered with plug-and-play connectivity, requiring no drivers and featuring a direct 5-pin MIDI output for external devices. The Hammer 88 can also operate standalone with an optional power adapter, while the SR18 setup requires additional hardware like a mixer board or sound card for audio output.","context":["I recently ran into minor but fixable problems setting up my Alesis SR18 up with FL Studio and I now have it working 100%! It only took me a couple of days to tinker with the settings and now I have FL Studio and the drum machine communicating with one another. This is how I did it.\nI will make a quick and brief tutorial here. For those hooking up the Alesis SR18 Drum Machine up with FL Studio (FL Studio), you will need:\n1. The drum machine and FL Studio of course.\n2. A USB MIDI cable. I have the Cakewalk UM-1G MIDI Interface, google it, it's the best because it has indicator lights.\n3. An optional pair of two 1/4 instrument cables. This is if you want to use the audio from the drums for your drum sounds. You will need a mixer board or a sound card with either 1/8 or 1/4 instrument cable inputs. A cheap Alesis MultiMix 4 desktop USB mixer works the best, and it's compatible with ALL versions of Windows including 32 and 64 bit of Windows Vista and Windows 7.\nSet up the drum machine and learn how to use it and set up patterns. Once you learn the basics, you're ready to work with FL Studio! Make sure you have the drivers installed for the Cakewalk MIDI interface adapter properly. It says it does not support 64 bit Windows on the box but it DOES. Go to their website and they have drivers for all the new versions of Windows. Plug the drum machine into the appropriate MIDI in and out outputs on the drum machine. Make sure the IN part of the cable goes into the IN output and the OUT goes into the OUT output!!!\nNow, if you are using the Cakewalk USB interface I have, make sure on the device that \"ADVANCED DRIVER\" is set to ON and \"MIDI THROUGH\" is set to OFF. Again, MIDI THROUGH is set to OFF, not on like you would think it should be...\nTurn on the drum machine and launch FL Studio. Press F10 and bring up the MIDI options. The UM-1G interface should show up on the list. Select it. Now go onto your drum machine and bring up the SYSTEM SETUP. Here are how the following settings should be:\nCLOCK IN: OFF\nDRUM OUT: ON\nDRUM IN: ONV1\nMIDI CH: 12\nThe rest of the settings can be left default. If you are using the drum machine as the master device and/or want to sync it with the tempo in FL Studio, you will have to play with the CLOCKOUT and CLOCK IN settings. Make sure you save your settings (hold the SAVE/COPY button and press the red \"REC (enter)\" button and the screen should say SAVED).\nIn your FL Studio MIDI settings screen, set the port number to 10 and all other channel numbers to 10. Make sure \"Enable\" is set for enabling input from the device. Select the generic controller, leave all other settings default, and close the window.\nNow go to \"Channels\" and \"Add One..\" and select \"MIDI Out.\" A new MIDI Output channel is created. Open it, change the MIDI Channel to 12 and the port number to 11. You should now be able to play notes in the FL Studio Piano Roll and you can now record MIDI patterns from the drum machine into FL Studio! How awesome is that?!\nWant to record the drum patterns neatly into FL Studio? Just hit \"Wait for input to start playing\" up top in the FL Studio toolbar and FL Studio will not play nor record until you hit play on the drum machine.\nI hope this tutorial helps others with the SR18 drum machine get started with their drum machine. This method should also work with the SR16 as well, which I learned to use in school (how do you think I know so much about the SR18?).\nDiscuss MIDI Controller support/issues here.\n10 posts • Page 1 of 1\n[You can only see part of this thread as you are not logged in to the forums]\nThanks for this tutorial. I have a noob questi...\nbhustan wrote:Thanks for this tutorial. I have...","5-Pin MIDI output to trigger external MIDI devices. Pitch bend, modulation, volume and ± controls for expressive performances.\nMultiple keyboard zones for layering, splits and 4-note chords with a single key press. Inputs for Sustain, Expression and Soft pedals. USB-powered, no power supply required. No drivers required, supports plug-and-play connectivity to your Mac or PC. Clean, professional editing with included Hammer 88 Controller Editor.Power adapter (optional) for stand-alone use. This Ones For The Player.\nIntroducing Hammer 88, a premium, no-compromise keyboard controller, ideal for professional, semi-pro players or students seeking the realistic grand piano feel to use with virtual instruments or sound modules. At the heart of Hammer 88 are 88 velocity-sensitive, fully-weighted hammer-action keys that are guaranteed to faithfully capture every subtle nuance of your performance, while providing the unmatched response of a traditional grand pianoclassic feel fused with the unrivalled sonic capabilities of virtual instruments. Its the perfect combination for performers and producers to experience their plugin collection like never before. Put expression center-stage with Hammer 88, thanks to a core range of controls, thoughtfully positioned to complement and enhance your performance. With a unity-locking pitch wheel and a separate modulation wheel, add portamento and glissando effects, or map the Mod-Wheel to any assignable parameter in your virtual instrument collection to reshape and evolve your sound on the fly.\nWith the dedicated Master Volume Fader, you can instantly control the intensity of your performance or MIDI map this to any compatible parameter in your DAW or virtual instrument collection for expanded, hands-on control. Use the assignable Up and Down buttons to select different programs, effects, MMC messages, jump between octaves and more.\nAdd some pedal-based performance articulation using the three dedicated inputs for Sustain, Expression and Soft. With Hammer 88s performance-centric feature-set, expressing yourself has never been so easy!Dont limit yourself to your computer-based virtual instruments! Take control of your studio equipment with Hammer 88. With its conveniently-located 5-pin MIDI Output, you can send all MIDI information to any compatible peripheral device, including hardware synthesisers, sound modules and drum machines.\nWith Hammer 88, its easy get the most out of your external sound generators. This is a controller designed from the ground up to place you firmly in the drivers seat. An instrument shouldnt limit your creativity, it should enhance it! Hammer 88 is USB-powered, class-compliant and supports USB-MIDI connectivity for rapid, hassle-free setup that lets you focus on whats importantyour performance. Pro Tools' First, the industry standard for recording software, is now included with M-Track C-Series Audio/MIDI interfaces, as well as our renowned CTRL, Code, Oxygen, Hammer and Keystation USB/MIDI keyboard controllers.\nThis amazing audio recording software helps inspire any artist, traveling musician or singer-songwriter to create, record and share all of their ideas across the world at any time. Pro Tools' First features Unlimited Busses, Elastic Time and Elastic Pitch, Offline Bounce and 1 GB of free cloud storage space for collaboration or accessing your projects from any computer anywhere that is connected to the internet.This truly amazing software recording package is primed to get your ideas out of your imagination and amplified out into the world, enjoy! Chop up and mix 2 gigs of samples ranging from deep ambient synth pad loops to vintage drum one shots. These are all designed to inspire your music production and provide professional sounding samples that will shine in your mix. Touch Loops perfectly captures the tone and intensity of each sample, so they enhance and complement any song, wherever you use them. This incredible value can be found in your M-Audio account, ready to be downloaded and chopped to your liking! Keys: 88, velocity-sensitive, weighted keys. Connections: USB port, sustain pedal, soft pedal, expression pedal, MIDI Out. Dimensions: 11.9 x 5.0 x 55.9. PC: Windows 7 or later. Mac: OS X 10.8 or later.\nThank you for shopping with us! \"We Sell Dreams\" is our core philosophy and we do this with integrity and transparency. The easiest and safe way to pay! This listing is currently undergoing maintenance, we apologise for any inconvenience caused.The item \"M-Audio Hammer 88 88-Key Hammer-Action USB MIDI Keyboard Controller\" is in sale since Friday, May 11, 2018. This item is in the category \"Musical Instruments & Gear\\Pianos, Keyboards & Organs\\Electronic Keyboards\". The seller is \"*sweetheartdeals*\" and is located in Spencer, Massachusetts.\nThis item can be shipped to United States, Canada, United Kingdom, Denmark, Romania, Slovakia, Bulgaria, Czech republic, Finland, Hungary, Latvia, Lithuania, Malta, Estonia, Australia, Greece, Portugal, Cyprus, Slovenia, Japan, China, Sweden, South Korea, Indonesia, Taiwan, Thailand, Belgium, France, Hong Kong, Ireland, Netherlands, Poland, Spain, Italy, Germany, Austria, Bahamas, Israel, Mexico, New Zealand, Philippines, Singapore, Switzerland, Norway, Saudi arabia, United arab emirates, Qatar, Kuwait, Bahrain, Croatia, Malaysia, Brazil, Chile, Costa rica, Panama, Trinidad and tobago, Guatemala, Honduras, Jamaica, Aruba, Belize, Dominica, Grenada, Saint kitts and nevis, Turks and caicos islands, Bangladesh, Bermuda, Brunei darussalam, Bolivia, Ecuador, Egypt, French guiana, Guernsey, Gibraltar, Guadeloupe, Iceland, Jersey, Jordan, Cambodia, Liechtenstein, Luxembourg, Monaco, Macao, Martinique, Maldives, Nicaragua, Peru, Pakistan, Paraguay, Reunion, Viet nam, Uruguay."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9191a683-a4b8-49e5-82da-4422c82508f0>","<urn:uuid:79edb081-eefd-43c7-be7e-c3d57ad46fe6>"],"error":null}
{"question":"Dime, cual genera menos impacto ambiental: la carne de pollo o la carne de res?","answer":"Chicken meat has a much lower environmental impact than beef. According to the Environmental Working Group (EWG), beef produces more than 13 times the emissions of vegetable proteins and two times as much emissions as pork. In contrast, chicken has the lowest environmental footprint of any meat. This is primarily because chickens are the most efficient converters of feed into meat among all land-based livestock species, requiring only about 1.6 kg of feed for every kg of body weight. Additionally, modern intensive chicken farming systems optimize growth and minimize resource use, resulting in more efficient use of feed, energy, and water, while producing less greenhouse gas emissions.","context":["It is not difficult to eat food that is healthy both for people and the environment. Fortunately most of the food that is wholesome, is also eco-friendly. You don't have to give up some favorites to lessen your impact; instead, choose your recipes with care to boost and extend environment protection through responsible food consumption. Small changes to your daily meals will add up.\n1. Chocolate Chip Organic Granola Bar\nA chocolate chip granola bar is organic and packed with healthy oats, nut and dry fruits and raw honey. It serves not just as breakfast, but also as a snack for adults and children. The recipe that uses boiling instead of baking cuts down the number of steps by half. Since a large amount is prepared in a batch and stored, this saves energy used for multiple meals.\nA straightforward way to eco-friendly food is to buy organic. The benefits to one's health and the environment make them nearly unbeatable. Any recipe can be made organic by using organic products purchased when they are in season or offered on discount.\n2. Strawberry Smoothies\nStrawberry smoothies are a no-cook breakfast that has five simple ingredients prepared in three steps. Instead of the usual vanilla flavored yogurt, tofu and soymilk are used to make this vegan meal for hurried mornings.\nThe noteworthy ingredients are the frozen fruits that extend the shelf life of abundant produce (creating less food waste, making it very eco-friendly) without impacting taste or nutritional value. Frozen fruits are also often cheaper to purchase.\n3. Pumpkin Soup\nPumpkin soup is vegetarian and the main element is a seasonal vegetable which can be purchased in farmers markets. Since local produce is picked at the right time, it is more nutritious, one is sure to get it fresh, and there are less risks of it getting contaminated during transport or storage. Conventional food transportation produces 5-17 times more carbon dioxide than food sourced locally in a radius of 100-400 miles, according to the Earth Institute of Columbia University. Choosing recipes with this in mind is an environmentally sound decision.\nIt is a great warm dinner for the family during autumn and winter.\n4. Mediterranean Diet Salad\nThis vegan Mediterranean salad is made of fresh tomatoes, bell peppers, and beans, with a no fuss recipe. The vegetables are all thrown together, and mixed with a basic dressing of olive oil, lemon, and salt. It makes an excellent light lunch with bread, and also goes well with grills.\nThe ingredient list is short so chances of getting them organic or from the farmers' market increase, and vegan food has the smallest carbon footprint when compared to several other popular diets.\n5. Pickled Cucumbers\nCucumbers can be pickled without the complicated water bath step, and with spices and herbs found in all kitchens. Even though the recipe is simple, it should be followed meticulously to avoid any form of contamination. The pickled cucumbers are great for dinner.\nPickling is a traditional method of storing vegetables from the garden, without resorting to freezing. And growing one's own vegetables and fruits, in gardens or pots on balconies, saves both the environment and money since transportation isn't needed to get to the food source.\n6. Penne Pasta With Tomatoes\nPenne pasta with tomatoes also has pinto beans, corn, chickpeas, olives, and scallions. A standard pasta sauce is made healthy with the addition of pulses, which are an important source of proteins for vegetarians who abstain from meat. The recipe is simple enough for kids to prepare and enjoy.\nVegetables and beans are some of the items that have the least carbon footprint according to Environment Working Group (EWG), so they can be enjoyed guilt-free.\n7. Baked Potatoes Topping\nA smoked salmon baked potato topping adds protein and flavor through the use of four common ingredients: cream cheese, salmon, capers, and onion greens. It is unique enough to be served at a party or to break the monotony at lunch; it is quick and easy to make, and is served at room temperature.\nLow fat varieties of cheese, like cream cheese, or soft cheese are less polluting among cheese, according a brochure released by EWG. Therefore, if you're going to eat cheese, pick a recipe that uses a soft version.\n8. Baked Beans Without Pork\nBaked beans can be vegetarian and is then a more environmentally sound breakfast than the usual variant made with pork. Along with tomatoes, garlic, molasses, there are extras like Worcestershire sauce and mustard, compared to other baked beans recipes. The baking time for this recipe is shorter than for others.\nPork is the fourth most polluting food item according to EWG. It is not meat per se but the frequency with which it is eaten that is a problem. There are anyway ample proteins in this bean recipe to give the day a healthy start without needing the additional amounts from pork, making this a much more earth-friendly option.\n9. Cornmeal Griddle Cake\nA breakfast similar to pancakes, these griddle cakes are made with eggs, flour, milk, sugar, cornmeal. Water used to prepare the batter makes it light. The cakes are sweet enough to be served plain or with some syrup and kids are sure to like them.\nNot everyone is ready to give up meat, so a balance with some non-vegetarian meals is the best way out for most people. While the recipe uses eggs, they have the smallest carbon footprint among animal proteins according to the EWG brochure, they can be used more. Eggs are also easily available as an organic produce or from a local free-range farm.\n10. Creamy Baked Chicken With Mushroom Gravy\nCreamy baked chicken with mushroom gravy uses a whole fowl cut it into small pieces. It is best used without skin and from an organic source to ensure it is healthy. The rest of the ingredients are common and inexpensive like butter, onion, mushrooms, evaporated milk and cream of mushroom soup. An hour is needed to bake this hearty dinner, and is best served with rice and vegetables.\nChicken has one-fourth the carbon footprint of beef, according to EWG. So if you're craving meat, look for one that will reduce your carbon footprint compared to others.\n11. Broiled Halibut\nBroiled halibut is quick and simple to prepare for lunch or dinner. Filets of halibut are the main ingredients and carry the dish practically single-handed helped only by some butter and shallots. Tarragon, salt and pepper are the only seasoning. It takes about 15 minutes to cook the fish and is served with wedges of lime and a side-dish of choice.\nFish is a healthy alternative to red meat, especially as the main dish. Fish can be farmed/caught sustainably and then certified. Lessen your environmental impact by purchasing a halibut labeled by the Marine Stewardship Council as sustainable.\n12. Pasta Carbonara\nPasta carbonara has noodles eaten with a cream sauce made with eggs, and small quantities of bacon for flavor. Shallot, garlic, cream and Parmesan cheese along with seasoning are the other equally easy to find elements of this dish. It takes around 30 minutes to prepare and cook this lunch.\nIn small portions and as a side dish red meat is a welcome addition without making the meal unhealthy or harming the environment as it would be when used as a main dish.\n13. Beef Fritters\nCold beef leftover from a roast can be used the next day as the basis for a lunch as delicious as the previous day's roast, but in a different avatar. Beef fritters uses only beef, eggs and flour with some seasoning. The batter is effortless to make, and the frying takes less than 10 minutes.\nIt is a good idea to use beef completely, and not waste it. Beef produces two times as much emissions as pork, according to the EWG brochure, and more than 13 times of vegetable proteins, so by using leftovers in a second meal, your second meal is much more eco-friendly than throwing out the food waste and starting with additional meat. Moreover, using leftovers saves time and energy needed for making multiple meals from scratch.\n14. Scottish Stovies\nThis traditional ware has its origin as a makeover of Sunday leftovers- meat, potatoes and other vegetables. Any ingredient not present as leftover needed for making Scottish stovies is used fresh and adds pep to a delectable lunch.\nRescue food waste at home using recipes like this one; don't be someone who contributes to the nearly 20% of the total meat produced that turns to waste.\n15. Fried Rice\nRice and other leftovers are combined with a few ingredients to provide a quick warm dinner. Rice is fried with onions, celery and eggs in oil. Soya-sauce, salt and pepper are the uncomplicated seasoning. The basic recipe for fried rice is versatile, and variations can be introduced with meat, seafood, or vegetables.\nA significant portion of waste could be reduced by consuming all food purchased and save precious resources. Tossing the foods into this kind of recipe helps reduce your environmental impact and keeps you from contributing to the large percentage of waste made by consumers. On an average, overall (not just meat) food waste by consumers amounts to 10-22% according to Shrink That Footprint. This waste includes inedible portions like skin of fruits and vegetables, or bones, products that go out of date, cooking waste, and plate waste.\nKnow Your Food Footprint\nFind the impact of your food choice by using The Food Carbon Emissions Calculator, designed by Clean Metrics. Learn how you can contribute to protecting the environment, which is a process that requires thought and planning, through your recipe and food choices.","With all agricultural industries across Australia looking at how they can operate more sustainably and considering whether, and when, a carbon neutral future may be possible, it is timely to reflect on what the chicken industry’s environmental impact is and celebrate its unique achievements.\nIt’s a fact that most commercial chicken meat production today is farmed intensively, but did you know that this helps to contribute to its very modest environmental impacts. In fact, chicken has the lowest environmental footprint of any meat.\nLet me explain why this is the case…\n- One of the biggest determinants of how much energy is used, and greenhouse gasses created, in the production of livestock products (like chicken and other meats and dairy products) is how efficiently the animals convert feed into edible product. This is because feed represents the biggest source of these impacts. The good news is that chickens are the most efficient converters of feed into meat of all land-based livestock species.\n- The way we rear most chickens today, where they are housed in large sheds or barns that have been designed so that they provide, as closely as is possible, their ideal climatic conditions, where food and water are laid on continuously and the birds are fed a diet which very precisely matches their ideal dietary nutrient profile for each stage of growth, all means that we can optimise the flocks’ growth and minimise the amount of feed the birds require to grow. In these farming systems we can also reduce the amount of energy that the chickens themselves need to put into maintaining their body temperature and in finding food and water.\n- All this translates to more efficient use of feed, energy and water to produce each kg of chicken meat – and less greenhouse gas emissions created.\n- The above also applies for free range production systems, where the chickens are also housed in large sheds, but also have access to an outdoor range area during daylight hours once they have reached an age where they are relatively safe from predation and can better cope with variable outside temperatures.\nAm I just making this up, or is there actual hard evidence of this? In fact, there is abundant evidence to support what I have said. Most of this has come from studies using Life Cycle Assessment methodologies.\nLife cycle assessment (LCA) is a tool which is being used worldwide to estimate the environmental impacts associated with producing a particular product from ‘cradle-to-grave’, taking into account all the environmental impacts involved in producing the product, including those generated in creating the inputs into a particular product, for example, the impacts associated with producing the grains that are fed to the chickens, or in generating the energy that might be used on farm or in the processing plant.\nResearch conducted on the environmental impacts of Australian chicken meat have confirmed that chicken meat production generates low levels of greenhouse gas emissions and results in modest levels of energy and water use. Click here to listen to Stephen Wiedemann, the Agricultural Scientist who worked on this research, talking about chicken meat production and its environmental impact. The report on this research, “Using Life Cycle Assessment to Quantify the Environmental Impact of Chicken Meat Production”, is available here. The results from the Australian LCA are broadly consistent with a significant body of overseas research that confirms that poultry production is the most environmentally sustainable (land-based) way to produce quality animal protein for human consumption.\nWhat about waste?\nIn terms of waste…there is virtually no wastage from chicken meat production!\nFor a start, as I’ve described above, chickens are amazingly efficient converters of feed into meat. On average, a meat chicken on a typical Australian commercial farm will consume about 1.6 kg of feed for every kg of body weight it puts on…and this ratio (kg of feed in : kg of body weight produced) is reducing all the time, due to superior bird genetics, better bird nutrition and improved bird health, husbandry practices and management. The more efficiently the chickens convert their feed (which is mostly grains) into meat, the less nutrients that get deposited in their manure. As I’ve said above, the way we grow chickens in Australia today maximises this efficiency.\nBut what of the nutrients that do end up on the shed floor? Well, the used litter (the bird’s bedding plus manure which remains on the floor of the chicken shed after a batch of chickens has been collected to be processed) is taken out of the shed, loaded into trailers and trucked off, generally for use in composts or in organic fertiliser for a range of different applications, such as horticulture, viticulture, broadacre farming, pasture and turf farms. (click here to listen to Stephen Wiedemann).\nEven better, the use of chicken litter as fertiliser reduces the amount of artificial or inorganic fertilisers, like urea and superphosphate (which require significant amounts of energy to produce), that need to be used across Australia to produce a diverse range of other food products and it also contributes valuable organic matter to soils. Chicken litter is therefore not a waste product… it’s a valuable resource used in the production chain of other crops and foods.\nAt the chicken processing plant, there is virtually no waste either! As a rough guide, a bit over two thirds of the weight of the live chicken that leaves the farm ends up in products for human consumption. What doesn’t end up in products for human consumption mostly ends up in a range of rendered or fresh products used in animal and pet foods.\nWhat about water use?\nBecause there is a very close relationship between how much an animal eats and how much it drinks (in the case of chickens, it’s about 2 litres of water for every kg of feed consumed), this means that the more efficiently an animal converts feed into product, the more efficiently it uses drinking water also…so modern poultry production wins again!\nThe area of commercial chicken meat production where most water is used is in the processing plant, where water is a critical input for the carcase cleaning and chilling processes. However, chicken processing companies have, over the past ten years, invested a significant amount of time, money and effort into finding and implementing new ways of reducing water usage in their plants, without compromising food safety. This has resulted in massive reductions in the amount of water used to process each chicken. Indeed, industry initiatives in this area have been recognised nationally. For example, in 2010 Inghams Enterprises was recognised with the Prime Minister’s Water Wise Award for its adoption of advanced water treatment technologies which enabled the company’s Brisbane plant to treat wastewater from the poultry processing plant to drinking water quality, thereby reducing its reliance on mains water by 70 per cent.\nTo put things into perspective, the Life Cycle Assessment conducted for Australian chicken meat (referred to above) concluded that the average 1.7 kg whole chicken requires less water to produce throughout the whole supply chain (up to the point that it leaves the plant for retail sale and ultimate cooking) than an average 4-minute shower.\nAdd this to the finding that the total greenhouse gas emissions associated with producing that chicken would be equivalent to the emissions that would be generated by driving a car to collect the same chicken from the local supermarket or chicken shop, and I think the case for the environmental sustainability of modern chicken meat production clearly stacks up!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:00cf8573-7b2b-4212-a649-9a2379825eb3>","<urn:uuid:35324ac9-8132-4b9f-a709-76ab1214574b>"],"error":null}
{"question":"What are common symptoms of stroke and how is carotid artery disease detected?","answer":"Common stroke symptoms include sudden weakness, numbness or paralysis on one side of the body affecting face, arm or leg, slurred speech, and sudden blindness in one eye. These symptoms may be temporary in TIAs (mini-strokes). As for detecting carotid artery disease, it can be identified through physical examination findings and a simple, painless ultrasound examination of the carotid arteries that takes about 20 minutes and can usually be completed during an office visit.","context":["Ask The Mayo Clinic: Treatment for blocked carotid arteries\nPublished 10:00 pm, Sunday, June 22, 2008\nDear Mayo Clinic: I have 50-percent blockage of my carotid arteries on both sides. What is the recommended treatment?\nA: The purpose of treatment for carotid artery disease is to prevent a stroke. But the type of treatment you receive depends on several factors, including the extent of the blockage, the signs and symptoms you're experiencing, and other medical conditions you may have.\nThe carotid arteries are a pair of blood vessels that deliver blood to your brain and head. When these blood vessels become clogged with fatty deposits (plaque) that restrict blood flow -- a condition known as carotid artery disease -- your risk of stroke increases. Most strokes associated with carotid artery disease are caused by small clots that form in the area of the restricted blood flow and travel to the brain.\nWith a 50-percent blockage, and if you haven't had any signs or symptoms of a stroke, I would generally recommend that your condition be treated with lifestyle changes and medication. Eating healthy foods, exercising, losing weight and, in some cases, lowering the amount of sodium in your diet, may help slow the progression of carotid artery blockage (stenosis). Cigarette smoking is a risk factor for carotid artery disease and stroke. If you use tobacco, you should permanently stop.\nCarolyn Hax Media OS Playlist\nEffectively managing chronic medical conditions also is important. High blood pressure (hypertension) is a key risk factor for stroke among people with carotid artery disease. Bringing your blood pressure down to approximately 120 over 70 mmHg can reduce your risk of stroke significantly. Controlling your blood sugar levels if you have diabetes and lowering your cholesterol levels with diet, exercise and, if necessary, a statin drug -- if you have high cholesterol -- may reduce your stroke risk, as well.\nTaking an aspirin every day is often part of treatment for carotid artery disease, too. Aspirin is an effective blood-thinning medication that can help prevent blood clots from forming in the narrowed carotid arteries. If you can't take aspirin, your doctor can prescribe another drug that will have a similar effect.\nYou don't mention if you've had signs or symptoms of a stroke, such as sudden weakness, numbness or paralysis on one side of your body affecting the face, arm or leg; slurred speech; or sudden blindness in one eye. If you experience any of these, seek immediate medical attention.\nEven if they last for only a short time, tell your doctor. Signs and symptoms of a stroke that last less than 24 hours, and after which you feel normal, may be the result of a temporary shortage of blood to part of your brain (transient ischemic attack or TIA). Having a TIA significantly increases your risk of having a stroke.\nFor people who have carotid artery disease and experience TIA or other stroke signs and symptoms, or for those who have a higher level of blockage than you do, such as 70 percent or more, lifestyle changes and medication usually aren't enough. Treatment in these situations typically involves a procedure to remove the blockage. A carotid endarterectomy is the most common operation. During this surgery, performed under general anesthesia, a surgeon opens the carotid artery and removes the plaque.\nIn some cases, a carotid endarterectomy may not be an option. The location of the blockage may be difficult to reach directly. Or, a patient may have a condition that makes surgery too risky, such as previous neck radiation or neck surgery, severe heart or lung disease, or kidney failure.\nIn these situations, carotid angioplasty and stenting may be appropriate. This procedure involves inflating a tiny balloon at the end of a long hollow tube (catheter) within the blocked artery, then inserting a wire-mesh metal stent into the artery to keep it open.\nThese are the general treatment guidelines for carotid artery disease but they don't apply in all situations. Factors such as your age, past medical history and underlying medical conditions also need to be taken into consideration. Talk to your doctor about a treatment plan that's best for you.\n-- Bruce Evans, M.D., neurology, Mayo Clinic, Rochester, Minn.","Home > Conditions We Treat > Vascular Conditions\nVascular care is the medical and surgical treatment of problems affecting the circulatory system (outside of the heart and brain) including arteries and veins, especially in the legs, arms, neck, and kidneys.\nPeripheral vascular disease (PVD) refers to blockages of the arteries outside of the heart composed of cholesterol plaques. As the condition progresses, patients frequently experience intolerance to walking because of discomfort or weakness in the legs. Worsening of the disease may result in pain at rest or sores in the legs. Ultimately, a person might require amputation of the affected limb.\nPVD is more common with increasing age, in smokers, and in diabetic patients. Simple, painless and inexpensive non-invasive tests can be performed in the office to detect the condition. Further evaluation may require advanced (CT or MRA) imaging. Catheter-based angiography may also be necessary to define the extent and severity of the disease.\nPVD is typically first treated with risk factor modification: smoking cessation, cholesterol lowering therapy, aspirin and possible other medications. A search for other organs affected by cholesterol plaques may be indicated. Rehabilitation is recommended for PVD patients, and may be the only form of treatment needed.\nMedications may help alleviate symptoms related to PVD. Patients who have inadequate relief with conservative management or who are at risk for sores or limb loss are frequently evaluated with the goal of aggressively improving blood flow.\nSurgery was once the only therapy available to achieve this goal. Now, minimally invasive therapy, including balloon angioplasty and stent implantation, has great success for many patients. This is typically an outpatient procedure that offers immediate relief for many patients.\nStroke is the third-leading cause of death and the leading cause of disability in the U.S. A stroke is an obstruction of blood flow to a part of the brain that results in that part of the brain dying. Symptoms of a stroke include weakness, loss of control, or numbness in an extremity, visual disturbances, or difficultly with speech. Stroke symptoms that resolve within one day are called \"transient ischemic attacks,\" or TIAs (frequently also referred to as \"mini-strokes\").\nFollowing a TIA, there is a very high risk of a completed stroke within the next year. Strokes most frequently, however, occur suddenly with no warning symptoms.\nAbout one in four strokes are the result of significant cholesterol plaque buildup (atherosclerosis) in the carotid arteries. Patients with other atherosclerotic disease are frequently screened for carotid artery disease.\nOften, physical examination findings suggest the presence of carotid artery disease. Screening for carotid artery disease is quite simple and painless. An ultrasound examination of the carotid arteries takes about 20 minutes and can usually be completed at the time of an office visit.\nTreatment of carotid artery disease begins with risk factor management including smoking cessation, blood pressure control, treatment of diabetes, and cholesterol lowering. Medical therapy most often involves blood thinners.\nCarotid surgery may be performed to remove the cholesterol plaque. More recently, a less invasive option has become available in the form of carotid stents. This procedure has been shown to have similar results to surgery in selected patients."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:46255662-8be0-4312-a7e3-b6b592e0ee86>","<urn:uuid:3be8fa2a-88e2-4fc0-ab6e-bd4e3217774c>"],"error":null}
{"question":"What are the main Hindu scriptures, and how do they describe the timing of cosmic cycles?","answer":"The main authoritative Hindu scriptures are the Prasthanatraya or 'Triple Canon' consisting of the Upanishads, Brahmasutras, and Bhagavad Gita. According to these texts and Hindu cosmology, cosmic cycles follow a precise timing: a Day of Brahma (kalpa) lasts 4,320,000,000 earth-years, divided into 14 manvantaras each presided over by a Manu. Each kalpa is further divided into 1000 yuga-cycles, with each cycle comprising four yugas (Satya, Treta, Dvapara, and Kali) of progressively shorter duration. After each Day of Brahma, there is a Night of equal length where the universe is dissolved, before the cycle begins anew.","context":["WHAT IS THE MAIN “BIBLE” OR SCRIPTURE FOR HINDUS? – WHAT IS THE HINDU VIEW OF GOD? – DO HINDUS BELIEVE IN CREATION AND TEACH CREATIONISM? – WHAT DOES HINDUISM SAY ABOUT THE END OF THE WORLD? – DOES THE WEST HAVE AN ACCURATE VIEW OF HINDUISM NOWADAYS?\nQ. What is the main “Bible” or Scripture for Hindus?\nA. There isn’t actually a main “Bible” or sole central scripture of Hinduism. There are many hundreds of different kinds of scriptures and spiritual texts belonging to the many different and diverse forms of the religion. Some which are well known include the Ramayana, the Vishnu Purana, Shrimad Bhagavatam (Bhagavata Purana), and Yoga Vasistha.\nThere are six divisions of Hindu scriptures and these – in order of general authority and importance – are (1) the Shrutis, (2) the Smritis, (3) the Itihasas, (4) the Puranas, (5) the Agamas, and (6) the Darshanas.\nHowever, there are three specific central scriptures which comprise the Prasthanatraya (“Triple Canon”) and these are the main and truly authoritative scriptures for a Hindu. They are the Upanishads, the Brahmasutras (Brahman Sutras), and the Bhagavad Gita.\nOf these three, the Brahmasutras are the least read, due to their being very abstruse and intellectually philosophical, and almost impossible to understand without the aid of a special commentary known as a Bhashya. Their aim and purpose is to expound and clarify the nature of Brahman, the Ultimate Reality, as presented in the Upanishads.\nRegarding the Upanishads, Swami Sivananda has written, “The Upanishads are the concluding portions of the Vedas or the end of the Vedas. The teaching based on them is called Vedanta. The Upanishads are the gist and the goal of the Vedas. They form the very foundation of Hinduism. … Even the Western scholars have paid their tribute to the seers of the Upanishads. At a time when the Westerners were clad in barks and were sunk in deep ignorance, the Upanishadic seers were enjoying the eternal bliss of the Absolute, and had the highest culture and civilisation.”\nThe Upanishads, the oldest of which were written at least 3,500 years ago, are timeless, beautiful, and inspiring treatises which all focus on the identity, unity, and literal sameness of the spiritual nature of the individual and the Divine Absolute Reality or, in other words, the Oneness of our Higher Self (Atman) with the Supreme Self (Brahman). The Upanishads describe Brahman as the ONE Infinite Divine Life and speak of the oneness, non-separateness, and inner divinity of all things. They are the source and the deepest and most profound expression in history of the spiritual teaching of non-duality and universal oneness.\nThe Bhagavad Gita, which literally means “Song of God” or “The Lord’s Song,” is without doubt the most popular and universally loved of all Hindu scriptures. It consists of a conversation between Krishna and Arjuna and is admired by multitudes around the world for its practical yet profound spiritual philosophy of life. It has been described as “the Manual of Life” and “the Gospel for the 21st Century.” Krishna is thought to represent the Higher Self and Arjuna the individual human soul, who must turn towards and seek refuge in that Self. Set on the scene of a battlefield, it in no way promotes or encourages war and violence as some enemies of Hinduism have ignorantly claimed, but rather symbolises the battlefield of life or “the war within,” which each of us must at some time face. Although not a particularly long book, it is truly an unforgettable classic of the world’s spiritual and religious literature.\nBeing part of the Vedas (the original foundational scriptures of Indian civilisation and the most ancient books known to man), the Upanishads are thus of the nature of Shruti, meaning direct revelation, and are thus ultimately the primary and final scriptural authority of the religion of Hinduism.\nQ. What is the Hindu view of God?\nA. The Upanishads very clearly and repeatedly explain that God – the Divine Absolute Reality – is not a Supreme Being but rather a Supreme Principle. It is referred to as Brahman, which can be translated as “infinite expansion.” It is acknowledged as being ultimately undefinable and indescribable but the Upanishads try their best by using such terms and phrases as “the divine Principle of existence,” “attributeless reality,” “the ground of existence,” “changeless, nameless, formless,” “Pure Consciousness,” “formless in the midst of forms, changeless in the midst of change, omnipresent and Supreme,” “the eternal Reality,” “without form and unconditioned,” “infinite and invisible,” “beyond all attributes,” “without body and without mind,” “without action or organs of action,” “beyond good and evil,” “beyond the reach of words and works,” “the indivisible unity of life,” and “That from which all words turn back and which thought can never reach.”\nThose are just a few examples out of hundreds of similar terms.\nSince Brahman is the ONE Life and the ONE Reality, it is said in Hinduism that there is nothing but Brahman and that thus Brahman is all there is, being all and in all. This is the main reason for Hindus revering all living beings and life itself as something sacred, precious, and divine. They know that Brahman is “the Only One,” “One without a second,” as the scriptures say.\nAnd so, in the highermost part of our being, in our essential nature as pure eternal spirit, we too are Brahman, this “Eternal Light” and “Infinite Life.” This is why the Vedanta philosophy speaks constantly of man’s identity and oneness with the Divine. Many Hindus will meditate upon and affirm the statement “Aham Brahmasmi” – “I am Brahman.”\nSince Brahman is recognised as being absolute, infinite, immutable, and unconditioned, It can thus have nothing finite, conditioned, or anthropomorphic about Itself, such as form, personality, physical appearance, name and human-like characteristics, etc. Rather than being a “He,” it is taught that Brahman is to be reverentially referred to as “IT” since to do otherwise would surely be an attempt to drag the purely Spiritual and purely Divine down to the physical, material, human level, whereas we should be endeavouring instead to raise and elevate ourselves in consciousness up towards the purely Spiritual and Divine, for That is who and what we truly and eternally are.\nHowever, as well as being an expressly monist or monistic religion, Hinduism is also pluralistic in that it not merely permits but actively encourages everyone to relate to God or the Divine in the way that suits them best and which appeals to them the most. The above may be somewhat unpalatable for some and so we also find many Hindus viewing and worshipping God as a Supreme Being with personal characteristics. Many acknowledge and accept that behind their anthropomorphic concept of God there is the Infinite Brahman Principle but they feel that it is more beneficial and helpful for them personally to give a more human and comprehensible touch to It, characterising It as Vishnu, Krishna (who is considered to have been the supreme Avatar of Vishnu), Shiva, Ganesha, Rama, one of the representations of the Divine Mother, or indeed anyone or anything else of their choosing.\nThis is accepted and celebrated in the vastly diverse and open religion of Hinduism, where many differing viewpoints and perspectives live harmoniously side by side, yet it cannot be denied that the Upanishads teach of a Supreme Pure Divine Consciousness which is everything and in everything, rather than a Supreme Being modelled in the image and likeness of us human beings.\nThe Hindu Trinity or Trimurti is comprised of Brahmā (note the important distinction between Brahmā and Brahman; they are not the same!), Vishnu, and Shiva. Their roles are popularly described as Creator, Preserver, and Destroyer. It is said that at the appointed time Brahmā-Vishnu-Shiva comes forth from Brahman and causes the universe to come into being. Brahmā performs the initial task of evolution into manifestation, Vishnu then preserves and sustains the whole universe until finally the universal life cycle reaches its appointed end, whereupon Shiva destroys it all.\nPhilosophically speaking, Brahmā, Vishnu, and Shiva are not three distinct “beings” or entities but are the three different aspects of the One all-ensouling Life of the Universe or, in other words, Brahmā-Vishnu-Shiva is the Living Universe itself…self-evolving (when it radiates forth from the absoluteness of Brahman), self-sustaining, and finally self-dissolving, whereupon everything is completely reabsorbed back into Brahman until eventually the universe has to be reborn, just as everything in Nature is constantly being reborn and following a cyclic pattern.\nQ. Do Hindus believe in Creation and teach Creationism?\nA. Although the terms “Creation” and “Creator” are used in various places in Hinduism, this is an unfortunate choice of word and in some cases is simply a mistranslation into English, since what Hinduism teaches is not Creationism but rather a very broad and far reaching system of Evolution.\nIt is widely unknown or at least often overlooked in Western civilisation that Hinduism, along with other Indian religions such as Buddhism and Jainism, believed in and taught evolution thousands of years before Darwin came along. Hinduism does not deny but readily affirms that evolution is a fact and that all manifestation is in a continual state of change, transformation, and progressive development into higher and better adapted forms, affecting the species of animals as well as the vegetable kingdom and everything else, humanity included.\nAnd since Hinduism upholds the idea of non-duality and the Oneness of Brahman, it has to deny creation in the literal sense of the word, since if Brahman is all there is then nothing – no new thing – can ever be created, since that would imply duality and multiplicity and thus negate the non-dualistic ideal as well as unphilosophically implying that infinite impersonal Oneness somehow has the intention and ability to personally make things out of nothing.\nIt is maintained that Brahman emanates Itself, through Itself, within Itself, and as Itself. It cannot be otherwise, since Brahman is the One and Only Reality; Absolute Infinite EXISTENCE Itself. Thus emanation and evolution are facts, whereas creation as commonly understood and portrayed in the Abrahamic religions is not.\nOne important difference between the modern scientific view of evolution and the Hindu view is that Hinduism says that the most important aspect of evolution is not the evolution of the outer material form but the constant gradual progression, development, unfoldment, and advancement of the inner life entity through a long series of changing physical forms and experiences, eventually working its way up to the human kingdom and beyond. In other words, inner evolution is much more important than outer evolution because ultimately only the inner survives and endures and everything external, material, and objective must eventually disappear and cease to be.\nQ. What does Hinduism say about the end of the world?\nA. Hinduism teaches an ever-ongoing process of the cyclic appearance and disappearance of the universe and everything in it. This again is an aspect of the cosmic and universal system of evolution as propounded in the Hindu religion. This alternating process of the existence and non-existence of the universe is known as Manvantara and Pralaya or the Days and Nights of Brahmā, since Brahmā represents the universe itself.\nFollowing the cyclically occurring dissolution of the universe, there is complete rest, absolute inactivity, nothing manifest or objective whatsoever, and a complete reabsorption in Brahman. This remains for the exact same duration as the universe had previously been in manifested objective existence, the life span of the universe being said to always be a period of 311,040,000,000,000 years.\nAfter that, the whole process of universal evolution begins all over again, with everything now raised to a higher level than it was before.\nThus the universe itself is ultimately only a temporary and impermanent thing (hence it being called “maya” or illusion) and its contents such as the solar systems and planets even more so, since they die and are reborn a multitude of times within the universal life cycle. Our world will therefore eventually come to its end, only to later be reborn or “reincarnated” in some way, but this will not and cannot happen until the appointed cyclic time and according to the calculations and calendars of the ancient Hindus that time is still an incredibly long way off.\nQ. Does the West have an accurate view of Hinduism nowadays?\nA. Unfortunately not. Despite being the world’s third largest religion and 15% of the world’s population identifying themselves as Hindus, the religion and its teachings and practices are still sorely misunderstood and misrepresented here in the West, with many people not actually knowing anything about it at all. To the average person in the street, the word “Hinduism” signifies a strange, impenetrable, chaotic, colourful world filled with hundreds or thousands of peculiar gods or deities with many limbs, many heads, and unnaturally coloured skin.\nIt’s true that a few terms and concepts that originate in Hinduism have filtered into popular Western usage, such as karma, reincarnation, yoga, mantras, and so forth, and quite a lot of people are aware that karma and reincarnation play some sort of role in the Hindu philosophy but that’s about all they know at the very most and those terms themselves are often either partially or severely misunderstood, particularly in the case of yoga. But then again, the average Westerner doesn’t know very much at all about Christianity either, despite most of the Western nations still being nominally Christian countries…so perhaps we shouldn’t be overly concerned about this at the moment.\nWhat is a cause of concern is that even the majority of spiritually interested people have a warped view of Hinduism, often falling into one of the two extremes of either erroneously equating tantric things such as Kundalini awakening and the chakras (which play an extremely minor and virtually inconsequential role in the teachings and general practices of Hinduism) with Hinduism at large or alternatively assuming that Hinduism as a whole is a type of Bhakti movement with the central focus on worshipful devotion of a Supreme Being figure and various deities.\nThe first erroneous view is largely due to the New Age movement which is famous for misrepresenting, misunderstanding, and grossly distorting numerous aspects and teachings of Eastern religions. New Agers spend their time – and money! – attempting to “cleanse,” “unblock” and “heal” their chakras, naively believing the claims of various writers and teachers that this is actually an ancient and prominent practice of the Indian sages, whilst in reality the Indian sages observe such nonsense with pity and cannot help but laugh. The second view is often formed by someone visiting a Hare Krishna temple, for example, and then automatically assuming that all of Hinduism is like that.\nThose who are genuinely and sincerely interested in gaining a clear and deep understanding of the heart of Hinduism, which is Vedanta, may perhaps have some idea where to look after reading this article. One excellent and influential new endeavour is the “Upanishad Ganga” television series from Chinmaya Creations, which is part of the organisation founded by the late Swami Chinmayananda, one of the most widely respected and beloved teachers of Advaita Vedanta in modern times. The first seventeen episodes of the 52 episode series are now widely available on DVD, complete with perfect English subtitles, and many more episodes other than those first seventeen can be freely watched, albeit without subtitles, on YouTube.\nDespite some of the current misunderstandings and misconceptions, it is believed that the teachings of Hinduism – or Sanatana Dharma – will soon begin to play a new and more influential role in the world than ever before. It’s surprising how many people say, once they learn more about Hinduism, that they have always believed and known such things to be true. Such is the nature of the world’s oldest and perhaps most universal religion.\n~ BlavatskyTheosophy.com ~\nSOME RELATED ARTICLES: Gandhi on Blavatsky and Theosophy, The Theosophy of the Bhagavad Gita, The Life & Times of Adi Shankaracharya, Ramalingam Pillai and the Theosophical Movement, The Three Gunas, Trimurti – The Hindu Trinity, The Mandukya Upanishad, The Seven Yugas, Our Mother India, Who are you, Madame Blavatsky?, Is Theosophy Hinduism, Buddhism, or Something Else? and Esoteric Symbolism.","Statue of Kalki in Gujarat\non a wall of Rani Ki Vav\n“In speaking here of the Hindu cosmology, it is chiefly our solar system that is to be understood; but it will be clear that similar principles are applicable to any other system, or to a whole universe composed of many systems. No original creation of the universe can be imagined; but there are alternations, partial and complete, of manifestation and withdrawal. At the commencement of a cycle (kalpa) the world is created by the Brahma aspect of Ishvara; during the cycle it is sustained by Vishnu; and at the end, as Shiva, he destroys it. This cosmic process takes place according to the following time scheme:\n“A cycle, or Day of Brahma, a kalpa, the period of the endurance of the solar system, is 12,000 years of the devas, or 4,320,000,000 earth-years. At the beginning of each Day when Brahma wakes, the \"Three Worlds\" so often spoken of in the myths, together with the devas, rishis, asuras, men, and creatures, are manifested afresh according to their individual deserts (karma, deeds); only those who in the previous kalpa obtained direct release (nirvana, moksha), or who passed beyond the Three Worlds to higher planes, no longer reappear. At the close of each Day the Three Worlds, with all their creatures, are again resolved into chaos (pralaya), retaining only a latent germ of necessity of remanifestation. The Night of Brahma is of equal length with the Day. The life of our Brahma or Ishvara is one hundred Brahma-years, at the end of which time not only the Three Worlds, but all planes and all beings Ishvara himself, devas, rishis, asuras, men, creatures, and matter are resolved into chaos (riiaha-pralaya, \"great-chaos\"), enduring for another hundred Brahma-years, when there appear a new Brahma and a new creation. It will be seen that both major and minor alternations of evolution and involution are represented as necessitated by natural law, the latent force of past action (karma). Causality governs all conditioned existence. The whole scheme is highly scientific.\n\"The Day of Brahma is divided into fourteen manvantaras, over each of which presides a Manu, or teacher. Each manvantara is followed by a Deluge, which destroys the existing continents and swallows up all living beings, except the few who are preserved for the repeopling of the earth. The name of our Manu is Vaivasvata, who is the source of the Laws of Manu, formulating the basic structure of Hindu society. The Day of Brahma is also divided into 1000 yuga-cycles (maha-yuga); each consisting of four ages, the Satya, Treta, Dvapara, and Kali yugas, of which the last three are periods of progressive degeneration from the first. The four yugas together last 4,320,000 years; the first 1,728,000, the second 1,296,000, the third 864,000, and the last 432,000. The present year (A.D. 1913) is the 5013th of the Kali yuga of the present maha-yuga ; this maha-yuga is the twenty-eighth of the seventh manvantara of our kalpa, called the Varahakalpa, because in it Vishnu incarnated as a boar (varaha); and this kalpa is the first day of the fifty-first year of the life of our Brahma.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:d62e8167-2724-4cb6-b86c-0d9c3b077f9b>","<urn:uuid:c98a9e88-4346-4a70-9580-fa3d1cc43da1>"],"error":null}
{"question":"How do the mental challenges and preparation strategies compare between cyclocross racing and virtual time trials?","answer":"Cyclocross racing demands intense mental preparation due to constant violent accelerations and maximum efforts, requiring specific training blocks including Foundation, Build and Peak periods to handle the anaerobic demands. The mental challenge comes from sustaining repeated maximum efforts throughout the race. In contrast, time trials present a different mental challenge of staying motivated while riding solo, requiring specific mental techniques like breaking down the effort into countable segments (e.g. counting down 30 10-second blocks). While both disciplines require careful preparation, cyclocross needs more comprehensive periodization over months, while time trial success relies more on in-the-moment mental focus and careful pacing strategies.","context":["A Look at the Demands of Cyclocross\nPeople used to always tell me that ’cross races are like time trials. Now, time trialing is something I’m very good at, and when I first tried ’cross, it was certainly an eye-opener! I’m a decent mountain biker, and I had good form for my first ’cross race, but it certainly was not like a time trial—and I fell apart badly. If I had to succinctly describe a true ’cross race, it’s like riding off the front of a technical crit: much more fast-twitch-reliant than people realize.\nTime trials demand a steady effort from beginning to end, with very small jumps in wattage—the lactate threshold (LT) system is the primary engine here, with most TT power output falling into the LT or “SupraThreshold” zones. The best time trialists can even produce negative splits, meaning they can put out more power in the second half of the event. But ’cross is a completely different animal, chock full of violent accelerations, max efforts out of turns and up power climbs, and attacks—the average power output may be similar to a time trial in the end, but how you get there is quite another story!\nIn TTs, the start is key—start too hard and you can risk ruining your performance. In ’cross, a hard start is imperative, and then you need to be able to stay with the leaders for the remainder of the event.\nPrecisely because ’cross events are shorter, in my opinion they demand as much if not even more preparation than typical road or MTB events. The anaerobic engine is hit so damned hard during these races, with max efforts constantly required, that if you don’t prepare properly, you’ll never be able to perform to your true potential, or you’ll only last half the ’cross season, with the body just falling apart.\nRecovery Time Between Seasons\nI coach four professional ’cross racers, and I make sure to deliberately give each of them perhaps more recovery time than necessary after their December racing. Many riders take too little time from season to season, and they can really pay the price in the spring when things start to heat up—these athletes aren’t prepared and never really peak, or they just erode into an overtrained state and ruin their seasons. In December after your last cyclocross race, it’s critically important to recharge the batteries, perhaps more so mentally than physically.\nWe’ll use one of the athletes I coach, Laura Winberry of Elite Endurance, as our concrete example. She won the New Jersey State Cyclocross Championships in 2009, and after the race she had two weeks completely off. Nada, nunca, zilch. Then I prescribed her two weeks of light cross-training, with possible workouts including mountain ￼biking, hiking, trail running, the Stairmaster elliptical or rowing machines at the gym, rollerblading, swimming, running, basketball, rock climbing and yoga. The goal was to keep any of these workouts to an aerobic pace and finish feeling fresh.\nThen Laura went through a typical five-week transition and preparation period where she started completing light gym work—which is periodized and specific for her cycling goals—and plenty of pedaling efficiency and leg speed drills, combined with light aerobic riding.\nThe Meat & Potatoes: Foundation Training\nAfter this block, she’s ready for Foundation work, which is the longest block at 10 to 12 weeks. In my opinion, this is the most important block in any athlete’s annual training plan. Here, the gym work continues to progress, we start to introduce plenty of tempo intervals while also incorporating various forms of force work, which consists of pushing a big gear at a low cadence—like lifting weights on the bike. I also like to sprinkle in light endurance rides and group rides with specific objectives.\nWhen Laura and I first started, I administered a fitness test to establish heart rate training zones—the test typically consists of a max 8-minute effort, full recovery, then 16 minutes at your highest aerobic level. I’ve found Laura’s lactate threshold heart rate to be around 185 beats per minute. With this information, I can create HR zones for her: Recovery, Endurance, Tempo, UltraTempo, SubThreshold, Lactate Threshold, VO2 Max, Anaerobic Threshold, and Max.\nDuring Foundation work, I really like to stress the two “Sweet Spots,” which for most athletes are longer 15+ minute Tempo intervals at 84-88% of LTHR, then shorter, 10-12 minute SubThreshold intervals at 93-96% LTHR. I’ve found that spending time in these two training zones yields the most bang for the buck.\nA typical Foundation week for Laura looks like this:\nMonday – 50+min (minute) Recovery Ride, rolling terrain, aerobic pace. 4 x 7min Leg Speed Drills at 105-115 rpms.\nTuesday – 75+min ride, hilly terrain. 9 Uphill Grinds, 1m efforts at sub-max effort, 50-60 rpms, 4+min recovery in between efforts.\nWednesday – 90min ride, rolling terrain. 3 x 12min SubThresholds, 172-178 HR (93-96% LTHR). These are done at her absolute highest aerobic level, not an easy effort, but not overly hard—challenging and uncomfortable, but won’t blow her out.\nThursday – Gym. Most workouts include squats, leg presses, lunges and step-ups. I’m a big fan of core, so my athletes usually do 3-4 supersets of core work at the end of each gym workout, plus some core exercises on separate days.\nFriday – 45+min Recovery Ride, flat terrain. Ride at a natural cadence, HR under 127 (68% of LTHR). 8 Corner Accelerations: As you cruise out of a corner, jump from the saddle and accelerate 100% for 5 seconds. These are short, max efforts. Corner Accelerations help develop leg speed, out-of-the-saddle jumps, and leg torque.\nSaturday – Group Ride. Sit in. No max efforts. Goal is to finish as fresh as possible. Enjoy the day. OK to stand while climbing. It’s OK to drop to the back or off the back if the pace is too harsh. Ride time under 2hrs.\nSunday – 2.5+hrs Endurance Ride, hilly terrain. 132-154 HR (71-83% of LTHR) on the flats. 4 x 3+min PowerClimbs, 65-75 rpms, 172+ HR (93+% of LTHR). Full recovery in between climbs. Then finish with 25+m Tempo over rolling terrain, 156+ HR (84+% LTHR). 94+ rpms, pushing a challenging pace on the uphills.\n￼For the weekend warrior with only a couple of days available for training, the Tuesday and Wednesday rides are the most important workouts in the example given, and could be shifted around to accommodate a rider’s schedule. The other days could be used to add in volume whenever that saddle time can be fit in, with the gym day a good addition for those interested in full-body strength—an important attribute for a successful cyclocrosser.\nDuring this training block, the workload is steadily increased, and the athlete is in a gray zone—never recovered, never overtired, just slowly continuing to move forward. I give Laura a ton of longer “Sweet Spot” intervals, which really strengthen her aerobic engine. In the end, the aerobic engine is the cornerstone for success in this sport, something so few riders comprehend.\nLaura’s Foundation block takes her into late-April but, depending on the goals, dedicated cyclocrossers could certainly begin this phase later in the spring, continue to train through summer rides and races and save their peak performances all for cyclocross season. Many riders cut short their Foundation blocks, either riding too easy or too hard, and these riders have a hard time holding back, wanting to be king of the local spring group rides and doing other things to achieve peak form sooner. But trust me: good things come to those who wait, especially if ’cross is on your menu.\nRemember: early-season heroes can be summer and ’cross zeroes.\nGood Things Come to Those Who Wait\nNow that Laura has true foundation work in her legs, she’s ready to turn it up with an eight-week Build block, which commonly finishes with a month-long Peak and Taper block before her biggest races. Volume is not what changes so much here—it’s how we’re starting to rev her engine. In Foundation, we were doing plenty of longer aerobic intervals and plenty of steady force work. But now we turn it up, and her body will respond much better to the increased intensity.\nDuring Build training, we will be doing plenty of Lactate Threshold work, intervals typically 7 to 11 minutes in duration, performed at an intensity where the rider is right at threshold, an 8 on a scale from 1 to 10. We also will complete plenty of VO2 Max intervals. VO2 Max is also known as known as “maximal oxygen uptake” or “maximal oxygen consumption,” and these intervals are very tough, but are also crucial for any successful ’cross racer. Here, having a power meter really helps make sure you’re doing these correctly. [See Hunter Allen’s “Power Training” article in Issue 11] Anaerobic Threshold intervals are shorter, typically 30 to 90 seconds in duration, and they’re essentially max- effort intervals with shorter recovery, which make them even more fun!\nLaura is prepared for this: she can push harder, will recover better between workouts and, most importantly, mentally she is 100 percent ready to rock and roll at a time when other athletes are perhaps starting to get drained or are starting to go backwards. Laura is truly ready to reach for her best.\nStarting slower and steadier in your training is important for all endurance athletes, but it’s especially important for cyclocross racers because they will need to be in peak form in early-winter. If the athletes don’t dose their efforts properly during the year, their knife will just be too blunt for ’cross, because ’cross is a mean, mean animal, requiring sharp preparation with VO2 Max, anaerobic threshold and max power intervals. Those are very potent workouts, and if you’re not prepared, you can’t complete these workouts properly.\nAlso, I like to get the ’cross racer on the cyclocross bike at least once a week during the spring and summer, just to stay sharp. ’Cross is much more than just riding hard—you need to acquire the technical chops to be able to use your engine at max effort. I typically like to prescribe recovery rides where the athlete heads to a park, double track or mild wooded trails with the ’cross bike, hits “barriers” (garbage cans work great), practices run-ups, tight turns, stair climbs, off-camber descents—all at an aerobic pace, just going through the motions.\n￼Planning Your Peaks\nI recommend two to three peaks a year for the cyclocross racer, preferably two. Many riders want a May-July-autumn peak, but I actually like to talk them into a June peak, then a full build and peak for ’cross. Remember, for most athletes, the second peak is the strongest. Even for riders who don’t target summer racing disciplines and only care about cyclocross competitions, it’s still a good idea to “come up for air” and take a break from big blocks of training. That means they’ll have great form—so it’s a good time to mix it up in a summer race, kill your local training buddies or just head somewhere cool and push the pace.\nBecause of our thorough preparation, Laura responds very well during Build and Peak training. I’ve coached her for many years, she’s been doing the right things at the right times, and now she’s a professional athlete. It’s beautiful seeing her respond so well to the training stress. With new athletes I coach, sometimes the gains don’t come as quickly, or I have to progress them more slowly, and this is because of their lack of true aerobic conditioning prior to our relationship.\nLaura had a tremendous 2010, earning eight top-10s in Pro cross country MTB races. This year, MTB Nationals are in July, and after that we’re shutting it down. This is a huge key for the ’cross athlete: knowing when to turn off the training, and doing it soon enough. Many riders fail to address this, not shutting it down at all, or shutting it down too late and not giving themselves enough time to prepare for a true ’cross peak, leading to mediocre performances.\nIn July, Laura will have two weeks off, and then I plan to run her through another nine-week Foundation cycle. She might not be flying for the early ’cross races, but in the past she still has always held her own with the solid Foundation work in her legs.\nThe key for the smart ’cross athlete is to be able to progress through the ’cross season. So many riders come out flying early on, enjoying road or MTB form only to fade away as the season is well underway. The best ’cross racers typically come into form by early-October, continuing to improve until the last races of the year, which are usually the most important.\nYou Can Never Plan Too Far Ahead\n’Cross Nationals are on January 8th, 2012, so we designed Laura’s plan to have her ready to rock at the country’s biggest event. We take that race date, then work backwards from there, making sure we leave no stone unturned. Through mid-October to late-November, Laura will be progressing through true Build work, able to hit her VO2 Max and Anaerobic Threshold intervals at 100 percent, recovering well, continuing to race and get stronger. Early- December will include two Peak weeks, where she’ll be pushing harder than ever, and then a solid two- to three-week taper for Nationals.\nI recommend such a long prep and careful selection of peaks for ’cross because in the fall you need to be completely ready to race and train: much easier said than done. Road racing requires more lactate threshold intervals, sprint work and endurance work, whereas true ’cross prep will include much more white-hot, full-metal training, and this needs to be very carefully prepared for—too much of this arduous training, or completing it too soon, and you run the risk of overtraining, running flat or not completing the season.\nThe Proof is on the Podium\nThe training program worked very well in 2010 as Laura took podiums at the monstrously popular Oregon Cross Crusade events. She then had the best race of her life at Cyclocross Nationals, taking 34th after starting near the back. As we all know, having a good start is critical in ’cross, and had Laura started in 34th, I know she could’ve placed top- 15, which would’ve been unbelievable. Laura reported having her best legs ever, and she was one of the few not to get lapped by Katie Compton. Laura impressively finished in front of many local pros who had beaten her all year, not far off a top-20—and this wasn’t by mistake. She was at her best when it truly counted—at Nationals, she has gone from 54th in 2009 to 34th in 2010. What will 2011 bring? We plan to race all the big UCI races, something we have never done before, seeing where we can take this dream.\nFor all you ’cross racers out there, I highly recommend planning out your season before it starts so you can truly be at your best for your favorite racing, something so few riders can actually do. If you continue to train efficiently, not wasting any days and continuing to move forward, there is no limit to what you can accomplish.","Want to up your racing game? Confused about the different types of races and how to ride them? We’re breaking down the most common Zwift race types to help you out.\nThe Race: Time Trial\nA time trial is a race between you and the clock. It’s the “Race of Truth.” No drafting, no teammates, no tactical games – just you and your bike, trying to cover a certain distance in the fastest time you can.\nThey work a little differently than other races on Zwift. When you sign up for a TT event, you’ll get an individual start time (equal to or after the event’s start time). Make sure to log in and join the event before that time, and your avatar will line up on one of the conveyor belts in the starting pen. These belts will move riders up and release them in staggered starts. Start pedaling before the countdown hits zero. When it’s time for your row to leave the pen, your avatar will speed up to 20 miles per hour for a rolling start. Cross the line and it’s go time!\nWhat Makes a Good Time Trialist?\nTTs can be very different depending on the length and the course. They might take you 5 minutes or 2 hours to ride. They could be pan-flat or take you up a big mountain.\nIn general, most time trials on Zwift are between 10 and 30 miles long and take place on flatter courses. These tend to favor riders who:\n- Are better at holding efforts around FTP (functional threshold power) versus sprinting or attacking\n- Can pump out big watts for a long time\n- May not be lightweight or have high watts per kilogram (unless the course has big climbs)\n- Pace themselves well\n- Can stay motivated riding solo\nDon’t be discouraged if that’s not you! A race against the clock is also about getting the best out of yourself. Let’s look at some tips for how to get your best time in a TT.\nAny route can be turned into a time trial! Three of the most popular choices for short TTs on Zwift are:\n- Watopia’s “Tempus Fugit” – Very flat\n- Watopia’s “Tick Tock” – Mostly flat with some rolling hills in the middle\n- “Bologna Time Trial” – Flat start that leads into a steep, medium-length finishing climb\nZwift racer Chris Bartley, a 2018 silver medalist in the British 25-mile Time Trial Championships, says knowing how to ride a course well can be a game-changer.\n“Sometimes you can be weaker but still win as you've ridden the course better. Just how to use your power takes a bit of experience, and constantly questioning how you feel - is this too hard? Can I keep pushing this? How much should I ease off? My advice would be always check the course profile and try and plan your effort first. It's easy to get things wrong, and only practise really teaches you what works the best for you.”\nChris Bartley, 2018 British 25-mile Time Trial Championships silver medalist\nHow to Race a TT\nFirst, make sure you know the distance and the route ahead of time. Try to figure out how much time it will take you to finish, and then work out the best power you can hold for that amount of time.\nNext, decide on your pacing. This is key in time trials!\nChris says: “For a flat, even-profiled TT I'd start perhaps at 5% under the average power I'd expect to be able to hold, and try and build throughout to hit my estimated average by the end. Generally speaking, if I've made it to the final third of the effort feeling reasonably okay, I'd expect to build well. I try not to overcook things in the first two thirds as a rule, whatever the duration!”\nAn even pace works well when you can hold a constant speed. It’s not the best strategy for every course, though.\nChris says: “Hills and slow sections make it more tricky though, as I'd want to push on during these sections (you tend to make up time this way, rather than completely flat-pacing things). Starting steadier with something in reserve will give more flexibility to push on if needed. I'd usually expect to push on perhaps 10% more than my expected average on slower sections, and then also back off maybe 10% on faster sections. A good rule of thumb for me is if I see speeds below my expected average speed, I'll push on, anything faster and I'll ease off.”\nFinally, choose the right gear for the job. It should be no surprise that a TT bike is usually the fastest choice for a time trial! It’s aerodynamic and will let you ride the quickest on flat or rolling routes. On courses with significant climbs, if the event allows it, you could try a lighter road bike – or even come to a stop and switch in the middle.\nThe Mental Game\nYou might be able to chase other riders on course, but remember – you can’t draft them, and they might be slower riders who started before you. Stick to your pace strategy, and use other racers as motivation to keep the power up when things get hard.\nChris has his own mental tricks he uses during a TT:\n“When the efforts really start to hurt, I'll usually count down clusters of seconds - for example I break 5 minutes up into 30 10-second blocks, and count down from 30 to zero. I might do this 3 times in a row which takes care of 15 minutes. I do a lot of counting! Breaking things up like this is always a good way to get through hard efforts.”\nAfter following this guide, you should know your course, have a pacing plan, and have a strategy to keep your mental focus. You’re ready for a Zwift time trial!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:ac07938e-3a35-4808-bf5c-26df6c976d59>","<urn:uuid:9fcf1342-38dc-45d7-a250-2fbac3203406>"],"error":null}
{"question":"What is the expected annual container handling capacity of Tuas Terminal when it is fully completed?","answer":"When Tuas Terminal is fully completed in 2040, it will be capable of handling up to 65 million TEUs per annum.","context":["Final Caisson Installed for Tuas Terminal Phase 1\nThe final caisson has been installed at Singapore's Tuas Terminal Phase 1 development, and reclamation works for Phase 1 on track to be completed by 2021.\nThe development now has 221 caissons and 8.6 kilometers (5.3 miles) of seawall installed. The caissons are among the largest in the world; pre-fabricated onsite, each one weighs 15,000 tons and measures 40 meters (131 feet) in length, 28 meters (92 feet) in width and 28 meters (92 feet) in height – as tall as a 10-storey building.\nOnce reclamation work is complete, PSA Corporation will begin construction of deep-water berths capable of handling about 20 million TEUs per annum.\nTogether with Dredging International Asia Pacific - Daelim Joint Venture and Surbana Jurong, the Maritime and Port Authority of Singapore (MPA) has adopted innovative solutions including the pre-installation of geotextiles on caissons on land instead of at sea, the use of artificial intelligence and the use of drones for site survey and progress monitoring. In particular, there are two innovations, which have pushed the frontiers of engineering:\nTemarock is an all-in-one rock mound construction vessel. The conventional process of rock mound construction requires multiple vessels for rock laying and compaction, one survey vessel to facilitate operation and is supported by divers. Temarock automates these tasks, eliminating the need for multiple vessels or divers’ assistance.\n2 Automatic Rebar Machine using Robotics System (ARMS).\nARMS automates the bending and cutting of reinforcement steel bars (or rebars, which are used to strengthen concrete) to the desired design and then transfers them onto the stacking area. The conventional practice requires workers to handle the rebar manually, which exposes them to mechanical hazards when they operate the rebar cutting and bar bending machines.\nTuas Terminal will open progressively from 2021, when the first two berths are expected to be completed. When fully operational by 2027, Phase 1 will see 21 deep-water berths that are able to handle about 20 million TEUs per year. When Tuas is fully completed in 2040, it will consolidate all of Singapore's port operations in a single location and be capable of handling up to 65 million TEUs per annum.\nThe port will be include a next generation vessel traffic management system that predicts congestion hotspots, assists vessels' crews to plan their routes to the berths and detects potential collision situations. It will implement the Maritime Single Window in two phases. Phase 1 will streamline submission process for faster port clearance. Phase 2 will have a Just in Time Planning and Coordination System to allow vessels to turn-around faster in the port and optimize the deployment of resources for port services.\nIt will also feature a Maritime Sense Making System which will optimize port operations and manage the growth of future shipping traffic by preventing illegal bunkering, detecting entry into prohibited areas and optimizing the utilization of anchorages. A Remotely Assisted Piloted Advisory solution incorporating internet-of-things sensors and communication systems will enable remote pilotage from shore-based stations. Shore-based marine pilots will use real-time video imagery and collision avoidance software to navigate ships safely."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:41eed65f-f893-4ca1-bec7-c0956e4e0406>"],"error":null}
{"question":"How do digital photo management systems like Lightroom contribute to efficiency, and what environmental benefits could their non-destructive editing approach bring to urban farming initiatives?","answer":"Digital photo management systems like Lightroom contribute to efficiency through their non-destructive editing approach, where original files remain unmodified while edits are stored as metadata in a catalog database. This system allows for efficient incremental backups since large original files don't need to be copied when edited. Similarly, this principle of preserving originals while building upon them can be applied to urban farming initiatives, which can help address environmental challenges by preserving and improving soil quality. Urban farms naturally sequester carbon in soil through photosynthesis, where plants absorb carbon dioxide and transfer it to soil microbes through their roots. This process maintains the original soil while enhancing its fertility over time, allowing for increased food production while simultaneously reducing atmospheric carbon emissions.","context":["Lightroom is the main software for many photographers and hobbyists. Even if other software such as Photoshop are used, Lightroom is designed to be the main place for organising files, keywording, and basic editing, while other editors work as external editors from there.\nWhile Lightroom is quite intuitive and easy to use, it works in very different way than other image editing software. Therefore, it can be a bit intimidating to start with and many give up before or because of not understanding how it actually works.\nLightroom (or LR here, referring to Adobe Lightroom CC Classic, not the new cloud version) does not work like other software where you just open a file, edit, and save. Instead, Lightroom has a catalog database where you import all photos so they are kind of always open. Edits are data stored in the catalog in real-time. When you need an actual JPEG photo, you need to export it from the catalog.\nCommon Lightroom questions I will try to give some answers to are:\n- How does Lightroom work? What is Catalog?\n- Where are my photos?\n- Where do I store my Lightroom files?\n- How to make it faster?\n- What should I backup?\nWhen you understand how Lightroom works, you can answer the rest of the questions.\nI do not consider myself as a Lightroom guru but I have been using it since version 2 and do know quite well how it works. I am not a professional so the amount of photos I have is not huge; however, these principles should be easy to scale up as well.\nAlso remember, Lightroom is not a replacement for Photoshop! They are often compared but have been designed to work together, that is why Adobe CC Photographer subscription has Lightroom and Photoshop. Especially for hobbyists Lightroom may be all that is required but if you want to do detailed editing and use layers, Photoshop is the way to go. Lightroom is still meant to be the main software where you jump to Photoshop from and come back when you are finished.\nI also have Photoshop (although cannot use it well), Aurora HDR 2018, and Nik Collection, and all of those are used as external editors and plugins from Lightroom.\nHow Adobe Lightroom Works?\nImage data in LR consists of following parts:\n- Originals (RAW, JPEG, TIFF, DNG)\n- Lightroom Catalog (database)\n- Smart Previews (optional)\n- Adobe Camera RAW (ACR) cache\n- Originals are only used as source data in the background and they are never modified\n- RAW file is not even an image per se but data required to create an image\n- Catalog is a database containing all photo information: file location, all edits, keywords, etc.)\n- Previews are actual photos shown to the user, created from Originals using Catalog information\n- Smart Previews are smaller lossless versions of Originals\n- ACR Cache contains some form of partly processed Originals used for faster access\nNext I try to open these a bit more.\nLR’s main idea is to keep the Originals intact despite all editing while still remaining fast enough. Non-destructive editing that does not modify original files have been there since the beginning, while for example Smart Previews were added later to make LR faster and more flexible.\nEdits are actually just instructions stored in Catalog along with Originals location, keywords, rating, and other metadata. Catalog is a database. When an image is shown, resized copy of the Original is created and all edits rendered on it as per Catalog instructions. On Export all edits are applied on a high-quality copy of the original image. Therefore, original files are not modified in the workflow.\nReading the original files and creating copies of them every time an image is edited or shown is slow. To ease the processing load, various forms of cache and previews are used. Previews are processed images of various sizes (e.g. screen-size and 1:1 full-size previews) which are loaded instead of reading and processing the large Originals every time. When edits are performed, new Previews are generated and shown to user.\nThink of Previews the photos you actually see on screen and Originals as underlying data that is never shown.\nCamera RAW cache is something between original photo and preview, some form of partly processed RAW file. Smart Previews also sit between Originals and Previews; when enabled they are used instead of original photos for generating Previews.\nSmart previews were introduced in Lightroom 5 and are not automatically generated in the background unless user chooses to do so. When adjustments are made and Smart Previews in use, LR renders new Preview using the Smart Preview instead of the original photo. This leads to one major difference: Originals are not needed when editing photos using Smart Previews. Only on Export original photos are required to export highest-quality files.\nThis means that Originals can be on external USB drive or NAS but photos are still available for editing even if the drive is not connected – very useful for laptop users. Only on Export the drive needs to be connected.\nBut even if Originals are available on LR (the drive connected all the time), there is a setting to use Smart Previews instead of Originals on editing. This can be used to boost performance if Originals are on slow drive.\nThis simple functional description is probably not 100% accurate but gives an idea how LR works. When a photo is opened in Develop module for editing and Smart Previews are enabled:\n- LR loads the Smart Preview of the photo as a starting point\n- Reads the edit instructions from Catalog\n- Applies the edits on (a copy of) the Smart Preview\n- Shows the photo and saves the Preview\nIf no further edits are made, next time the same photo is shown LR loads the already-generated Preview. This likely also happens in Library module when just viewing photos. Note that Originals are not accessed at all.\nIf Smart Previews are not used, large and heavy Originals need to be read and processed in the first step although Camera RAW Cache helps this in some way.\nEnabling and Generating Smart Previews\nI recommend using Smart Previews even if your original files are available all the time. For example, my Originals are on NAS which is connected to my desktop computer always but I use Smart Previews stored on SSD for better performance.\nWhile normal previews are generated automatically as they are part of normal Lightroom funtionality, Smart Previews are not. When you take them in your workflow, it is best to create them on Import. Just tick “Build Smart Previews” box on Import, or save it in your import preset (I recommend using one). I also generate full-sized 1:1 on Import. It takes time to generate all these for larger batch of photos but I rather do it once and then enjoy smoother browsing of photos.\nIf you did not create Smart Previews on Import, it can be done later on Library module.\nThis is all that is needed for using Smart Previews – have them generated and ensure Lightroom uses them on editing.\nWhere to Store Lightroom Files\nIn terms of performance, we can decide how to store all files after understanding how they are used.\nIn the example above, Smart Previews, Previews, and Catalog are accessed during normal workflow. As long as these files are stored on fast SSD, there should be no serious performance bottlenecks. Therefore, when using Smart Previews, large RAW files can be stored on slow but cheap HDDs, USB drive, or NAS, without sacrificing performance. And that is why we like Smart Previews even on desktop workstations!\nIf Smart Previews are not used, LR needs to access the original file in the first step, which becomes bottleneck if these are on slow drive. Camera RAW cache helps this in some way but I cannot find much information how it exactly works. Anyway, Camera RAW Cache should be stored on SSD as well although I am not sure if it is even used when using Smart Previews.\nOne file type not mentioned yet is Presets. Presets can be used for everything: import, copyright information, develop settings, export, etc. There is a box in Preferences to store presets with Catalog. Then the Catalog, Presets, Previews, Smart Previews are all stored in the same folder, making backing up easier.\nWhat Files to Back Up?\nIt should be clear at this point that in order not to lose any work, you must backup Catalog and Originals. You probably want to backup Presets as well but the rest – Previews, Smart Previews, and ACR Cache – are optional as these can be re-generated.\nSome things that came to my mind when writing this.\n- Use only one Catalog – unless you have A LOT of photos. One Catalog should work fine at least with tens of thousands of photos, even more. Obviously huge number of photos make Catalog and Previews folders big as well, in which case you may want to consider using multiple Catalogs at least if you need to move them around.\n- Do not move files outside Lightroom. Among other information Catalog stores location of Originals. If you move them outside LR, it loses track. You can move files but just do it on LR.\n- Do not copy Originals if you want to give someone a copy of your image (unless you deliberately want to give the Original without any modifications). Export is the way to do it as it combines Originals and Catalog data. You can export not only JPEGs but also TIFFs, DNGs, and even Catalogs.\n- Because of this Catalog-based operation you can go back in editing history. You can also create Snapshots of edits or create Virtual Copies to have multiple different edits of the same photo. Snapshots and Virtual Copies do not copy the Original, just the Catalog entry. The same goes for Collections – they are nothing but new entries in the Catalog.\n- The fact that Originals remain unmodified is extremely useful for backup. Incremental backup does not need to copy your large files even if you edit them as the edits are stored in the Catalog. This is very useful if using cloud backup.\n- In Lightroom Catalog settings there is a tickbox “Automatically write changes into XMP” or on Library module you can choose “Metadata -> Save Metadata to File”. This saves Catalog data to the Original file itself. I never use this as I do not want to modify the Originals. You just need to take good care of your Catalog and its backups – see next tip.\n- Backup your Catalog often! Set Lightroom to ask for Catalog backup every time it closes (but tidy the backup folder occasionally as it becomes large). Remember, all your work is in the Catalog – which may get corrupted if LR crashes! Make sure you do not lose too much work if LR cannot recover the corrupted Catalog.\nNot sure how good job I did on answering the original questions but here is a recap:\n- How does Lightroom work?\n- Impossible to put in one sentence but it is vital to understand this in order to answer the rest of the questions.\n- Where are my photos?\n- All over the place! Originals are where you put them and remain unmodified. All editing, keywords, and anything done on Lightroom are stored in Catalog as metadata instead. Therefore, these should not be separated. Final images (e.g. JPEG) are created on Export using Originals and Catalog data.\n- Where do I store my Lightroom files?\n- See below.\n- How to make it faster?\n- Use Smart Previews, then store your Catalog, Previews, and Smart Previews on fast SSD drive. Large Originals can be on HDD, USB drive, or NAS.\n- Generate Smart Previews and 1:1 Previews for all photos before editing them to make browsing smoother (especially if you want to check photos 1:1 if they are sharp).\n- Obviously having decent hardware makes it faster. I have written another post regarding this.\n- What should I backup?\n- Catalog and Originals. You probably want to backup Presets as well.\nHope this helps! Feel free to drop a line if you have something to add or ask.","When you turn on the tv and watch the news, you’re quickly bombarded with bad news about climate change destroying the world as we know it and billions of people facing potential food and water shortages. It’s daunting, and it’s tough to assess where the data ends and sensationalism takes over, making it even more difficult to do something about the situation. It’s easy to feel insignificant when looking at the scale of the problem, which in turn leads to a lot of people feeling like they can’t contribute to finding a solution. This isn’t the case. There are opportunities to take control of your life, and in turn have a positive effect on the environment as well as your local community. One of these opportunities comes in the form of Urban Farming, and it’s starting to gain traction.\nAn urban farm is “a part of a local food system where food is cultivated, produced and marketed to consumers within that urban area.” These farms can take a variety of forms, including non-profit gardens and for-profit businesses, which provide jobs, job training, health education, as well as contributing to better nutrition and health for the community by providing locally grown, fresh produce. In addition, these urban farms can be used to help revitalize any abandoned or underutilized urban area and remediate the soil in brownfields. This remediation tackles the problem of contaminated land that may have negative consequences on our ground water and increases the amount of usable, fertile land.\nAddressing food shortages on a global scale presents a situation with an extremely difficult problem to solve, but when we focus on individual communities having access to healthy food options and eliminating the presence of food deserts, this problem becomes much more manageable. By creating community farms we can build a space for citizens of that community to grow some of their own produce, as well as create a connection with the origin of their food. This also contributes to peace of mind as they would know that their food is free of harmful pesticides. Taking this a step further, urban agriculture can be used to help feed the community at a larger scale, by producing a higher yield and a greater diversity of products. A distribution network can be created to either deliver these products directly to the citizens or redirected to a local farmer’s market in which these locally grown goods are available to purchase. Because of the close proximity to the source, costs will be reduced and the produce would be available for a longer window of time before they begin to spoil. Incorporating vertical farming and hydroponics into this system reduces the need for large plots of land while still providing the ability to produce a variety of fruits and vegetables.\nUrban farming is not only about growing produce though, as it can also include “animal husbandry (e.g., breeding and raising livestock), beekeeping, aquaculture (e.g., fish farming), aquaponics (e.g., integrating fish farming and agriculture), and non-food products such as producing seeds, cultivating seedlings, and growing flowers.” If these were to be incorporated it would go a long way in ensuring that the community can be self-sustaining, and therefore the community would no longer need to wait on policies to come to their rescue. Urban farming would also create economic opportunities for citizens, and the money would stay local so that further development and revitalization of the community could take place. It would also have a positive impact on reducing the amount of carbon emissions that have an effect on climate change.\nSequestering carbon in soil is a natural way of removing carbon dioxide from the atmosphere with fewer impacts on land and water, less need for energy, and lower costs than other means of capturing carbon emissions. The Earth’s soils contain about 2,500 gigatons of carbon, more than three times the amount of carbon in the atmosphere and four times the amount stored in all living plants and animals. Traditional agricultural practices— such as tilling, planting mono-crops, removing crop residue, excessive use of fertilizers and pesticides, and overgrazing— expose the carbon in the soil to oxygen and allows it to burn off into the atmosphere. Soil sequestration occurs through the process of photosynthesis, by which plants absorb carbon dioxide from the atmosphere and use water and sunlight to turn the carbon into leaves, stems, seeds and roots. Some carbon is then released into the atmosphere through respiration and some is exuded into the soil as a sugary substance through the roots, which then feeds the microbes that live underground. These microbes are beneficial for the viability of the soil and increase the fertility of the land so more plants can then be planted and more produce can be cultivated. This means that we can help to feed our communities with healthy, locally grown foods while at the same time helping to reduce the amount of carbon emissions in the atmosphere, solely through the use of nature.\nIn a time when our problems seem too grandiose for us to be able to do anything about them, it’s important for us to realize that there are solutions that we can implement ourselves that would not only benefit our local communities, but can also have an aggregate benefit to the rest of the world. There are plenty of resources out there that provide information on how to practice urban farming, whether at a small,at-home scale or on a larger, community oriented development, such as the Urban Agriculture Toolkit and courses offered at various institutions, including the Huntington Library Botanical Gardens. All that’s required is for the community to step up and take matters into our own hands."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:182bb60c-0417-4ecc-9636-8ba274e5ebcc>","<urn:uuid:8483995d-2308-4eb0-9f33-73b04dab63b7>"],"error":null}
{"question":"How do church planning initiatives and SPRED differ in their approaches to achieving community transformation?","answer":"Church planning initiatives achieve transformation through systematic community analysis and targeted interventions, as demonstrated in Montreal where research led to specific programs addressing issues like high school dropout rates and intergenerational poverty. They focus on understanding community needs through detailed demographic and social analysis. In contrast, SPRED achieves transformation through building intimate faith-based relationships, emphasizing individual dignity and integration into parish life. Their approach centers on personal bonds between faith companions and friends with learning difficulties, creating transformation through shared spiritual experiences and mutual growth in faith.","context":["When we use the word “missional,” we are affirming that the Church by its very nature is sent into our neighbourhoods and cities as people living the implications of the Gospel. In order for the church to do this effectively, it is beneficial to know how to “exegete the neighbourhood,” in much the same way that they have learned to study a biblical text. Reading your community is the first step to demonstrating a commitment to partnering with the community and establishing relational trust. This fosters a greater openness to a discussion of deeper, spiritual needs, and to Christ who meets all needs.\nTo begin the process of reading one’s community, let me propose steps to consider. These are best undertaken by teams—usually ecumenical “task forces”—that try to understand their community context. After the “exegesis” or community assessment, it will be important to prioritize the initiatives that congregations will undertake.\nThese steps can be divided into two sections – 10 Steps to Understand Context & 10 Steps to Consider Church Planting Initiatives.\n10 Steps to Understand Context:\n- Compile a list of significant historical events that inform the community’s identity. These could be historic conflicts that took place, unifying events such as the city coming together to fight a massive fire, decisions that leaders made such as the building of a community centre, or something that happened that gave people hope, such as a person doing something heroic or selfless. These will provide clues to the best way for the church to focus its energy.\n- Study the growth patterns of the city. One can find this information in libraries, city councils, museums, bookstores, local newspapers and on local websites.\n- How and why is the city growing?\n- Who are (and were) the immigrants to the city? Where did they come from and where are they settled? Where are they employed?\n- Study the design of the city.\n- Understand clearly the sections or zones that make up the city.\n- Find out from city planners and real estate offices where city populations are expected to move, where commercial and industrial zones will develop, and which areas are slated to undergo major changes.\n- Study the neighbourhoods: their ethnic, social and economic composition, religious affiliations, occupational patterns, younger and older populations, concentrations of the elderly, young professionals, singles, and problem groups. To understand a neighbourhood, you must walk the streets and talk to people, insiders and outsiders. Census data is important, but onsite observation is best. People groups criss-cross in the community.\n- Determine and analyze the power centres in the community: the political figures, the police department, the business leaders and the Chamber of Commerce, the religious leaders. Who controls the media—TV, radio, the newspapers? Who controls commerce and finance? The schools and the arts? What are the religious and moral commitments of the people in positions of power?\n- Analyze the dreams, aspirations, hopes, and expectation of specific people groups within the community. Observe the obstacles that various groups identify to attain these aspirations. In some communities, such things as personal illness, loneliness, physical hardships, insecurity in terms of housing, property rights, and the threat of losing one’s dwelling are very real. You are looking for indications of receptivity and “keys” which may unlock doors to homes and hearts.\n- Examine the traffic flow. Just as successful advertisers know where to place their signs, practitioners need to know where to begin their ministries, where they can readily be seen and reached. Find out where these sites are located: community service centres, libraries, police stations, fire stations, city hall, shopping centres, sports facilities, and any other significant places.\n- Seek to discover how news and opinion spread in the community, and in particular groups. Mainly through conversation? By radio or TV? Who are the idea-people, the opinion-makers? Subscribe to the weekly publication in the area. Read it faithfully.\n- Examine the relationship between city-dwellers and the rural, small-town communities outside the city. Do certain segments of the urban population maintain strong ties with their rural cousins? Is there a lot of travel and visiting between city and village? What are the present immigration patterns from the countryside? How might the urban-rural interaction be used for the spread of the gospel and multiplication of churches? Most of this information is available in census data.\n- Identify and map the local ministries and churches in the community; identify them by denomination, size, and age. What transformational ministries and social services are already taking place through these ministries and churches? Reflect on what the church map shows.\nSteps to Consider for Church Planting Initiatives:\n- Analyze the various types of existing churches. Find out the growth patterns (if any) of the various churches: attendance, membership, and rate of growth. Try to determine the nature of the growth: is it by transfer, conversion, or by births? One can often locate this information by chatting with congregational leaders.\n- Inquire about church planting efforts and church closures in the past several years. Which churches have closed? Why? Who has started new churches, and why and where did they succeed? Learn all you can from them.\n- Who is currently planning to start new churches? Where, and among which people groups? Find out all you can from church and mission sources as to what is being planned for the community.\n- Strategies: what has been tried in the past, what has failed, and what was effective in starting churches and stimulating growth? Analyze the information you receive. In the light of recent church growth studies, what has been “done right” in this community, and where ought things to have been done differently?\n- Christians and non-Christians: where are the Christians located (of course, this may not be where they attend church)? Identify areas of the city where relatively few Christians live.\n- Identify Christians in positions of influence in the city: in business, politics, the media, education, entertainment, and sports. Analyze their potential for wider spread of the gospel and assistance in planting churches.\n- List and analyze the parachurch ministries (if any) operating in and to the community. How might each contribute something to the overall strategy? Are there some you may want to avoid because they might have a negative influence on church multiplication?\n- Make an inventory of all possible personnel resources that might be tapped for the carrying out of your church planting strategy. For example, are there Bible College or seminary students available to help with door-to-door calling? Could interns be borrowed from existing churches to help younger congregations?\n- Evaluate all known methods for church planting in light of what you know about this community—its history, people, existing churches, and particular characteristics. What methods have proven effective elsewhere and appear appropriate for this community—and are within the capabilities of your resources.\n- List and evaluate the community agencies (private, religious and civic) that are designed to meet particular needs (literacy, overnight shelter, emergency food and clothing, and so on) and consider how their help can be incorporated into your overall strategy.\nApplication in Montréal\nExegeting a community in Monreal brought transformation and fruitful community-wide collaborative ministry opportunities. In 2006, Christian Direction, in conjunction with Roman Catholic parishes and Protestant congregations, published a 45-page study of the east end of Montréal, using these twenty steps. This borough is the poorest continuous borough in Canada going back to the 1871 census!\nNumerous initiatives have been launched as a result of that study. For example, to address the 55% high school drop-out in the neighbourhood, in collaboration with another social service agency, we started a centre to work with adolescents, their families, and the local schools to promote school success. Together with others in the borough, we advocate with local businesses so they do not employ kids during the school day. To address the relational poverty, there are four events each year to break the deep solitude people experience. To break the cycle of intergenerational poverty, three “financial capability” projects have been started for young people and their families. In a short time, all three were generating income. At the heart of the vision are the key indicators of a transformed community. This same experience is now being repeated in 18 other boroughs on the island of Montréal and four other cities in Québec.\nThis example emphasizes the efficacy of how research can and must inform transformational strategies. In order to further the church’s mission in our communities, we have to learn first of all to read those communities.\nThis article is an excerpt from a more detailed paper by Glenn Smith: Reading your Community: Towards an Authentic Encounter With the City.\n In Québec, Christian Direction has worked with congregations in ten different cities and boroughs to implement these steps. To obtain a copy of one of the studies, visit our website: www.direction.ca.\n David Ley, “The Inner City,” in Canadian Cities in Transition: Local Through Global Perspectives, ed. Trudi Bunting and Pierre Fortin (Don Mills, Ontario: Oxford University Press, 2006) 195.","S.P.R.E.D. – Special Religious Development.\ne were born into the community of a family and extended family. We depended on this community for the fulfilment of all our needs, physical, emotional, social, moral, economic, and religious. Our community of family went a long way towards making us who we are. When we grow up we can move away from home, get a job, set up a home; in other words become independent. Our friends with learning difficulties do not have these options in the same way. They really do need a community that will allow them to be individuals who have dignity, choice,and a sense of giving as well as receiving, a sense of being part of a community. They need friends outside their families and work places. The first community we experience outside the family is that of the Christian community when we are brought to church to receive the Sacrament of Baptism.\nEach Baptised person is welcomed into the community of the parish and is given the opportunity to develop his/her faith, initially by receiving the sacraments and then by ongoing attendance at the Eucharist and other liturgical events. Our aim in Spred is to integrate our friends into the liturgical life of the parish. Our faith is our greatest gift from God. Our friends have as much right as we have to grow in their faith. We, in Spred, can give them a chance to experience the spiritual in life, also a chance to grow in the faith that they received in seed at baptism. Faith education is only possible if our friends feel they are loved. We are opening a door that allows our friends to appreciate who God is for them. Often it is our friends who opens the door for us. It is by giving to those who have learning difficulties that we not only receive but also enable them to give. Then, their joy, simplicity, and honesty, for example, may be seen and experienced by others. Others are consequently modified by being with them. With them we can be weak and receive of their strengths, of their love for us, of their trust, of their approval, if we give them time and space to be with us.\nSpred is all about friendship. We are referred to as volunteers while we ourselves are known as faith companions because that is our role- to be a companion in faith to our faith friends. We all need friends outside of our family. We all need community. It takes time to build up this bond of friendship. We realise to be a faith companion we need to accept one another as we are, to be loving, sincere, honest, genuine, compassionate, kind and authentic. Our friends can easily discern if we are lacking in these attributes. We respect one another and this can be seen in the way we relate to one another. We are sharing our faith. We are faithful companions on a journey with our friends. The depth of our bonding as a small faith community is a symbol of the depth of God’s love for us. Through Spred our friends can experience the Church as a group of believers who come together to share life.\nA Christian community, is the web of relationships that provides the sacred, safe space where all members can discover and develop their uniqueness, their giftedness. It is the locus where God, acting through others, removes the scales from our eyes so that we can discover God’s presence in every respect and dimension of our lives. Through this process, we become seers of God in everything and in everyone and this vision impels us to go out of ourselves to serve others. We often forget that the English word ‘church’ translates the Greek word’ ekkl-sia’, which designates a gathering of people. A great metaphor for community is given to us by St. Paul in his first letter to the Corinthians (12:12-27). Paul uses the image of the human body to explain Christ’s relationship with believers. Paul explains the need for diversity of function among the parts of a body without threat to its unity. Community is not just ‘a place’; it is a living organism, dynamic, alive, and always changing, just like a family. In a healthy family system, the role of each member is not defined by his or her level of productivity, but by the love that is given and received.\nJesus teaches us clearly that God calls us, not just as individuals, but as a community and that how we relate to each other is just as important religiously as how we relate to God. Or, more accurately, how we relate to each other is part of how we relate to God. For Jesus, the two great commandments, to love God and love neighbour, can never be separated. In Spred we have a wonderful opportunity to reflect on some aspect of our lives each week. We also read a small section from God’s Holy Book. In Spred we share our own experiences of our daily lives. They may be little happenings that have stayed deep within us. Meaning is derived from the shared experience. The event, the experience, can be entered into time and time again. The re- telling of the story, its re-enactment, can put us in touch with the primary experience. In the theology of Karl Rahner, “God is experienced in the ‘mysticism of everyday things – not in the distilled essence of things, not the highest abstractions from the world but the experience of God’s life at the very heart of the world, in flesh, in time and in history. Presence to self, presence to the world, and presence to God are all aspects of one and the same experience, the experience of God’s real presence in the world which he created to be his real symbol”.*\nSpred companions and friends grow in faith through the process known as symbolic catechesis or the method Vivre. This process is to interpret life situations within a group, through the process of evocation, in faith, in the light of the Word, moving toward communion and witness. We recognize the gift of each other; that each of us is a gift to the other. Like any gift, none of us is perfectly wrapped – we all have our faults, failings and problems, but we are happy to be a gift to another who accepts us as we are. It is only by giving that you are able to receive more than you have already.\nWe have all come to be companions for different reason, many and varied, but at the heart of these surely must be the willingness and urge to share the gift of faith that we have by which we live our lives. All faith companions come with a wish to help, encourage, and share what they had experienced on their faith journey with others. All companions can speak of the gifts they have received on that journey and want others to be able to experience the same. Faith companions tell of how much they get from their faith friends and the other group members; about the gifts they receive which they never expected; and how they all hope that the giving and receiving of all these amazing , wonderful gifts will just go on. This experience is also felt by the families of the faith friends and their own families and the wider community because the community of Spred have become more loving people or as Pope Francis said – we are part of the ‘Revolution of Tenderness’.\nReferences: *The Achievement of Karl Rahner; W. Dych. 1984.\nThe Way of the Learning Disabled; J. McClorry. 1995.\nThe Spirituality of Community; A.J. Gonzalez. 2009."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:975c56aa-e6c3-4aa8-869f-c99523b5357b>","<urn:uuid:7b83812e-f8e5-44c6-b657-041babb42a04>"],"error":null}
{"question":"What strategies did the North Vietnamese use to control and support the Pathet Lao movement during the 1950s and 1960s?","answer":"The North Vietnamese employed several key strategies to control and support the Pathet Lao. In 1954, they maintained forces along Laos's border areas despite Geneva Conference agreements requiring withdrawal. They formed specialized units: Group 100 at Ban Nameo in 1954 to direct the Pathet Lao movement, and Group 959 in 1959 to supply, train and militarily support the Pathet Lao. Their typical battle strategy involved North Vietnamese regulars attacking first, then having Pathet Lao forces claim victory afterward. By the mid-1960s, North Vietnam launched a multi-division invasion, reducing the Pathet Lao to an auxiliary force of the North Vietnamese army.","context":["The Pathet Lao (Lao: ປະເທດລາວ pa thēt lāo, \"Lao Nation\") was a communist political movement and organization in Laos, formed in the mid-20th century. The group was ultimately successful in assuming political power in 1975, after the Laotian Civil War. The Pathet Lao were always closely associated with Vietnamese communists. During the civil war, it was effectively organized, equipped and even led by the People's Army of Vietnam. They fought against the anti-communist forces in the Vietnam War. Eventually, the term became the generic name for Laotian communists.\nFlag of the Pathet Lao\n|Active||1950 – 2 December 1975|\n|Battles and war(s)||Indochina War|\nLaotian Civil War\nCommunist insurgency in Thailand\nThe most important source of military aid to the movement (as was also the case for the Vietnamese communists) was China; under orders from Mao Zedong, the People's Liberation Army provided 115,000 guns, 920,000 grenades and 170 million bullets, and trained more than 700 of its military officers.\nThe political movement of the Pathet Lao was called first the \"Lao People's Party\" (1955–1972) and later the \"Lao People's Revolutionary Party\" (1972–present).\nThe political wing of the Pathet Lao, called the \"Lao Patriotic Front\" (Lao: Neo Lao Hak Xat) served in multiple coalition governments, starting in 1956. Through the 1960s and 1970s the Pathet Lao battled the Royal Lao government during the Laotian Civil War, gaining control of the north and east of Laos. The Pathet Lao gained power throughout the country by the spring of 1975. In December, the US-backed Vientiane government fell and the Lao People's Revolutionary Party formed a new government.\nThis section needs additional citations for verification. (December 2014) (Learn how and when to remove this template message)\nThe organization can trace its roots from the Second World War, just as can the Khmer Issarak in Cambodia and the Viet Minh and Vietnam People's Army in Vietnam. Originally the Lao Issara, an anti-French, non-communist nationalist movement formed on October 12, 1945, it was renamed the \"Pathet Lao\" in 1950, when it was adopted by Lao forces under Souphanouvong, who joined the Viet Minh's revolt against the colonial French authorities in Indochina during the First Indochina War.\nSouphanouvong, who had spent seven years in Nha Trang during his sixteen years in Vietnam, met Ho Chi Minh, married a Vietnamese woman while in Vietnam, and solicited Viet Minh aid in founding a guerrilla force.\nIn August 1950, Souphanouvong joined the Viet Minh in their headquarters north of Hanoi, Vietnam, and become the head of the Pathet Lao, along with its political arm dubbed \"Neo Lao Issara\" (Free Lao Front). The Pathet Lao founded resistance government with members: Souphanouvong (prime minister, minister of the foreign), Kaysone Phomvihane (minister of the defence), Nouhak Phoumsavanh (minister of the economy), Phoumi Vongvichit (deputy prime minister, minister of the interior), Souk Vongsak, Sithon Kommadam, and Faydang Lobliayao. This was an attempt to give a false front of authority to the Lao communist movement by claiming to represent a united non-partisan effort. Two of its most important founders were members of the Indochinese Communist Party, which advocated an overthrow of the monarchy as well as expulsion of the French.\nIn 1953, Pathet Lao fighters accompanied an invasion of Laos from Vietnam led by Viet Minh forces; they established a government at Viengxay in Houaphan province in northeast Laos. The communists began to make incursions into central Laos with the support of the Viet Minh, and a civil war erupted; the Pathet Lao quickly occupied substantial sections of the country.\nThe 1954 Geneva Conference agreements required the withdrawal of foreign forces, and allowed the Pathet Lao to establish itself as a regime in Laos's two northern provinces. The Viet Minh and North Vietnamese, in spite of the agreement, never really withdrew from the border areas of Laos, and the Pathet Lao continued to operate almost as a branch organization of the Viet Minh. Two months after the conference, the Viet Minh-North Vietnamese formed the unit Group 100 with headquarters at Ban Nameo. The unit effectively controlled and directed the Pathet Lao movement.\nIt was formed into an official party, the Lao Patriotic Front (Neo Lao Hak Sat), in 1956. Its stated goal was to wage the communist struggle against capitalism and Western colonialism and imperialism. Unstated was its subordination to the Communist Party of Vietnam. A coalition government was established in 1957 between the monarchists and communists, but it collapsed in 1959, bringing about a resumption of fighting.\nBy the late 1950s, North Vietnam had occupied areas of eastern Laos. The area was used as a transit route for men and supplies destined for the insurgency in South Vietnam. In September 1959, North Vietnam formed Group 959 in Laos with the aim of building the Pathet Lao into a stronger counterforce against the Lao Royal government. Group 959 openly supplied, trained and militarily supported the Pathet Lao. The typical strategy during this era was for North Vietnamese regulars to attack first but then send in the Pathet Lao at the end of the battle to claim victory.\n1960s and 1970sEdit\nIn the 1960s, more attempts at neutrality agreements and coalition government were attempted but as North Vietnam had no intention of withdrawing from Laos, these agreements all failed. By the mid-1960s, the country had fallen into proxy warfare between pro-US and pro-Vietnamese irregular military groups. In 1968, the Army of North Vietnam launched a multi-division invasion of Laos. The Pathet Lao were pushed to the side in the conflict and reduced to the role of an auxiliary force to the North Vietnamese army. Unable to match the heavy Soviet and Chinese weapons in addition to the numerical strength of the Vietnamese forces, the Royal Lao Army took itself out of the conflict after heavy losses.\nThe communist forces battled the Royal Lao Army, US irregular forces (including Air America and other contract employees and Hmong commandos), and Thai volunteer forces in Laos winning effective control in the north and east. The government itself was effectively powerless. The Pathet Lao held hundreds of US \"detainees\" as prisoners of war during and after the Vietnam War (Second Indochina War).\nIn December 1974, they were responsible for the killing of Charles Dean and Neil Sharman.\nThe coalition government envisaged by the treaty lasted only two years. The Pathet Lao refused to disarm and the North Vietnamese Army did not leave the country. In late May 1975, the Pathet Lao, with the direct assistance of the North Vietnamese Army, began attacking government strongholds. With the fall of the South Vietnamese government to the North Vietnamese on 30 April 1975 and the fall of the Cambodian government to the Khmer Rouge on April 17, the non-communist elements of the national government decided that allowing the Pathet Lao to enter power would be better than to have them take it by force. On 23 August 1975, Pathet Lao forces quietly entered the Lao capital city of Vientiane.\nOn 2 December 1975, the Pathet Lao firmly took over the government, abolishing the monarchy and establishing the Lao People's Democratic Republic. Shortly thereafter, the Pathet Lao signed an agreement with Vietnam that allowed Vietnam to station part of its army in the country and to send political and economic advisors into Laos. Vietnam afterward forced Laos to cut any remaining economic ties to its other neighbours, including Thailand and Cambodia.\nAfter the Pathet Lao took over the country in 1975, the conflict continued in isolated pockets. In 1977, a communist newspaper promised the party would hunt down the \"American collaborators\" and their families \"to the last root\". With the demise of the Soviet Union, control of Laos by Vietnam waned at the end of the 1980s. Today, \"Pathet Lao\" is often invoked as a general term signifying Lao nationalism.\n- Andrea Matles Savada, ed. (1994). \"The Pathet Lao\". Laos: A Country Study. GPO for the Library of Congress. Retrieved August 8, 2011.\n.... The basic stance of this front's propaganda was the united struggle against the French without reference to political parties or ideology. Illustrative of this stance was the use henceforth of the name Pathet Lao (Lao Nation).CS1 maint: extra text: authors list (link)\n- Brazinsky, Gregg A. (2017) Winning the Third World: Sino-American Rivalry during the Cold War, p. 249, The University of North Carolina Press\n- \"Pathet Lao\". britannica. Retrieved 9 March 2013.\n- At War in the Shadow of Vietnam: U. S. Military Aid to the Royal Lao Government, 1955 - 1975. p. 7.\n- Laos: The Pathet Lao Library of Congress Country Studies\n- At War in the Shadow of Vietnam: U. S. Military Aid to the Royal Lao Government, 1955 - 1975. pp. 7, 142–143.\n- Stalker, John N. (1974). \"Laos\". The World Book Year Book 1974. Chicago: Field Enterprises Educational Corporation. p. 375. ISBN 0-7166-0474-4. LCCN 62-4818."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ee879159-7cfe-4e17-bb87-c127487466d2>"],"error":null}
{"question":"Could you compare traditional portrait photography with more experimental approaches regarding eye contact? What are the differences?","answer":"Traditional portrait photography typically has the subject looking directly into the camera lens to make eye contact with the viewer. In contrast, experimental approaches involve breaking eye contact by having the subject look at something outside of the frame or at another object within the frame, such as their hands. This alternative approach can create powerful images that convey more emotion.","context":["Great portrait photography is notoriously difficult to accomplish. People having their photographs taken are often uncomfortable at first, which can result in stilted looking photos; portrait photographers also depend on variables in the surrounding environment, including the weather, to help make a great portrait. There can be so much happening at any given moment the photographer can become distracted and overwhelmed by details. However, you should remember that no matter what happens there are no hard and fast rules for making a great portrait. These tips will help you snap photos beyond the standard headshot for truly amazing prints.\n1. Break eye contact.\nTraditional portrait photography usually has the subject looking directly into the camera lens, so that they appear to be making eye contact with the viewer of the final photo. With digital photography, you can experiment with breaking eye contact without breaking the bank on film. Try having your subject look at something outside of the frame, or even at another object within the frame, such as their hands. This can create powerful images that really pop with emotion.\n2. Introduce props.\nUsing props in portrait photography can create additional interest, and when the prop reflects an aspect of the subject’s personality, the person you are photographing might even feel more comfortable, leading to more candid shots. Props can be meaningful, like family memorabilia, or just fun, like toy balls. These types of portraits are also the most fun to have printed as canvas prints or digital prints for wall art!\n3. Pay attention to your Composition techniques.\nMost portrait photography follows the Rule of Thirds an imaginary grid of nine blocks, across the center of which one envisions a cross going horizontally and vertically. Following this rule, the main photo has more interest when the subject is not placed in the center of the photo. Experiment with placing your portrait subject off to the side of the frame, or on the bottom of the frame looking up, or even holding the camera at a slight angle so that your subject fills the frame diagonally, for new and interesting portraits. Basically, get your subject out of the middle of the frame and you will instantly have more interesting portraits.\n4. Play with motion.\nNot everything in a portrait needs to be in focus. In addition to using selective focus, why not play with motion to add interest and drama to your portrait photography? You can have the subject move, frame the subject against a moving background, or even move the lens of your camera during a shot to introduce interesting blurs and a sense of liveliness to your photos. Shooting with a tripod will help you achieve these effects without additional, unintentional movement that can distract from the composition. Great for sport portraits or for super energetic child portraits.\n5. Learn how to do your own edits.\nIt’s all too easy to rely on Instagram and similar services to edit your digital photography. While the filters these services offer can be eye popping and have their place, viewers can usually tell the difference between a carefully edited portrait that relies on its own merits and a portrait that has been edited with an instant filter. Taking the time to learn how to do edits in Photoshop, Corel, and other semi-professional to professional level photo editing suites will be more than rewarding, and will help you take your portrait photography to the next level."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:b6e2b6e9-7df9-4f96-ba71-c8e4e78a85c9>"],"error":null}
{"question":"What are the key challenges in managing building systems, and how do professional expertise requirements differ between smart buildings and restoration projects?","answer":"In smart buildings, key challenges include optimal placement of sensors affecting their performance, integration of space data with sensor data, and complex documentation management. For restoration projects, professionals need expertise in architectural survey, decay detection, and understanding of historical building features. Both fields require interdisciplinary knowledge - smart buildings demand expertise in integrated control systems and network design, while restoration requires knowledge of historical construction techniques, materials science, and conservation principles. Both areas emphasize the importance of proper documentation and data management for successful implementation.","context":["Survey & Restoration Course\nThe course (in English) provides the basics of theory and history of restoration; it provides architectural survey, decays detection and conservative intervention capabilities and informs on the legislative framework; it defines the design drawings needed for restoration.\nRegarding Survey knowledge, the course deepens the knowledge of laser scanners and photogrammetry methodologies, for the construction of technical drawings for the drafting of restoration and heritage conservation projects.\nBased on the skills gained during the course, the student will have to demonstrate, through laboratory exercises, the assimilation of theoretical notions by transforming them into operational practice.\nThe purpose of the course is to implement critical skills and operational tools to improve the care process towards the existing building. The nodal point of this path is the achievement of an adequate degree of awareness of the features that the intervention on the buildings presents: the operational contents and the main interdisciplinary connections, but also the issues related to its historical, cultural and contemporary growth resource significance.\n- Definitions and basic concepts.Foundations, objectives and application fields of the discipline.\n- The role of knowledge in the historical path of restoration discipline.\n- The restoration project as a project of knowledge. Current methodological developments for analysis and diagnosis.Standards and restoration c Procedures and bureaucratic iter;the European and international situation.\n- Relations with the specialized disciplines and the fields of study involved.\n- Collection, management and interpretation of Diagnosis data for the restoration project.\n- Drafts and representation for restoration.\n- Analysis and interpretation of the state of conservation: recognition of pathologies, identification of causes, evaluation of evolutionary processes of phenomena.\n- Analysis and interpretation of the structural instability: recognition of the forms of instability, identification of the causes, evaluation of the phenomena of evolutionary processes.\n- The restoration project.\nAt the end of the course the student must know:\n- Fundamentals of theory and history of restoration\n- Constructive features of historic building and typological solutions.Structural conception and constructive techniques in their historical development. The Italian legislative framework;\n- Diagnosis of the restoration. Instrumental Diagnostics\n- Survey for restoration and direct analysis of buildings.\n- Materials and structures: analysis, recognition, degradation, disruption, and diagnostics.\n- The restoration project as a process of conservation of materials, preservation of structures, reuse and functional rehabilitation.\nAt the end of the course the student must learn:\n- The management of 3D databases\n- Knowing adequately the main notions of theory and history of restoration.\n- Recognize and analyze the main constructive techniques.\n- Develop appropriate considerations between forms of decays and causes.\n- Interpret the structural behavior of a historic building.\n- Know and evaluate the new guidelines for the restoration and reuse of historic\n- Realize and adequately present a restoration project that evaluates the building as an unicum, enhancing its potential.\nThemes developed in the Didactic Course\n- a.a. 2014/2015 – Capo d’Orlando municipal palace- Nocca Palace in Barbianello\n- a.a. 2015/2016 – Nocca Palace in Barbianello\n- a.a. 2016/2017 – Rocca S. Silvestro fortress – Monumental staircase of Central University\n- a.a. 2017/2018 – Porta Nuova monument in Verona’s military wall system\n- a.a. 2018/2019 – Churches and monumental sites along Upper Kama route (Russia)","“Smart buildings are the solution to check the energy consumed by buildings & the carbon emission caused by them. They are more than automatic lighting & CCTVs. These buildings are controlled by an intelligent network of sensors and actuators with computing & communication systems that greatly increases the efficiency of buildings while supporting the inhabitants in their day-to-day activities. Everything from energy consumption, water consumption to air conditioning, security & lighting are automated.”\n– Annrin K Thomas, Environmentalist & Civil Engineer at Advenser Engineering Services\nThe foundation of a smart building is made out of data. In order to leverage the most out of smart buildings everything starting from design, procurement, fabrication, assembly & implementation to commissioning, operation & maintenance has to be done through a fully integrated platform.\nWhere does BIM come in the case of smart buildings?\nUnlike constructing conventional buildings, constructing a smart building involves the integration of the core systems of a building such as lighting, electricity meters, water, Heating Ventilation & Air conditioning systems, firefighting system, water pumps with a network of sensors and an integrated control system.\nThis is where BIM comes into play!\nBuilding Information Modeling (BIM) is all about integrating the several aspects that go into construction and it is the pre requisite in constructing a smart building. Which makes BIM the right tool in the construction of smart buildings.\nHow does a Smart Build Environment (SBE) work?\nBefore we dwell deep into the benefits of incorporating the various BIM services in the construction of smart buildings, let’s have a quick look into how a smart build environment work.\nTraditional buildings have provided the inhabitant’s shelter, protection from extreme weather & safety at the same efficiency level for years. Unlike these buildings, smart buildings can be considered as living organisms connected to an intelligent network.\nSmart buildings are constantly evolving while significantly improving efficiency. There will be dozens of smart objects like sensors and actuators installed in the smart buildings. These smart objects will constantly monitor & interact with their immediate environment and will be in communication with each other either through Ethernet cables or over an internal wireless network.\nSmart buildings are known to make its inhabitants more productive with better lighting, thermal comfort, better air quality and better security at a lower cost and environmental than the traditional buildings.\nChallenges faced in a Smart Build Environment\nLike we discussed, dozens of smart objects will be ubiquitously installed in an SBE to perform sensing and control of the immediate environment. The three mail challenges faced in an SBE are\nChallenge 1: The physical location where smart objects such as sensors are embedded can greatly affect their performance and ability tocarry out certain tasks. For e.g. The location of light detecting sensor can greatly affect the ambient light or occupancy it senses.Apart from this, the wireless network of the Ethernet cable through which these smart objects communicate should be designed to enable perfect communication between them.\nChallenge 2: The way these smart objects embedded in SBEs interact with their environment is very crucial. These objects can’t really function solely on the data they collect from their immediate surrounding rather they need to be fed the space data such as floor plans of the buildings too. In the case of conducting a performance analysis or energy analysis of the SBE building, along with the information from the sensors & meters data on building architecture and geometry are also needed.\nChallenge 3: Maintaining the smart objects in the building and properly documenting the data from an SBE building is not a walk on the beach. In an SBE building, the data will be more complicated than the traditional structures\nBattling the challenges with BIM\nIn the pre-construction stages of a project, BIM can be extremely vital. The data rich graphical model can be utilized to identify any shortcomings before first breaking ground.\nThe primary advantage of designing an SBE building with BIM lies in the fact that a data rich BIM model can be used to plan the layout and placement of the sensors, actuators, tags and meters inside the SBE building. The performance of the SME building can be verified against the known parameters and the layout and location can be optimized for the best functional performance.\nIncorporating BIM in SBE buildings can be the answer to the challenge of asset tracking & maintenance during the post construction phases. A BIM model can store the physical information of smart objects & their installed location these data can be visualized in 3D.\nIt is time to embrace the fact that the buildings we work & live in shape us! A better building always influences how happy and productive the inhabitants are. Smart buildings are the future of the construction industry.\nThe journey to constructing these buildings start long before first breaking ground. BIM is the only futuristic technology that can drive that journey."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3b9a7fbb-7214-47df-b2c8-bd2077a6e67b>","<urn:uuid:2c8fbb53-810a-4c2d-a156-c9e9abd7e5dc>"],"error":null}
{"question":"What factors affect the choice between carbide and HSS tools for both woodworking and metalworking applications?","answer":"The choice between carbide and HSS tools depends on several factors. In woodworking, HSS is often better for soft woods like cedar, pine, and poplar as it provides a sharper cutting edge. In metalworking, the decision depends on factors like hole size (carbide drills above 1/2 inch are expensive), number of holes to be produced (HSS might be more cost-effective for low quantities), and workpiece material (carbide is better for abrasive materials). Additionally, machine capabilities matter - carbide tools require rigid machine tools with minimal spindle runout, while HSS tools can be more forgiving in less precise setups.","context":["|Home » Forums » Solid Wood Machining Forum » Message||Login|\nYou are not logged in. Consider these WOODWEB Member advantages:\nMetal Detector For Staples!2/14/17\nI plane alot of rough sawn clear cedar, and I'm getting tired of ruining my knives when hitting staples, even after we check and double check for them! Just dropped off 3 sets of knives for sharpening and it's getting old so old.\nSo I need a better detector the the cheap harbor freight one I have, hopefully one that's specifically for scanning rough lumber. Would anyone have any suggestions for me?\nanother question, we often find little round pieces of something hard embedded in the lumber that's also chipping the knives I bet, what is that, road grit from Canada?\nThe grit is small bits of gravel coming from the forks and yard of the folks that handle the Cedar before you get it. Watch how the handle it, often setting one end in the gravel, then picking it up again. This puts hundreds of little bits into the wood, to be found by your knives.\nDavid, I've heard carbide knives aren't as sharp as regular steel knives, and therefore not the best choice for planning soft wr cedar?\nIt is true that carbide does not have as sharp of a cutting edge as HSS. For many species of wood, HSS knives are a better choice. Poplar, many pines, firs and many cedars will finish better with steel versus carbide.\nI teach it this way , the softer the wood the more you want to slice it , the harder the wood the more you want to scrap it. With this said using steel for soft to cut woods generally works better. With the introduction of DGK, a DLC coating you can achieve the finish quality of steel with the life of the tool that will approach that of carbide.\nDo you have a link for those DGK knives? When I google it all I see is Dirty Ghetto Kids :-)\nwww.moulderservices.com. Has the info on the DGK material\nI'm not sure where it's made, but we use a Garrett metal detector purchased from Amazon. It works great, even on \"cut-backs\" (i.e. 9' boards in a 10' pack).\nHowever, I have this tidbit to add. We have long been annoyed by staples in wood. The metal detector was Step 1. Then we found a company that had developed Composite Staples - you can machine them with no damage to woodworking tools! Rather than wait for the Trickle Down effect, I decided that we would take the helm and show our suppliers that our money was where our mouth was, and we Wanted Composite Staples. So we bought the Special Gun (about double the price of a regular one) and bought a case of staples (more than double - don't remember off-hand) and started using them.\nFirst impression - good. They worked every bit as good as metal ones.\nHowever, one of our suppliers started using composite staples last year. Since they have no Iron in them, the Metal Detector doesn't detect them (d'oh!), so we don't even bother. However, the ones that this particular supplier is using are hard enough that they wreak havoc on our HSS and even Carbide Insert knives... Not as catastrophic as standard metal staples, but it would almost be better if they were, because it makes someone wonder \"is this bad enough to have to do something about?!!\"..","October 2013 / Volume 65 / Issue 10|\nConsiderations when selecting a drill\nBy Christopher Tate, Savannah Machinery Works\nThe variety of drills on the market seems limitless, and each salesman says he has an offering that is better than the rest. The substrates, point geometries and coatings vary greatly—along with the price. I consider several factors when purchasing a drill and try to find the balance between price and performance that gives the best cost advantage while maintaining part quality and productivity.\nThe first consideration is the number of holes to be produced. A common misconception is that carbide is always the best substrate choice. An expensive, high-performance carbide drill may provide the highest penetration rate and shortest cycle time. However, if the number of holes being drilled is low, there may be a cost advantage to applying a less-expensive alternative, such as a HSS drill, because the tool cost savings can trump the increased cycle time.\nCourtesy of All images courtesy C. Tate\nAlso consider hole size. Carbide drills above ½ \" (12mm) in diameter are expensive, and indexable-insert drills have limited selection in sizes below 5/8 \" (16mm). Therefore, a HSS drill might be better for making holes from ½ \" to 1 \". On the other hand, an indexable drill may be more practical than a HSS or carbide drill for producing holes larger than 1 \".\nBecause high-performance carbide drills can hold tight size and geometric tolerances, I have opted for them when they eliminate a secondary operation, such as reaming. In addition, an indexable drill can be applied like a boring bar on a lathe. After drilling the hole, the indexable drill is stepped off center so the periphery insert enlarges the hole to the desired diameter, possibly eliminating the need for a boring tool.\nBesides reducing the number of tools by combining operations, reducing setup is a common goal when machining. If a drill can be used on a variety of parts, selecting an expensive, high-performance drill to extend tool life and minimize tool changes generally cuts costs. It may be possible to produce various hole sizes by drilling one size and circular interpolating with an endmill to enlarge holes that are larger than the drill diameter all in one setup.\nWorkpiece material is another consideration. A basic HSS drill performs well in carbon steel but tool life is poor when drilling abrasive materials like cast iron or die cast aluminum, justifying a costlier alternative like carbide. And P/M HSS drills effectively cut austenitic (300 series) stainless steels and similar materials. In these materials, P/M drills often have penetration rates similar to carbide and cost significantly less.\nMatching the drill to the machine tool is yet another consideration. High-performance drills require rigid machine tools that have spindles with minimal runout. Vibration from loose ways and excessive spindle runout can damage the fragile edges of carbide drills, causing premature failure. Large-diameter drills, especially large indexable drills, may require more torque than the spindles of some light-duty machines can supply. This scenario may require drilling smaller-than-specified holes and enlarging them via circular interpolation or boring.\nToolholders play a critical role in drill performance. The toolholder is the interface between the drill and the spindle, so it is important to balance this relationship. Low-quality toolholders can allow drills to run out, which reduces geometric accuracy and tool life. If the holder’s grip is insufficient, the drill can slip. This causes incorrect hole depth, putting other tools at risk of failure when they enter holes that are too shallow. Insufficient grip can also allow chatter, damaging the cutting edges on carbide drills and ultimately causing catastrophic failure. The best machine tool is only as good as the toolholder in its spindle.\nFinally, consider the machining environment and a shop’s practices. If the machinists, toolmakers and programmers are accustomed to using a particular type of drill, it may not be practical to introduce alternatives. For example, I tried to introduce high-performance carbide drills at a machine shop where I used to work. HSS jobber drills had been the standard. The operators applying the HSS tools were accustomed to running them at speeds many times slower than the speeds at which carbide drills can run. I was unable to change the culture, so there was no benefit to using costlier, high-performance carbide drills.\nJobs, work materials and production requirements vary greatly and there is no one definitive method for selecting the proper drill. The correct drill is the one that enables a high-quality part to be profitably shipped on time. CTE\nAbout the Author: Christopher Tate is manufacturing engineering lead for machining at Mitsubishi Power Systems, Savannah (Ga.) Machinery Works, a global builder of gas and steam turbines. He has 19 years of experience in the metalworking industry and holds a Master of Science and Bachelor of Science from Mississippi State University. E-mail: firstname.lastname@example.org.\nCUTTING TOOL ENGINEERING Magazine is protected under U.S. and international copyright laws. Before reproducing anything from this Web site, call the Copyright Clearance Center Inc. at (978) 750-8400.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:42cd5c28-0d44-4b33-847f-7e3f39c21eee>","<urn:uuid:6069aebc-f3fc-4f27-b26c-867f7fe91a11>"],"error":null}
{"question":"How do alcoholic beverages and sugary sodas compare in terms of their harmful effects on oral health?","answer":"Both alcoholic beverages and sugary sodas have significant negative impacts on oral health. Alcohol contributes to decreased salivary flow through sialadenosis of the parotid gland, reduces immune response, and creates an environment conducive to rapid progression of periodontal disease and caries. Additionally, alcoholic patients often consume high amounts of sweet drinks with simple sugars. Sugary sodas are particularly harmful because they combine high sugar content with acid, while also being frequently sipped over long periods, which maintains constant acid levels in the mouth. The bacteria in the mouth metabolize these sugars and produce acid as a waste product, which dissolves teeth creating cavities. The frequency and length of exposure to both substances amplify their damaging effects on dental health.","context":["Alcohol consumption is accepted around the world. About 70% of the US population drinks alcohol, and 1 in 10 will suffer from problems of alcoholism,1 a statistic that is probably mirrored in the Canadian population. Patients who consume alcohol can be divided into 3 categories: those who consume alcohol in moderation, those who abuse alcohol and those with alcohol dependence.1,2\nThere is no agreed-upon definition of moderate drinking, but most authorities believe that less than 1 drink per day for women and less than 2 drinks per day for men can be considered moderate. People who abuse alcohol experience repeated episodes of alcohol intoxication severe enough to alter mood and impair judgment. Their alcohol consumption prevents them from fulfilling their societal obligations and often results in dangerous behaviour.3,4 Alcohol dependence is described as a physiological dependence resulting in tolerance and/or withdrawal symptoms (tremor, weakness, sweating and delirium tremens).3,5\nThe etiology of alcoholism is unknown, although both genetics and environment probably play a role. Whatever the cause, it is likely that every practising dentist will treat at least one alcoholic patient or recovering alcoholic patient in his or her career, so dentists must be aware of the special needs of these patients.4 Furthermore, dentists should be prepared to refer patients to appropriate centres or professionals for additional treatment when appropriate.\nEffects on Teeth and Periodontium\nAlcoholic patients frequently consume more than 50% of their daily caloric intake in the form of ethyl alcohol,3,6 with much of the remaining caloric intake coming from sweet drinks high in simple sugars.3 Alcohol is also the most common cause of sialadenosis of the parotid gland, a peripheral autonomic neuropathy occurring in 30%–80% of patients with cirrhosis. This condition manifests with noninflammatory swelling of the parotid gland and decreased secretion of saliva, which in turn reduces the ability to neutralize cariogenic acid.2,7,8\nDecreased immune response secondary to alcoholic cirrhosis,6 combined with a cariogenic, nutritionally poor diet, poor oral hygiene, decreased salivary flow and a high incidence of smoking among these patients, provides an environment conducive to rapid progression of periodontal disease and caries.\nEffects on Mucosa and Underlying Tissues\nNutritional deficiencies are detrimental to the immune system but also have orofacial presentations. Common nutritional deficiencies in alcoholic patients include deficiencies of protein, minerals, trace elements, folic acid (causing mucosal ulcers), riboflavin (causing glossitis, filiform atrophy, pallor of oral commissures and dry scaly skin), pyridoxine (causing anemia and mucosal ulcerations) and vitamin E.1,3,5,8 Altered absorption of nutrients exacerbates the poor diet. More specifically, alcohol decreases gastrointestinal absorption of folic acid, riboflavin, niacin, thiamine (causing neuropathologies), vitamin D (causing osteoporosis) and vitamin K (causing coagulopathies).1,3,5 Alcohol also enhances the excretion of magnesium and zinc. Poor intake and poor absorption combined with increased excretion frequently lead to glossitis, angular cheilitis, candidal infection, oral ulceration and acute necrotizing ulcerative gingivitis.3,5\nPatients who consume alcohol are at increased risk of many cancers, including oral cancers. Among those who continue to drink after a diagnosis of oral cancer, the risk that a second primary tumour will develop increases by up to 50%.9 The exact mechanism by which alcohol increases the risk of oral cancer is still unclear. Alcohol is not itself a carcinogen, but acetaldehyde, produced when ethanol is metabolized by alcohol dehydrogenase (an enzyme produced locally by oral bacteria9), is a potent carcinogen. Acetaldehyde is also produced systemically by the breakdown of alcohol in the body and is subsequently secreted by the salivary glands.9 Smoking, the carcinogenic effects of acetaldehyde and the ability of alcohol to increase permeability of the mucous membranes and solubility of carcinogens3 may combine to promote tobacco-initiated tumours.5 Therefore, all oral ulcerations in alcoholic patients, including leukoplakia and erythroplakia, should be treated with suspicion, especially if the patient smokes.\nThe effect of ethanol on the liver can be viewed as a 3-stage phenomenon.4,7 The first stage, fatty liver (caused by steatosis), is the result of alcoholic insult to the liver. It usually has few signs or symptoms, is reversible after 2–4 weeks of abstinence4,7 and is seen histologically as the buildup of fat within the hepatocytes.4,7\nThe second stage, acute or chronic hepatitis, may occur if the patient has an exceptionally high intake of alcohol. Mild cases present without symptoms and may be diagnosable only by abnormalities on liver function tests. Severe cases may present with jaundice, hepatic encephalopathy, ascites, bleeding esophageal varices, abnormal clotting and coma. Although hepatitis is usually reversible, it may also progress to cirrhosis.3,5\nThe third stage is cirrhosis, the development of fibrous nodules within the liver, which occurs with repeated damage and healing with scar tissue. A cirrhotic liver is thus unable to perform its normal functions, and production of several clotting factors (V, VII, IX and X), synthesis of bile, control of glycemia, metabolism of cholesterol and detoxification are all impaired. Congestion or hypertension of the portal vein in cirrhosis leads to esophageal varices, rectal hemorrhoids and splenomegaly. Splenomegaly results in sequestration of blood cells, and sequestration combined with alcohol-related coagulopathies can lead to fatal bleeding.3\nLong-term alcohol use can lead to mild hypertension and increases in both triglycerides and low-density lipoproteins. In people without alcoholism, this triad has been related to increased risk for coronary artery and cerebrovascular disease. No studies have examined these risks in people with alcoholism, but extrapolation of these risks to this population seems reasonable. Excessive ingestion of alcohol may also damage the myofibrillar architecture, which could eventually lead to congestive heart failure. An evaluation of the cardiovascular system may therefore be warranted before any dental treatment is undertaken.3\nAlcoholic patients may seek treatment for pain or infection only in emergency situations. Immune deficits, including deficiency of complement, impaired adherence of Kupffer cells and neutrophils, and impaired motility and phagocytotic activity,5 leave these patients at greater risk for infection and necessitate aggressive management while limiting the portals of entry. For example, osteomyelitis of the mandible following simple dental extractions has been documented.10\nIn alcoholic patients, the liver's ability to metabolize drugs is usually impaired, which may result in subtherapeutic or toxic doses of prescribed medications.6,11 Dentists should be aware of several direct drug–alcohol interactions (Table 1).4,8 In addition to these problems, patients with cirrhosis have decreased albumin levels. The dosages of drugs that bind albumin in the blood are based on binding percentages. Therefore, if albumin levels are decreased, plasma levels of these drugs will be increased relative to patients with normal levels of albumin. This phenomenon is especially important for drugs such as warfarin, which have high protein-binding percentages.6,11\nTable 1 Drugs that interact directly with ethyl alcohol4,8\n|Acetylsalicylic acid, NSAIDs||\n|Metronidazole, cephalosporins, ketoconazole||\n|Barbiturates, opioids, benzodiazepines||\nCNS = central nervous system, GI = gastrointestinal, NAPQI = N-acetyl-p-benzoquinone imine, NSAID = nonsteroidal anti-inflammatory drug.\nRecovered Alcoholic Patients\nThe management of pain for recovered alcoholic patients represents a tight balance between too little and too much pain control. Excess pain (if pain management is insufficient) or overmedicating the patient could cause relapse of the alcoholism.11 The recovered alcoholic patient may eventually require liver transplantation, which entails immunosuppression. Among these patients, screening for cancer and aggressive dental therapy are especially important, because of increased susceptibility to infection and development of malignancy.12 In addition, Wernicke-Korsakoff psychosis, caused by thiamine deficiency, can make proper history taking and behaviour management difficult.3,5,11\nAcute Alcohol Withdrawal\nTreatment of acute alcohol withdrawal is beyond the scope of this article, but it should be remembered that alcohol abusers who stop drinking may suffer life-threatening delirium tremens. Therefore, alcoholic patients who wish to stop abusing alcohol must undergo detoxification in a monitored hospital setting. Dentists treating any patient who abuses alcohol should ask the patient when he or she last consumed alcohol. Patients who have not had alcohol recently should be monitored for tachycardia, confusion, sweating, arrhythmias or any other signs of withdrawal, which can lead to a life-threatening medical emergency.\nIncreased risk of malignancy and coagulopathy, altered drug metabolism, neuropathology, nutritional deficits, compromised immune function, Wernicke–Korsakoff psychosis and cardiovascular disease are just some of the many areas of concern for patients who currently consume alcohol or have previously abused or been dependent on alcohol. Many of these patients will seek treatment only in emergency situations. It is therefore important that practising dentists have open lines of communication with medical and dental specialists, so that treatment can be rendered in a timely and effective manner to control infection, diagnose disease and alleviate pain.\n- Maserejian NN, Joshipura KJ, Rosner BA, Giovannucci E, Zavras AI. Prospective study of alcohol consumption and risk of oral premalignant lesions in men. Cancer Epidemiol Biomarkers Prev. 2006;15(4):774-81.\n- Bullock K. Dental care of patients with substance abuse. Dent Clin North Am. 1999;43(3):513-26.\n- Friedlander AH, Marder SR, Pisegna JR, Yagiela JA. Alcohol abuse and dependence: psychopathology, medical management and dental implications. J Am Dent Assoc. 2003;134:731-40.\n- Robb ND, Smith BG. Chronic alcoholism: an important condition in the dentist-patient relationship. J Dent. 1996;24(1-2):17-24.\n- Friedlander AH, Norman DC. Geriatric alcoholism: pathophysiology and dental implications. J Am Dent Assoc. 2006;137:330-338.\n- Novacek G, Plachetzky U, Pötzi R, Lentner S, Slavicek R, Gangl A, et al. Dental and periodontal disease in patients with cirrhosis – role of etiology of liver disease. J Hepatol. 1995;22(3):576-82.\n- Mandel L, Hamele-Bena D. Alcoholic parotid sialadenosis. J Am Dent Assoc. 1997;128(10):1411-5.\n- Riedel F, Goessler U, Hörmann K. Alcohol-related diseases of the mouth and throat. Best Pract Res Clin Gastroenterol. 2003;17(4):543-55.\n- Miller PM, Day TA, Ravenel MC. Clinical implications of continued alcohol consumption after diagnosis of upper aerodigestive tract cancer. Alcohol Alcohol. 2006;41(2):140-2. Epub 2005 Nov 24.\n- Davis HT, Carr RJ. Osteomyelitis of the mandible: a complication of routine dental extractions in alcoholics. Br J Oral Maxillofac Surg. 1990;28(3):185-8.\n- Lindroth JE, Herren MC, Falace DA. The management of acute dental pain in the recovering alcoholic. Oral Surg Oral Med Oral Pathol Oral Radiol Endod. 2003;95(4):432-6.\n- Glassman P, Wong C, Gish R. A review of liver transplantation for the dentist and guidelines for dental management. Spec Care Dentist. 1993;13(2):74-80.","You probably know that eating or drinking sugar and not brushing and flossing your teeth is how you get cavities, but it might be helpful to understand a little more about how that process actually occurs.\nYour mouth, like all parts of your body, has a population of natural bacteria at all times. There are many, many types of bacteria, and the numbers and ratios of each must be kept in balance. These bacteria are simple creatures, and enjoy metabolizing the simplest food of all: Sugar.\nIf you feed these bacteria, they grow in number. The amount you’re feeding your bacteria can be increased in three ways: Amount, Length and Frequency\nAmount: This is where dietary choice comes into play. Sugar, Crackers, Cookies, and Carbohydrates that get stuck in the grooves of your teeth are major offenders, but what you drink is often even worse, because the amount of sugar in soda, juice, coffee and dessert beverages is often super high. The worst dietary choice out there is sugary soda due to the amount of sugar, and the tendency to sip over long periods of time, increasing the frequency of feeding the bacteria.\nFrequency: How often you consume the above types of food and drink also increases your bacteria. If you sip a soda, if you’re taking a sip every 30 minutes, the acid level (that bacteria thrive under) in your mouth is the same as if you were to continuously hold the soda in your mouth.\nLength: As above, frequency can turn into length, but the other component that affects length is how often you remove the food debri and bacteria from your mouth with brushing and flossing. We recommend you brush 2-3 times per day and floss at least once per day. In some cases greater frequency is required due to underlying conditions. The greatest of these being, dry mouth. The saliva in the mouth works throughout the day to remove (some of) the food debri and bacteria. We find that people with dry mouth (due to radiation or prescription side effects) to have a very difficult time controlling the bacteria, thus the decay in their mouths, because carbohydrates and bacteria simply stay on the teeth longer.\nWhat exactly do bacteria do to make these cavities? I mentioned earlier, that bacteria like acidic environments and thrive in them. This is natural because bacteria MAKE ACID. It is actually the acid that is produced as a waste product when they consume food debri that dissolves the teeth creating cavities. This is why sugar and acid together (soda) is super bad, but also why acid alone (diet soda) is also harmful to teeth.\nHow can we address these three issues to keep bacteria and cavities at bay? We will look at the three categories and find good choices and habits!\nAmount: Choose foods low in sugar and carbohydrates like Meats, Vegetables, Whole Fruits, Whole grains . Cheese and Nuts have been shown to have a negative effect on bacteria, thus a positive effect on teeth!\nFrequency: Keep snacking under control, and it’s especially important to choose wisely (cheese, nuts, whole fruits, veggies, beef jerky) when it comes to snack items. If you do drink soda or juice, keep it to mealtimes only, so you’re not prolonging the acid environment.\nLength: Brush at least 2 times per day. Brush and Floss last thing before bed, only consuming water after. This gives your teeth the whole night free of acid attack. See your dentist for regular cleanings at least every 6 months. If you do have dry mouth, you might need to work with your dentist on a special plan."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ebd5b743-5862-49df-800f-163a03fc09a8>","<urn:uuid:0676abd7-5a88-48d0-bfb2-172f7e696664>"],"error":null}
{"question":"How do the simulation goals differ between modeling fluid dynamics and programming language implementations?","answer":"The simulation goals differ significantly between these domains. In fluid dynamics simulation, the focus is on computational modeling of physical phenomena like oscillating gas flow and dealing with numerical problems, with special attention to validity issues (as notably, two-dimensional fluid-dynamics models are considered invalid). In contrast, programming language implementation simulation focuses on studying fundamental concepts through interpreters and compilers, with emphasis on features like higher-order functions, data structures, and type checkers to determine program validity. While fluid dynamics aims to create accurate physical representations, programming language simulation seeks to understand and implement language concepts through toy languages and principled compilers.","context":["Modeling and Simulation: The Computer Science of Illusion / Edition 1 available in Hardcover\n- Pub. Date:\nSimulation is the art of using tools – physical or conceptual models, or computer hardware and software, to attempt to create the illusion of reality. The discipline has in recent years expanded to include the modelling of systems that rely on human factors and therefore possess a large proportion of uncertainty, such as social, economic or commercial systems. These new applications make the discipline of modelling and simulation a field of dynamic growth and new research.\nStanislaw Raczynski outlines the considerable and promising research that is being conducted to counter the problems of uncertainty surrounding the methods used to approach these new applications. It aims to stimulate the reader into seeking out new tools for modelling and simulation.\n- Examines the state-of-the-art in recent research into methods of approaching new applications in the field of modelling and simulation\n- Provides an introduction to new modelling tools such as differential inclusions, metric structures in the space of models, semi-discrete events, and use of simulation in parallel optimization techniques\n- Discusses recently developed practical applications: for example the PASION simulation system, stock market simulation, a new fluid dynamics tool, manufacturing simulation and the simulation of social structures\n- Illustrated throughout with a series of case studies\nModelling and Simulation: The Computer Science of Illusion will appeal to academics, postgraduate students, researchers and practitioners in the modelling and simulation of industrial computer systems. It will also be of interest to those using simulation as an auxiliary tool.\nAbout the Author\nStanislaw Raczynski is a professor of control theory, electronics and computer simulation at the Panamerican University in Mexico City. He is also the international director of the Socirty for Modelling Simulation in San Diego. He has authored 1 book, “Simulacion por computadora” and over 70 journal articles & conference papers.\nTable of Contents\nChapter 1. Basic Concepts and Tools.\n1.1. Modeling and simulation: What is it ?.\n1.2. Validity, credibility, tractability and verification.\n1.3. System state and causal systems.\n1.4. Classification of dynamical systems.\n1.5. Discrete and continuous simulation.\n1.6. Evolution of simulation software.\nChapter 2. Continuous simulation.\n2.2 Ordinary differential equations and models of concentrated parameter systems.\n2.3. Continuous simulation with analog computers.\n2.4. Numerical methods for ordinary differential equations (ODE).\n2.5. Signal flow graphs.\n2.6. Bond graphs.\n2.7. Alternative modeling tools and dynamic uncertainty.\n2.8. Distributed parameter systems.\n2.9. System dynamics.\n2.10. Galactic simulations and the N-body problem.\nChapter 3. Discrete and combined simulation – example of PASION implementation.\n3.1. Are discrete models valid?.\n3.2. PASION simulation system.\n3.3. Queuing Model Generator QMG.\n3.4. Complex system simulator of PASION.\nChapter 4. Differential inclusions in Modeling and Simulation.\n4.1. Differential inclusions.\n4.2. Possible applications.\n4.3. Differential inclusion solver.\n4.4. Application in uncertainty treatment.\n4.5. Uncertain future and differential inclusions.\n4.6. Conclusions and future research.\nChapter 5. Fluid dynamics – simulating oscillating gas flow.\n5.1. Computational fluid dynamics.\n5.2. Numerical problems.\n5.3. The simulation tool.\n5.5. Oscillating gas flow.\n5.6. Two-dimensional fluid-dynamics models are invalid.\nChapter 6. Simulating Phenomena of General Relativity.\n6.1. Some basic concepts.\n6.3. The simulation tool and model time.\n6.4. Simulation experiments.\nChapter 7. Interactions between hostile hierarchical structures: simulation of the struggle between terrorist and anti-terrorist organizations.\n7.2. The model.\n7.4. The tool and the model implementation.\n7.5. Simulation experiments.\nChapter 8. On a metric structure in the space of dynamic system models.\n8.3. Distance between models.\nChapter 9. Simulation optimization: A case study of a parallel optimization algorithm.\n9.2. Problem statement.\n9.3. Simulation experiment.","Principles of Programming Languages\nIn this open book, our goal is to study the fundamental concepts in programming languages, as opposed to learning a range of specific languages. Languages are easy to learn, it is the concepts behind them that are difficult. The basic features we study in turn include higher-order functions, data structures in the form of records and variants, mutable state, exceptions, objects and classes, and types. We also study language implementations, both through language interpreters and language compilers. Throughout the book we write small interpreters for toy languages, and in Chapter 8 we write a principled compiler. We define type checkers to define which programs are well-typed and which are not. We also take a more precise, mathematical view of interpreters and type checkers, via the concepts of operational semantics and type systems. These last two concepts have historically evolved from the logician's view of programming.\nThe material has evolved from lecture notes used in a ...\nProgramming DSLs in Kotlin\nCreating your own domain-specific languages (DSLs) is both challenging and exhilarating. DSLs give users a way to interact with your applications more effectively, and Kotlin is a fantastic language to serve as a host for internal DSLs, because it greatly reduces the pain and effort of design and development. But implementing DSLs on top of Kotlin requires understanding the key strengths of the language and knowing how to apply them appropriately. Learn to avoid the pitfalls and leverage the language while creating your own elegant, fluent, concise, and robust DSLs using Kotlin.\nInternal DSLs remove the burdens of implementing a full blown language compiler. The host language quickly becomes your ally to creating DSLs, but the syntax you can choose for your DSLs is limited to what the host language allows. You can work around the limitations by tactfully bending the rules and exploiting the language capabilities. Learn the power of Kotlin and ways to design with it, in the context o ...\nA Common-Sense Guide to Data Structures and Algorithms, 2nd Edition\nIf you thought that data structures and algorithms were all just theory, you're missing out on what they can do for your code. Learn to use Big O Notation to make your code run faster by orders of magnitude. Choose from data structures such as hash tables, trees, and graphs to increase your code's efficiency exponentially. With simple language and clear diagrams, this book makes this complex topic accessible, no matter your background. This new edition features practice exercises in every chapter, and new chapters on topics such as dynamic programming and heaps and tries. Get the hands-on info you need to master data structures and algorithms for your day-to-day work.\nAlgorithms and data structures are much more than abstract concepts. Mastering them enables you to write code that runs faster and more efficiently, which is particularly important for today's web and mobile apps. Take a practical approach to data structures and algorithms, with techniques and real-world scenari ...\nProgramming iOS 14\nIf you're grounded in the basics of Swift, Xcode, and the Cocoa framework, this book provides a structured explanation of all essential real-world iOS app components. Through deep exploration and copious code examples, you'll learn how to create views, manipulate view controllers, and add features from iOS frameworks.\nCreate, arrange, draw, layer, and animate views that respond to touch; Use view controllers to manage multiple screens of interface; Master interface classes for scroll views, table views, collection views, text, popovers, split views, web views, and controls; Dive into frameworks for sound, video, maps, and sensors; Access user libraries: music, photos, contacts, and calendar; Explore additional topics, including files, networking, and threads.\nStay up-to-date on iOS 14 innovations, such as: Control action closures and menus; Table view cell configuration objects; Collection view lists and outlines; New split view controller architecture; Pointer customization on i ...\niOS 14 Programming Fundamentals with Swift\nMove into iOS development by getting a firm grasp of its fundamentals, including the Xcode 12 IDE, Cocoa Touch, and the latest version of Apple's acclaimed programming language, Swift 5.3. With this thoroughly updated guide, you'll learn the Swift language, understand Apple's Xcode development tools, and discover the Cocoa framework.\nBecome familiar with built-in Swift types; Dive deep into Swift objects, protocols, and generics; Tour the life cycle of an Xcode project; Learn how nibs are loaded; Understand Cocoa's event-driven design; Communicate with C and Objective-C.\nIn this edition, catch up on the latest iOS programming features: Multiple trailing closures; Code editor document tabs; New Simulator features; Resources in Swift packages; Logging and testing improvements; And more! ...\nOptions and Derivatives Programming in C++20, 2nd Edition\nMaster the features of C++ that are frequently used to write financial software for options and derivatives, including the STL, templates, functional programming and numerical libraries. This book also covers new features introduced in C++20 and other recent standard releases: modules, concepts, spaceship operators, and smart pointers.\nYou will explore how-to examples covering all the major tools and concepts used to build working solutions for quantitative finance. These include advanced C++ concepts as well as the basic building libraries used by modern C++ developers, such as the STL and Boost, while also leveraging knowledge of object-oriented and template-based programming. Options and Derivatives Programming in C++ provides a great value for readers who are trying to use their current programming knowledge in order to become proficient in the style of programming used in large banks, hedge funds, and other investment institutions. The topics covere ...\nThe world runs on code written in the C programming language, yet most schools begin the curriculum with Python or Java. Effective C bridges this gap and brings C into the modern era - covering the modern C17 Standard as well as potential C2x features. With the aid of this instant classic, you'll soon be writing professional, portable, and secure C programs to power robust systems and solve real-world problems.\nRobert C. Seacord introduces C and the C Standard Library while addressing best practices, common errors, and open debates in the C community. Developed together with other C Standards committee experts, Effective C will teach you how to debug, test, and analyze C programs. You'll benefit from Seacord's concise explanations of C language constructs and behaviors, and from his 40 years of coding experience.\nYou'll learn: How to identify and handle undefined behavior in a C program; The range and representations of integers and floating-point values; How dynamic memor ...\nBeginning Ada Programming\nDiscover the Ada programming language by being gently guided through the various parts of the language and its latest available stable release. The goal in this book is to slowly ease you into the different topics. It is understood that you do not always have ample free time, so the text is easy to digest and concepts are spoon fed to the reader.\nStarting with the simplest of topics, detailed explanations demonstrate the how and why of Ada. You are strongly encouraged to experiment and break things (without which the learning process is linear and quite dull). At the end of Beginning Ada Programming you will have an excellent understanding of the general topics that make up the Ada programming language and can tackle far more challenging topics.\nEach chapter builds on what was previously described. Furthermore, each code example is independent of others and will run all by itself. Instructions are provided where you can obtain an Ada compiler and how to deb ...\nModern Programming Made Easy, 2nd Edition\nGet up and running fast with the basics of programming using Java as an example language. This short book gets you thinking like a programmer in an easy and entertaining way. Modern Programming Made Easy teaches you basic coding principles, including working with lists, sets, arrays, and maps; coding in the object-oriented style; and writing a web application.\nHands-On C++ Game Animation Programming\nAnimation is one of the most important parts of any game. Modern animation systems work directly with track-driven animation and provide support for advanced techniques such as inverse kinematics (IK), blend trees, and dual quaternion skinning.\nThis book will walk you through everything you need to get an optimized, production-ready animation system up and running, and contains all the code required to build the animation system. You'll start by learning the basic principles, and then delve into the core topics of animation programming by building a curve-based skinned animation system. You'll implement different skinning techniques and explore advanced animation topics such as IK, animation blending, dual quaternion skinning, and crowd rendering. The animation system you will build following this book can be easily integrated into your next game development project. The book is intended to be read from start to finish, although each chapter is self-contained and can be read ...\nEffective AWK Programming, 5th Edition\nWhen processing text files, the awk language is ideal for handling data extraction, reporting, and data-reformatting jobs. This practical guide serves as both a reference and tutorial for POSIX-standard awk and for the GNU implementation, called gawk. This book is useful for novices and awk experts alike.\nIn this thoroughly revised 5th edition, author and gawk lead developer Arnold Robbins describes the awk language and gawk program in detail, shows you how to use awk and gawk for problem solving, and then dives into specific features of gawk. System administrators, programmers, webmasters, and other power users will find everything they need to know about awk and gawk. ..."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4c27f785-b98a-4725-9898-24bc77cf9440>","<urn:uuid:1f7257e3-b07e-4b5e-95ad-4da5c2c48eb6>"],"error":null}
{"question":"What are the benefits of using two-factor authentication, and how does it compare to biometric security? 😊","answer":"Two-factor authentication provides protection against phishing attacks, ensures against hacking incidents, increases flexibility for shared system access, and reduces the probability of unauthorized access through unique verification codes. Biometric security, on the other hand, uses unique patterns like face, voice, or fingerprints, converting them into mathematical maps that are almost impossible to replicate. Biometric methods include 3D recognition with specific identifiers like skin texture analysis and geometric sensors, making them highly secure for authentication purposes.","context":["With the ever-increasing risk of cyberattacks, the probability of a user’s data being compromised has never been greater. Cybercriminals and their methods are becoming increasingly more sophisticated by the second, meaning security measures and protocols are becoming easier to circumvent thus exposing sensitive information, often personal information of customers and employees alike. When it comes down to it, the responsibly to stem this growing tide solely rests on the shoulders of the businesses and customers themselves. Thankfully, white hat security experts are fighting tirelessly on the frontlines of this cyberwar and are building more defensive security measures that boost data privacy and keep information safe and secure through cryptographic and encryption technology. Which leads us to the pressing question at hand: What’s being done about it? Glad you asked! Here are some recent developments in encryption standards aimed at data privacy with the goal of keeping you and your data safe.\nWith advancements in the field of quantum mechanical properties, computer scientists and cryptographers have been able to harness the power of the proton and light for the transmission of encrypted passwords and keys. The most well-known method of quantum cryptography is what’s called quantum key distribution, which is a back to back encryption method to be utilized by the sender and receiver without the possibility of a man-in-the-middle attack, even if the would-be thief had access to the parties’ communications. This is made possible due to the principles of quantum mechanics. The mere act of attempting to eavesdrop requires measuring a system in its quantum entangled state, which produces measurable anomalies in the system the two communicating parties will be able to detect, thus automatically changing the decryption key.\nPhysical security measures\nWhile not as glamorous as its counterparts, physical-based information security is still one of the most effective ways of keeping sensitive information secure on your website and boosting your data privacy.\nBy simply adopting adequate security measures such as keeping access to a proprietary server severely restricted and properly vetting those with access to such information with extensive background checks and identity verification, you can essentially air gap such a server from the Internet and prevent forms of intrusion other than physical.\nIn addition, if you’re a small business with little control over a physical server, good security practices start with choosing a proper web hosting service that continuously updates their security protocols.\nA honeypot is a deception security mechanism that is designed to trick and lull unauthorized users by providing information, generally in the form of data, that appears to be the real thing but is actually an isolated trap of sorts that’s being constantly monitored. There are several forms of honeypot, which can be utilized depending on the type of information you wish to capture in your trap. Limited use honeypots, or production honeypots, are designed to be placed inside of networks as a general security measure under low-risk circumstances. Due to the low level of interaction required for production honeypots, they’re often easier to manage but also collect less information on attackers and their potential motivations.\nThe second form of honeypot is a little more complex to maintain and gives less value to a system’s overall security measures, but research honeypots are designed and deployed with the intention of collecting information such as attackers’ identities, their tactics and to collect intelligence on how to improve the systems which are currently in place. Research honeypots are often used by governmental agencies, such as intelligence services, the military, and research and development firms that need to constantly be aware of who the enemies are and what they want.\nWhat’s more important is the data collected on how a system was breached and the research that’s gathered to fix the weaknesses in the system. Honeypots aren’t as effective as they once were with the advent of honeypot detection systems, which are counter cyberweapons that detect and identify a honeypot through the unique signatures they leave. This can be a double-edged sword in the sense that the mere presence of a honeypot may be enough to ward off any brute-force attacks.\nBiometrics and two-factor authentication technology\nKnowing your grandma’s middle name is no longer enough to keep your information safe, but your face and voice sure can be. If you’ve ever used a smartphone, the odds are you’ve been able to unlock it with a thumbprint or by pointing the camera at your face. This is possible through the use of biometric technology that measures unique patterns associated with the user and churns them into a mathematical face/thumbprint map that is unique to you. Techniques such as 3D recognition look for specific identifiers such as skin texture analysis and geometric sensors that can identify the little idiosyncrasies that are unique to the person with access, which are almost impossible to replicate unless the person who has access is present.\nTwo-factor authentication is along the lines of biometrics in the sense that the user must be present for access to the system to be achieved. This can be through proximity, like having a wearable tech that communicates with and unlocks the computer, or through combining biometrics like facial recognition and a thumbprint scanner in order to achieve access.\nData privacy: The battle is never won\nThough these may seem like groundbreaking innovations in modern cryptography, black hats aren’t fools and learn and adopt new tactics at an alarming rate. With advancements in quantum computing and blockchain technology still a ways off due to the complexity and uncertainty of future applications, encryption techniques, both digital and physical, reign supreme when it comes to data privacy. Everyone must take it upon themselves to make sure the information they hold dear is safe.\nFeatured image: Shutterstock","With the growing advancement in technology and the hacking techniques, both safety of the online accounts and the risk to lose them are increasing rapidly. Perfect authentication systems help secure your online accounts from hackers or the bad guys, and determine ‘whom to give access’. Lesser the security level, higher is the possibility that the intruders will gain access to your online account with no trouble. But, if the authentication system is intricate, restraining, or hard to compromise, then no unauthentic user will be able to gain access. The time has come to strengthen the digital assets security as far as growing cyberattacks are concerned, so it is better to enable Two Factor Authentication on all your valuable online accounts.\nBeing an authentic end user, if you want to log in to your online account, then you need the valid username and password combination for verifying your identity. But, what if someone else manages to guess your secret password. By doing so, he can easily get in to your online account. Therefore, it is necessary to add an extra security layer, so that no unauthorized user can access the account even if the password gets compromised. Two Factor Authentication security method helps to keep the prying eyes away from your online accounts and confidential data.\nA brief overview of this method\nTwo Factor Authentication, also known as 2FA or Two Step Verification is an authentication mechanism that adds an additional layer of security in the login procedure to verify identity of the user. This security method requires two authentication factors; unique verification code or OTP (One Time Passcode) and secret password, to verify whether the user is authentic or not. The verification code is immediately sent to the registered mobile device of the user as he enters the correct username and password combination associated with his account, and is valid only for a few seconds. Two Factor Authentication security method creates the perfect balance between online accounts security and whom to give access.\nIn internet security, the possible types of authentication factors are:\nSomething the user knows (the knowledge factors) – username, password, PIN.\nSomething the user have (the possession factors) – credit & a debit card, mobile device, a token.\nSomething the user is (the inherence factors) – biometric characteristics of the user such as iris, retina, face scan, voice recognition, fingerprint.\nWhat are the major benefits of enabling Two Factor Authentication?\nProtection against social engineering phishing attacks.\nEnsures no more hacking incidents on the valuable online accounts and data of the authentic end users.\nIncreased flexibility and productivity as Two Factor Authentication method allows different users to securely log in or gain access into a shared system or database.\nReduced probability of an intruder or hacker gaining access to your online account, resulting in no or minimal security breaches. This is achieved through a unique verification code used in the login procedure to ensure a strong second line of defence.\nHope, after going the above-listed benefits of Two Factor Authentication security method, you might have understood its importance. So, enable it today and secure your online accounts from different types of cyberattacks."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a6b84f1a-5c1d-46cd-9119-b1f491485b66>","<urn:uuid:d76358c7-9b3b-4ddc-a47e-acad6c52d366>"],"error":null}
{"question":"What role do international initiatives play in addressing both literacy challenges in Laos and carbon market development?","answer":"International initiatives play crucial roles in both areas. In literacy development, foreign individuals like American businessman Sasha Alyson have established projects like Big Brother Mouse to address the severe shortage of books in Laos, producing the first local-language children's books to promote reading. Similarly, in carbon market development, multilateral financial institutions are actively supporting developing countries by providing capacity-building resources, implementing result-based climate finance programs, and helping verify carbon credits. These institutions can assist countries in developing proper institutional frameworks for carbon market convergence and help track new rules and climate neutrality targets. The World Bank, for instance, has begun purchasing carbon credits and providing revenue to private sector clients via carbon markets, while helping countries navigate the evolving space of carbon pricing and trading.","context":["Highs and Laos of being a bestselling author\nI first heard about Colin Cotterill on a family holiday in the old royal capital of Laos, called Luang Prabang.\nIt’s a small and atmospheric city of softly spoken residents and half-hidden architectural treasures.\nWeather-beaten temples and French colonial-era mansions dot the streets and side-streets and down at the river bank, the waters of the Mekong slide silently by.\nThere are pockets of activity here, on the main street where the tourists congregate and in a rickety building, on a dusty alleyway, that the locals call “Big Brother Mouse”.\nVisitors who venture through the front door are greeted with a large book shelf packed with picture books and children’s stories – and usually, there is a good supply of youngsters making use of them in a room off the side.\nThe books are written and produced here, for the ramshackle building on Hiaphong Street is an unlikely looking publishing house.\nIt was founded by an American called Sasha Alyson, a retired businessman from Boston.\nWhile holidaying in Laos in 2003, he failed to spot a single book in the country – so he decided to do something about it.\nThree years later, ‘Big Brother Mouse’ produced the first batch of children’s books written in the local language. The idea behind the project is simple – raise literacy levels and promote the idea that reading can be fun.\nThe land of no books\nYet the task before the Big Brother Mouse team is huge, for this isolated and impoverished nation continues to be, in many ways, the land of no books.\nFew people could afford the $7 or so required to buy a popular novel – but then again, they are unlikely to find anything in Lao on the shelves anyway.\nA government-approved six volume set called “The History of Laos” did make it into a handful of shops this year, but we were told by one book seller that ‘it’s not selling very well.’\nWhen I asked Mr Alyson about the paucity of reading material for adults, he did offer a few words of encouragement however; “you do know that Colin Cotterill is trying to get his books published here, don’t you?”\nI didn’t – but I decided to find out more.\nMr Cotterill is the critically acclaimed author of an eight book crime-fiction series set in Laos. The London native got to know the country in the early 1990’s when he took a job re-writing the national university’s English course – it had been designed some years earlier by a team of East Germans and was in need of some fairly major repair.\nA shaky start\nStill, the author told me this rather exotic sounding opportunity got off to a shaky start : when I arrived in the country someone on the airplane leaned over and said: “do you realise that you have hepatitis?”\n“I went into the bathroom and looked at this big yellow glowing person and I realised that I was arriving in Laos sick. So on my first day I went to the hospital and spent three months there for treatment – and I liked the place so much that I moved in.”\nCotterill spent two years at the Mahosot Hospital in Lao capital, Vientiane and the experience serving as the inspiration for books about “Dr Siri Paiboun”.\nThis doc is a curmudgeonly figure, dreaming of a long and uninterrupted retirement when, to his undisguised disgust, officials from Laos’ one and only political party – the Communists – order him to become the national coroner.\nStill, Dr Siri does as he is told, casting a wry eye at “revolutionary” Laos as he solves all manner of crimes and mysteries.\nThe series is now available in 14 languages and seems ideally suited to the local book-starved market – but making them available here has proven a challenge of almost Mekong-length complexity.\nStruggle to get published\nFor starters, getting the books translated in Laos has been anything but simple. Cotterill told me: “I have been looking for six, seven, eight years for someone to translate my books but I’ve never been able to find anyone who could actually get the jokes.”\nAfter asking “20 to 30 people around the world’”to have a go, Cotterill thinks he has finally found someone who gets the jokes in both languages.\nHis name is Neil Garforth, an electrician from Rochdale, who has been travelling the highways and waterways of Southeast Asia for twenty years.\nGarforth has developed an ear for languages over the years and now works as a legal translator in Vientiane.\nIt is pretty dry stuff he told me – contracts and laws and memoranda of understanding – but at least it is predictable.\nMr Cotterill’s novels offer a different sort of challenge; “Dr Siri Paiboun was described as a short-arsed man,” said Garforth, reading out-loud from the book. “I mean, how do I translate short-arsed into Lao? It’s not like I have got a lot of words to choose from,” he chuckled.\nLess than a dollar a page\nMr Garforth is not doing it for the money – less than a dollar a page he said – and he seems to have inherited a fairly weighty burden; “when I do work on it, sometimes I re-write a paragraph 20 times because I’m not happy.” When I asked Garforth how far he’d actually got, he said he had finished the first chapter of the first book.\nIf the translated text of the first novel, “The Coroner’s Lunch” is produced, Mr Cotterill will need to get the book printed into the Lao language – but he is running into problems in that department as well.\nUnsurprisingly perhaps, the “country of no books” doesn’t have a whole lot of printing factories either.\nIn fact there is only one printer in Laos who uses equipment that could reasonably be described as ‘contemporary’ and he happens to be a devout Christian who insists on reading over the material before it touches his machines.\nHis name is Mana Jangmook, a self-described “unusual businessman”. “I turn down millions of dollars of business every year,” he told me; “We won’t print everything. Like magazines, like sex appeal – we totally deny, we reject completely. Or maybe horoscopes, we deny (that too).”\nQuite possibly, he will also deny Colin Cotterill’s books which contain plenty of references to Buddhism and local animist traditions – the sort of thing Mr Mana doesn’t look particularly favourably on.\nIf you thought that marked the end of this literary obstacle course, there is one more challenge to face – and it is a rather formidable one at that.\nIf the books do get translated and printed, they have to be sent on to the “state censors” for approval before anyone can sell them.\nThe censors work in a curtained-off room at the Laos Ministry of Culture and one of our contacts was kind enough to take a picture of the outer door – but the mere mention of this building in Vientian produces an array of anxious or terrified responses. When I suggested to one local that we would really like to film it for our report, he started to visibly shake.\nColin Cotterill seems to be taking this particular threat in his stride however. “I don’t know how the Ministry will react to these books. I don’t know what will be left or how long it will be. It might be a six page book by the time everyone’s put their hands on it,” he said.\nStill, the London-native is quite sure that he is doing the right thing – even if the final chapter in this particular “adventure-in-publishing” seems a long way off; “I want to see how they will react to a story set in their own country.\n” They watch foreign movies but they can’t imagine that Laos could ever produce anything like that and I want to give them that feeling – the feeling that their lives and their stories are just as important as anything they see in Hollywood.”\nFollow @c4sparks on Twitter.","Disclaimer: The views expressed in this blog are those of the authors and do not necessarily reflect the views of the Asian Development Bank, its management, its Board of Directors, or its members.\nFacing twin crises—climate change and biodiversity loss, corporate actions on climate change are a new norm for business. Multilateral financial institutions function through international treaties and have been quick to respond with their commitment to the Paris alignment.\nThis blog shares thoughts about valuing carbon as a commodity and developing the currently immature carbon market. It provides insights into how developing countries can recognize opportunities from the carbon market and leverage the support from MFIs in their decarbonization pathways.\nCarbon pricing is the act of putting a price on carbon emissions to mitigate climate change. It comprises of carbon taxes, carbon border adjustments, emission trading system (ETS), and carbon crediting. There are currently 73 instruments, most of which are carbon taxes and ETSs in Europe and North America. The People’s Republic of China (PRC) started running its national ETS in 2021.\nCarbon prices have hit record highs recently in many markets. The price of an EU ETS credit topped at more than 99 euros in August 2022, while the carbon tax in Singapore stayed at about 17 euros. To prevent carbon leakage, the EU is introducing the carbon border adjustment mechanism (CBAM) to require importers in selected industrial sectors to purchase certificates equivalent to the weekly EU carbon price. Higher carbon prices are necessary to keep global heating to below 2°C—the upper end of the limit agreed in the Paris Agreement. The Report of the High-Level Commission on Carbon Prices identified a USD 50-100/ton range as the price needed by 2030 to keep global heating to below 2°C—the upper end of the limit agreed in the Paris Agreement—as part of a comprehensive climate policy package. Further, more recent estimates indicate even higher prices may be needed to reduce emissions to net-zero by 2050.\nUnder the new rules of Paris Agreement Article 6, governments can decide on the type of projects in their countries and whether to authorize carbon emission reductions from those projects. These rules can lead to further divergence in approaches, credit types, and prices for carbon.\nSource: Asian Development Bank. 2023 price is not included because it is subject to change.\nActions that developing countries can take to leverage carbon market opportunities for government revenue enhancement and decarbonization pathways:\n- Enhancing the measurement and verification of carbon credits for market opportunities:\nADB is applying carbon price in economic analysis, set at US$49.8/ton in 2022, to quantify the difference in emissions between a with-project scenario and a without-project scenario.\nWith its huge volume and rising price, carbon has the potential to become the most valuable commodity. The World Bank has begun implementing result-based climate finance programs through the purchase of carbon credits or providing revenue to private sector clients via the carbon markets. For carbon credits to be traded, they must be issued to eligible projects that recognize quantified emission reductions that are real, additional, permanent, and below a baseline scenario.\nDeveloping countries can engage MFIs’ independent role in the measurement and verification of carbon credits issued to domestic and regional projects. Then, governments can ensure that their verified carbon credits are good for trading activities in the market.\n- Developing the carbon markets through supporting the convergence of the compliance and voluntary markets\nThe carbon market is an evolving space and immature market. The Paris Agreement Article 6 enables governments to independently choose projects and whether to authorize emissions reductions in their countries. It is unclear how Article 6 will shape the carbon market, but global climate change ambition can only materialize if governments and MFIs act proactively together.\nThe components of the carbon market are the voluntary market and the compliance market. The smaller voluntary carbon market, valued at only $2 billion in 2021, enables corporates to offset hard-to-abate emissions to achieve net-zero and carbon-neutral ambitions. The much larger compliance market, valued at $751 billion in 2021 and dominated by EU ETS, is essentially a policy instrument mandating the reduction of greenhouse gas emissions. Currently, compliance and voluntary carbon markets operate largely independently of one another, but the blurring of lines has occurred. There is already a convergence between voluntary and compliance markets. Various compliance schemes, such as the PRC and Republic of Korea ETS, allow for a limited use (i.e., 5%) of carbon offsets. Voluntary programs can also participate in international compliance mechanisms such as Article 6.\nBy leveraging MFIs’ capacity building resources, developing countries can make the convergence of the voluntary and compliance markets happen sooner through proper institutional framework designs.\nDecarbonization and carbon market development are challenging processes that take place together. They are constantly updated processes as new rules come out. Developing countries can work with MFIs to keep track of the rules and use their own individual climate neutrality targets to effectively cope with the carbon market.\n 2023. State and Trends of Carbon Pricing, World Bank. https://openknowledge.worldbank.org/handle/10986/39796\nThis article was first published as an ADB blog."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:112fbc5f-da80-4c65-91c4-2ccb9f395ec4>","<urn:uuid:9c2c65f2-e3e1-4572-a35b-2cdbef1ba8ea>"],"error":null}