{"question":"How do I set up a basic aquaponics system for growing vegetables and fish at home?","answer":"To set up a basic aquaponics system, you'll need several key components. First, you'll need a fish tank (a 55-gallon drum or 225-gallon bin works well) and growing beds with a soilless medium like perlite, vermiculite, or fine gravel. You'll also need a pump to move water between the tank and growing area, and an aerating device to maintain oxygen levels for the fish. Install a water distribution system using PVC pipes with 1/4-inch holes every 6-8 inches. The growing surface area should be at least twice the surface area of the tank. Place the system in full sun with flat ground and easy access to water. For beginners, stock one fish per 10 gallons of water, and start with easy crops like lettuce, kale, or other leafy greens.","context":["Aquaponics combines aquaculture, the raising of edible fish, with hydroponics, a method of growing plants in a soilless medium. It is one the most efficient and sustainable approaches to food production capable of producing hundreds of pounds of healthy, low-fat protein and organic vegetables in an area the size of a small room.\nAquaponics systems are modeled on natural aquatic systems, where the waste products of the fish are the source of nutrients for the plants, which in turn purify the water for the fish. It is a model of self-sufficient food production, as it requires little in the way of external inputs and lessens reliance on large-scale unsustainable food systems.\nAquaponics is a lot more involved than a typical vegetable garden, but it is a worthwhile project within reach of any motivated homesteader. You can try your own DIY system or work with a ready-made kit. The key is to keep your system as small and simple as possible until you master the basic techniques.\nIndoor Versus Outdoor Systems\nAquaponics systems are often incorporated into greenhouses in order to keep them going year round. They can even be used indoors under artificial lighting. There are pro and cons for each approach.\nBecause indoor systems produce food continuously in any climate, they are inherently more productive than outdoor systems. However, building a greenhouse or setting up an artificial lighting system adds a great deal of complexity and expense to an aquaponics system, not to mention the cost of heating the greenhouse.\nIn mild winter climates, fish and vegetables can be produced all year outdoors, but in temperate climates the system generally has to be shut down for winter. This is potentially a big issue if the fish have not yet reached a harvestable size, though some species are able to overwinter.\nSome people choose to have a very small system indoors for breeding fingerlings in the winter that are then released into the outdoor tank once the water has warmed sufficiently.\nAll aquaponics systems have certain features in common although the exact materials used can vary significantly depending on the approach.\nMany vegetables grow perfectly well without soil, as long as nutrients are provided in an aqueous solution. This is the basis of hydroponics, which is one half of the equation in an aquaponics system. Perlite, vermiculite, sand, fine gravel, coco coir and many other inert growing mediums are available. You can make your own recipe with some of these ingredients or make use of a commercially available hydroponic growing medium.\nThe options for containers to hold the growing medium are equally diverse. Ordinary plastic pots are the simplest DIY approach; at the other end of the spectrum are things like hydroponic towers that allow increased productivity by making use of vertical space. In between are a myriad of products from floating rafts to simple plastic growing boxes.\nOne popular approach is to build rectangular wooden boxes just like those used for ordinary raised bed vegetable gardens. These are then lined with pond liner and filled with fine gravel or some other free-draining growing medium.\nBarrel and Tank Options\nAn ordinary 55-gallon drum or a square 225-gallon bin are easy, inexpensive options for an aquaponic fish tank. Many people have used aboveground swimming pools for larger backyard systems. Specialty suppliers offer heavy duty plastic tanks or you can improvise your own system using pond liner in a raised wooden structure or set into the ground.\nIf you're considering recycled plastic bins for your tank, make sure they were used for food products or other non-toxic materials and clean them thoroughly before adding water and fish.\nOther Critical Components\nProfessional aquaponics systems can be high-tech and quite complex. Still, the backyard gardener needs a lot more than just a fish tank and growing beds.\nFirst off, a pump is necessary to move water between the tank and growing area. A standard fountain pump sold for backyard water features is generally sufficient.\nSecond, and equally critical, is an aerating device to maintain oxygen level for the fish. This is a mission critical component - if it stops running for even a few hours, a fish kill may occur. This can take the form of a bubbler device that sits at the bottom of the tank or agitators that swish the water around on the surface to add oxygen.\nYou will also need a system to distribute the water from the fish tank to the vegetables (unless floating beds are used). Again, there are numerous products designed for this purpose, though many backyard growers simply drill 1/4-inch holes every six or eight inches in pieces of 1/2-inch PVC pipe as a rudimentary drip system to deliver the water. In the raised wooden box system described above, the PVC pipes would be laid parallel to each other about 12 inches apart. A seedling can be planted at each of the drip holes in the PVC pipe.\nFish Species to Consider\nThe species you choose depends on whether your system will be kept outdoors or indoors.\nCatfish are a reliable choice for a recirculating aquaculture system and are the top species used in systems that are allowed to overwinter. As long as a small area on the surface is kept free of ice wit an agitator, catfish will survive the winter in a semi-dormant state just as they do in nature. Once the water temperature hits 70 degrees in spring, they will wake up and start feeding again.\nTilapia are the most widely farmed fish in the world and are perfectly adapted to aquaponics systems. The only disadvantage is their inability to survive cold weather. They are a tropical species and will die if the water temperature dips below 50 degrees. The water needs to be at least 70 degrees for them to thrive and reproduce. It is possible to grow them out to a harvestable one-pound size in a six-month period of warm weather from April to September, though there is no guarantee they will grow that fast, especially for a beginner. For this reason tilapia are typically used in greenhouse systems.\nYellow perch are another species used in aquaponics systems that are known for their excellent culinary qualities, as well as their ability to grow in cool waters.\nBest Choices for Hydroponic Vegetables\nLettuce, kale, arugula, spinach and other leafy greens are by far the easiest crops to grow in an aquaponic system along with herbs like basil and dill. Essentially, anything harvested in the leaf stage, as opposed to a fruit, does well with the mix of nutrients contained in fish waste. Tomatoes, peppers, cucumber and strawberries are commonly grown in hydroponic systems as well, but they need nutritional supplements to produce a crop.\nAssembling Your System\nFull sun, flat ground and easy access to a water supply are the three main requisites when choosing a site for an aquaponics system. There are many possible configurations depending on the chosen approach.\nOne of the main decisions is whether to have the grow beds higher in elevation than the tank or vice versa. The lightweight growing medium makes it possible to elevate the grow beds above the tank on a table if desired. For this method, a system of piping has to be devised to collect the water and funnel it back to the tank. In this case the pump will be in the bottom of the tank. If the beds are on the ground, however, a second pump is needed to return the water that drains from them to the fish tank.\nAnother big question in designing an aquaponics system is the ratio of growing space to tank space. In general, the more growing space the better, as it it's actually the growing medium, not the plants, that do most of the filtering. A good rule of thumb is to plan for the growing surface to be at least two times the surface of area of the tank.\nAn Abundant Harvest\nAquaponics systems can be stocked at up to one fish per gallon of water, making them incredibly efficient at food production. It's recommended that beginners start with one fish per 10 gallons of water while learning the ropes of aquaculture, but that's still a lot of fish that can be grown in a very small space. With the veggies you'll harvest from the growing beds, your family will have the freshest food imaginable."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:09a7388e-fe50-4057-aceb-a7cfd2988909>"],"error":null}
{"question":"How do biological and psychological factors contribute to addictive behaviors in both ethical decision-making and Internet usage patterns?","answer":"In both ethical decision-making and Internet addiction, multiple factors influence behavior. From an ethical perspective, people weigh consequences and benefits, considering the greatest good for the greatest number while attempting to minimize harm. Similarly, Internet addiction involves biological predispositions, including deficient levels of dopamine and serotonin, which affect the brain's pleasure center. The prefrontal region of the brain, responsible for prioritizing tasks and decision-making, can be physically altered by excessive Internet use. Psychological factors such as anxiety, depression, and social awkwardness can drive both ethical choices and Internet addiction, with individuals often seeking to fill emotional voids or escape from problems. The Variable Ratio Reinforcement Schedule theory explains how unpredictable rewards in Internet usage can lead to addictive behavior, similar to how different consequences influence ethical decision-making.","context":["Philosophers and ethicists distinguish among three types of ethical reasoning: deontological, or rules-based reasoning; virtue ethics; and consequential reasoning. Consequential reasoning involves looking at the consequences of an action or decision to determine its moral value. This type of reasoning has application in making moral judgments in personal decisions as well as the fields of political, business, medical and engineering ethics.\nIn the history of philosophy, the main type of consequential reasoning is called utilitarianism. The philosopher Jeremy Bentham first put forth this line of thinking in the late 1800s, and it became popular due to the expanded work in utilitarian ethics carried out by Bentham’s protégé John Stuart Mill. Bentham considered utilitarianism doing the greatest good for the greatest number of people affected by a decision, while minimizing any possible harm. Mill elaborated this theory further by arguing that a decision or action is right if it provides the greatest happiness for the greatest number while causing the least amount of unhappiness.\nConsequential reasoning involves several steps to arrive at a morally right or just decision or action. The first step involves specifying the action under consideration. The second step identifies all those affected by an action. Step three determines the benefits and harm produced by the action. The fourth step weighs the amount of good, or happiness, brought about by the decision. If the good achieved by an action outweighs any harm that may result, utilitarian theory considers the action a morally right one.\nIn business ethics, a chief executive officer might ask whether it's morally justifiable for her business to lower its environmental standards to save money. She would then determine the individuals affected, which would include the workers in the company, the company’s shareholders, the local ecology and any inhabitants that might incur harm from the pollution. The CEO must then identify the benefits or harm that each affected party would receive or suffer. Benefits may include increased profits by saving money, which could lead to better wages and larger dividends for the workers and shareholders, respectively. Harmful effects may consist of damage to the local ecosystem and possible health problems for residents living near the company. The CEO would also need to consider any potential jail time she might receive if she broke environmental laws. According to utilitarian theory, the correct decision is the one that maximizes benefits while minimizing harm or costs.\nEthical thinkers have criticized consequential ethical reasoning for ignoring fundamental moral categories. In the above example about industrial pollution, consequential analysis does not take into account that people have rights, such as the right not to be harmed, and that these rights entail duties, such as the “duty not to harm others unjustly.” A utilitarian could potentially argue that a corporate strategy that saves a company a great deal of money could offset the problems associated with pollution that harms very few people to a very small degree. Critics of utilitarian theory would argue that no amount of good offsets any harm intentionally inflicted on others.\n- Jupiterimages/Photos.com/Getty Images","What Is Internet Addiction?\nDo you play video games on the Internet in excess? Are you compulsively shopping online? Can’t physically stop checking Facebook? Is your excessive computer use interfering with your daily life – relationships, work, school? If you answered yes to any of these questions, you may be suffering from Internet Addition Disorder, also commonly referred to as Compulsive Internet Use (CIU), Problematic Internet Use (PIU), or iDisorder. Originally debated as a “real thing,” it was satirically theorized as a disorder in 1995 by Dr. Ivan Goldberg, M.D. who compared its original model to pathological gambling. Since this hoax of sorts, the disorder has rapidly gained ground and has been given serious attention from many researchers, mental health counselors, and doctors as a truly debilitating disorder. Though not officially recognized as a disorder in the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV), its prevalence in American and European cultures is staggering – affecting up to 8.2% of the general population. However, some reports suggest it affects up to 38% of the general population. The widely variable difference in prevalence rates might be contributed to the fact that no true and standardized criteria has been selected for Internet Addiction Disorder. It is researched differently among scientists and mental health professionals. And, it is researched differently across ethnic cultures.\nThe advancement in study of Internet Addiction Disorder has been negatively impacted by the lack of standardization in this area. It has been generally accepted among researchers, however, that Internet Addiction is only a subset of technology addiction in general. As the name states, its concentration is on compulsion with the Internet – as other areas of media addiction can be seen in television addiction, radio addiction, and other types of media addiction. Due to the explosion of the digital age, Internet Addiction Disorder has taken the reigns as the top culprit is technology addiction as of late. The troubling thing about this disorder is that if you are suffering from it, you are endlessly surrounded by technology. In the digital age, the Internet has taken over. Most of what we do, as a general population, can be done on the Internet. Can’t find that shirt you want in the store? No worries – the Internet has it! Need to place an order for pizza? Why call? Complete an online order! Can’t call over a friend to play a video game at 3am when you’re suffering from insomnia and can’t go back to sleep? I bet there’s someone across the globe that is awake and ready to play! That’s, in essence, why this disorder can be so troubling – even treatment-wise. It’s hard to live these days by getting rid of the Internet. We’re always surrounded by it – and for most of us, we use it daily.\nJust because you use the Internet a lot – watch a lot of YouTube videos, shop online frequently, or like to check social media does not mean you suffer from Internet Addiction Disorder. The trouble comes when these activities start to interfere with your daily life. In general, Internet Addiction Disorder is subdivided into varying categories. The most commonly identified categories of Internet Addiction include gaming, social networking, email, blogging, online shopping, and inappropriate Internet pornography use. Other researchers suggest that it is not the amount of time spent on the Internet that is particularly troublesome – rather, it is how the Internet is being used. That is, the riskiness of Internet use can be just as important as the amount of time spent. Do you have a teenager using teen dating sites that could have child molesters lurking on the site? This is risky – and one of the multidimensional aspects of Internet Addiction Disorder. Other identified multi-dimensional risk factors of Internet Addiction Disorder include physical impairments, social and functional impairments, emotional impairments, impulsive Internet use, and dependence on the Internet.\nWhat Causes It?\nLike most disorders, it’s not likely to pinpoint an exact cause of Internet Addiction Disorder. This disorder is characteristic of having multiple contributing factors. Some evidence suggests that if you are suffering from Internet Addiction Disorder, your brain makeup is similar to those that suffer from a chemical dependency, such as drugs or alcohol. Interestingly, some studies link Internet Addiction Disorder to physically changing the brain structure – specifically affecting the amount of gray and white matter in regions of the prefrontal brain. This area of the brain is associated with remembering details, attention, planning, and prioritizing tasks. It is suggested one of the causes of Internet Addiction Disorder is structural changes to the prefrontal region of the brain are detrimental to your capability to prioritize tasks in your life, rendering you unable to prioritize your life, i.e., the Internet takes precedence to necessary life tasks.\nInternet Addiction Disorder, in addition to other dependency disorders, seem to affect the pleasure center of the brain. The addictive behavior triggers a release of dopamine to promote the pleasurable experience activating the release of this chemical. Over time, more and more of the activity is needed to induce the same pleasurable response, creating a dependency. That is, if you find online gaming or online shopping a pleasurable activity and you suffer from an addiction to the Internet, you will need to engage in more and more of the behavior to institute the same pleasurable feeling prior to your dependency.\nThe variable reinforcement effects of Internet addiction is another cause of this behavior. According to the Variable Ratio Reinforcement Schedule (VRRS) theory, the reason why you might be so addicted to Internet activity (e.g., gaming, gambling, shopping, pornography, etc.), is because it provides multiple layers of rewards. That is, your constant surfing of the Internet leads to multiple rewards that are unpredictable. Perhaps your addiction to Facebook provides a multiple and unpredictable layer of rewards in the sense that every time you sign on to read your updates, you get repeated and unexpected good news. Maybe you found out one of your great friends just got engaged. The next time you sign on, you learn another friend just had a baby! Or, perhaps the man you are really interested in just posted an update that he and his longtime girlfriend just broke up. Each sign on gives you unpredictable results that keep you entertained and coming back for more. Certain games, such as MMROPGs (massively multiplayer online roleplaying games) – including World of Warcraft and Everquest may lead to Internet addiction because, in effect, they never end.\nBiological predispositions to Internet Addiction Disorder may also be a contributing factor to the disorder. If you suffer from this disorder, your levels of dopamine and serotonin may be deficient compared to the general population. This chemical deficiency may require you to engage in more behaviors to receive the same pleasurable response compared to individuals not suffering from addictive Internet behaviors. To achieve this pleasure, individuals may engage in more behavior to the general public, increasing their chances for addiction.\nPredispositions of Internet addiction are also related to anxiety and depression. Oftentimes, if you are already suffering from anxiety or depression, you may turn to the Internet to relieve your suffering from these conditions. Similarly, shy individuals and those with social awkwardness might also be at a higher risk of suffering from Internet addiction. If you suffer from anxiety and depression, you might turn to the Internet to fill a void. If you are shy or socially awkward, you may turn to the Internet because it does not require interpersonal interaction and it is emotionally rewarding.\nWhat are the Symptoms?\nSigns and symptoms of Internet Addiction Disorder may present themselves in both physical and emotional manifestations. Some of the emotional symptoms of Internet Addiction Disorder may include:\n- Feelings of guilt\n- Feelings of Euphoria when using the Computer\n- Inability to Prioritize or Keep Schedules\n- No Sense of Time\n- Avoidance of Work\n- Mood Swings\n- Boredom with Routine Tasks\nPhysical Symptoms of Internet Addiction Disorder may include:\n- Carpal Tunnel Syndrome\n- Poor Nutrition (failing to eat or eating in excessively to avoid being away from the computer)\n- Poor Personal Hygiene (e.g., not bathing to stay online)\n- Neck Pain\n- Dry Eyes and other Vision Problems\n- Weight Gain or Loss\nWhat are the effects of Internet Addiction Disorder? If you are suffering from this disorder, it might be affecting your personal relationships, work life, finances, or school life. Individuals suffering from this condition may be isolating themselves from others, spending a long time in social isolation and negatively impacting their personal relationships. Distrust and dishonesty issues may also arise due to Internet addicts trying to hide or deny the amount of time they spend online. In addition, these individuals may create alternate personas online in an attempt to mask their online behaviors. Serious financial troubles may also result from avoidance of work, bankruptcy due to continued online shopping, online gaming, or online gambling. Internet addicts may also have trouble developing new relationships and socially withdraw – as they feel more at ease in an online environment than a physical one.\nHow is it Diagnosed?\nThough it is gaining traction in the mental health field – and recently added to the Diagnostic and Statistical Manual of Mental Disorders as a disorder that needs more research, a standardized diagnosis of Internet Addiction Disorder has not been discovered. This is also a significant contributing factor to the overall variability in the disorder as a whole and wide range of prevalence in the population from 0.3% to a whopping 38%.\nOne of the more accepted diagnostic assessments of Internet Addiction Disorder has been proposed by KW Beard’s 2005 article in CyberPsychology and Behavior. Beard proposes five diagnostic criteria in the identification of Internet Addiction Disorder in the general population:\n- Is preoccupied with the Internet (constantly thinks about past use or future use)\n- Needs to use the Internet with increased amounts of time to gain satisfaction\n- Has made unsuccessful efforts to control, cut back, or stop use of the Internet\n- Is restless, moody, depressed, or irritable when attempting to control Internet use\n- Has stayed online longer than originally intended\nIn addition, Beard (2005) suggests at least one of the following must also be present in a diagnosis of Internet Addiction Disorder:\n- Has jeopardized or risked the loss of a significant relationship, job, educational, or career opportunity because of the Internet\n- Has lied to family members, therapists, or others to conceal their involvement with the Internet\n- Uses the Internet as a way of escaping from problems or to relieve a dysphoric mood (e.g., guilt, anxiety, depression, helplessness)\nIf you have sought help with an Internet Addiction Disorder, you have likely been given a mental test or questionnaire of some sort to assess your dependency on the Internet. The most common assessment tools used to help make a diagnosis of Internet Addiction Disorder include:\n- Young’s Internet Addiction Test\n- the Problematic Internet Use Questionnaire (PIUQ)\n- the Compulsive Internet Use Scale (CIUS)\nWhat are the Treatment Options?\nThe first step in treatment is the recognition that a problem exists. If you do not believe you have a problem, you are not likely to seek treatment. One of the overarching problems with the Internet is that there is often no accountability and no limits. You are hidden behind a screen – and some things that you may say or do online are things you would never do in person.\nThere is debate in the literature whether treatment is necessary in the first place. Some believe Internet Addiction Disorder to be a “fad illness” and suggest that it usually resolves itself on its own. Studies have show that self-corrective behavior can be achieved and successful. Corrective behaviors include software that controls the Internet use and types of sites that can be visited – with the majority of professionals in agreement that total abstinence from the computer is not an effective method of correction.\nSome professionals argue that medications are effective in the treatment of Internet Addiction Disorder – because if you are suffering from this condition, it is likely that you are also suffering from an underlying condition of anxiety and depression. It is generally thought that if you treat the anxiety or depression, the Internet Addiction may resolve in step with this treatment approach. Studies have shown that anti-anxiety and anti-depressant medications have had a profound affect on the amount of time spent on the Internet – in some cases decreasing rates from 35+ hours a week to 16 hours a week. Physical activity has also been indicative of effective in increasing serotonin levels and decreasing dependency on the Internet.\nSome of the more common psychological treatments of Internet Addiction Disorder include:\n- Individual, group, or family therapy\n- Behavior modification\n- Dialectical Behavioral Therapy (DBT)\n- Cognitive Behavioral Therapy (CBT)\n- Equine Therapy\n- Art Therapy\n- Recreation Therapy\n- Reality Therapy\nBecause of the prevalence of the disorder in the general population, treatment centers and programs have started to pop up in the US and across the globe. In some cases, electro-shock therapy was used to wean individuals off the Internet – this method has since been banned. The ReSTART residential treatment facility was started in 2009 in Seattle, WA for pathological computer use. In 2013, a USB-connected keyboard device was created to provide a very low voltage shock to users who visited particular websites. In other places nationwide and internationally, de-addiction centers have been started to aid individuals suffering from Internet Addiction Disorder.\nIn many instances, multimodal treatments have been employed to treat Internet Addiction Disorder. In this method of treatment, if you are suffering from this condition, you might be prescribed both medications and psychotherapy to treat your addiction to the Internet.\nContinued or Questionable Existence?\nThough originally diagnosed as a “hoax” disorder – the increased digital age has propelled us into the Internet age and Internet addiction has become a truly real “thing.” However, many researchers are uncertain of whether Internet Addiction Disorder is a disorder in its own existence or rather a symptom of other underlying conditions.\nCreating an even more problematic interaction is the fact that everything is online nowadays. It’s hard to make a distinction between online and offline worlds. Everything is Internet-based. From ordering food, interacting with friends, playing games, and even watching tv. Adding an additional layer of confusion and distinction is that other digital technology is taking over the world as well – make access to computers even easier. Now, we don’t have to be physically sitting in front of the computer – we can do anything from anywhere with just our phones, tablets, or other electronic devices.\nStill, other researchers question whether excessive Internet use is an addiction or an obsessive-compulsive or impulse-control disorder. Indeed, the Diagnostic and Statistical Manual of Mental Disorders is correct in its acknowledgement that much more research is needed to study this disorder."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d53426bb-3b2a-403f-be26-de2d60664971>","<urn:uuid:4521d3a8-4d42-4c7e-ac3b-e2011c4a66cd>"],"error":null}
{"question":"What characteristics differentiate playas formed in intermontane valleys versus the Salar de Uyuni in terms of their water sources and formation periods?","answer":"Playa lakes in intermontane valleys primarily form from groundwater percolation and are influenced by alluvial fans that block drainage in desert mountain regions. In contrast, the Salar de Uyuni formed from the transformation of prehistoric lakes between 30,000-40,000 years ago, specifically from Lake Minchin which became Paleo Lake Tauca. While intermontane playa lakes can be seasonal or permanent depending on groundwater levels, Salar de Uyuni developed into a permanent salt flat with a thick salt crust containing significant lithium deposits.","context":["A ephemeral lake in an arid or semiarid region, most of whose\nwater comes from groundwater percolating through the soil\nof higher surrounding areas.\nMany playa lakes form in intermontane valleys in mountainous desert\nregions, such as North America's Basin and Range country. Others form in deflation hollows or pans which have eroded down to the water table.\nIn a desert, the rocky debris of erosion only travels as far as there\nis water above ground to push it along, or as far as there is a steep slope\nto roll down. Debris builds up into alluvial fans along the bases\nof the mountains. Alluvial fans are much larger than they first appear:\nThe steep, visible part of the fan is made up of large rock fragments.\nThe bases of the fans, however, are made up of finer particles such as silt,\nsand, and clay. The fans' lower slopes are very gentle, so gentle\nthat a hiker might not think that he or she is hiking on a hill slope.\nIn certain intermontane valleys, fans merge to block all drainage\nout of the valley. Any blockage of drainage traps water, even in\na desert. Sometimes, enough water collects in the lowest portion\nof the valley for a lake to exist year-round. Periodic floods distribute\nthe finest sediment to flatten the valley floor even further, resulting\nin vast lakes miles across and a quarter inch deep.\nDuring the last Ice Age, many intermontane environments were not as\ndry as they are now. There was more water, sometimes flowing\ndirectly from melting glaciers. Consequently, the lakes were much\nlarger. Although these lakes have receded since the Ice Age, they\nhave not lost all of the water from that time. Playa lakes\nthat have had assistance from historic periods of increased rainfall are\nknown as pluvial lakes. The best-known pluvial lakes are the Great\nSalt Lake and Yellowstone Lake.\nIn other places, such as the southern Great Plains, playa lakes are\nlow spots in the soil where groundwater collects. The formation\nof these playa lakes is the subject of debate; there are tens of thousands\nof them and no alluvial fans to block drainage. One theory\nholds that they lie above weak spots in an impermeable layer of stone,\nwhere water percolates through into the underlying deep Ogallala Aquifier.\nStill other desert lakes are fed from continent-sized regions of internal\ndrainage. Lakes such as Lake Eyre in Australia, Lake Chad in\nAfrica, and the Aral Sea and Lake Balkhash in Central Asia are\nmore complex than their smaller cousins, being fed from both groundwater\nand surface drainage. Such lakes are not properly called playa lakes,\nhowever, they sometimes act like their smaller counterparts, going through\n\"playa phases\", When the Mediterranean Sea dried up during the Miocene epoch, the largest playa lakes\nin Earth's history were formed!\nSalts dissolved from rock, or leached from the soil during percolation\nhave nowhere to go but the lake, and the lakes frequently build up high\nsalinity. Quite often, large sections of playa lakes dry up completely,\nleaving vast salt pans. such as the Bonneville Salt Flats.\nHowever, even in a dry lake bed, groundwater is closer to the surface\nhere than in other places. In places where salt has not built\nup to toxic levels, the lakes and lakebeds provide vital water for vegetation.\nThese are wetland oases for wildlife and humans.\nMany playa lakes are home to salt-resistant crustaceans that crawl through\nthe lake bed; flocks of millions of flamingoes feed upon them.\nCheyenne Bottoms in central Kansas is a temperate playa wetland, a\nhaven for migrating birds. Lake Texcoco's wetlands caused the nomadic\nAztec to settle down, build their city Tenochtitlán, and eventually","Salt pans are large flat grounds covered with salt and other minerals. They are common in deserts and shine under the sun. Salt flats are formed by the evaporation of water bodies such as lakes or ponds, especially in places where the rate of evaporation is considerably higher than the rate of precipitation such as in deserts. Water that cannot be infiltrated into the ground often evaporates, living behind minerals and salt particles. The salts and minerals form on the surface over several years creating a salt pan. Salt pans can be dangerous if they conceal mud beneath it. The concealed mud can engulf a truck.\n5. Salar de Uyuni\nSalar de Uyuni covers an area of about 4,085 square miles. The salt flat is found in Bolivia next to the area around the Andes in Daniel Campos Province. It resulted from the transformation of several prehistoric lakes that dates between 30,000 and 40,000 years ago. Lake Minchin transformed into Paleo Lake Tauca whose depth was about 459 feet. When the lake completely dried, two other lakes were left behind with two salt flats, Salar de Coipasa and the Salar de Uyuni, also formed. The latter is covered with a thick crust of salt which is a major source of salt. The crust also accounts for about 60% of the known lithium deposit in the world.\n4. Bonneville Salt Flats\nBonneville Salt Flats covers parts of the Tooele County, Utah. It is one of the several salt pans covering the western parts of the Great Salt Lake. The area covered by the salt pan was once the site for Lake Bonneville. The salt flats are about 12 miles long and cover an area of about 45 square miles. The depth of the crust increases towards the center of the flat, reaching about 5 feet at the thickest point. Bonneville Salt Flats is under the management of the Bureau of Land Management and it is opened to the public. The salt flat has hosted motorcar racing since 1914 with the racing taking place at Bonneville Speedway.\n3. Etosha Pan\nEtosha Pan is part of the Kalahari Basin located in northern Namibia. It measures 75 miles long and is protected by the Etosha National Park. The pan is often dry but the heavy rains form a thin layer of heavily salted water. The salt flat is characterized by white and greenish surface spreading over 1,850 square miles. The salt pan was developed over 10 million years ago through a tectonic plate activity.\n2. Cono de Arita\nSalar de Arizaro is the sixth largest salt flat in the world and the second largest in Argentina. The salt flat is situated in Los Andes Department, Salta Province. It covers an area of approximately 618 square miles. The salt pan is an important mine for metallic and non-metallic minerals including salt, copper, iron, and marble. An outstanding feature on Salar de Arizaro is the conical hill known as Cono de Arita, a small volcano that lacked the strength to burst through the crust so it never developed crater.\n1. Devil's golf course\nDevil's golf course covers the surface of the Mojave Desert which is within the Death Valley National Park. The valley was initially covered by Lake Manly to a depth of about 30 feet. However, the lake’s water evaporated with time leaving behind the salt flat containing minerals that were part of the lake’s water. The Devil's golf course is located several feet above the valley floor causing it to remain dry most of the time and allowing the weathering process to disintegrate salt into complex formations."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:2471bd6e-32a6-45b4-bed4-16bc8c90f825>","<urn:uuid:19a0819d-f359-430a-b322-d125a4892a7a>"],"error":null}
{"question":"Why do shelf/slope exchange processes affect marine biodiversity, and how does temperature variability influence species distribution in these areas?","answer":"Shelf/slope exchange processes affect marine biodiversity by transferring heat, salt, and organic matter between continental shelves and the deep sea, which is critical in structuring continental shelf food webs. These exchanges attract megafauna like whales to locations of intense exchange, likely due to food availability. Temperature variability influences species distribution through its impact on metabolic oxygen demand - as temperatures rise, marine organisms require more energy for normal life processes, while oxygen availability decreases. This creates a stringent trade-off in energy usage for marine species. Research on tropical species has shown that extreme temperature events, rather than average conditions, determine habitat selection. Species face particular challenges in warmer waters, which hold less oxygen than cooler waters, leading to potential habitat exclusion when temperature and oxygen conditions become too stressful.","context":["Understanding the spatial and temporal complexity of the coastal ocean is a long-standing challenge. Quantifying the interactions between atmospheric and terrestrial forcing, and coupled physical, chemical, and biological processes, is critical to elucidating the role of coastal margins in the global carbon cycle, and developing strategies for managing coastal resources in a changing climate.\nLearn more about the various themes within Coastal Ocean Dynamics and Ecosystems:\nShelf/slope exchange processes are important mechanisms that transfer significant amounts of heat, salt, and organic matter between continental shelves and the deep sea. These mechanisms, however, are highly variable in space and time. Many may span the area of kilometers in the horizontal dimension, but may only span a few meters vertically into the water column. Exchange may last only a few days. Extreme events such as storms appear to play a large role in sustaining exchange and dissipating heat, salt, and organic matter.\nTraditional shipboard sampling cannot provide sufficient resolution in time or space to fully examine and quantify these shelf/slope processes. As a result, it is not possible to derive robust budgets for carbon, heat, salt, and other properties on continental shelves.\nWithout a complete understanding of shelf/slope processes, scientists have been unable to quantify the flux of carbon between continental shelves and the deep sea. Additionally, shelf/slope exchanges are critical in structuring continental shelf food webs. For example, megafauna, such as whales, are often known to congregate at locations of intense exchange, driven there likely by the availability of food.\nHigh-frequency spatial and temporal data collected by the OOI Network on the U.S. east and west coasts will enable scientists to quantify these exchange mechanisms and identify their impact on shelf/slope biogeochemistry.\nBecause exchange can vary in location along the shelf/slope due to the passage of offshore features (e.g., warm/cold core rings) it is very important to collect real-time, in situ data. Additionally, the OOI will have mobile assets, AUVs, deployed in these areas who can be program to navigate towards these temporary features.\nProfilers will collect high-frequency data to characterize the water column, from the seafloor to the sea surface. In doing so they will be able to capture properties of water masses that pass through the array. Meteorological measurements will be taken to study the impact of wind forcing on exchange.\nGiven that the many of the shelf/slope waters are optically complex, the OOI sampling strategy will include optical characterization of dissolved and particulate material (e.g., sediment, phytoplankton, and detritus) in the water column.\nTo enable future studies of other shelves and processes, the Pioneer array itself will be re-deployable at the completion of the initial study.\nRelated Science Questions\n- How do shelf/slope exchange processes structure the physics, chemistry, and biology of continental shelves?\n- What processes lead to heat, salt, nutrient, and carbon fluxes across the mid-Atlantic Bight shelf-break front?\n- What is the relationship between the variability in shelf-break frontal jets and along-front structure in phytoplankton distributions?\n- What aspects of interannual variability (in stratification, offshore circulation patterns, jet velocities, and wind forcing) are most important for modulating shelf/slope exchange of dissolved and particulate materials?\nHypoxia on Continental Shelves\nDissolved oxygen concentrations in continental shelf waters are influenced by a variety of physical, chemical, and biological processes. These processes occur over spatial scales of meters to thousands of kilometers, and from hours to decades. Dissolved oxygen in seawater is crucial to the survival of marine animals. The coastal ocean is vulnerable to low oxygen (hypoxic) zones and events that may involve significant marine die-offs.\nUnlike hypoxic events fueled by anthropogenic nutrients and limited circulation of semi-enclosed estuaries or embayments, hypoxia on the continental shelf, such as off the coast of Oregon, is driven by atmospheric forcing, upwelling/downwelling, and variability in ocean circulation.\nThe formation and duration of hypoxic areas are subject to climate variability and variations in upwelling/downwelling and oceanic flow on seasonal, interannual, El Niño Southern Oscillation, and inter-decadal scales. Understanding hypoxic events and impacts to marine ecosystems requires the ability to observe physical, chemical, and biological conditions across the continental shelf to slope waters, for periods spanning years (seasonal to interannual change) to decades (El Niño Southern Oscillation and Pacific Decadal Oscillation shifts).\nOOI’s distributed network of fixed and mobile platforms will permit studies of the frequency, intensity, and mechanisms driving the invasion of low dissolved oxygen water on continental shelves. Large, three-dimensional data volumes collected by gliders will provide detailed information for making maps of the low dissolved oxygen waters. Gliders will also adaptively map the spatial extent and morphology of the low dissolved oxygen intrusion.\nFor studies of coastal ocean processes, from event-scale changes, to interannual variability, to interdecadal trends, data will be collected by permanent and movable instrumented arrays that have sufficient power and bandwidth to support multidisciplinary sensors. These nodes will also collect time series of the gradients in physical and biogeochemical properties across the continental shelf and slope.\nBy combining these data with simultaneous observations of the meteorological forcing and oceanic flows measured at high vertical resolution, scientists will be able to study the corresponding chemical and biological response to the low dissolved oxygen water.\nRelated Science Questions\n- What are the dynamics of hypoxia on continental shelves?\n- What are the relative contributions of low-oxygen, nutrient-rich source water, phytoplankton production from local upwelling events and along-shore advection, and local respiration in driving shelf water hypoxia in the Northern California Current?\n- What are the impacts of shelf hypoxic conditions on living marine resources?\n- How are wind-driven upwelling, circulation, and biological responses in the coastal zone affected by the El Niño Southern Oscillation, water mass intrusions, and inter-decadal variability?","What can brittle stars tell us about the response of marine species to climate warming?\nA recent study conducted on coral reefs along the coast of Panama is helping to shed light on how marine species are impacted by an increasingly warming planet. The study uses unprecedented high-frequency data to examine the effects of temperature and oxygen on the species’ biological demands and habitat requirements. The results of this study have implications for the diverse marine animals that live in coral reefs and may help scientists better understand the complex dynamic between climate change and ecosystems.\nScientists have for a while now known that climatic warming has deleterious effects on marine life. One important reason is that warmer temperatures increase a species’ metabolic oxygen demand while decreasing oxygen availability. Temperature and oxygen play a central role in limiting the habitability for animal species, because together these environmental conditions determine how ‘breathable’ or ‘metabolically viable’ the water is for animals of all kinds.\n“As temperatures rise, marine organisms use more energy to maintain normal life processes,” said Curtis Deutsch, a professor of geosciences and the High Meadows Environmental Institute at Princeton University and coauthor of the study. “When the ocean gets warmer, the availability of oxygen doesn’t go up, it goes down. This means that species face a much more stringent trade-off in how they use energy—they need more of it but have less of it.”\nNowhere is this more concerning than in tropical locales, which contain the largest number and greatest diversity of species on Earth, and whose warmer waters hold less oxygen than cooler, more temperate ocean waters. But the complex relationship between thermal tolerance, temperature variability and the response of many species to warming trends in tropical waters is not well understood. Most of our current understanding, Deutsch explained, has come from large-scale studies with data collected outside the tropics.\nTo fill the gap, Deutsch teamed up with Noelle Lucey, a marine biologist at the Smithsonian Tropical Research Institute in Panama. Lucey had been collecting high-frequency environmental data from tropical coral reefs and conducting the types of biological experiments needed to understand how animals are impacted by the variations in their environment.\nThe results of this collaborative study appear in this week’s issue of PLOS Climate, and they promise to untangle some of these relationships. In essence, the researchers wanted to look at how oxygen and its relationship with temperature is changing in these tropical systems, and, moreover, if these environmental changes are causing marine animals to leave their home reefs.\nThe research builds on more than a decade of environmental monitoring data collected by the Smithsonian Tropical Research Institute’s Physical Monitoring program and covers a portion of the Caribbean coast in Panama, called Almirante Bay. The data show a persistent difference in environment between the inner part of the bay, which is warmer and lower in oxygen, and hosts a different mix of species, relative to the outer part of the bay. Over a decade of weekly changes of water conditions can now be combined with hourly changes collected by sensors deployed by Lucey and her students.\nTo investigate the relationship between these environmental conditions and the distributions of reef species, Lucey conducted controlled experiments in the lab at Bocas del Toro, Panama to measure the sensitivity of species to these water properties. “These controlled experiments consisted of relatively simple and well-known assays geared toward understanding what the animal’s physiological limits are to different conditions,” Lucey said. “For instance, we’d place the animal in a beaker of sea water and increase the temperature to the point where the animal was unable to move, which we determined was the animal’s thermal limit. We also did this for oxygen.” These experiments, used for decades by marine biologists, had rarely been done on tropical species.\nThe researchers settled on brittle stars as an ideal case study for answering their broader questions. “Brittle stars are Echinoderms that are related to sea stars and sea urchins, and they look like long, skinny starfishes,” said Lucey. “They are common invertebrates on coral reefs, and they are both beautiful and active, making them great models to understand physiological limits to oxygen and temperature stress.”\nLucey first documented which reefs these species were living on and then with Piero Calosi and students from the Université of Québec in Rimouski, Canada, specializing in animal physiology, started to develop new tolerance and behavior assays that could be used for the brittle stars. These studies were designed to evaluate the brittle stars’ ability, for example, to sense elevated sea temperature or low oxygen and, if so, to move away from these offending conditions.\n“This involved exercising these animals to measure how much energy they use in stressful environments, very much the same way a doctor can measure how fast you are breathing before and after running on a treadmill to figure out how much oxygen you need when you are active,” Lucey said.\nBy combining these data, the researchers were able to observe a number of trends that helped them answer a variety of questions. Chiefly, they found that it is extreme conditions, rather than the average conditions, that largely determine which parts of the reef they choose to inhabit.\nThey found that really stressful conditions occur not on the average but during extreme episodes. Lucey noted, for example, that for a few days, especially at night and when the brittle stars are particularly active, certain parts of the reef experience extremely low oxygen levels. If the brittle stars inhabited these areas at that time, they would not be able to be active.\n“If the brittle stars just experienced these places in their average state, they would be fine,” explained Deutsch. “But they don’t, of course, experience the average; they experience every instantaneous condition that nature throws at them—and the extremely stressful conditions are frequent enough and just long enough to prevent them from living there in the long term.”\nFor a long time, scientists have argued that it is not the averages in nature that frequently matter most. It is, more often than not, the limiting factors at the extremes that have the greatest effect on things like reproduction, demography and habitat preference.\n“Our research is one of the nicest, most holistic examples of where we can put all the pieces together—the physiological tolerance of the organisms, the high-frequency measurements, and the spatial patterns of inhabited versus uninhabited—and say ‘yes, indeed, these extreme episodes are what’s really important,’” said Lucey. “The extreme conditions are really what’s driving the pattern of where the organisms live and where they don’t live.”\nThe second important finding—and the one that has particular relevance for climate studies—came when the researchers began to examine the long-term data. They found that as the waters of Almirante Bay became warmer over the course of the past decade the frequency and intensity of the low oxygen conditions, a condition known as ‘hypoxia,’ has increased.\n“This means that as climate gets warmer, the periods in which there is not enough oxygen for some species to breathe can become more pronounced, even if the amount of oxygen on average is not changing,” said Deutsch.\nThis has potentially profound implications not only for brittle stars but also for many marine species. “We also show how future warming will increase oxygen extremes and dramatically reduce habitability for even the most hypoxia-tolerant species on the reef,” Lucey said. “Low oxygen habitat exclusion might be much more widespread in the future, even if average oxygen doesn’t go down at all.”\nThe researchers don’t yet know if this increasing frequency of hypoxia is a widespread phenomenon, but if it is, they contend it could mean that scientists have underestimated the climate risks especially for species in the tropical oceans, where the average oxygen level is expected to be relatively stable.\nConclusions like this underscore the urgent need to understand the complex dynamics of climate change, temperature, oxygen-level fluctuation, and species biogeography.\n“The oceans are getting warmer and we need to understand how that impacts its ecosystems,” said Deutsch. “The oceans have been under sampled for a long time, and major international initiatives are needed to monitor ocean health in the face of mounting challenges from climate and other human impacts.”\nOther members of the research team included Marie-Hélène Carignan, Fanny Vermandele, and Piero Calosi of the Marine Ecological and Evolutionary Physiology laboratory, Département de Biologie, Chimie et Géographie, Université du Québec à Rimouski, Rimouski, Canada; Mary Collins and Rachel Collin of Smithsonian Tropical Research Institute, Balboa Ancon, Panama; and Maggie D. Johnson of the Smithsonian Tropical Research Institute and the Red Sea Research Center, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia. The research was supported by the Smithsonian Tropical Research Institute, the Smithsonian Women’s Committee, Natural Sciences and Engineering Research Council of Canada Discovery Program, Canada Foundation for Innovation, and the University of Quebec in Rimouski.\nThe study, “Climate warming erodes tropical reef habitat through frequency and intensity of episodic hypoxia,” by Noelle M. Lucey, Curtis A. Deutsch, Marie-Hélène Carignan, Fanny Vermandele, Mary Collins, Maggie D. Johnson, Rachel Collin, and Piero Calosi was published online in the journal PLOS Climate on March. 1, 2023.\nArticle link: https://journals.plos.org/climate/article?id=10.1371/journal.pclm.0000095"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:d916f0e1-c1c9-4691-90c7-86b7d9760152>","<urn:uuid:9c381904-4d2b-419a-b0f8-f8d9fb428cdd>"],"error":null}
{"question":"Compare the customization capabilities of therapeutic polymer coatings versus phosphate glass fiber coatings for medical implants.","answer":"Both coating types offer significant customization capabilities. Therapeutic polymer coatings can be customized through polymer selection (durable vs degradable), glass transition temperature adjustment, and drug-polymer compatibility to control drug release rates and duration. They can also be modified to cover specific portions of devices like luminal or abluminal surfaces. Phosphate glass fiber coatings can be customized through fiber composition to control degradation rates, fiber orientation to direct cell growth, and can incorporate additional components like drugs or growth factors in their polymer component. The glass fiber formulations can be tweaked to achieve specific resorption timeframes, ranging from 9-12 months or longer.","context":["[Image above] Credit: biologycorner; Flickr CC BY-NC 2.0\nWhen it comes to materials for biomedical implants, ceramics have something its materials brethren, metals and polymers, completely lack—bioactivity.\nSure, metals and polymers may have the strength and flexibility that ceramics can sometimes lack, but when it comes to the ability to bond to living tissues, it’s no contest.\nBioactivity is an important parameter for helping the body to rebuild—cells and tissues needs to be able to bond with an implant to allow repaired joints and bones to function like they once did. A bioinert material like metal doesn’t promote that bonding, preventing the body from being able to integrate with its new hardware.\nLuckily for metal, however, the material’s not totally out of the implant game—a glass coating and a ceramic processing technique apparently can work wonders for an otherwise bioinert material.\n“We developed a novel and facile approach to modify metallic surfaces with random and aligned bioactive phosphate glass fibers by electrophoretic deposition, a well known processing technique in ceramic processing,” Aldo Boccaccini, ACerS Fellow and professor of biomaterials and head of the Institute of Biomaterials at the Department of Materials Science and Engineering at the University of Erlangen-Nuremberg, Germany, explains via email.\nBoccaccini is senior author of a recent ACS Applied Materials & Interfaces paper describing how the glass fiber coating may be able to improve the biocompatibility of metal implants, performed by a team of researchers from the Institute of Biomaterials at University of Erlangen-Nuremberg, Germany; the Department of Mechanical, Materials and Manufacturing Engineering at the University of Nottingham, U.K.; and the Key Laboratory for Space Bioscience and Biotechnology, School of Life Sciences and Northwestern Polytechnical University in Xi’an, China.\nElectrophoretic deposition—which uses an electric field to control deposition of charged particles—is a low-cost processing method, performed at room temperature, that can easily be scaled-up. So “there are very good possibilities for the commercial exploitation of electrophoretic deposition-based coating techniques in the biomedical field” Boccaccini says.\nAnd by tailoring the technique to form coatings of phosphate glass fibers embedded in a polymer that can be deposited on even irregular metal surfaces, the researchers are finding that those possibilities might include the ability to significantly enhance the biocompatibility of metal implants and promote healing through improved tissue-to-implant interfaces.\nUsing electrophoretic deposition, the scientists demonstrated that they can lay down layers of 1–2-mm-long phosphate glass fibers in a poly(acrylic acid) polymer solution. And by varying parameters of the process, the technique can control both coating thickness and fiber orientation, depositing the glass fibers in neat parallel rows.\nControlling fiber orientation is critical when it comes to implants. And it’s not just for a neat appearance—fiber orientation actually has a significant effect on how cells behave, impacting the shape of cells and how they differentiate, or change, into other specialized cell types—a factor that is critical to the body’s natural healing processes.\nPlus, phosphate glass fibers already have a good track record for biocompatibility. In addition to providing both structural and biological benefits, the fibers are completely resorbable in the body as well—offering new possibilities for engineering more biologically friendly biomedical implants that can evolve along with the body’s natural healing processes.\nThe idea is that these EPD-deposited phosphate glass fiber coatings would provide an interface on the otherwise bioinert metal implant for the body’s own tissues to bond to—prompting critical initial growth and healing after the implant has been installed.\nBut it’s important to note that these interfacial coatings aren’t a permanent metal-masking solution, as the phosphate glass fibers are eventually resorbed by the body.\n“The proposed biodegradable coating is thus mainly used to strengthen the link between bone tissue and the implant at the early stage of implantation,” Boccaccini explains.\nThen, once the glass fibers resorb away, the bone’s healing processes with the new implant are well underway.\n“It is anticipated that by this stage the implant would have made a good union with the surrounding hard tissue,” Boccaccini continues. “Having a polymer component present could also enable incorporation of other useful components—such as drugs, for example, to address infections, and even growth factors to stimulate bone growth.”\nThe team’s experiments so far, using human cells cultured in the lab, show that oriented glass fibers coatings can enhance and direct proliferation of bone-related precursor cells and enhance osteoblast activity, indicating that bone will be able to bond onto the surface.\nThese cells, called pre-osteoblast cells, eventually become osteoblasts. And because osteoblasts are the cells that help lay down bone tissue, these are precisely the ones you want to be active when it comes to healing bone.\nThe scientists are next measuring in vivo responses to the glass fiber coatings, examining the effect of different fiber compositions, which have different degradation rates, according to Boccaccini.\nAn added bonus is that the glass fiber coatings are completely customizable, affording possibilities of implants that are tailor-coated for patients’ individual healing needs.\n“We can tailor the degradation rates for phosphate glass fibers, which is what makes them really unique and versatile materials,” Boccaccini writes. “The formulations that are proving to be highly cytocompatible would take approximately 9–12 months to resorb, or even longer if required (by simply tweaking the formulations).”\n“The possibility of tuning the fiber composition as well as the versatility of the coating technique open interesting opportunities for the biofunctionalization of metallic implants, for example bone implants, in the future.”\nThe paper, published in ACS Applied Materials & Interfaces, is “Electric field-assisted orientation of short phosphate glass fibers on stainless steel for biomedical applications” (DOI: 10.1021/acsami.8b01378).\nWant to read more articles like this? Subscribe to the Ceramic Tech Today newsletter to continue to receive the latest news in the ceramic and glass industry right in your inbox! Visit this link to get started.","Therapeutic Coatings for Medical Device Implants\nDrug delivery coatings are not a new technology to the medical device industry. However, as more implantable devices are tasked with achieving a greater level of healthcare, they do offer great benefit to design engineers. This article reviews drug coating technology and looks at application areas where it has made a significant impact.\nThe development and adoption of therapeutic coatings for medical devices has created new product categories, enhanced the functionality of devices, and improved patient outcomes. The therapeutic effect, duration of therapy, and deliverability of these combination devices are influenced by the choice of therapeutic agent, coating materials, and the means by which coatings are applied to the medical device (Figure 1).\nTherapeutic coatings have been incorporated onto implantable medical devices for a variety of acute and chronic applications. Perhaps the best known is the drug-eluting stent. The polymeric coating on a drug-eluting stent provides controlled, site-specific delivery of anti-restenotic drugs that dramatically lower the restenosis rates found with bare metal stents. More recently, percutaneous angioplasty balloons coated with anti-restenotic drugs and excipients (drug coated balloons or DCBs) (Figure 2) have been developed to quickly transfer a therapeutic drug from the device to the arterial site, then provide longer-term control of restenosis. Other devices take advantage of surface modification to impart their therapeutic effect. For example, embolic protection devices are enhanced by the incorporation of a hemocompatible coating that, while not eluting a therapeutic agent from the device, reduces clot formation on the surface of the device.\nThe coating materials and formulations available make possible the delivery of a wide range of therapeutic drugs and the ability to elicit a range of tissue responses. For instance, a device that is highly prone to infection may benefit from a coating that delivers antimicrobial drugs to stop foreign cell growth, while a device that requires rapid incorporation into the surrounding tissue may call for a coating that provides a surface onto which cells may easily attach and grow. The desired therapeutic outcome impacts the time over which the therapeutic coating acts. For example, an anti-inflammatory compound may need to be delivered for weeks or months in order to prevent restenosis, while an antimicrobial may only need to stay resident at the site for several days in order to effectively prevent infection.\nSelection of the appropriate coating material enables a device to deliver the proper therapeutic effect. For passive therapeutic coatings, such as polymeric hydrogels, the ability to covalently bond a molecule to the surface of a device provides for a more durable hemocompatible effect. The active peptides and proteins of some therapeutic coatings elicit a stronger response from the body only when they are presented in a certain orientation, such as end-point attachment. An example of this approach is the bio-engineered OrbusNeich Genous coronary stent, which is coated with immobilized anti-CD34 monoclonal antibody that captures beneficial cells circulating in the blood stream to enhance endothelialization, protect against blood clots, and minimize restenosis.\nMaterials for the active delivery of a therapeutic agent include a wide range of polymers and small-molecule matrices. For many drug-polymer systems, the release of the drug is dominated by the diffusion of the drug through the polymer matrix. In these cases, the glass transition temperature (Tg) of the polymer has a large effect on the drug release, with lower Tg polymers releasing the drug faster than high Tg materials. The compatibility of the drug-polymer system should be evaluated as this will give information on how quickly the drug will come out of the system. As a general rule, the more similar the hydrophilicity or hydrophobicity between the drug and polymer, the longer the drug will stay in the polymer matrix (Figure 3).\nMedical device designers have a choice of durable or degradable polymer materials for drug delivery coatings. Durable polymers were used in the early generation drug-eluting stents (e.g., Cordis Cypher, Boston Scientific Taxus, Boston Scientific Promus, Abbott Xience, and Medtronic Resolute stents), which have been successfully implanted in millions of patients (Figure 4). Durable polymers can allow for longer delivery times and the underlying matrix does not change over time. In contrast, recent commercial drug-eluting stents (e.g., OrbusNeich Combo stent) use degradable polymer coatings (Figure 5). These materials enable control over the release of the drug by both diffusion (similar to durable systems) and degradation of the polymer, which can change the polymer Tg, coating thickness, and encapsulation of the drug over time. Some devices utilize both durable and biodegradable coatings, such as a stent coated on the inner and outer lumen for two different drug delivery modes of action.\nEqually important to the ability of the polymer to control the drug release is the ability of the coating system to ensure delivery of the therapy to the treatment site. Coating materials should be evaluated for adhesion to the underlying substrate and resistance to flaking or cracking as the device is deployed to the desired location. In some instances, such as with drug-coated balloons, coating materials need to balance the retention of the coating during transit with the need to deliver the drug very quickly once the device reaches the treatment site.\nThe means by which a therapeutic coating is applied to an implant can significantly impact the performance of the coating. A variety of coating techniques enable conformal coatings as well as coatings that cover only a portion of the device, such as a luminal or abluminal coating, which enables targeted delivery to specific tissues in contact with the device. Coating techniques offer control over the thickness of the coating, which can be adjusted to provide a sufficient reservoir for the therapeutic agent while not causing a large dimensional change to the device that may hinder delivery, placement, or other attributes. For solvent-based coating processes, the rate at which the solvent is dried from the coating can affect the structure of the coating and the drug release rate. Slower dry times, for example, can allow for greater segregation of the drug from the polymer, resulting in an enrichment at the surface of either the drug or polymer compared to the bulk coating composition. Solvent evaporation and other aspects of the coating process can also impact the form of the drug (crystalline, amorphous, etc.), which can, in turn, influence how quickly the drug elutes from the coating and how rapidly the drug is absorbed into the surrounding tissue.\nThe use of therapeutic coatings on medical devices has led to enhanced device performance and improved patient outcomes. The selection of coating materials, therapeutic agent, and processing methods all play a critical role in the deliverability, therapeutic effect, and duration of therapy. A skilled development team can help navigate the interconnectedness of the coating choices to ensure the desired therapeutic properties are achieved.\nFor more information, visit www.surmodics.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:2c47ba42-4d78-41a4-af77-eb6b926fc458>","<urn:uuid:ff8c1553-f100-4a81-b10d-cab8fb4b044f>"],"error":null}
{"question":"How do the principles of discoverability in everyday objects compare to those in data visualization maps, such as the NYC subway system visualization?","answer":"Both everyday objects and data visualizations share key principles of discoverability. For everyday objects, discoverability means making it clear what actions are possible and how to perform them - like making a faucet look pushable if it should be pushed. Similarly, in data visualizations like the NYC subway map, discoverability means creating clear visual presentations that reveal complex information at a glance through elegant layers of detail. Both domains emphasize natural mappings and clear signifiers to make their use obvious - whether it's the relationship between stove controls and burners, or the visual representation of subway lines and stations that efficiently generate insight and understanding.","context":["Note: Data Dialogues is an occasional series that looks at what works well—and what doesn't work so well—in the world of data visualization. The series aims to foster a positive and constructive discussion.\nI’ve been educated and inspired recently by the best-selling design classic The Design of Everyday Things by UX guru Don Norman.\nYou really have to read the entire book, which applies to all types of objects that people design, from chairs to doors, to software, to organizational structures. It provides thoughtful and practical principles that guide designers to design all of those things well. And by “well” he means “products that fit the needs and capabilities of people” (p.218).\nAs I read it, it occurred to me that data visualizations—even richly interactive ones viewed on tablets and phones—are “everyday things” now, too. That has only become the case in the past half-decade or so.\nYes, examples can be traced back to the early days of the internet, but the recent explosion of data, software tools, and programming libraries has caused their proliferation.\nAnd I found that point after point, principle after principle in Don’s book applied directly to data visualization. I’d like to call out five points that struck me as particularly relevant to recent discussions in the field of data visualization.\n1. Good Visualizations Are Discoverable and Understandable\nDon starts his book by describing two important characteristics of all designed products:\n- Discoverability: Is it possible to even figure out what actions are possible and where and how to perform them?\n- Understanding: What does it all mean? How is the product supposed to be used? What do all the different controls and settings mean?\nHe talks about common things that are often anything but discoverable and understandable, such as faucets, doors, and stovetops. One of my favorite quotes in the book is about faucets:\n\"If you want the faucet to be pushed, make it look as if it should be pushed.\"\nRegarding doors, Vox published a great video on a particularly poorly-designed door on the 10th floor of the Vox Media office. The video references and even includes interview footage with Don Norman himself. And it’s funny. You should watch it.\nIt occurred to me that the typical stovetop design snafu has a direct translation into the world of data visualization. To explain, let’s start with the problem with stovetops. Ever turn on the wrong burner? Why? Because you’re stupid? No, because there are often poor mappings between the controls and the burners. The burners are often arranged in a two-by-two grid and the controls are often in a straight line, like this:\nWhat does that have to do with data visualization? We often use similar controls—radio buttons, combo boxes, sliders, etc.—to filter and highlight the marks in the view. When there are multiple views in a visualization (a dashboard), there is a similar opportunity to provide clear, or natural, mappings.\nDon gives the following advice for mappings:\n- Best mapping: Controls are mounted directly on the item to be controlled.\n- Second-best mapping: Controls are as close as possible to the object to be controlled.\n- Third-best mapping: Controls are arranged in the same spatial configuration as the objects to be controlled.\nOften the software default places the controls on the right-hand side. Here’s my attempt to show these options on a generic data dashboard, where the four different views are labeled A, B, C, and D, and the controls that change them are labeled according to the views they modify:\nThis is a relatively straightforward example, and the job of the designer of a more complex visualization is to make it similarly clear what can be done and how to do it. Designers use things like affordances, signifiers, constraints, and mappings to make it obvious. Note that it takes a lot of effort to make the complex obvious.\n2. Don’t Blame People for Getting Confused or Making Errors\nA fundamental principle that Don drives home a number of times in the book is that human error usually isn’t the fault of humans, but rather of poorly-designed systems. Here are two great quotes on the topic:\n\"It is not possible to eliminate human error if it is thought of as a personal failure rather than as a sign of poor design of procedures or equipment.\"\nAnd again on the same page:\n\"If the system lets you make the error, it is badly designed. And if the system induces you to make the error, it is really badly designed. When I turn on the wrong stove burner, it is not due to my lack of knowledge: it is due to poor mapping between controls and burners.\"\nDon differentiates between two types of errors: slips and mistakes.\n- Slips are when you mean to do one thing, but you do another.\n- Mistakes are when you come up with the wrong goal or plan and then carry it out.\nBoth types of errors happen when people interact with data visualizations. In the world of mobile, slips are so common. Maybe I meant to tap that small icon at the edge of my phone screen, but the phone and app recognized a tap of an adjacent icon instead.\nMistakes are also common. Maybe it made sense to me to filter to a subset of the data to get my answer, but in reality I was misleading myself by introducing a selection bias that wasn’t appropriate at all. If someone makes the wrong decision based on misinformation they took from your visualization, that’s your problem at least as much as it is theirs, if not more so.\nHow to make sure your readers avoid slips and mistakes? Build and test. Iterate. Watch people interact with your visualization. When they screw up, don’t blame them or step in and explain what they did wrong and why they should’ve known better. Write it down and go back to the drawing board.\nIf the person who agreed to test your visualization made that error, don’t you think many more likely will? And you won’t be there to tell them all what they did wrong. Your only chance to fix the error is to prevent it.\n3. Designing for Pleasure and Emotion Is Important\nI’m a big believer in this principle. Don states that “great designers make pleasurable experiences”:\n\"Experience is critical for it determines how fondly people remember their interactions. Was the overall experience positive, or was it frustrating and confusing? \"\nHow can an experience with a data visualization be pleasurable? In lots of ways. It can make it easy to understand something interesting or important about our world. It can employ good design techniques and artistic elements. It can surprise us with a clever or funny metaphor, or some combination of these and more.\nWhat about emotion—the “e word” to which the analytical folks in our midst are allergic? Cognition gets a lot of play in the world of data visualization, but emotion does not. But these two horses of the chariot that is the human spirit are actually inextricably yoked:\n\"Cognition and emotion cannot be separated. Cognitive thoughts lead to emotions: emotions drive cognitive thoughts.\"\nI also love the following quote:\n\"Cognition attempts to make sense of the world: emotion assigns value … cognition provides understanding: emotion provides value judgments.\"\nSo let’s embrace emotions. Some data visualizations piss us off. Some crack us up. Some are just delightful to interact with. These elements of the experience should be part of the discourse in our field, and not ignored just because they don’t match the left-brained predisposition of the bulk of the so-called experts. If we take them into consideration, we’ll probably design better stuff.\n4. Complexity Is Good. Confusion Is Bad\nThere’s a trend in our field to move away from the big, complex dashboards of 2010 and toward “lightweight” and uber-simple individual graphs, even GIFs.\nWhy? A big part of the reason is that they work better on mobile. It’s true, and what we’ve learned in the past few years is that the complexity of those big dashboards isn’t always necessary.\nThis is a great development, and I’m all for it, but let’s just remember that there was often a great value to the rich interaction that is still possible on a larger screen.\nInstead of abandoning rich interactivity altogether, I believe we should be looking for new and innovative ways to give these advanced capabilities to readers on smaller devices. When those capabilities will help us achieve some goal, we’ll be good to go. We’re not there yet.\nAfter all, it’s not the complexity of the detailed, filterable dashboard that’s the problem on the phone. It’s that we haven’t figured out how to give these capabilities to a reader using a phone yet, and the experience is confusing. Does this make sense to you?\nI actually see this as a good thing. Our generation has the chance to figure this out for the generations to come. The growth of the numerical literacy of our population will be well worth the effort.\n5. Absolute Precision Isn’t Always Necessary\nI have to be honest. This one is my hot button.\nThere’s a school of thought that says that the visualization type that gives the reader the ability to guess the true proportions of the thing visualized with the greatest accuracy is the only one that can be used. Some go so far as to declare it immoral to choose a visualization type that introduces any greater error than another (they all have some error).\nI found this great visualization about visualizations in Tamara Munzner’s book, Visualization Analysis and Design:\nThe problem with this line of reasoning is that absolute precision isn’t always necessary for the task at hand.\nDon uses the example of converting temperature from Celcius to Farenheit. If all you need to do is figure out if you need to wear a sweater when you go outside, a shortcut approximate-conversion equation is GOOD ENOUGH. It doesn’t matter whether it’s 52, 55, 55.8, or 55.806. In all four cases, you’re wearing a light sweater.\nAnd let me repeat the point: There are errors associated with every visualization type. We aren’t machines and perfect decoders of pixels or ink. Sometimes it’s okay that a general understanding is achieved.\nAnd for goodness sake, if absolute precision is required, then use labels, or just show a table of exact values.\nData Viz: The Catch-All Discipline\nI hope this was helpful for you! I love doing this kind of thing—pulling lessons from other amazing writings and seeing how they apply to data visualization, which I see as the “catch-all” discipline.\nIt’s part numeric, part editorial, part graphic. To do it well, we need to embrace the principles of good design. I’ve tried to outline a few here from a true expert who we should all be familiar with. If you do read Don Norman’s book, you’ll find that there are many more.\nWhat do you think? Do you agree with my points—that absolute precision isn't always necessary, that designing for emotions is an important part of visualizing data? Share your thoughts in the comments below.","Visualization is the graphic presentation of data -- portrayals meant to reveal complex information at a glance. Think of the familiar map of the New York City subway system, or a diagram of the human brain. Successful visualizations are beautiful not only for their aesthetic design, but also for elegant layers of detail that efficiently generate insight and new understanding.\nThis book examines the methods of two dozen visualization experts who approach their projects from a variety of perspectives -- as artists, designers, commentators, scientists, analysts, statisticians, and more. Together they demonstrate how visualization can help us make sense of the world.\n- Explore the importance of storytelling with a simple visualization exercise\n- Learn how color conveys information that our brains recognize before we're fully aware of it\n- Discover how the books we buy and the people we associate with reveal clues to our deeper selves\n- Recognize a method to the madness of air travel with a visualization of civilian air traffic\n- Find out how researchers investigate unknown phenomena, from initial sketches to published papers\nNick Bilton,Michael E. Driscoll,Jonathan Feinberg,Danyel Fisher,Jessica Hagy,Gregor Hochmuth,Todd Holloway,Noah Iliinsky,Eddie Jabbour,Valdean Klump,Aaron Koblin,Robert Kosara,Valdis Krebs,JoAnn Kuchera-Morin et al.,Andrew Odewahn,Adam Perer,Anders Persson,Maximilian Schich,Matthias Shapiro,Julie Steele,Moritz Stefaner,Jer Thorp,Fernanda Viegas,Martin Wattenberg,and Michael Young.\nTable of contents\n- Beautiful Visualization\n1. On Beauty\n- What Is Beauty?\n- Learning from the Classics\n- How Do We Achieve Beauty?\n- Putting It Into Practice\n2. Once Upon a Stacked Time Series\n- Question + Visual Data + Context = Story\n- Steps for Creating an Effective Visualization\n- Hands-on Visualization Creation\n- Wordle's Origins\n- How Wordle Works\n- Is Wordle Good Information Visualization?\n- How Wordle Is Actually Used\n4. Color: The Cinderella of Data Visualization\n- Why Use Color in Data Graphics?\n- Luminosity As a Means of Recovering Local Density\n- Looking Forward: What About Animation?\n- References and Further Reading\n5. Mapping Information: Redesigning the New York City Subway Map\n- The Need for a Better Tool\n- London Calling\n- New York Blues\n- Better Tools Allow for Better Tools\n- Size Is Only One Factor\n- Looking Back to Look Forward\n- New York's Unique Complexity\n- Geography Is About Relationships\n- Sweat the Small Stuff\n- 6. Flight Patterns: A Deep Dive\n- 7. Your Choices Reveal Who You Are: Mining and Visualizing Social Patterns\n- 8. Visualizing the U.S. Senate Social Graph (1991–2009)\n- 9. The Big Picture: Search and Discovery\n- 10. Finding Beautiful Insights in the Chaos of Social Network Visualizations\n- 11. Beautiful History: Visualizing Wikipedia\n- 12. Turning a Table into a Tree: Growing Parallel Sets into a Purposeful Project\n- 13. The Design of \"X by Y\"\n- 14. Revealing Matrices\n- 15. This Was 1994: Data Exploration with the NYTimes Article Search API\n16. A Day in the Life of the New York Times\n- Collecting Some Data\n- Let's Clean 'Em First\n- Python, Map/Reduce, and Hadoop\n- The First Pass at the Visualization\n- Scene 1, Take 1\n- Scene 1, Take 2\n- The Second Pass at the Visualization\n- Visual Scale and Other Visualization Optimizations\n- Getting the Time Lapse Working\n- So, What Do We Do with This Thing?\n- 17. Immersed in Unfolding Complex Systems\n18. Postmortem Visualization: The Real Gold Standard\n- Impact on Forensic Work\n- The Virtual Autopsy Procedure\n- The Future for Virtual Autopsies\n- References and Suggested Reading\n19. Animation for Visualization: Opportunities and Drawbacks\n- Principles of Animation\n- Animation in Scientific Visualization\n- Learning from Cartooning\n- Presentation Is Not Exploration\n- Types of Animation\n- Staging Animations with DynaVis\n- Principles of Animation\n- Conclusion: Animate or Not?\n- Further Reading\n20. Visualization: Indexed.\n- Visualization: It's an Elephant.\n- Visualization: It's Art.\n- Visualization: It's Business.\n- Visualization: It's Timeless.\n- Visualization: It's Right Now.\n- Visualization: It's Coded.\n- Visualization: It's Clear.\n- Visualization: It's Learnable.\n- Visualization: It's a Buzzword.\n- Visualization: It's an Opportunity.\n- A. Contributors\n- B. Colophon\n- About the Authors\n- Title: Beautiful Visualization\n- Release date: June 2010\n- Publisher(s): O'Reilly Media, Inc.\n- ISBN: 9781449379865\nYou might also like\nThe Big Book of Dashboards\nThe definitive reference book with real-world solutions you won't find anywhere else The Big Book of …\nStorytelling with Data\nInfluence action through data! This is not a book. It is a one-of-a-kind immersive learning experience …\nVisualize This: The FlowingData Guide to Design, Visualization, and Statistics\nPractical data design tips from a data visualization expert of the modern age Data doesn't decrease; …\nStorytelling with Data: A Data Visualization Guide for Business Professionals\nDon't simply show your data—tell a story with it! Storytelling with Data teaches you the fundamentals …"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:6b665c77-75e9-4800-b7aa-ccb6255d58b2>","<urn:uuid:28f63afd-9b72-4ccb-bfb9-b2150554ef0d>"],"error":null}
{"question":"What are the most frequent foot problems that occur from doing Zumba?","answer":"The most common foot problems from Zumba include plantar fasciitis (heel and arch pain), neuromas (pain and numbness between toes), Achilles tendonitis, stress fractures in the heel or metatarsal bones, and numbness/tingling in feet from prolonged pressure on the balls of the feet.","context":["Preventing Zumba Foot Pain and Injuries with the Correct Shoes\nWe treat a lot of patients who participate in Zumba, Brazilian butt lift, and other dance based exercise routines. One problem I have seen a lot of lately is patients complaining of foot or knee pain that is associated with the shoes that they are using during Zumba.\nBest Shoes for Zumba\nBecause these activities involve pivoting on the ball of the foot, standard athletic shoes are often not appropriate for Zumba-type workouts. Standard athletic shoes soles can catch when you try to pivot and that can increase stress on the knee and the rest of the lower extremity. Because of this, many Zumba participants use dance shoes or shoe marketed exclusively for Zumba. Unfortunately, these shoes tend to be extremely unstable and for many people this leads to excessive collapse of the foot during the activity and excessive rotation at the knee. To prevent these types of problems, these people should wear more stable shoes. Unfortunately, it is almost impossible to find shoes that are stable but also provide the non-stick forefoot sole that is necessary for Zumba.\nThe perfect shoe for these patients would be a stable cross-trainer type athletic shoe with a low-friction sole on the forefoot (like a dance shoe). We have not found this type of shoe on the market but we have come up with several solutions to create the perfect shoe for Zumba.\nTo get the perfect shoe for Zumba, first get one of the cross trainer shoe we recommend on our recommended shoe list. Then use of the devices listed below. The products below are the ones we recommend to our patients and they are also affiliate links so we may receive a small commission at no additional cost to you if your order from the link.\nYou can make a cross trainer or other athletic shoe a viable dance option and much safer for your knees by adding a “Slip-on Dancer“. These slip over the rubber sole of your athletic shoes to make the sole on the front of the shoe slick such as you would find on a dance shoe. This great device allows you to use a stable athletic shoe for dance activities like Zumba. You can find the Slip-on Dancer here.\nAnother brand we like is called DanceSocks. These are a little less expensive as they do not have the strap that goes\naround the heel. Because of this they seem a little more likely to slip off.\nRegardless of which one you buy, we highly recommend one of these devices for anyone who is wearing standard athletic shoes for dance activities such as Zumba. By decreasing forefoot friction they help protect your knees.\nShoe Modifications for Zumba\nIf the Slip-on Dancer doesn’t work for you for any reason, an athletic shoe can be modified to make it work for Zumba and other dance activiies. I have found it very effective for to have patients get a stable cross-training shoe, and then I write a prescription to have it modified with a dance shoe sole under the forefoot. You can find suggestions on the best Zumba cross trainers on our recommended shoe list.\nBefore having the shoe permanently modified, we will often have patients use a Slip-on Dancer or apply a temporary low friction material to the sole of the shoe under the ball of the foot and have my patient try that in Zumba for one or two sessions. In most cases, this eliminates the pain these patients were having. If this works well for the patient, then we will write the prescription directing a shoe repair shop on how to make the permanent modification to the shoes.\nIf you are in the Seattle area and are having foot pain related to Zumba or other dance type activities, make an appointment to see us. Please be sure to bring two your visit any shoes or arch supports that you are currently using on a regular basis, particularly those you were using for exercise activities including Zumba.\nArch Supports for Zumba\nWe feel every good shoe needs an over the counter or custom orthotic to help prevent problems that my slow or stop your work out. Research has shown that orthotics must be made from non-weightbearing casts of your feet and tightly conform to your arch to provide the best function of your joints. This is called a “total-contact orthotic“.\nHowever, if you are not able to obtain a custom orthotic we suggest the FootChair Podiatrist Designed Adjustable Arch Orthotic.\nThis has the best arch height we have found on an over-the-counter arch support to transfer pressure off your joints as it is the only OTC orthotic to have an adjustable arch.\nThe arch can be adjusted for maximum support and comfort via pads that can be inserted under the cover.\nWhat are the Most Common Zumba Foot Problems?\nHere are some of the most common Zumba foot problems from your Seattle Foot Doctor:\n- Plantar fasciitis – Heel and arch pain caused by inflammation of the plantar fascia, a thick band of connective tissue on the bottom of your foot.\n- Neuromas – Pain, burning, tingling, or numbness in the front of your foot, usually between the 3rd and 4th toes, caused by thickening and enlargement of a nerve in the foot as a result of compression and/or irritation.\n- Achilles tendonitis – Pain caused by injury and inflammation of the Achilles tendon, which is where the calf muscle attaches to the heel bone. Zumba-induced tendinitis may also affect the posterior tibial tendon.\n- Stress fractures – sometimes referred to as a “hairline fracture,” caused by a cracked or incompletely broken bone in the foot. The most common stress fractures in our Zumba-loving patients affect the heel bone or the metatarsal bones. Stress fractures are overuse injuries, caused by repetitive stress to the lower extremities and are a common cause of foot pain in people who begin a new exercise program – and who do too much too soon.\n- Numb and tingling feet – Caused by too much pressure on the toes and the balls of your feet. This is very common among Zumba-lovers, as the movements required in the class require you to remain on the balls of your feet almost indefinitely.\nIf you’ve already checked with your physician before beginning a new exercise program, congratulations for being conscientious. Now it’s time to make an appointment with us, so we can help you prevent the most common Zumba footproblems."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:082412d6-13df-4006-bd23-fce2807b51b9>"],"error":null}
{"question":"Why do people readily accept risks of familiar activities but show extreme caution toward new technologies, and how does this relate to attitudes about space colonization?","answer":"People evaluate risks differently based on factors beyond just statistical probability. While they readily understand and accept risks of familiar voluntary activities, they are more concerned about risks that are imposed without consent or are difficult to personally control. This explains why there's greater public acceptance of known risks like alcohol consumption compared to newer technologies. This same dynamic appears in space exploration, where entrepreneurs like Elon Musk and Jeff Bezos must emphasize human survival benefits to gain public acceptance, rather than focusing on technical scientific aspects. The public's risk perception is heavily influenced by trust in managing institutions and whether they feel they have control over the risks, which shapes attitudes toward both everyday hazards and ambitious projects like space colonization.","context":["Is the purpose of deep space exploration pure science or proving humanity’s worth?\nby Shalina Chatlani\n|It’s not just private sector moguls that are sensationalizing doomsday scenarios to justify humanity’s move into the cosmos.|\nThese are the same images, pounded into our heads by pop culture, media, and science fiction novels, which have informed our relationship to everything unearthly. And, they get played out over and over again with each new discussion on the future of human spaceflight. It’s a hopeful narrative that has been adopted by almost every leading figure in the space industry, but with an explicit worldly prerogative that’s often pushed before concrete scientific discovery: “We need to get to space because…”\nConsider Elon Musk, the SpaceX CEO and poster boy for commercial spaceflight, who regularly espouses such ultimatum rhetoric, arguing that we must colonize Mars to save the human race from the impending doom of ecological destruction or certain extinction on Earth. For him, it’s a matter of life or death.\nOr, think toward Jeff Bezos, the head of Amazon and creator of Blue Origin, who said at a conference that he didn’t want a “Plan B” for Earth; rather, he wanted to use space to save Earth, by turning the extraterrestrial realm into an industrial complex. And, it’s not just private sector moguls that are sensationalizing doomsday scenarios to justify humanity’s move into the cosmos.\nUS presidents since the beginning of the Space Age have used anthropocentric language, alongside common securitizing and frontiers logic, to advocate for the importance of not only space exploration, but also human influence in the extraterrestrial realm. Four years after the Soviets had made their presence known with the launch of Sputnik I in 1957, President John F. Kennedy called upon Congress to turn its attention toward a space program with “urgent need”:\nIf we are to win the battle that is now going on around the world between freedom and tyranny, the dramatic achievements in space which occurred in recent weeks should have made clear to us all, as did the Sputnik in 1957, the impact of this adventure on the minds of men everywhere, who are attempting to make a determination of which road they should take…\nNow is the time to take longer strides—time for a great new American enterprise—time for this nation to take a clearly leading role in space achievement, which in many ways may hold the key to our future on Earth.\nPresident Obama echoed this sentiment in a similar recent essay. He appealed to the public’s childish imagination, while also hinting toward a need for the United States to reaffirm its commitment to building NASA’s space program and getting humans to Mars for the sake of security:\nOne of my earliest memories is sitting on my grandfather’s shoulders, waving a flag as our astronauts returned to Hawaii. This was years before we'd set foot on the Moon…\nWe have set a clear goal vital to the next chapter of America’s story in space: sending humans to Mars by the 2030s and returning them safely to Earth, with the ultimate ambition to one day remain there for an extended time.\nTo be clear, the scientific community believes going to Mars is a genuine possibility. For instance, the NASA Authorization Act of 2010 and US National Space Policy outline a plan to have astronauts going to asteroids by 2025 and then on to the Red Planet not too long after. Research by the agency in the years since has been directed towards achieving the long-term Mars goal.\n|Do we see space as a venue for scientific intrigue and discovery, or does the public view deep space exploration and Mars as the platform for another Earth?|\nMost will agree that going to Mars should absolutely be a goal of space travel. And, there’s no doubt that putting humans on Mars will lead to more scientific discovery. But, fewer people will recognize the importance of developing space science for the sake of science.\nA simple search of Mars-related articles will show that when it comes to reaching other worlds, the state of humanity is a top concern. Government leaders, Musk, Bezos, and other private sector space leaders have continuously pushed humans to the forefront of their mission statements.\nThis reality and our empirical relationship to spaceflight begs yet another question on the human relationship to science: Do we see space as a venue for scientific intrigue and discovery, or does the public view deep space exploration and Mars as the platform for another Earth? And, what are the consequences of the answer?\nTo explore the question and whether there are side effects to an anthropocentrically-driven mindset toward space, we must consider our historical ties to the Space Age, imagination, and science. The consequence of the current paradigm is a potentially dangerous undervaluing of scientific knowledge. The decision to deemphasize scientific reality may actually render space unsafe for humanity.\nHumanity’s imagination has been, and will always be, a central theme to space exploration. It’s the curiosity of what could be known that intrinsically drives us to explore frontiers and search for answers. But, there’s another factor involved as people advance into different environments: personal desire.\nHoward E. McCurdy, in his book Space and the American Imagination, charts over the development of the Space Age where these two factors intersect and how it has affected American decision-making when it came to planetary exploration. McCurdy says that, more often than not, the intricacies of elements like “cosmic radiation or toxic atmospheres” are replaced by “metaphors” of innovation and discovery. These ideas were so compelling and satisfying for the desires of the American public that even the wildest notions extraterrestrial exploration seemed believable.\nHowever, as he comments on a photo of the 1997 Pathfinder mission to Mars, “the reality of space exploration, during the early years of venture,” was far different from the “romantic vision offered by advocates of cosmic flight.” This reality, he writes, means that “truth or validity of the vision” is often irrelevant to the feasibility of its undertaking.\nOnce scientists embarked on space research, they could not have anticipated how truly costly and difficult the process would have been, or the fact that finding lifeforms in other worlds would have probably fallen short. Yet, this pristine vision of exploration could persist beyond the scientific actuality. And, one thing was always clear: when it came to Earthlings considering space travel, the Earth still had to somehow be involved.\n|NASA could have very well developed an “impressive program of scientific discovery,” centered on “satellite technology, or the emerging science of remote sensing,” but it would not have satisfied the public’s hungry imagination to see itself in a new light.|\n“If technological civilizations engage in space exploration for sufficiently long periods of time (say millions of years), they may do so in forms that are unrecognizable to species just beginning the venture,” wrote McCurdy. “Yet for earthlings contemplating the possibility of space travel in its initial stages, the vision of human travel to and from the Moon and inner planets dominated their collective imagination.”\nThis empirical root of a human-centric mindset could explain why for so many space industry leaders the statement, “We need to get to space because…” almost certainly ends with a clause on humanity or the fate of the Earth.\nAs McCurdy even says, NASA could have very well developed an “impressive program of scientific discovery,” centered on “satellite technology, or the emerging science of remote sensing,” but it would not have satisfied the public’s hungry imagination to see itself in a new light. The human simply could not be taken out of the equation.\nSubsequently, leaders of rocket societies, lacking an eminent US scientist to serve as a spokesperson, committed themselves to the grandiose goal of human spaceflight well before the technology had even been conceived.\nAnd today, this divide between the scientist and the American public seems to persist. For instance, the Pew Research Center found in its 2015 report, “Public and Scientists’ Views on Science and Society,” that while both groups value contributions of science, there are large gaps on how each perceive technical aspects of scientific issues. The center’s 2014 survey of adults and scientists from the American Association for Advancement of Science, found that 59 percent of Americans “view that human astronauts are an essential part of future U.S. space exploration,” while only 47 percent of AAAS scientists say they are essential.\nEven more reflective of the divide between the public and scientific experts is the fact that the original Space Act, which laid out the eight main objectives of the US civilian space program. contained no mention of humanity’s hold over space as an environment.\nRather, at the forefront of NASA’s mission was the “expansion of human knowledge of phenomena in the atmosphere and space,” the establishment of studies that would use aerospace activities “for peaceful and scientific purposes,” and “the preservation of the role of the United States as a leader in aeronautical and space science and technology,” according to research from the Committee on Human Spaceflight. NASA’s plan intended for a balanced program of science and the possibility of human flight to the Moon beyond 1970.\nBut it was this drive within the American imagination and a desire for international primacy that McCurdy writes about, which pushed Congress to increase NASA’s budget by 89 percent and extraordinarily put a man on the Moon safely before the end of the decade. The spirit of the human undoubtedly resulted in one of the nation’s greatest scientific triumphs of the century. This reality cannot be underplayed.\nStill, in 1986, the National Commission on Space in a report to Congress made it very clear that that while there is “no doubt that exploring, prospecting and settling Mars should be the ultimate goal of human space exploration,” NASA should maintain a strategy to “continue an orderly expansion outward from Earth.” The authors of the report note that every scientific advisory panel that has come together on human spaceflight says that first and foremost the mission of the program should be to “promote science, technology, engineering, mathematics, and education.”\nMcCurdy later goes on to outline this prevailing gap between NASA’s actual experience in trying to build technology and a persistent desire within the public for the romanticized “space station.” Many looked toward this technology as a pathway for colonizing other worlds, perhaps providing an escape from the difficulties that exist on Earth.\nHe summarizes the point succinctly in another piece for Technology In Society, where he writes, “ultimately, the willingness to invest in human space flight gained justification from a general cultural desire to extend some part of humanity into the cosmos.” He quotes NASA official Frank Martin, who also perfectly characterizes public reaction to technological innovations that don’t involve humans: “We don’t give ticker tape parades for robots.”\n|The subject of human-prioritized space exploration, which emphasizes the importance of preserving humanity, has a potential consequence of not only undervalued scientific knowledge, but also manipulated knowledge.|\nThese statements suggest that humans will always be at the forefront of space travel and that entrepreneurs interested in getting to space, like Musk, are perhaps not so nefarious in spreading doomsday rhetoric. Rather, they are exploiting a clear reality—the only way to make their missions a possibility in the public’s eye is for the “technical” aspects of science to sit in the backseat.\n“Given sufficient time, a new space exploration paradigm may emerge. Over the years advocates of robotic and human cooperation have envisioned the two exploring space together,” writes McCurdy in his article. “But even in this vision, the two remained separate entities—master and servant, owner and slave, flesh and machine.\nThe empirical evidence of emphasis on humanity within space exploration is likely to elicit a response of “So what?”\nIt’s valid for anyone to buy into the lofty sentiments of space entrepreneurs who essentially say that leaving Earth is the only option for humanity’s survival. “I really think there are two fundamental paths [for humanity]: One path is we stay on Earth forever, and some eventual extinction event wipes us out,” said Musk at the 67th International Astronautical Congress in Guadalajara, Mexico, in September. “The alternative is, become a spacefaring and multi-planetary species.”\nEven Hannah Arendt, a Jewish-American philosopher, as early as 1958, pondered the possibility of re-creating life beyond the Earth when she witnessed the Soviets launch Sputnik I into cosmos just a year before. She predicted in her book The Human Condition the emergence of theories that are now being supported by leading space figures:\nForemost in our minds at this moment is of course the enormously increased human power of destruction, that we are able to destroy all organic life on earth and shall probably be able one day to destroy even the earth itself…\nHowever, no less awesome and no less difficult to come to terms with is the corresponding new creative power…we have begun to populate the space surrounding the earth with man-made stars, creating as it were, in the form of satellites new heavenly bodies, and we hope that in a not very distant future we shall be able to perform what times before us regarded as the greatest, the deepest, and holiest secrets of nature, to create or re-create the miracle of life…\nThe subject of human-prioritized space exploration, which emphasizes the importance of preserving humanity, has a potential consequence of not only undervalued scientific knowledge, but also manipulated knowledge, she writes. Scientific pursuits, especially the conquest of space, move forward with an explicit point of proving the “stature of man.”\nFor instance, when it comes to Mars exploration, Robert Markley in his article, “Red Planet Scientific and Cultural Encounters,” charts NASA and the public’s interest in continuing to research even the possibility of going to the Red Planet. He argues that each renewed endeavor was the product of whether or not the NASA was able to find details on Mars that pointed toward life. Whether or not a mission was too much of an investment depended on public interest so much so that decisions were made even if it wasn’t the right time.\nThe Mars Mariner program, he writes, was “an ambitious prelude to a heroic but overhyped strategy to colonize Mars before 2020.” There wasn’t enough valuable emphasis on the technological advances or feasibility of the idea. And of course, the US had to scale it back because it was too far of a leap.\nThe same mentality persists now and will have the consequence of undermining the importance of scientific certainty. Arendt, in her book, offers a historical example of Galileo’s discovery of the telescope. Rather than being seen as having great potential for scientific innovation and discovery, it was turned into an affirmation of humanity’s potential. Its main purpose was therefore undermined:\nInstead of the dichotomy between earth and sky we have a new one between man and the universe, or between the capacities of the human mind for understanding and the universal laws which man can discover and handle without true comprehension.\n|With eager private sector leaders now seeing space an absolute for human beings, there is a grave potential that failure to check ambitions with fact and scientific reality will render space unusable.|\nWhen it comes to outer space, the threat of astrophysics was the “abstraction of thought from reason and common sense,” writes Lisa Messeri in her book Placing Outer Space: An Earthly Ethnography of Other Worlds. For her, the proof lies within the real gap between public views and the scientific community’s views the feasibility of certain space activities.\nMesseri shows how Arendt’s point about the manipulation of scientific knowledge for the purpose of proving humanity’s worth is still a valid concern:\nIn searching for habitable planets, astronomers confront the changing ways they can inhabit their places of science. No longer do they simply dwell at the observatory; they also inhabit increasingly distributed sociotechnical networks…\nGoing to the observatory will not soon fade from practice, but it will be driven more and more by a need to be in place and connect to history of the pression less and less by a scientific need… the making of habitability as the definitive metric of an exoplanet that would have the greatest human significance.\nThe increasing insignificance of science in favor of lofty humanistic ambitions presents a general disservice to society. And it’s a trend that will ultimately be our undoing in space. While there is discussion about the need to go to deep space, there is far less conversation on what must be done and considered from a scientific point before traversing new environments. Continued de-emphasis on the research will mean that we miss important realities on the feasibility of our endeavors.\nTo put into perspective the real world impact of such ignorance just on one level, officials at NASA now estimates that there are more than 500,000 pieces of debris in Earth orbit. A hypervelocity impact with one of these objects produces enough kinetic energy that even the smallest fragment could completely destroy a satellite.\nNASA scientist Donald Kessler recognized the danger of space debris as early as 1978, predicting that debris could cascade in a chain reaction that made low Earth orbit unusable for spacecraft. With eager private sector leaders now seeing space an absolute for human beings, there is a grave potential that failure to check ambitions with fact and scientific reality will render space unusable.\nThus, seeing space as the Plan B and making humans the point of exploration may have an unintended consequence: environmental and commercial destruction on Earth could be manifested within this extraterrestrial environment. Perhaps, an emphasis on scientific reality could mitigate the side effects of our unchecked ambitions.","Why does alcohol pose an acceptable risk while GM foods call for caution? It is all about trust and control, says Nick Pidgeon.\nRisk and the way people respond to and perceive risk has, over the past five years or so, reached the top of the public policy agenda. We can all point to examples where people are apparently unconcerned by some very serious public health problems, such as alcohol or smoking. On the other hand, some hazards, which appear to pose a comparatively low risk according to some experts, such as applications of genetics research, are the focus of considerable controversy and concern.\nA first point to make is, of course, that the statistical assessment of risk is relatively easy to conduct for everyday activities that many people are routinely engaged in, such as smoking, but much more difficult for many of the controversial risks that stretch far into the future or of which we have little experience. Accordingly, the risks from technological advances may be fraught with considerable and profound uncertainty - until the technology is developed, by which time it may be too late to prevent some unforeseen consequences - or surrounded by disagreement among experts.\nRegarding public understanding of risk, the research evidence from two decades of social science work on \"risk perception\" shows us quite clearly that people have a good grasp of what is likely to kill them tomorrow, but when asked about risk, they bring other factors into the account. For example, risks are seen as less acceptable, or in need of more attention, if they are difficult to personally control - for example, flying - or imposed on people without their consent - for example, industrial facilities.\nRisk acceptability can also vary with people's cultural or demographic characteristics (there are many \"publics\", each with differing views on risk), or in relation to judgements about fundamental values, such as whether the risks and benefits are unevenly distributed across a society in either space or time. A particular problem comes when activities bring present benefits but pose risks to future generations.\nSociological work, in particular, has highlighted an important aspect of people's risk concerns - that of trust or distrust in risk-managing institutions. Again, matters here are far from simple, and we have yet to develop a sufficiently deep understanding of this issue. For example, surveys show that some professionals, such as doctors or the emergency services, are consistently accorded high levels of trust across society. Politicians, perhaps not surprisingly, are often accorded very poor levels of trust, while government agencies that manage risk on our behalf can have quite varied trust profiles depending on their perceived function and performance history.\nIn the United Kingdom and Europe, trust questions have been central to people's concerns about BSE, and, more recently, genetic modification of foodstuffs. Some sociologists would argue that our reliance on expertise and institutionalised risk management has become a defining feature of a modern \"risk society\". Reading the risk stories that constantly appear across all sections of the media, one could hardly disagree, and it is also clear that there are many issues today (for example, the privacy consequences of the increasing use of large, interconnected databases) that would not in the past have been labelled as \"risk issues\".\nTrust is important for a number of reasons, not least because it will impact upon the many \"risk communications\" we receive daily. Most obviously, results from persuasive communication research show us that if we do not trust the messenger we may not believe the message. And institutional trust (accorded to governments or business) may be lost following a serious incident or disaster, particularly if a cover-up or less than full efforts to learn the lessons is suspected.\nIt is for these reasons that many commentators argue that we need to go beyond simple efforts to \"educate\" people about risks, or improve public understanding of science. Particularly, in mapping the future path of the more socially conflicted risk questions (such as human genetics research and its development), we may need a form of analytic-deliberative process that allows open interrogation of the basic science of the matter plus some acknowledgement that people's values and beliefs about risk and trust should count. This has been tried for a range of technologies in forms such as citizens' juries and consensus conferences. Whatever the vehicle and the issue, it is important to recognise that society's decisions about risk will often have to involve a judicious blend of sound science and public values. This is the real risk challenge of the new millennium.\nNick Pidgeon is director of the centre for environmental risk at the school of environmental sciences, University of East Anglia. This article is based on a presentation given to the British Association for the Advancement of Science in London."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:3133e39e-71a7-44e7-85fa-61048cb16a0b>","<urn:uuid:0457dfe7-9a96-47f5-9251-b516358dc259>"],"error":null}
{"question":"How do the safety requirements for handling corrosive materials compare between WHMIS and GHS compliance standards, particularly in terms of warning labels and workplace protocols?","answer":"Both WHMIS and GHS systems require clear warning labels and protocols for handling corrosive materials. Under WHMIS, corrosive materials are Class E substances that require a specific symbol showing two test tubes pouring liquid on a bar and a hand inside a circle. These materials can burn eyes and skin on contact and cause irreversible damage to human tissue. For GHS compliance, businesses are legally required to display specific pictograms, HAZMAT diamond placards, and specially designed warning labels. Both systems emphasize the importance of proper signage to prevent workplace accidents and injuries. Failing to maintain compliance with either system can result in serious consequences - under GHS, violations can lead to fines, financial penalties, and loss of business licenses, while WHMIS was established specifically to prevent injuries, illnesses, deaths, medical costs, and fires.","context":["The Workplace Hazardous Materials Information System (WHMIS) is Canada’s national workplace hazard communication standard. This legislation applies to all workers involved in manufacturing, or work with or are in proximity to controlled substances or products in their place of work. The WHMIS is a comprehensive plan that provides information on the safe use of hazardous materials. These safety information are delivered to the involved personnel by means of:\n- Cautionary labels found on the containers of controlled products\n- Providing material safety data sheets (MSDS) for each controlled product; and\n- Educating workers and providing site-specific training programs on hazardous substances\nExposure to hazardous substances can cause or contribute to various serious health issues such as dizziness, lung or kidney damage, cancer, sterility, burns, and rashes. Some hazardous materials are also safety hazards, and can cause fire or explosions. WHMIS was established to help prevent injuries, illnesses, deaths, medical costs, and fires caused by hazardous products. The end goal is to create a safer workplace by making sure you and your workers are knowledgeable about these hazards and have the proper tools to work safely.\nWHMIS has established eight different classes to identify chemical hazards. Each class has a corresponding symbol for identification:\nCompressed gas is any gas placed under constant pressure or chilled, contained by a cylinder. A sudden release of high pressure can be deadly; it can puncture the skin and cause fatal embolism. Heat exposure can also cause it to explode. Leaking cylinders are also a danger because the gas that comes out may cause frostbite. Compressed gas includes carbon dioxide, oxygen, compressed air, ethylene, and welding gases. The hazard symbol is an image of a cylinder surrounded by a circle.\nAny material that will burn, explode, or catch fire easily at normal temperature (below 37.8 °F) are considered flammable. Combustible materials must be heated first before they will ignite while reactive flammable materials are those that suddenly start burning when it touches air or water. Materials under this class can be solid, liquid or gas such as acetone, turpentine, ethanol, propane, butane, kerosene, spray paints, and varnish. The symbol for this is a flame with a line under it inside a circle.\nOxygen is necessary for combustion. Oxidizers do not burn but can cause other materials that normally do not burn to suddenly catch fire by providing oxygen. They can be in the form of gases such as oxygen and ozone, liquids such as nitric acids and perchloric acid solutions, or solids such as sodium chlorite. The symbol for oxidizing materials is an “o” with flames on top of it inside a circle.\nMaterials under this division are very poisonous and can cause immediate death or serious injury if inhaled, digested, absorbed, or injected into the body. Most Class D-1 materials will also cause long term effects. Some examples of D-1 materials include sodium cyanide, carbon monoxide, sulphuric acid, acrylonitrile, and 4-diisocyanate (TDI). The symbol is a skull and crossbones inside a circle.\nThese are poisonous materials as well but their effects are not as quick-acting and only temporary. Those that don’t have immediate effects may still have serious results such as cancer, liver or kidney damage, birth defects, or reproductive problems. Some of these materials include asbestos, mercury, benzene, acetone, and cadmium. The symbol for Class D-2 looks like a “T” with an exclamation point at the bottom inside a circle.\nThese are toxins or organisms such as viruses, bacteria, fungi, and parasites, that can cause diseases in people or animal. Biohazardous infectious materials are usually found in hospitals, laboratories, healthcare facilities, veterinary practices, and research facilities. Any person handling specimens or samples in these environments should assume that they are contaminated and should treat them accordingly. Examples of these materials include Hepatitis B, AIDS/HIV virus, and salmonella. A Class D-3 symbol looks like three “c”s joined together with a small circle in the middle, all inside a circle.\nCorrosive materials, such as acids and bases will burn eyes and skin on contact. It can also cause irreversible damage to human tissue such as the eye or lung. It will also attack clothes and other materials like metal. Common corrosive materials include sulphuric and nitric acids, ammonium hydroxide, and caustic soda. The symbol for Class E is an image of two test tubes pouring liquid on a bar and a hand inside a circle.\nMaterials under this class are unstable, and may burn, explode or produce dangerous gases when mixed with incompatible materials. A Class F material manifests three different properties or abilities:\n- It can react very strongly and quickly with water to make toxic gas;\n- It will react with itself when it gets shocked, or if the temperature or pressure increase; and\n- It can undergo polymerization, decomposition, or condensation.\nExamples of this class include vinyl chloride, ethyl acetate, ethylene oxide, and picric acid. The symbol for this class is a picture of a test tube with sparks coming out of the tube surrounded by a letter “R” inside a circle.\nA safe environment is important for everyone, and knowing how to properly handle hazardous materials in your facility is one way to achieve that. A thorough WHMIS education coupled with a strong PPE training program will help you in achieving a safe and efficient workplace.\nConnect with Maria Marnelli G. Medina on Google+","GHS pictograms, workplace hazard warnings and warning labels can provide essential safety information for employees and associates of organisations and businesses that utilize or transport potentially hazardous materials. Failing to maintain a GHS compliant working environment can become a serious issue, one which may increase the risk of a workplace accident or injury. Signs and labels designed to warm employees regarding the potential health and safety risks of hazardous materials play a vital role in workplace safety and can be especially important in day to day operations that involve transporting dangerous goods or working with industrial chemicals.\nCreating and Maintaining a Safe Workplace\nWorkplace safety is a concern that no business owner, employer or management professional can afford to overlook. Failing to display appropriate warning labels and GHS pictograms that provide essential safety information can drastically increase the risks of a workplace injury. Businesses that fail to maintain a GHS compliant environment by posting clearly marked workplace hazard warnings and ensuring that all employees take proper precautions and follow establish protocols when working with or transporting dangerous goods may find themselves facing steep fines and penalties. Failing to create and maintain a safe working environment often means that businesses are more likely to be faced with legal action resulting from a preventable accident or injury.\nRegulations Governing Workplace Hazard Warnings\nThere are numerous regulations pertaining to the signage and warning labels that must be displayed in environments that contain potentially hazardous materials. Businesses are legally required to utilize the Hazardous Materials Identification System, GHS pictograms and Material Safety Data Sheets whenever working with or transporting dangerous goods, industrial chemicals and hazardous materials. Maintaining a GHS compliant working environment involves understanding and following numerous regulations and guidelines in order to ensure workers and bystanders are not being placed at greater risk.\nThe Globally Harmonized System is a set of guidelines established to ensure the safe manufacture, handling, transport and disposal of materials that may pose a potential health and safety risk. GHS pictograms are designed to be universal and easily understood in order to reduce the potential confusion caused by language barriers. GHS regulations require businesses and organisations to display specific pictograms, HAZMAT diamond placards and other specially designed warning labels when working with or transporting hazardous materials. Violating GHS regulations or failing to comply with established rules, guidelines and protocols can result in numerous legal consequences that may include fines, financial penalties and even loos of certain business licences.\nEnsuring all working environments, transport operations and workflow process are GHS compliant can be of paramount importance. Transporting dangerous goods in an improper manner or failing to display the correct workplace hazard warnings is a serious OSHA violation, one that may result in greater instances of work-related accidents or even place the health and safety of bystanders at risk. Businesses who fail to make workplace safety a top priority are far more likely to be charged with compliance violations that can result in fees, fines and even legal action.Back to top"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:ef7f93ea-f149-43d6-8e24-8135f9714ece>","<urn:uuid:031871cb-39ca-4caa-b449-ec9af22d7fbc>"],"error":null}
{"question":"What are the theological implications of eternity in relation to God's temporality, and how does this concept intersect with compatibilist views on free will?","answer":"Eternity in theological context has two main implications: divine temporality and future existence. Regarding divine temporality, there are two perspectives - God existing within time (temporal) and outside of time (atemporal). The Bible presents both views: God living innumerable days and doing human-like things suggests temporality, while depictions of God as apart from creation suggest atemporality. This theological understanding intersects with compatibilist philosophy, which maintains that free will and determinism can coexist. Compatibilists, including medieval scholastics like Thomas Aquinas, argued that human actions can be free even if determined by rational cognition or divine decrees through 'physical premotion' - a deterministic intervention of God into human will.","context":["Seeing the word eternity may conjure up the image of an outstretched horizon receding as far as we can see, or a type of heavenly realm—an Elysian\nplain with warm sun and cloudless sky. However, eternity is actually a complex idea that relates to how people understand God and their place in the universe.\nWhile eternity is not something one can actually experience, biblical writers use the concept to describe two related issues: First, biblical writers use eternity as a way to describe divine temporality\n(as in a quality of God, e.g., Gen 21:33; Deut 33:27; Isa 40:28). Second, they also use eternity to describe the timeframe of future existence for people (as in an afterlife, Tob 3:6; John 3:16).\nEternity as Divine Temporality\nConcepts of eternity are rooted in how people perceive time, especially as it relates to God’s interaction with time. There are essentially two options: God is temporal, meaning that he exists in time; and God is atemporal, meaning that he exists apart from time. Because of the difficulty of describing eternity in human language, the Bible seems to hint at both of these options. For example, the psalmist depicts God as living an innumerable number of days (Pss 88:29, 90:2; 1 Chr 16:36) and contrasts God’s life with the brevity of a person’s life (Pss 39:5, 90:3–10, 103:15–17, 144:4). Similarly, God seems to exist in time when he does human-like things such as change his intentions (e.g., Exod 32:14) and dwell among people in the incarnation\n(John 1:14). In contrast, the Bible also depicts God as apart from creation and, therefore, outside of time (Gen 1:1–5; Deut 33:27; Eccl 3:11; Isa 43:13; Rom 1:20). New Testament\nbooks such as John afford Jesus a divine temporality as a function of his divinity (John 1:1, 8:58; and looking back to the Old Testament\n, see Prov 8:22–31; Isa 9:6; Mic 5:2).\nEternity as Future Existence\nThese concepts of eternity also affect the way in which people describe their future existence. Because God is eternal, his covenant with his people will also be eternal (Gen 9:16, 17:7; Lev 16:34; 2 Sam 23:5; Sir 44:18, 45:7; Bar 2:35; Heb 13:20). Since the final resting place for those possessing this covenant with God is with God (Dan 12:2; 2 Macc 7:9; 2 Cor 5:1; Rev 21), then it is also, by extension, eternal as God is eternal (Ps 49:9; Eccl 12:5; Isa 45:17). For the New Testament, the culmination of this covenant coming through right belief is eternal life (John 3:16). Thus, when the New Testament speaks of eternity, it also views eternity not just as innumerable days but also as the mode of life with God in the age to come (Mark 10:20; John 4:14). Just as heaven is eternal, so does hell appear to be also (Matt 25:41; 4 Macc 9:9; 2 Thess 1:9; Jude 6–7; Rev 20:10).\nThe biblical writers could not see or touch eternity; it was only something they could grasp in their mind’s eye. Even when Jesus spoke of heavenly things, the crowds did not grasp his meaning (John 3:12). Yet concepts of eternity allow the reader to contrast the limitations and weakness of their present existence with an outstretched horizon of God’s eternal goodness and love (John 3:16).","Compatibilism is the belief that free will and determinism are mutually compatible and that it is possible to believe in both without being logically inconsistent.\nCompatibilists believe freedom can be present or absent in situations for reasons that have nothing to do with metaphysics.They say causal determinism does not exclude the truth of possible future outcomes.\nSimilarly, political liberty is a non-metaphysical concept.Statements of political liberty, such as the United States Bill of Rights, assume moral liberty: the ability to choose to do otherwise than one does.\nCompatibilism was championed by the ancient Stoicsand some medieval scholastics (such as Thomas Aquinas). More specifically, scholastics like Thomas Aquinas and later Thomists (such as Domingo Báñez) are often interpreted as holding that a human action can be free even though the agent in some strong sense could not do otherwise than he did. Whereas Aquinas is often interpreted to maintain rational compatibilism (i.e., an action can be determined by rational cognition and yet free), later Thomists such as Báñez develop a sophisticated theory of theological determinism, according to which actions of free agents, despite being free, are, on a higher level, determined by infallible divine decrees manifested in the form of \"physical premotion\" (praemotio physica), a deterministic intervention of God into the will of a free agent required to reduce the will from potency to act. A strongly incompatibilist view of freedom was, on the other hand, developed in the Franciscan tradition, especially by Duns Scotus, and later upheld and further developed by Jesuits, esp. Luis de Molina and Francisco Suárez. In the early-modern era, compatibilism was maintained by Enlightenment philosophers (such as David Hume and Thomas Hobbes).\nDuring the 20th century, compatibilists presented novel arguments that differed from the classical arguments of Hume, Hobbes, and John Stuart Mill.Importantly, Harry Frankfurt popularized what are now known as Frankfurt counterexamples to argue against incompatibilism, and developed a positive account of compatibilist free will based on higher-order volitions. Other \"new compatibilists\" include Gary Watson, Susan R. Wolf, P. F. Strawson, and R. Jay Wallace. Contemporary compatibilists range from the philosopher and cognitive scientist Daniel Dennett, particularly in his works Elbow Room (1984) and Freedom Evolves (2003), to the existentialist philosopher Frithjof Bergmann. Perhaps the most renowned contemporary defender of compatibilism is John Martin Fischer.\nCompatibilists often define an instance of \"free will\" as one in which the agent had freedom to act according to their own motivation . That is, the agent was not coerced or restrained. Arthur Schopenhauer famously said, \"Man can do what he wills but he cannot will what he wills.\"In other words, although an agent may often be free to act according to a motive, the nature of that motive is determined. This definition of free will does not rely on the truth or falsity of causal determinism. This view also makes free will close to autonomy , the ability to live according to one's own rules, as opposed to being submitted to external domination.\nSome compatibilists will hold both causal determinism (all effects have causes) and logical determinism (the future is already determined) to be true. Thus statements about the future (e.g., \"it will rain tomorrow\") are either true or false when spoken today. This compatibilist free will should not be understood as some kind of ability to have actually chosen differently in an identical situation. A compatibilist can believe that a person can choose between many choices, but the choice is always determined by external factors.If the compatibilist says \"I may visit tomorrow, or I may not\", he is saying that he does not know what he will choose—if he will choose to follow the subconscious urge to go or not.\nAlternatives to strictly naturalist physics, such as mind–body dualism positing a mind or soul existing apart from one's body while perceiving, thinking, choosing freely, and as a result acting independently on the body, include both traditional religious metaphysics and less common newer compatibilist concepts.Also consistent with both autonomy and Darwinism, they allow for free personal agency based on practical reasons within the laws of physics. While less popular among 21st century philosophers, non-naturalist compatibilism is present in most if not almost all religions.\nA prominent criticism of compatibilism is Peter van Inwagen's consequence argument.\nCritics of compatibilism often focus on the definition(s) of free will: incompatibilists may agree that the compatibilists are showing something to be compatible with determinism, but they think that this something ought not to be called \"free will\". Incompatibilists might accept the \"freedom to act\" as a necessary criterion for free will, but doubt that it is sufficient. Basically, they demand more of \"free will\". The incompatibilists believe free will refers to genuine (e.g., absolute, ultimate) alternate possibilities for beliefs, desires, or actions, rather than merely counterfactual ones.\nCompatibilism is sometimes called soft determinism (William James's term) pejoratively .James accused them of creating a \"quagmire of evasion\" by stealing the name of freedom to mask their underlying determinism. Immanuel Kant called it a \"wretched subterfuge\" and \"word jugglery\". Kant's argument turns on the view that, while all empirical phenomena must result from determining causes, human thought introduces something seemingly not found elsewhere in nature—the ability to conceive of the world in terms of how it ought to be, or how it might otherwise be. For Kant, subjective reasoning is necessarily distinct from how the world is empirically. Because of its capacity to distinguish is from ought, reasoning can 'spontaneously' originate new events without being itself determined by what already exists. It is on this basis that Kant argues against a version of compatibilism in which, for instance, the actions of the criminal are comprehended as a blend of determining forces and free choice, which Kant regards as misusing the word \"free\". Kant proposes that taking the compatibilist view involves denying the distinctly subjective capacity to re-think an intended course of action in terms of what ought to happen. Ted Honderich explains his view that the mistake of compatibilism is to assert that nothing changes as a consequence of determinism, when clearly we have lost the life-hope of origination.\nFree will is the ability to choose between different possible courses of action unimpeded.\nDeterminism is the philosophical belief that all events are determined completely by previously existing causes. Deterministic theories throughout the history of philosophy have sprung from diverse and sometimes overlapping motives and considerations. The opposite of determinism is some kind of indeterminism or randomness. Determinism is often contrasted with free will.\nWill, generally, is the faculty of the mind that selects, at the moment of decision, a desire among the various desires present; it itself does not refer to any particular desire, but rather to the mechanism responsible for choosing from among one's desires. Within philosophy, will is important as one of the parts of the mind, along with reason and understanding. It is considered central to the field of ethics because of its role in enabling deliberate action.\nIncompatibilism is the view that a deterministic universe is completely at odds with the notion that persons have a free will; that there is a dichotomy between determinism and free will where philosophers must choose one or the other. This view is pursued in at least three ways: libertarians deny that the universe is deterministic, the hard determinists deny that any free will exists, and pessimistic incompatibilists deny both that the universe is determined and that free will exists.\nFatalism is a philosophical doctrine that stresses the subjugation of all events or actions to Fate or destiny, and is commonly associated with the consequent attitude of resignation in the face of future events which are thought to be inevitable.\nIndeterminism is the idea that events are not caused, or not caused deterministically.\nLibertarianism is one of the main philosophical positions related to the problems of free will and determinism, which are part of the larger domain of metaphysics. In particular, libertarianism is an incompatibilist position which argues that free will is logically incompatible with a deterministic universe. Libertarianism states that since agents have free will, determinism must be false. One of the first clear formulations of libertarianism is found in John Duns Scotus; in theological context metaphysical libertarianism was notably defended by Jesuit authors like Luis de Molina and Francisco Suárez against rather compatibilist Thomist Báñezianism. Other important metaphysical libertarians in the early modern period were René Descartes, George Berkeley, Immanuel Kant, and Thomas Reid. Roderick Chisholm was a prominent defender of libertarianism in the 20th century, and contemporary libertarians include Robert Kane, Peter van Inwagen and Robert Nozick.\nHard determinism is a view on free will which holds that determinism is true, and that it is incompatible with free will, and, therefore, that free will does not exist. Although hard determinism generally refers to nomological determinism, it can also be a position taken with respect to other forms of determinism that necessitate the future in its entirety. Hard determinism is contrasted with soft determinism, which is a compatibilist form of determinism, holding that free will may exist despite determinism. It is also contrasted with metaphysical libertarianism, the other major form of incompatibilism which holds that free will exists and determinism is false.\nPredeterminism is the philosophy that all events of history, past, present and future, have been already decided or are already known, including human actions.\nPeter van Inwagen is an American analytic philosopher and the John Cardinal O'Hara Professor of Philosophy at the University of Notre Dame. He is also a Research Professor of Philosophy at Duke University each Spring. He previously taught at Syracuse University and earned his PhD from the University of Rochester in 1969 under the direction of Richard Taylor. Van Inwagen is one of the leading figures in contemporary metaphysics, philosophy of religion, and philosophy of action. He was the president of the Society of Christian Philosophers from 2010 to 2013.\nRobert Hilary Kane is an American philosopher. He is Distinguished Teaching Professor of Philosophy at the University of Texas at Austin, and is currently on phased retirement.\nTheological determinism is a form of predeterminism which states that all events that happen are pre-ordained, and/or predestined to happen, by one or more divine beings, or that they are destined to occur given the divine beings' omniscience. Theological determinism exists in a number of religions, including Jainism, Judaism, Christianity and Islam. It is also supported by proponents of Classical pantheism such as the Stoics and Baruch Spinoza.\nIn philosophy, moral responsibility is the status of morally deserving praise, blame, reward, or punishment for an act or omission performed or neglected in accordance with one's moral obligations. Deciding what counts as \"morally obligatory\" is a principal concern of ethics.\nKantian ethics refers to a deontological ethical theory ascribed to the German philosopher Immanuel Kant. The theory, developed as a result of Enlightenment rationalism, is based on the view that the only intrinsically good thing is a good will; an action can only be good if its maxim—the principle behind it—is duty to the moral law. Central to Kant's construction of the moral law is the categorical imperative, which acts on all people, regardless of their interests or desires. Kant formulated the categorical imperative in various ways. His principle of universalizability requires that, for an action to be permissible, it must be possible to apply it to all people without a contradiction occurring. Kant's formulation of humanity, the second section of the Categorical Imperative, states that as an end in itself humans are required never to treat others merely as a means to an end, but always, additionally, as ends in themselves. The formulation of autonomy concludes that rational agents are bound to the moral law by their own will, while Kant's concept of the Kingdom of Ends requires that people act as if the principles of their actions establish a law for a hypothetical kingdom. Kant also distinguished between perfect and imperfect duties. A perfect duty, such as the duty not to lie, always holds true; an imperfect duty, such as the duty to give to charity, can be made flexible and applied in particular time and place.\nFrankfurt cases were presented by philosopher Harry Frankfurt in 1969 as counterexamples to the principle of alternate possibilities (PAP), which holds that an agent is morally responsible for an action only if that person could have done otherwise.\nMetaphysics is the branch of philosophy that investigates principles of reality transcending those of any particular science. Cosmology and ontology are traditional branches of metaphysics. It is concerned with explaining the fundamental nature of being and the world. Someone who studies metaphysics can be called either a \"metaphysician\" or a \"metaphysicist\".\nIllusionism is a metaphysical theory first propounded by professor Saul Smilansky of the University of Haifa. Although there exist a theory of consciousness baring the same name (illusionism), It is important to note that the two theories are concerned with different subjects. Illusionism as discussed here, holds that people have illusory beliefs about free will. Furthermore, it holds that it is both of key importance and morally right that people not be disabused of these beliefs, because the illusion has benefits both to individuals and to society. Belief in hard incompatibilism, argues Smilansky, removes an individual's basis for a sense of self-worth in his or her own achievements. It is \"extremely damaging to our view of ourselves, to our sense of achievement, worth, and self-respect\".\nFree will in antiquity is a philosophical and theological concept.\nDickinson Sargeant Miller was an American philosopher best known for his work in metaphysics and the philosophy of mind. He worked with other philosophers including William James, George Santayana, John Dewey, Edmund Husserl, and Ludwig Wittgenstein.\nIn the theory developed by Domingo Báñez and other 16th and 17th century Thomists, physical premotion is a causal influence of God into a secondary cause which precedes and causes the actual motion of its causal power : it is the reduction of the power from potency to act. In this sense, it is a kind of divine concursus, the so-called concursus praevius advocated by the Thomists."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:ee763ef6-4132-4da5-915b-d70fcbda3e13>","<urn:uuid:9483c194-c173-4ae0-bd8b-a82912451b17>"],"error":null}
{"question":"Are there similarities between Black Lab Sports' and Kelley Drye & Warren's business models in terms of their approach to athlete services?","answer":"Yes, both Black Lab Sports and Kelley Drye & Warren share some similar approaches in serving athletes, though through different channels. Both organizations focus on long-term success beyond active playing careers - Black Lab Sports provides a platform for athletes to become mentors and advisors in sports product development, operating from a 22,000-square-foot facility in Boulder with multiple resources including performance labs and office spaces. Similarly, Kelley Drye & Warren positions itself as a long-term business adviser, helping athletes transition into post-playing careers while providing comprehensive legal and business services. Both organizations emphasize creating sustained value and opportunities for athletes beyond their active playing days.","context":["By Lisa Zimmerman, Player Engagement Insider\nDuring his 12-year NFL career, former defensive tackle Justin Bannan was wholly focused on the job at hand. A fifth-round NFL Draft pick in 2002 by the Buffalo Bills, Bannan went on to play for the Baltimore Ravens, Denver Broncos, St. Louis Rams and Detroit Lions. However, a few years in, the University of Colorado graduate realized that his financial education was lacking and he was going to need to learn how to prepare himself for life after football.\n“My financial advisor kept me from doing dumb things throughout my career,” Bannan said. “But it hit me like a lightning bolt, you don’t know what your money’s doing. Do you want people to view you as a dumb football player? That’s when I started to pay attention to getting educated. As an athlete, I think you get so pigeon-holed that when you’re done [if you don’t prepare] you don’t have a lot of options.”\nIn 2013, Bannan attended the NFL’s executive education program at Stanford University. It is an experience he calls “life-changing.”\n“It definitely generated something in my brain,” he said. “It put those thoughts in my head. It was like a validation. What I struggled with was the fear factor of stepping outside my box. People pitch things to you and you act like you know but you don’t.”\nWith a variety of offers and opportunities swirling around him, he found his curiosity sparked by the tech industry and began studying it. He quickly realized that his NFL pedigree could open doors and he began taking advantage of that, setting up meetings with as many people as he could to further educate himself.\nAll that networking paid off when Bannan received an email from JP O’Brien. O’Brien was an entrepreneur with a background in building businesses, especially those that were tech-focused. In 2014, the two kicked off their partnership, with Bannan becoming a Pro Advisor for a new product and company O’Brien helped launch called isplack. Described as a game-day products company, isplack eyeblack was their first entry into the market. It is chemical-free and, unlike traditional eyeblack products, can be removed with just soap and water.\nThat initial relationship has evolved and Bannan and O’Brien are now partners in Black Lab Sports, which is a go-to-market platform, whose overarching mission is to help fast-track already-designed sports products to market. The company has built a 22,000-square-foot facility in Boulder, Colorado where entrepreneurs who have sports-related products can come and interact with professional athletes who act as mentors and test those products and offer insight and advice. Black Lab Sports then helps expedite the process of getting the product on shelves.\nThe facility itself features office spaces, a performance lab with physical therapists and trainers, and an art studio space, which features in-resident artists.\nO’Brien described the thought process behind the combination of resources.\n“We’re an innovation center and innovation center is sparked by art,” he said. “We’re trying to be super creative on the business side. It’s important to have those traits next to us.”\nBlack Lab Sports cultivates relationships with other entrepreneurs, investors, and mentors including professional athletes. People like former NFL quarterback Jake Plummer, former NFL tight end Daniel Graham and Olympic swimmer Chloe Sutton are part of the team.\nThe company also has a charitable component. Their motto is, “Give First” and to that end, they work with several causes, including the organization Team Impact, which supports children dealing with life-threatening and chronic illnesses.\nFor Bannan, his new path is both gratifying and exciting and hopes that he can help others navigating a similar road.\n“It’s very important to be humble when you’re done (playing in the NFL) and be willing to learn,” he said. “Just because you have a name that grants you access, that’s going to be temporary. Sooner or later you have to create value. Every day you have to come out and say, ’How do I get better?’ You have to evolve. You can’t rest on your laurels.”\nLisa Zimmerman is a long-time NFL writer and reporter. She was the Jets correspondent for CBSSports.com, SportsNet New York’s TheJetsBlog.com and Sirius NFL Radio. She has also written for NFL.com.","As law firms nationwide scramble to find new sources of revenue in a flat legal market, Kelley Drye & Warren, the New York-based law firm with about 125 attorneys in Washington, thinks it has found a bankable solution: professional athletes.\nThe firm recently hired a pair of attorneys from Dow Lohnes, Adisa Bakari and Jeffery Whitney, to start its sports and entertainment group — a new practice for a firm best known for bankruptcy, tax and estate law.\nBakari and Whitney double as sports agents and attorneys, going beyond just negotiating contracts for athletes to doling out legal advice on issues such as wealth preservation, taxes, real estate, and trusts and estate planning — essentially anything that the chief executive of a company would want from their lawyer.\n“We view [athletes] as high net-worth clients, as executive clients,” said Bakari, who chairs the sports and entertainment group. “We sell long-term success and business protection.”\nBakari and Whitney cast themselves as an alternative to the traditional sports agency model, which they say fails athletes by not supporting them once their playing career is over. According to one estimate, nearly 80 percent of National Football League players are broke or bankrupt within three years after retiring.\nThe formula is one the duo, who met as classmates at the University of Wisconsin Law School in the 1990s, perfected at their former firm, Dow Lohnes. Bakari, a Washington native who played football at Delaware State, founded the sports and entertainment division at Dow Lohnes 15 years ago, and four years ago recruited Whitney to join him. When Dow Lohnes announced last month that it would combine some of its Washington practices with the Silicon Valley-based law firm Cooley, Bakari and Whitney’s group, along with Dow Lohnes’ lobbying practice and others, were not part of the deal.\nThe group’s clients include Redskins running back Evan Royster, Indianapolis Colts safety Antoine Bethea, Chicago Bears running back Matt Forté, Jacksonville Jaguars running back Maurice Jones-Drew and IBF junior welterweight boxing champion Lamont Peterson.\nProfessional athletes call for an unconventional approach, they said. Bakari and Whitney typically try to meet with a player’s parents first, pitching themselves as long-term business advisers who will stick around once an athlete’s playing career is over to help them transition into a second career, such as broadcasting or franchising.\n“A kid being drafted shifts the financials for the family,” Bakari said. “We get them to look at themselves as an executive who runs their own business.”\nBeing part of an established law firm — Kelley Drye was founded in 1836 and is one the nation’s top 200 firms by revenue — helps with the sell, they said.\n“No sports agency has been around as long as [Kelley Drye],” Bakari said. “It’s immediate credibility.”\nFor years, Washington law firm Williams & Connolly was one of the few firms in town that had a significant practice representing athletes, primarily NBA players, including Grant Hill, Tim Duncan and Shane Battier. The leader of that group, sports lawyer Jim Tanner, recently left to open his own sports agency in Arlington.\nKelley Drye’s sports and entertainment group, which in addition to Bakari and Whitney include two publicists, will help drive business to the firm’s existing practice areas, said Lewis Rose, managing partner of Kelley Drye’s Washington office. The firm’s intellectual property lawyers, he cited as an example, will come into play when negotiating how a company uses an athlete’s name and image in advertising materials.\nSuch work is a way to leverage the firm’s existing skills, part of the motivation behind the deal.\n“We thought, ‘How do we find a new inventory of high net worth clients?’” Rose said."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:7ea2ff62-a8c0-490f-88f1-e232a62ba756>","<urn:uuid:0043784e-9995-42d0-9f5a-60b9472f4112>"],"error":null}
{"question":"What are some examples of different colors of gold alloys used in jewelry making, and how are they achieved?","answer":"Different colors of gold are achieved through various alloys: Rose gold is made by combining 18 parts gold with 6 parts copper. Blue gold is created by alloying with iron, while purple gold is made by alloying with aluminum, though these are rare in jewelry. Green gold results from alloying 14kt or 18kt gold with silver, creating a greenish-yellow appearance. White gold can be made with either palladium or nickel, though nickel versions are restricted in Europe due to toxicity concerns.","context":["Aluminum is a soft, durable, lightweight, malleable metal with appearance ranging from silvery to dull grey, depending on the surface roughness. Aluminium is remarkable for the metal's low density and for its ability to resist corrosion. It is easily machined, cast, drawn and extruded.\nBrass is an alloy of copper and zinc. Since copper is a highly active metal, tarnishing is quite common. Many people notice that their skin will turn green from wearing brass. This is the result of a chemical reaction between the copper in the alloy and the normal acidic condition of the skin. To clean, use a copper/brass cleaner. Brasso and Bon Ami are suitable, but will slightly reduce the original high gloss finish.\nBronze is a metal alloy consisting primarily of copper, usually with tin as the main additive, but sometimes with other elements such as phosphorus, aluminum, or silicon. It is hard and brittle, and it was particularly significant in antiquity, giving its name to the Bronze Age.\nCopper is one of only two elemental metals that have color. Since copper is a highly active metal, tarnishing is quite common. Copper often comes with a patina and therefore care should be used in cleaning. Copper cleaners will remove patinas. If no patina has been applied you can use a copper cleaner or rouge cloth.\nBecause of the softness of pure 24kt gold, it is usually alloyed with base metals for use in jewelry, altering its hardness, ductility, melting point, color and other properties. 22kt, 18kt, 14kt or 10kt gold contain higher percentages of copper, other base metals, silver or palladium in the alloy and lower amounts of gold. Copper is the most commonly used base metal, yielding a redder color. Jewelry with higher gold content is more prone to scratch or bend.\nRose gold is an alloy of gold and copper, typically 18 parts gold, to 6 parts copper.\nBlue gold can be made by alloying with iron and purple gold can be made by alloying with aluminum, although rarely done except in specialized jewelry. Blue gold is more brittle and therefore more difficult to work with when making jewelry.\n14kt and 18kt gold alloys with silver appear greenish-yellow and are referred to as green gold.\nWhite gold alloys can be made with palladium or nickel. White 18kt gold containing 17.3% nickel, 5.5% zinc and 2.2% copper is silvery in appearance. Nickel is toxic, however, and its release from nickel white gold is controlled by legislation in Europe. Alternative white gold alloys are available based on palladium, silver and other white metals, but the palladium alloys are more expensive than those using nickel. High-carat white gold alloys are far more resistant to corrosion than are either pure silver or sterling silver.\nThe Japanese craft of Mokume-gane exploits the color contrasts between laminated colored gold alloys to produce decorative wood-grain effects.\nGold fill is a term used to describe a material that has been laminated with a karat gold before it is made into sheet or wire for use in jewelry making. Karat gold is bonded to a rod or brick of bronze alloy (copper and tin) with silver solder, heat and pressure. The metal is then annealed and rolled into wire or sheet. Gold filled standards require the karat gold layer to comprise 5% of the total weight of the piece. Because gold fill is a bonded metal (as opposed to plated metal), it will not lose its gold appearance through wear or polishing.\nGold fill wears well and gives the look of gold at a fraction of the price. It is a more expensive material than sterling and more difficult to handle in the process, hence the higher cost. Gold fill should be cleaned with a rouge cloth.\nGold plating is a process in which a thin gold layer is electroplated onto a base metal comprised usually of aluminum and tin. Sometimes a coating of laquer is used to protect the plating. Gold plate should not be rubbed with a polishing cloth as the gold layer is very thin and will come off over time. To remove tarnish, wash with mild soap and warm water and pat dry with a soft cloth. For objects which rub against the skin like rings and bracelets, everyday wear is not recommended.\nNiobium is a rare, soft, grey, malleable metal. Niobium can be electrically heated and anodized, resulting in a wide array of colors using a process known as reactive metal anodizing which is useful in making jewelry. The fact that niobium is hypoallergenic also benefits its use in jewelry.\nPalladium has been used as a precious metal in jewelry since 1939, as an alternative to platinum or white gold. This is due to its naturally white properties. The principal use of palladium in jewelry is as an alloy in the manufacture of white gold jewelry.\nAs a pure metal, platinum is silvery-white in appearance, lustrous, ductile, and malleable. Platinum's resistance to wear and tarnish is well suited for making fine jewelry. Platinum finds use in jewelry, usually as a 90-95% alloy, due to its inertness and shine.\nSterling silver is an alloy of silver containing 92.5% by weight of silver and 7.5% by weight of other metals, usually copper. Fine silver (99.9% pure) is generally too soft for producing functional objects; therefore, the silver is usually alloyed with copper to give it strength, while at the same time preserving the ductility and beauty of the precious metal. Other metals can replace the copper, usually with the intent to improve various properties of the basic sterling alloy such as reducing casting porosity, eliminating firescale, and increasing resistance to tarnish. These replacement metals include germanium, zinc and platinum, as well as a variety of other additives, including silicon and boron.\nSometimes called the \"space age metal\", it has a low density and is strong, lustrous, corrosion-resistant with a silver color.\nVermeil is the process of coating a silver object with gold, usually 23-karat gold over sterling silver. Sometimes vermeil is lacquered so that no cleaning should be necessary. Never use a rouge cloth on vermeil. Vermeil should be cleaned by dipping into a solution of warm water and mild detergent. Gently pat dry. Do not rub. Rubbing the surface will remove the gold finish with time. For objects which rub against the skin like rings and bracelets, everyday wear is not recommended as the gold layer will wear off."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:32201e7f-de5f-46c3-9f1b-f58cb40d1f4f>"],"error":null}
{"question":"What are the cognitive benefits of playing sports, and are there risks from head impacts in youth football?","answer":"Playing sports provides several cognitive benefits, including improved ability to process sound by reducing background noise. Athletes' brains show greater ability to dampen neural static and better understand auditory signals, which helps them hear coaches over crowd noise. However, regarding head impacts in youth football, research on 112 players aged 9-18 found that cumulative sub-concussive impacts during a single season were not associated with declines in neurocognitive function. The study measured various outcomes including neuropsychological testing, balance, and symptoms, finding no correlation between cumulative impacts and worsening performance.","context":["The level of high school sports participation is at an all-time high.1 Students who play competitive sports enjoy physical and mental benefits. It reduces the rate of obesity, Type 2 diabetes and high blood pressure. It also improves cardiovascular and pulmonary function and increases the potential that young people will go on to stay physically active throughout their life.2\nResearchers also discovered that athletes who regularly play sports are less likely to use drugs and less likely to smoke cigarettes. Female athletes are 80% less likely to become pregnant during high school than their peers who don’t participate in sports. Students playing sports perform better in the classroom with higher grade point averages, better attendance and a greater chance of going to college.\nIn one survey of 75 Fortune 500 companies, researchers found that 95% of the vice presidents played high school sports. One of the risks of playing full contact sports, though, is a head injury, which experts estimate occurs 300,000 times annually during participation in high school and intercollegiate sports.3\nPlaying Sports Develops Mental Fitness and Sound Response\nMany of the benefits of playing sports are related to mental fitness or mental flexibility. Neurobiologists from Northwestern University focus on this in their studies of the brain’s response to sound.4 By hooking a series of electrodes to the scalp they can record the electrical response to sound and play it through a speaker.\nLead researcher Nina Kraus commented that this methodology gives insight into the health of the nervous system. Her team has found that those who are exposed to language and musical stimulation while growing up are less likely to have neural static or the generation of excess electrical activity.\nOn the other hand, growing up in a musically or linguistically limited environment, the brain may be excessively noisy, which interferes with your ability to understand auditory information. Their results suggest that playing sports gives an athlete’s brain greater ability to turn down background noise.5 Kraus explained:6\n“The brain is hungry for information and it actually creates electrical activity when it doesn’t get enough. But it creates random and staticky activity, which in the end is more of a problem because it gets in the way of making sense of sound.”\nThe researchers used a cross-sectional study design involving 988 student athletes. They evaluated the athletes’ auditory processing by measuring the frequency-following response (FFR) using electrodes applied to the scalp.\nThe FFR was used as it is sensitive to experience, and researchers have found that injury reduces the amplitude. They measured the FFR amplitude of the brain’s response, of the background noise and the ratio between these measures, and found athletes had a larger response than nonathletes, and concluded:7\n“These findings suggest that playing sports increases the gain of an auditory signal by turning down the background noise. This mode of enhancement may be tied to the overall fitness level of athletes and/or the heightened need of an athlete to engage with and respond to auditory stimuli during competition.”\nSports May Dampen a Noisy Brain\nKraus believes the ability to hear direction during competitive sports helps “tune the brain to better understand one’s sensory environment.”8 This gives an athlete the advantage of being able to hear their coach yelling from the sidelines above the noise of the spectators by dampening the background noise.\nDr. Richard Isaacson from the Alzheimer’s Prevention Clinic at Weill Cornell Medical College was intrigued with the results.9 He commented that the researchers demonstrated that athletes enjoyed mental fitness from playing sports, and he expressed an interest in further studies to differentiate between noncontact and contact sports.\nThe Northwestern researchers theorized that within a healthy nervous system, athletes may be able to handle injury and other health problems better than non-athletes. Kraus explained that making sense of what is heard may be one of the most difficult functions of the brain. The pitch, timing and harmonics of sound must be meshed with understanding the meaning within microseconds of having heard it.\nThe study is one of the latest in neural processing sponsored by the National Institutes of Health concerning sound in sports concussions. The hope is to use the analyzation of electrical responses following a concussion to determine when an athlete may be ready to return to full contact sports without an increased potential for greater damage to the brain.\nBrain Benefits From Language and Music\nThe researchers also found since head injuries disrupt auditory processes it may be important to understand how this enhancement may reduce potential brain injury. Kraus said playing a musical instrument or learning a new language can help strengthen the brain’s ability to process the signal, yet it does not affect the background noise. In other words:10\n“They all hear the ‘DJ’ better but the musicians hear the ‘DJ’ better because they turn up the ‘DJ,’ whereas athletes can hear the ‘DJ’ better because they can tamp down the ‘static.”\nThe obvious benefit of being bilingual is the ability to communicate with people from around the world. Most people in the U.S. believe learning a second language is valuable, though not necessarily essential. However, as a challenging task for your brain, learning a new language is beneficial.\nWhen evaluated, those who are bilingual have more gray matter involved in cognition and have enhanced cognitive control, greater mental flexibility and a better ability to handle tasks that involve switching and conflict monitoring. In the elderly, this may offer greater advantages since bilingual older adults have a greater cognitive reserve that helps the brain cope with pathology.\nIn those who currently are experiencing a neurological disorder such as Alzheimer’s, the simple act of listening to music can help them reconnect with the world around them. Some of this benefit appears to be rooted in familiarity. Imaging studies show when you listen to","Cumulative Subconcussive Impacts in a Single Season of Youth Football Not Associated With Declines in Neurocognitive MeasuresCumulative Subconcussive Impacts in a Single Season of Youth Football Not Associated With Declines in Neurocognitive Measures https://pediatricsnationwide.org/wp-content/uploads/2021/03/AdobeStock_53945526.jpg-football-1024x575.jpg 1024 575 Abbie Roth Abbie Roth https://pediatricsnationwide.org/wp-content/uploads/2021/02/062019ds5821_abbie-profile-new.jpg\n- October 15, 2018\n- Abbie Roth\nIn an investigation of head impact burden and change in neurocognitive function during a season of youth football, researchers find that sub-concussive impacts are not correlated with worsening performance in neurocognitive function.\nEach year, more than 3 million children in primary and high school play tackle football in the United States. Growing concern about the possible negative effects of repetitive sub-concussive head impacts led to an increased number of physicians and parents who counsel against youth participation in full-contact sports.\nA research team, led by Sean Rose, MD, pediatric sports neurologist and co-director of the Complex Concussion Clinic at Nationwide Children’s Hospital, followed 112 youth football players age 9-18 during the 2016 season in a prospective study.\n“When trying to determine the chronic effects of repetitive sub-concussive head impacts, prospective outcomes studies are an important complement to the existing retrospective studies,” says Dr. Rose. “In this study of primary school and high school football players, a battery of neurocognitive outcomes tests did not detect any worsening of performance associated with cumulative head impacts.”\nThe pre- and post-season assessments used to measure outcomes included:\n- Neuropsychological testing\n- Symptoms assessment\n- Vestibular and ocular-motor screening\n- Balance testing\n- Parent-reported ADHD symptoms\n- Self-reported behavioral adjustment\nSensors placed in the helmets recorded sub-concussive head impacts during practices and games. Researchers added the impact g-forces to yield a cumulative impact measure. According to the study, cumulative impact did not predict changes (from pre-season to post-season) in any of the outcome measures.\nAdditionally, Dr. Rose notes, having sustained one or more concussions prior to entering the study was not associated with worse pre-season testing.\nIn their secondary analysis, they found that younger age and reported history of attention deficit hyperactivity disorder (ADHD) predicted score changes on several cognitive testing measures and parent-reported ADHD symptoms. Additionally, a reported history of anxiety or depression predicted changes in scores of symptom reporting.\n“We expected repetitive impacts to correlate with worsening neurocognitive function, but we found that sub-concussive head impacts sustained over the course of a single season were not associated with neurocognitive functional outcomes. And also surprising, sustaining isolated high g-force impacts was also not associated with worse outcome,” says Dr. Rose. “The lack of a significant association may reflect the need for longer follow up – so we are continuing to follow kids across multiple seasons.”\nThis publication is the first analysis in a four-year prospective cohort study. Dr. Rose will be presenting data from the second year of the study at the upcoming Child Neurology Society meeting in mid-October. The team is currently collecting data for a third year.\nRose SC, Yeates KO, Fuerst DR, Ercole PM, Nguyen JT, Pizzimenti NM. Head impact burden and change in neurocognitive function during a season of youth football. The Journal of Head Trauma Rehabilitation. [Epub ahead of print.]\nImage credit: Adobe Stock\nAbout the author\nYou might also like\nUnique Approach Helps Child Neurology Residents Improve Communication SkillsUnique Approach Helps Child Neurology Residents Improve Communication Skills https://pediatricsnationwide.org/wp-content/themes/corpus/images/empty/thumbnail.jpg 150 150 Natalie Wilson Natalie Wilson https://pediatricsnationwide.org/wp-content/uploads/2021/06/Natalieheadshot3-2.png\nUnderstanding the Social Neural NetworkUnderstanding the Social Neural Network https://pediatricsnationwide.org/wp-content/uploads/2021/03/AdobeStock_77919065-kid-brain-header-1024x575.gif 1024 575 Natalie Wilson Natalie Wilson https://pediatricsnationwide.org/wp-content/uploads/2021/06/Natalieheadshot3-2.png\nLong-Term Treatment of Pediatric Chronic Inflammatory Demyelinating Polyneuropathy with Pulse Oral Corticosteroid TherapyLong-Term Treatment of Pediatric Chronic Inflammatory Demyelinating Polyneuropathy with Pulse Oral Corticosteroid Therapy https://pediatricsnationwide.org/wp-content/themes/corpus/images/empty/thumbnail.jpg 150 150 JoAnna Pendergrass, DVM JoAnna Pendergrass, DVM https://pediatricsnationwide.org/wp-content/uploads/2021/03/pendergrass_01.jpg"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:3ce5e368-bde5-42da-8388-e567814fc213>","<urn:uuid:eeb137cf-fa26-4ebd-8177-ae2ce3e10c30>"],"error":null}
{"question":"What are the key differences between high and low frequency metal detection in terms of target sensitivity and depth?","answer":"Low frequency metal detection has longer wavelengths that penetrate the ground more deeply and work better for detecting high conductivity targets like silver. However, it struggles with small targets and low conductivity metals like gold and iron. In contrast, high frequency detection uses shorter wavelengths that excel at finding small objects like tiny gold nuggets and low conductivity targets, but achieves less depth and is more sensitive to ground mineralization interference. Most hobby detectors strike a balance by operating in the 6-8 KHz range to optimize both depth and sensitivity.","context":["Metal detecting is used by different professionals and hobbyists for various purposes. The process can help in finding hidden metal objects, gold, coins, and jewelry below the ground’s surface. A metal detector is the main tool used for that purpose. The good part is that you do not even need to know how a metal detector works to use it correctly.\nThe real simple answer is the metal detector transmits an electromagnetic field from the search coil at the end into the ground, and any metal objects, gold, coins, or jewelry within that field will send a tone or a signal back to the detector. That tone sent back will have its own unique properties depending on what kind of metal it is.\nMost people that use a metal detector just have a very basic knowledge of how it works. It is not necessary to know the mechanics of the device if you want to be a successful operator, however, a better understanding can help you to master the skills and achieve maximal results. That’s why you should definitely continue reading.\nWho Can Use a Metal Detector?\nThe simple answer is virtually anyone can pick up one and within a short time frame can learn how to use it and find hidden objects below the ground. Metal detecting is an activity performed by military and law enforcement. For example, they usually use metal detectors to discover mines and bombs hidden below the ground. Also, they might search for any other potentially dangerous item or material in the same way. A metal detector is also a popular tool of various archeologists who use it to find historically important artifacts and items that stay below the ground.\nAlso, many adventurers and hobbyists use the tool on beaches, yards, and parks in search of lost people’s valuable items, such as gold, pieces of jewelry, and coins for example. These people should have basic skills about this tool in order to operate it successfully. Metal detecting is, fortunately, simple to learn and master so virtually anyone can gain those skills. Still, very few of those people really know the details and inner workings of the device.\nDevelopment of Metal Detectors Throughout History\nThe development of metal detectors started at the end of the 19th century by different engineers and scientists. They began using their expanding knowledge about electrical theory trying to create a device with the capabilities of locating and discovering metal items. The intention was to support the miners who searched for ore-bearing rocks in different areas in the country. The scientists wanted to accelerate the exploration of various areas and maximize the results with an efficient tool of that kind.\nFirst metal detectors were pretty basic and not so ideal. They used a lot of energy so very few batteries could support their work in a normal way. That’s one of the reasons why these devices worked in a limited range.\nGustave Trouve was one of the first inventors of a metal detector. In 1874, he achieved success by producing a handheld machine for detecting and locating metal items in human bodies. This tool was mostly used to discover bullets in the bodies of patients who were wounded.\nAlexander Graham Bell was inspired by the success of this inventor so he created a similar device attempting to detect similar things and improve the medicine capabilities of that time. James A Garfield, an American president, was the first patient who was subjected to the investigation with that newly developed device. The bullet was detected in his chest, and the machine worked properly. However, it was not possible to complete the operation by locating the bullet precisely because the device was confused by the metal construction of a bed that the president used during the exploration.\nThese were some of the first devices of this kind, however, further, development continued throughout the 20th century. Gerhard Fisher, for example, created a device based on radio-direction exploration in 1920. The machine was extremely successful and the most powerful metal detector ever made. However, the inventor noticed particular anomalies at locations with ore-bearing rocks.\nFisher concluded that it would be achievable to develop a device with abilities to detect metal objects utilizing a search coil echoing a radio frequency if metal distorted a radio beam. The development of this product was lasting for several years and Fisher applied for the registration of a first metal detector patent of this type.\nThe application was accepted by the responsible authorities, and the inventor became the first scientist with a granted patent of that type. However, Shirl Herr was the first engineer who applied for the metal detector patent almost one year earlier than Fisher. Still, his application waited for more than four years to get accepted by the authorities and that happened in July 1928.\nShirl Herr was an assistant of Italian Prime Minister Benito Mussolini in discovering metal artifacts and items remaining from the period of Emperor Caligula. The exploration took place at the bottom of Lake Nemi in Italy in 1929.\nHerr’s device had, however, quite a wider purpose, and it was used by various investigation expeditions during the early 20th century. Admiral Richard Byrd was, for example, one of them. He used the machine during the Second Antarctic Expedition that occurred in 1933. The main intention was to discover items and objects lost by previous expeditors in the area. Herr’s machine was efficient at a depth of 8 feet.\nIt was clear the detector is a very practical and useful tool that could be used for various explorations. The military noticed the capabilities of the machine so they started using it as a mine detector on the battlefield. Lieutenant Jozef Stanislaw Kosacki was the first one who performed such a task with the Herr’s detector and from then on the Polish army used the device for that purpose.\nHowever, the device still needed some modifications and improvements that happened later on. For example, the detector units were pretty heavy so they took a lot of energy during the operations. They also needed a lot of battery power so separate packs were a necessity in order to work continuously.\nThe further development of metal detectors has continued to this very day, and at a moment, there are pretty efficient models on the market. They have excellent specifications and provide great results. It is good to expect the development of these devices will proceed further and each new model will be stronger than the current ones.\nHow Detectors Work\nThese devices have the capability to transmit an electromagnetic field directly into the ground. The metal objects then become energized, and they start creating and retransmitting their own small electromagnetic fields. The search coil receives the impulses of retransmitted signal, and that’s the moment when it sends a notification to the user in a form of a specific sound or tone. Metal detectors have different kinds of settings and features so they might locate only some specific items developed of some particular type of metal while avoiding all others.\nThat’s a very suitable option because it gives a researcher a chance to focus on valuable discoveries only. The ground is always full of trash, so it is good to ignore such objects, but ignoring the trash can also lead to not finding something valuable. For example old pull tabs are in the same tone range as a gold ring, so if you do not ‘dig for trash’ you could be passing up a gold ring.\nMetal detectors are composed of several important parts and each of those components has some specific purpose.\nThe Control Box is, for example, the place where the electronics are housed. This part of the detector is responsible for transmitting the signal to potential objects and receive a signal from those targets in a form of a response. That means it is the brain of the device in charge of the main part of work.\nThe Search Coil has the purpose of sending the electromagnetic field to the ground and catch the field of a targeted object. This part of the metal detector acts as a helping component of the control box.\nThe Target is any metal object that might be a potential find depending on a researcher’s selection. People usually want to discover valuable items so you may want to limit your search to maximize efficiency and save your time.\nUnwanted Targets are normally considered to be ferrous objects, which just means they are attracted to magnets. These are normally things like nails, but there are items that are non-ferrous that are attracted to magnets like bottle caps.\nThe Receive Electromagnetic Field is technically not part of the metal detector, but it is the signal that is sent back by the item, thus giving you a tone once it reaches the control box.\nThe detector’s frequency is one of the most important aspects when it comes to determining the efficiency of the device. It is usually a rule that high frequency is more sensitive to small objects while low frequency is far better for discovering big objects hidden in the ground.\nDifferent metal detectors might use different frequency technologies and it depends on the producer’s choice. There are, however, some new and advanced models that transmit multiple frequencies at once so they may successfully discover all types of targets regardless of their sizes and other related aspects.\nImportant Features and Settings\nGround Balance is also one of the important features that every good metal detector possesses. As you know, there are different kinds of grounds and some of those might be overly mineralized by various nature influences. For example, such a ground can contain a lot of salts like wet beach sand or red earth. These minerals are, however, not very helpful when it comes to metal detecting because they can mask and hide various small objects and a metal detector would be unable to locate them in those conditions.\nFortunately, the Ground Balance feature gives you the possibility to correct this issue by removing the ground signals. When you do this, the interference stops, and the metal detector can fully focus exclusively on the targets. The settings might be different depending on a particular model you currently use, and there are generally three types of Ground Balance.\nManual Ground Balance is the first one, and it gives you a chance to set up that option manually. It is an efficient type that maximally reduces the sound of mineral soil.\nAutomatic Ground Balance provides an option to perform the same task automatically. That’s definitely a hassle-free procedure so it is quicker and more accurate than the manual settings.\nTracking Ground Balance is the third type on the list, and it gives a possibility to the device to adjust appropriate settings in accordance with the ground’s conditions. It means the researcher does not need to do anything for themselves because the detector is capable of resolving a potential issue on its own.\nOne thing to keep in mind about ground balancing your metal detector, normally, but not always, you have to do a ground balance every time you turn your metal detector on. Also when you move to a different type of ground it is best to ground balance again. For example, if you are at a park and you move from the sand around the playground to the grass in the park it is best to ground balance in the new area, or moving from the grass to a beach, you will need to run a ground balance again.\nDiscrimination is also a very useful feature that helps you improve your performance and get the best results. The ground may contain a lot of potential targets, however, some might be totally unusable for the purpose. This feature exists to help you avoid unwanted targets by setting appropriate adjustments. It is necessary to mention, there are four types of discrimination, and they are explained better further in the text.\nVariable Discrimination is the first one, and it is the easiest sort of discrimination that allows you to use a control knob to make appropriate adjustments.\nIron Mask is the second type, and it is usually used by detectors that search for gold. The feature gives them an opportunity to avoid the iron trash from the ground.\nNotch Discrimination is third on the list, and it serves to allow or reject a potential target based on their characteristics.\nSmartfind is the last discrimination type, and it is the most advanced sort of all four different types. The feature provides a visualization of targets on a small display so it is far easier to make the right selection during detection.\nDifferent Types of Metal Detectors\nOf course, it is necessary to understand not all metal detectors have the same features and capabilities. That’s because there are different types of metal detectors, and the newest models are typically the most efficient and best. Some of those sorts are very common on the market, while others might appear as a real rarity. Very low frequency, multi-frequency, and pulse induction metal detectors are the three most common types today.\nVery low-frequency metal detectors are good when it comes to detecting bigger targets in the ground. They might have a display where the objects are visualized, but it depends on the particular model you use. These detectors are usually affordable, easy to use, lightweight, and with a strong battery.\nMulti-frequency detectors are even better. They can successfully discover both large and small objects so they are definitely a better solution when it comes to professional use.\nPulse induction metal detectors are, however, the best on the list. They can also successfully identify targets of various sizes and are totally immune to ground mineralization. They allow deep detection and that’s why they are excellent for gold hunting.\nSo as you can see, you can fairly easily use your metal detector without really knowing all the details of how it actually works. At the same time, some of the higher-end metal detectors have many different features and settings that can be used and adjusted to fine-tune your searches.\nIt is always best to read through your manual for the metal detector that you are using to get familiar with its specific settings. Then practice, practice, practice. Once you have dug about 100 targets out of the ground you should be at a point where you are very familiar and comfortable with your particular metal detector, its settings, and tones.","What does frequency mean when metal detecting?\nA common definition of frequency is as follows: The number of waves per unit of time measured in Khz or kilohertz. In a metal detector this is the number of electronic waves sent into the ground to detect metal.\nExample: 10 Khz means your detector will send and receive 10 000 times per second.\nWhy is Frequency important?\n- Both Pulse induction and beat frequency oscillator type metal detectors use frequency of electronic fields or pulses sent into the ground.\n- Different frequencies have different advantages and disadvantages when metal detecting.\n- Metal detector frequencies range between 3 – 100 Khz as a general rule.\n- Have longer wavelength.\n- Gets greater depth as long waves penetrate the ground more easily.\n- Better for detecting high conductivity targets like silver.\n- Not good for finding smaller targets.\n- Not good for low conductivity targets like gold or Iron.\n- Have shorter wavelengths.\n- Great for detecting small objects like tiny gold nuggets.\n- Better for low conductivity targets like gold and Iron.\n- Less depth is achieved than low frequency.\n- Higher accuracy, closer to the surface.\n- More sensitive to ground mineralisation interference.\nMost hobby detectors use a middle ground to try and get the best of both frequencies in the Goldilocks zone around 6 – 8 Khz for the best depth and sensitivity trade off. Some detectors even let you manually set the frequency on your detector. Others even use multiple frequencies at one time.\n2 types of Frequencies:\nSingle Frequency operation: Also called continuous wave has only one frequency selection. This is usually found in the Beat Frequency Oscilators like the Garrett Ace 250. This type is most often found in Entry level machines.\nMultiple or Dual Frequency operation: Some more advanced metal detectors make use of more than one frequency at the same time. Examples of this can be found in the Minelab Excalibur II, Etrac and CTX 3030. This is often called full band spectrum frequency technology allowing the user to get the best depth and accuracy at the same time. This is more often found on Pulse induction machines.\nMetal Detector Frequency Comparison Chart:\n|Garrett||Ace 150||Single||6.5 Khz||Beat frequency.|\n|Garrett||Ace 250||Single||6.5 Khz||Beat frequency.|\n|Garrett||Ace 350||Single||8.25 Khz||Beat frequency.|\n|Garrett||Sea Hunter||Multiple (manually adjustable)||7.5 Khz||Pulse induction.|\n|Garrett||AT Pro||Single||15 Khz||Beat frequency.|\n|Garrett||Infinium||Multiple (manually adjustable)||7.3 Khz||Pulse induction.|\n|Garrett||GTI 2500||Multiple (manually adjustable)||7.2 khz||Pulse induction and Beat frequency.|\n|Garrett||AT Gold||Single||18 Khz||Beat frequency.|\n|Minelab||GPX||Multiple (manually adjustable)||Multiple specially calibrated to conditions selected.||Pulse induction.|\n|Minelab||Eureka Gold||Triple||6.4, 20, 60 Khz||Pulse induction.|\n|Minelab||Xterra 305||Single||7 Khz or 18.5 Khz with a different coil.||Pulse induction.|\n|Minelab||Xterra 505||Single||7 Khz or 18.5 Khz with a different coil.||Pulse induction.|\n|Minelab||Xterra 705||Single||7 Khz or 18.5 Khz with a different coil.||Pulse induction.|\n|Minelab||Xcalibur 2||Multiple (17 concurrent)||1,5 – 25,5 Khz||BBS Broad band spectrum Pulse induction.|\n|Minelab||Etrac||Multiple concurrent||1,5 – 100 Khz||FBS Full band spectrum Pulse induction.|\n|Minelab||CTX 3030||Multiple concurrent||1,5 – 100 Khz||FBS Full band spectrum Pulse induction.|\nFrequency and Conductivity.\nMetal objects in the ground all conduct electricity better or worse. This may be plotted on a scale from Ferrous iron on the low side to Non Ferrous silver on the high side with gold and foil somewhere in the middle. This is often the basis for the LCD screen layout as you can see on the Garrett Ace.\nHobby metal detectors can tell the type of metal by how well it conducts electricity of the frequency pulses it sends into the ground (usually the speed that the signal sent back decays over time.) This allows the modern hobby metal detector to discriminate between metals and not only indicate the type of metal it thinks it’s found but also allows you to screen out certain metals like iron if you want to stop the detector from picking them up.\nWhether it is a simple single frequency machine like the Ace 350 or a complex machine like the Etrac using 1 – 100 Khz at the same time frequency is an important consideration when making your purchase.\nWe hope this helps your choice and understanding of a hobby metal detector and frequency!\nFind this page useful? Hit the g+ button bellow 🙂"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:f25a4197-18e7-4009-9f49-75b79599819e>","<urn:uuid:5b7e9660-2b30-4969-be4f-d4bd3dfdf7cc>"],"error":null}
{"question":"SANS和CompTIA Security+认证哪个入门门槛更高？","answer":"SANS的入门门槛明显更高。SANS课程费用非常昂贵,只有少数人能够参加,且课程资料受到严格保护。相比之下,CompTIA Security+认证没有前置经验要求,任何人都可以直接报名考试,费用也相对较低(约300美元),还可能获得学生折扣。Security+认证只需要通过一次考试即可获得。","context":["موسسه SANS یکی از معتبرترین و بزرگترین مراکز آموزشی دوره های امنیت سایبری در دنیا می باشد. موسسه SANS دوره های خود را در گرایش های مختلف اعم از تست نفوذ، جرائم رایانه ای، امنیت شبکه و بازرسی سیستم ها ارائه می دهد. مدارک شرکت SANS را GIAC صادر می کنند.کمتر کسی نسبت به دوره های شرکت SANS اطلاعات کافی را دارد، زیرا این دوره های با حفاظت بسیار زیادی از سوی این شرکت برگزار می شود و در برخی موارد تنها فایل های صوتی کلاس در اختیار دانشجویان قرار می گیرد. البته این موضوع را هم در نظر داشته باشید که دوره های شرکت SANS از نظر هزینه ای بسیار بالا بوده و افراد محدودی می توانند در دوره های SANS شرکت نمایند.\nمجموعه آموزشی SANS SEC573: Automating Information Security with Python (v2017 PDF and USB)\nحجم مجموعه : 3.75 گیگابایت\nرمز فایل: technet24.ir\nAll security professionals, including Penetration Testers, Forensics Analysts, Network Defenders, Security Administrators, and Incident Responders, have one thing in common. CHANGE. Change is constant. Technology, threats, and tools are constantly evolving. If we don’t evolve with them, we’ll become ineffective and irrelevant, unable to provide the vital defenses our organizations increasingly require.\nMaybe your chosen Operating Systems has a new feature that creates interesting forensics artifacts that would be invaluable for your investigation, if only you had a tool to access it. Often for new features and forensics artifacts, no such tool has yet been released. You could try moving your case forward without that evidence or hope that someone creates a tool before the case goes cold…or you can write a tool yourself.\nOr, perhaps an attacker bypassed your defenses and owned your network months ago. If existing tools were able to find the attack, you wouldn’t be in this situation. You are bleeding sensitive data and the time-consuming manual process of finding and eradicating the attacker is costing you money and hurting your organization big time. The answer is simple if you have the skills: Write a tool to automate your defenses.\nOr, as a Penetration tester, you need to evolve as quickly as the threats you are paid to emulate. What do you do when “off-the-shelf” tools and exploits fall short? If you’re good, you write your own tool.\nSEC573.1: Essentials Workshop with pyWars\nSEC573.2: Essentials Workshop with MORE pyWars\nSEC573.3: Defensive Python\nSEC573.4: Forensics Python\nSEC573.5: Offensive Python\nSEC573.6: Capture the Flag\nOther Courses People Have Taken\nWhat You Will Receive\nThis Course Will Prepare You To\nGood scripting skills are essential to professionals in all aspects of information security. Understanding how to develop your own applications means you can automate tasks and do more, with fewer resources, in less time. As penetration testers, knowing how to use canned information security tools is a basic skill that you must have. But knowing how to build your own tools when the tools someone else wrote fail is what separates the great penetration testers from the good ones. This course is designed for security professionals who want to learn how to apply basic coding skills to do their job more efficiently. The course will help take your career to the next level by teaching you the essential skills needed to develop applications that interact with networks, websites, databases, and file systems. We will cover these essential skills as we build practical applications that you can immediately put into use in your penetration tests.","It does not matter what cybersecurity certification is done, the question of if the certification is really worth the effort and time would inevitably arise.\nThe CompTIA Security+ certification is no doubt one of the most popular certifications out there, and it is no wonder that numerous prospective certification applicants ask if the Security+ certification is truly worth it.\nSo, is the Security+ certification worth it? A short answer would be yes.\nThe CompTIA Security+ certification is definitely worth the time and effort if you have intentions of pursuing a career in cybersecurity, or if you are interested in adding security qualifications to your CV.\nIt is also useful if you require knowledge of network security for your current role.\nWhile this might sound pretty straightforward, it still doesn’t help answer what exactly it is about Security+ that makes the effort put in worthwhile, at least compared to the many other certifications.\nThe aim of this article is to delve deeper into the numerous aspects of Security+ to find out why it is a great certification to attain.\nWhat is CompTIA Security+ Certification?\nThe Security+ certification is an IT security certification which develops your expertise and skills in network and computer security domains such as network security, IT risk management and cybersecurity. It should be seen as a lower level cybersecurity certification which covers the following topics:\n- How to recognise risks in a connected network\n- How to utilise tools, techniques and technologies to safeguard hardware and software assets from hostile parties and hackers.\nA Security+ certification can help you gain jobs in the network security and cybersecurity fields, with roles such as information security specialist, penetration tester, security engineer, security administrator, and network administrator.\nWhat are the skills measures by the Security+ certification?\nThe integral skills that the CompTIA Security+ certification validate include threat analysis, configuration and installation of secure applications, cryptography, risk mitigation techniques, understanding types of attacks and proffered solutions, cybersecurity law and policy, network protocols and layers, mobile security, forensics and architecture design with maximum risk mitigation.\nEvery one of the skills covered is bound to make you knowledgeable of the fundamentals to the point where you are able to recognise the possible risks found in cybersecurity.\nYou will also understand the required solutions that every organisation can utilise to safeguard their data, hardware and software assets.\nGiven that the Security+ certification comes with performance and lab-based questions, you have the opportunity to gain hands-on introspect and experience in discovering solutions to complicated problems associated with cybersecurity and modern networks.\nYou do not need experience to attempt the Security+ exam\nWhen you climb up the ranks in the cybersecurity industry, you will discover that quite a number of the renowned certifications come with prerequisites like having a particular number of years of recorded experience.\nIt could also be that you have to have completed a sanctioned training before you can even take the CompTIA Security+ certification exam. There are also certain exams that require you to be sponsored by an individual that has already been certified.\nA wonderful benefit that the Security+ certification offers is that there really is no previous training or experience required before you are able to take the exam. You can simply select a date for the exam and take it.\nWhat this means, however, is that you have to be adequately prepared before taking the test. Nevertheless, this is a wonderful benefit for individuals just entering the computer networking or cybersecurity industry, as they are able to attain a certification in a time when adequate roles for job opportunities can be scarce and college degrees can take longer to finish.\nThe CompTIA Security+ certification is something that can be done in about 60 days, as long as you prepare and study well.\nSecurity+ meets 8570 Requirements and is approved by the United States Department of Defence\nThe US DoD has a directive known as 8570 which offer guidance on how its direct employees and those working on behalf of the Department of Defence in information assurance or cybersecurity tasks have to be trained and gain certification.\nThis directive ensures that Security+ is the foundational certification for the majority of the work. What this means is that the Department of Defence understands and recognises the value and validity of the Security+ certification. This is clearly evident that it actually requires it for particular roles.\nWhen a certification is recognised and cleared by such an important part of the national governments, contractors and vendors are likely to reap the rewards of having a CompTIA Security+ certification.\nSecurity+ training can be offered almost everywhere\nThe Security+ certification is so prevalent that the majority of universities and colleges offer training for the exam at certain times in their academic calendar. It is even possible to use online training resources.\nMoreover, there are numerous resources for online courses available, so prospective Security+ candidates can prepare and study for the exam. The quality offered by these courses tend to vary, so it is important to pay close attention to the training resources you utilise.\nNow that the wonderful features that make the CompTIA Security+ certification a worthwhile endeavour, it is now time to consider other questions you might have concerning this certification.\nCompared to numerous other certifications, Security+ is cheaper\nCertifications are not cheap by a long shot, and certain cybersecurity-related certifications tend to cost a minimum of $500, which is quite a large sum of money to part with if you are not sure you are going to pass.\nThankfully, CompTIA seems to understand this and has ensured that its prices are reasonable, particularly for Security+ certification which costs a bit more than $300.\nThere are also discounts that can be had as CompTIA lets students take the Security+ certification for about $200 in certain instances.\nIs it possible to retake the Security+ if you don’t pass the first time?\nCompTIA enables certification candidates to retake the Security+ exam if you don’t pass. It even offers the opportunity to purchase a testing voucher with a retake option.\nNevertheless, if you are unable to pass the exam on the second try, you will be temporarily blocked from taking the exam. Generally, this block lasts about 2 weeks.\nHow much time do you have to devote to studying for the Security+ certification?\nHow much time you need to devote to studying and the effort required typically differs from one individual to the next.\nIt depends on your experience and knowledge, however, a great yardstick is typically around 50 hours of devoted study, before you are truly ready to take the Security+ certification exam.\nYou only have one exam to take with the Security+ certification\nThis is a great benefit of this certification. You only require a single exam. Certain mid to high-level certifications like Microsoft’s MCSA or Cisco’s CCNP requires numerous exams.\nWhile there is nothing wrong with that, those multiple exams do not necessarily equal a large increase in either career prospects or salary. This can be a negative considering just how much work is required for them.\nWith a certification that has just one exam like the CompTIA Security+, you only need to prepare for a single exam and once you pass it you are certified. That makes it a much more enticing prospect to take on.\nWill having a Security+ certification instead of the Network+ raise eyebrows in the job market?\nQuite a lot of people become concerned about having certain certifications and skipping others.\nIn the majority of cases, employers do not seem to question why a prospective candidate doesn’t have a particular certification.\nThe lack of a supposed lower certification is not something that is held against a prospective employee.\nThe majority of employers actually value certifications, particularly higher-level ones and they tend to presume the candidate in questions is knowledgeable enough in the lower aspects.\nDo I require experience to successfully take the Security+ certification?\nAs stated earlier in the article, it is possible to take the exam at any point, with no need for previous work experience. However, this does not mean it should be the case.\nIf you are interested in taking the Security+ exam and be successful without any previous cybersecurity-related work experience, it is imperative that you have a robust knowledge of computer networking.\nThis knowledge should include the understanding of protocols, the OSI model, port numbers as well as the functions of networking hardware components like routers, firewalls and switches.\nWhat this means is that you have to be particularly strong on the majority of CompTIA’s Network+ topics before you attempt to take the Security+ exam.\nThis, however, doesn’t mean you have to gain this knowledge from an actual cybersecurity-related job, or that you need to first take the Network+ certification exam.\nWhat it does mean is that you will have to show proficiency in these areas, before you can move onto the topics Security+ has."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:753b8e16-bf8e-44ec-9630-2b65f205fc56>","<urn:uuid:e3a92fdb-7b05-45a0-a561-d03b1e8b521a>"],"error":null}
{"question":"How do modern psychiatric practices impact individual liberty, and what role does digital media play in mental health stress relief among teenagers?","answer":"Modern psychiatric practices can threaten individual liberty through practices like civil commitment and involuntary institutionalization, with psychiatric interventions sometimes being used as a coercive arm of the state. Regarding digital media and stress relief, surveys indicate that 58% of teens use screens for stress relief, with social media and texting used by almost half of teenagers. However, this high media usage (averaging 9 hours daily for US teens) can have negative consequences, including cyberbullying (affecting 23% of teens, especially girls), 'Facebook depression,' sleep disruption, and reduced family communication time, potentially contributing to increased stress and anxiety rather than effectively relieving it.","context":["Genre(s): Philosophy, Law, Psychology, Psychiatry\nSzasz further believes that anyone brought to trial for a criminal offense should be allowed to stand trial instead of, as sometimes happens, being submitted to a pretrial psychiatric examination and then being committed to a mental institution. In fact, he would have the plea of insanity abolished. Nor does he accept dangerousness to oneself as a legitimate basis for institutionalization. He writes: \"In a free society, a person must have the right to injure or kill himself.\" As for dangerousness to others, Schur notes that Szasz expounds on those not incarcerated who are equally as dangerous to others, and cites drunken drivers as one example. Schur writes: \"A person's 'dangerousness' becomes a matter for legitimate public control, Szasz argues, only when he actually commits a dangerous act. Then he can be dealt with in accordance with regular criminal law.\"\nOther psychiatrists have called his work \"reckless iconoclasm,\" \"reprehensible,\" and \"dangerous.\" Lawyers, including Arthur Goldberg, praise him. His sole concern, says Schur, is the protection of the individual. Szasz believes that \"the poor need jobs and money, not psychoanalysis. The uneducated need knowledge and skills, not psychoanalysis.\" Though his arguments are often stated in their extreme forms, Schur believes that Szasz \"quite probably . . . has done more than any other man to alert the American public to the potential dangers of an excessively psychiatrized society.\"\nOf his own work Szasz says: \"I have tried to make two separate and yet connected points. The first point is that not only is mental illness not 'like any other illness,' as conventional wisdom now has it, but that mental illness does not exist: the term is a metaphor and belief in it and its implications is a mythology - indeed, it is the central mythology of psychiatry. The second point is that as a profession and as a social enterprise, psychiatry is neither a science nor a healing art but is rather a powerful arm of the modern nation state. The paradigmatic functions of the psychiatrist are inculpating and imprisoning innocent persons, called 'civil commitment,' and exculpating guilty persons and then often imprisoning them too (ostensibly for the 'treatment' of the illness that 'caused' their criminal conduct), called the 'insanity defense' and 'insanity verdict.'\n\"On conceptual, moral and political grounds I oppose these and all other coercive uses of psychiatry. Involuntary psychiatry is an enemy of liberty and responsibility. Morally and legally the only sexual relations we now regard as legitimate are those between consenting adults. Similarly, we should regard only psychiatric relations between consenting adults as morally and legally legitimate.\"\nLiberty and responsibility are two sides of the same coin. No policy - public or private - can increase or decrease one without increasing or decreasing the other. Human behavior has reasons, not causes.\nWho is Thomas Szasz? He is Professor of Psychiatry Emeritus at the State University of New York Health Science Center in Syracuse, New York, Adjunct Scholar at the Cato Institute, Washington, D.C., author and lecturer. His classic The Myth of Mental Illness (1961) made him a figure of international fame and controversy. Many of his works - such as Law, Liberty, and Psychiatry, The Ethics of Psychoanalysis, - are regarded as among the most influential in the 20th century by leaders in medicine, law, and the social sciences.\nReviewing Law, Liberty, and Psychiatry in the American Bar Association Journal, former Associate Justice of the Supreme Court of the United States Arthur J. Goldberg wrote: \"Dr. Szasz makes a real contribution by alerting us to the abuses - existing and potential - of human rights inherent in enlightened mental health programs and procedures. He points out, with telling examples, shortcomings in commitment procedures, inadequacies in the protections afforded patients in mental institutions and the dangers of over-reliance on psychiatric expert opinion by judges and juries.\" Charles D. Aring, M.D., Professor of Neurology, University of Cincinnati, praised the book in these words: \"One of the most important statements since the publications of Freud. Law, Liberty and Psychiatry is likely to rank among the classics of psychiatry.\" In an introduction to his writings and thought, Professors Richard E. Vatz and Lee S. Weinberg wrote (1983) \"Throughout his distinguished career . . . Thomas S. Szasz has steadfastly defended the values of humanism and personal autonomy against all who would constrain human freedom with shackles formed out of conceptual confusion, error, and willful deception.\"\nJohn Leo, social science editor for U.S. News & World Report, wrote in 1993 that \"No one attacks loose-thinking and folly with half the precision and zest of Thomas Szasz.\" Paul Roazen, author of Encountering Freud, wrote in 1993 that \"Thomas Szasz remains unique among contemporary observers of the social, ethical, and political implications of psychiatry: every argument he makes, and each word he chooses, are deserving of our closest attention.\"\nSee the entire catalog of books online by Thomas Szasz.","Hansa Bhargava MD FAAP\nStaff Physician, Children's Healthcare of Atlanta\nMedical Editor, WebMD\nPediatricians are seeing more and more teens suffering from stress. Whether they are complaining of it or having somatic symptoms such as headaches and stomach aches, it seems that stress and anxiety are on the rise. We know that over scheduling, homework, and the pressures of getting into college can contribute to this. But can media also affect it? Is screen time and media a stressor or a remedy for stress?\nIn a recent WebMD survey published in their Teens and Stress report, 54% of teens were stressed according to parents. Interestingly, 40% of parents turned to the screen for family stress relief while 58% of teens did. Social media and texting was used as stress relief by almost half the teens. This is on the heels of the Common Sense Media survey reporting that US teens were using media for 9 hours a day. Other recent reports have shown that 94% of teens with mobile devices are online daily with many online constantly.\nSo it seems that stress is on the rise and media use is on the rise. Although there may not be a direct relationship, some real issues impact stress and anxiety. Consider this: 23 % of teens report cyberbullying, especially girls. There have been reports of “Facebook depression” and loneliness, as kids who aren’t in social media conversations may feel left out. Other negative consequences can also have an impact: many teens are in front of a screen late at night or ‘sleep text’, both of which can contribute to lack of sleep, which in turn can decrease focus and potentially cause irritability and depression. And last but certainly not least, what about the time media consumes?\nTime spent on media is time often not spent communicating with family. Lately, when I’ve gone into a restaurant, I’ve observed that as soon as a family sits down, everyone pulls out a mobile device. No one is really talking. So even the short amount of time not doing homework, playing soccer, or at school is being compromised. Psychologists, community leaders and experts have long reported that family time can contribute to less depression, less anxiety, better academic performance and generally happier kids. But what if that family time is on media??\nAs the AAP reviews our screen time recommendations, I feel that we, as pediatricians should continue to advise parents about basic principles.\nParents need to lay down some parameters about when and how media is used. Media is a centerpiece of teens’ lives and is not going away, but just as we don’t give our kids a set of keys to our car and say “just drive”, we need to enforce appropriate media use. And good modeling is also critical: parents need to put down their mobile devices and simply communicate with their kids. Old fashioned parenting and just talking to your kids can build the foundation to a less stressful childhood and hopefully a happier life."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:730d301a-bc3f-4755-927b-d51289bb56cf>","<urn:uuid:9ee5a44c-ac0e-4471-bc2a-f3b31e423c46>"],"error":null}
{"question":"How many lunar eclipses are possible in a single calendar year, and when was the last time this maximum number occurred?","answer":"The maximum number of lunar eclipses possible in a year is five, though this is quite rare. The last time five lunar eclipses occurred in one calendar year was in 1879, and the next time this will happen will be in 2132.","context":["Lunar Eclipses: What They Are and When To See Them\nA lunar eclipse is one of the most breathtaking astronomical events — our natural satellite gets engulfed by the Earth’s shadow and changes its color to red. Learn what types of lunar eclipses there are, why they occur, and when the next lunar eclipse happens.\n- What is a lunar eclipse?\n- Types of lunar eclipses\n- Lunar eclipse vs. solar eclipse\n- When is the next lunar eclipse?\n- How often do lunar eclipses happen?\n- How to watch a lunar eclipse?\nWhat is a lunar eclipse?\nA lunar eclipse occurs when the Earth fully or partially blocks the Sun’s light from reaching the Moon. This can only happen during the Full Moon phase. If the Earth, the Moon, and the Sun form a straight line, a total lunar eclipse occurs. If their alignment is not exact enough, observers will see a partial or penumbral lunar eclipse — or no eclipse at all.\nWatch our short explanatory video to better understand how lunar eclipses work.\nTypes of lunar eclipses\nThere are three types of lunar eclipses: total, partial, and penumbral.\nA total lunar eclipse is the most spectacular of the three types. It happens when the Sun, the Earth, and the Moon are precisely aligned in space. The Earth comes between the Moon and the Sun and covers the entire Moon with the inner part of its shadow, called the umbra. Interestingly, our natural satellite doesn’t completely disappear during a total eclipse but turns dark red. Why does it happen?\nAlthough the Earth blocks all direct sunlight, a small portion of the light gets refracted by the Earth’s atmosphere and reaches the Moon’s surface. Our planet’s atmosphere scatters the blue-colored light but lets the red-colored light through. That’s why the lunar disk becomes red. Because of the distinctive reddish hue, a total lunar eclipse is often called a Blood Moon.\nA partial lunar eclipse occurs when only a part of the Moon gets covered by the Earth’s umbral shadow. This happens when the Sun, the Earth, and the Moon are not perfectly aligned. During this type of eclipse, only a portion of the Moon gets dark and reddish.\nA penumbral lunar eclipse occurs when the Moon passes through the penumbra — the outer part of the Earth’s shadow. It’s the least noticeable type of eclipse: for a keen-eyed observer, the Moon will look only slightly darker than usual.\nLunar eclipse vs. solar eclipse\nWhat’s the difference between a lunar eclipse and a solar eclipse? Both events involve three celestial bodies: the Sun, the Earth, and the Moon. During a lunar eclipse, which happens at night, the Moon gets covered by the Earth’s shadow. During a solar eclipse, which happens in the daytime, the Sun gets covered by the Moon’s disk. Lunar eclipses occur only at a Full Moon, and solar eclipses — only at a New Moon.\nA lunar eclipse can be observed from wherever the Moon is above the horizon — anywhere on the night side of the Earth. This is due to the fact that the Earth’s shadow is very large compared to the Moon. A solar eclipse is much more challenging to see, as the Moon’s shadow is much smaller than the Earth’s. Therefore, solar eclipses are visible only from specific locations where the Moon’s shadow falls.\nInteresting fact: eclipses always come in pairs. A solar eclipse occurs approximately two weeks before or after a lunar eclipse.\nIf you tend to confuse solar and lunar eclipses, remember this: when the Sun gets dark, we call it a solar eclipse, and when the Moon gets dark, we call it a lunar eclipse.\nWhen is the next lunar eclipse?\nThe next lunar eclipse will occur on November 8, 2022 — it will be a total lunar eclipse. The eclipse will be visible from Asia, Australia, North America, most of South America, and parts of northern and eastern Europe. The totality phase will begin at 10:16 GMT (6:16 a.m. EDT) and reach its maximum at 10:59 GMT (6:59 a.m. EDT).\nIf you want to learn about all lunar eclipses that will occur in the upcoming decade, use the eclipse calendar made by NASA. You can also check our infographic, where we list five future eclipses with their timelines and visibility maps.\nHow often do lunar eclipses happen?\nAs we’ve already mentioned, a lunar eclipse always happens at a Full Moon. However, not every Full Moon comes with a lunar eclipse. Here’s why: the Moon’s orbit is tilted at about five degrees to the Earth’s orbit, so our natural satellite usually passes above or below the Earth’s shadow at a Full Moon. On average, two lunar eclipses occur every year. The maximum number of lunar eclipses in a year is five, though it happens quite rarely. The last time five lunar eclipses occurred in one calendar year was in 1879; the next time it will happen in 2132.\nHow to watch a lunar eclipse?\nYou don’t need any special equipment to observe a lunar eclipse — though you can use binoculars to see more details on the red-shaded lunar surface. All you need to enjoy this astronomical event is a clear sky and an unobstructed horizon. To learn when the eclipse begins in your location, use the Eclipse Guide app.\nHow rare is a total lunar eclipse?\nTotal lunar eclipses account for about 29% of all lunar eclipses. On average, a total lunar eclipse occurs every 2.5 years in any given location.\nWhy is the Moon red during a lunar eclipse?\nThe eclipsed Moon looks red because sunlight gets refracted and scattered by the Earth’s atmosphere, with only red-colored light reaching the lunar surface.\nIs a lunar eclipse safe to look at?\nUnlike watching a solar eclipse (for which you need special solar filters), observing a lunar eclipse is absolutely safe for your eyes. That’s because the Moon doesn’t emit its own light but only reflects sunlight.\nHow do lunar eclipses affect humans?\nYou might hear that lunar eclipses can increase the risk of skin disease, badly affect pregnant women, or be harmful to digestion. However, there is no scientific evidence that lunar eclipses have any physical effect on people.\nWe hope you’ve learned something new about lunar eclipses from our article. Don’t hesitate to share it with your friends if you like it. You can also test your knowledge about solar and lunar eclipses with our challenging quiz. We wish you clear skies and happy stargazing!"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:8b795aec-02e6-4499-9d1c-8e90b39f5194>"],"error":null}
{"question":"What are the basic requirements to become a Catholic godparent? I want to make sure I qualify.","answer":"To become a Catholic godparent, you must meet several requirements: you must normally be at least 16 years old, be a member in good standing in the Catholic Church, lead a life in harmony with the faith and role, and have received all three sacraments of Christian initiation (Baptism, Confirmation, and Eucharist). Parents cannot be sponsors. While there are usually two godparents (a godfather and godmother), there may be just one Catholic godparent. A baptized non-Catholic may serve as a Christian witness, but there must be at least one Catholic godparent.","context":["There are seven Catholic sacraments:\nThe seven sacraments are ceremonies that point to what is sacred, significant and important. They are special occasions for experiencing God's saving presence. The sacraments touch all the stages and all the important moments of Christian life: they give birth and increase healing and mission the Christian’s life of faith. There is thus a certain resemblance between the stages of natural life and the stages of the spiritual life.\n\"Amen, amen I say to thee, unless a man be born again of water and the Holy Ghost, he cannot enter into the kingdom of God.\" For Catholics, the sacrament is not a mere formality; it is the very mark of a Christian, because it brings us into new life in Christ. Holy Baptism is the basis of the whole Christian life, the gateway to life in the Spirit and the door which gives access to the other sacraments. Through Baptism we are freed from sin and reborn as children of God; we become members of Christ, are incorporated into the Church and made sharers of her mission.\nSelection of God Parents – Careful consideration of the choice of Godparents is an important element for the sacrament of Baptism.\n† Normally at least 16 years old\n† A member in good standing in the Catholic Church\n† Leading a life in harmony with the faith and the role to be undertaken\n† Have received all three of the sacraments of Christian initiation – Baptism, Confirmation, and Eucharist\n† Parents may not be sponsors.\nUsually there are two godparents, a godfather and a godmother. However, there may be just one Catholic godparent. A validly baptized non-Catholic may serve as a Christian witness, but there must be a Catholic godparent\n\"Turn away from sin and be faithful to the Gospel.\" In the Sacrament of Penance the faithful who confess their sins to a lawful minister and are contrite for those sins, receive from God, through the absolution given by that minister, forgiveness of sins committed after Baptism, and at the same time they are reconciled with the Church.\nThroughout his life, Jesus was sought out by men and women who, distressed and humbled, said, \"Yes, I have sinned, have mercy on me.\" Their faith drew them to believe in Jesus' power of forgiveness - the living presence of the compassionate God. When we confess our sins, we are exercising the gift of our own free will the choice to tell God that we have wronged Him and others and seek to have our relationship with them restored. Through confession, God reaches back to us in a most real way, love and mercy in hand.\nThe Sacrament of Holy Eucharist is the source and summit of all worship and Christian life. In this Sacrament, Christ the Lord himself is contained, offered and received. In the Eucharistic Sacrifice, the memorial of the death and resurrection of the Lord, the sacrifice of the cross is forever perpetuated. By this Sacrament, the Church continually lives and grows and the unity of the People of God is signified and brought about.\nGod’s love sent us his only-begotten Son into this world of sinners to redeem us by his ultimate gift of love -- the sacrifice of his own life for our redemption. On the night before he died for the redemption of man’s sins, he shared bread and wine with his disciples, changing the bread and wine into his own body and blood. Each time we participate in the Eucharistic sacrifice, we ourselves become transformed and living members of the One True Christ, who is present in the Eucharistic, the Bread and Wine. All who are properly disposed are invited to experience Jesus' true presence, body, soul and divinity in the Sacrament of the Eucharist.\nLearn more about the Catholic faith\nLinks to the two primary documents encompassing our faith are provided below:\nIn addition to these references, we invite you to explore our faith at Guardian Angels Church. Please join us in prayer this coming weekend and take time to meet the Guardian Angels family."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:a3c63444-cf5d-4628-a210-6df69dd458fa>"],"error":null}
{"question":"What are the different approaches between small-scale home wildlife care versus organized rescue centers?","answer":"Home wildlife care typically handles individual cases as they arrive, with a focus on immediate emergency care - like in the case described where a joey needed urgent rehydration and temperature regulation. In contrast, organized rescue centers have systematic approaches with multiple dedicated facilities, trained veterinary staff, and specialized rehabilitation programs. They maintain permanent facilities since 1999, with professional protocols for health checks, pre-release conditioning, and careful planning for reintroduction to the wild. The rescue centers also have formal authorization through Memorandums of Agreement to receive and rehabilitate confiscated, donated, and rescued wildlife.","context":["Into September and seeing an almost never ending arrival of young, still pouch dependent\nas well as a large number of out-of-pouch but still very dependent macropods found alone\nas well as others suffering injuries after being found caught in fences. At the time of writing\n[4th Sept] the only feathered patients in care are a clutch of Masked Lapwings, here almost 3\nweeks arriving as newly hatched chicks. There have also been magpies as well as several\nof the larger parrot species, all too badly injured to be saved.\nThere were several comments about the piece in Edition 62 about how changes to the\nenvironment are impacting on birds. It’s a very interesting area to look into and always\nneeds to be considered when taking wildlife into care. There have also been queries\nregarding birds nest building and courtship displays so I have included in this edition the list\nof some of our common species and their breeding times first printed in Issue 53.\nThis will be the second last edition of The Wild Connection from me. It is way past time\nsomeone younger and more energetic took over. Perhaps in the future I’ll have another go\nbut the brain is currently well and truly running on empty, barely sparking, probably brought\nabout by many sad occurrences during this year that have left me finally giving in to the need\nto do other things. Time for me to be replaced I think. I remember the day way back in 1997\nwhen I was asked to do Edition number 3 and a new Editor would be ready to take off with\nEdition 4. I have worn the editor’s cap for 18 years and 61 Editions. If the new Editor\npermits I would like to send in the occasional piece for publication.\nWildlife needing care have been a constant at home for 30 years, at the time of writing the\nnumber arriving has almost reached 5000, if arrivals continue as they are at present we may\npass that milestone before Christmas.\nI had quite a number of comments about the article on ‘illegal arrivals’. Since that edition\nwent out there have been several others arriving for care, all except one have survived and\nwill eventually be released. On arrival the non-surviving joey was suffering hypothermia –\nthe cold could be felt coming from deep within his body – he was also critically dehydrated\nand had no muscle tone, nothing more than bones with a covering of tatty unkempt fur. he\nwas also laying in a large amount of his own stinking faeces. First thought was he’d been\nfound alone, mum probably killed several days earlier, the cold weather we were\nexperiencing at the time would certainly create the problems he was seen to be suffering\nfrom. The joey was given a temporary clean up and placed into a clean pouch. A large\namount of fluid was given sub-cutaneously and he was placed on a soft heat pad and\ncovered with a heated blanket. The dry eyes were treated with a lubricant to ease his\ndiscomfort. After an hour or two there was some movement, the ears were twitching and\nfollowing sounds. Maybe there is a chance I thought and maybe this one had just lost its\nmum and been left alone even though the ‘poo’ was all wrong for a wild joey.\nAs I knelt beside the joey to feel the body I had one of those ‘moments’ and thought ‘I bet\nhe’s got parrot beak’ and sure enough there was the definitive proof this poor animal had\nbeen kept in horrific care and for an amount of time I hate to think about. Long enough for\nthe jaw to become deformed so the lower jaw sat about 1cm shorter than it should, this\nmeans the lower incisors don’t connect with the upper palate on the inner surface of the\ncutting area of the upper incisors against the teeth enabling the animal to pull and cut\nthrough tough grasses. When the molars have erupted the mouth won’t close as the upper\nand lower molars meet together rather than top and bottom teeth alternating in the jaw this\nprevents the jaw closing and once this stage has occurred there is nothing that can be done\nto save an animal regardless of how successful we may be with the rehydrating, reheating\nand nutrition. The molars of this joey had all erupted – clearly it was older than it appeared\nto be – and the abnormal placement of teeth due to the shortened lower jaw meant it could not close its mouth and once again euthanasia was the only choice. Again, I ask all who\ncare about out beautiful creatures to constantly remind members of the public they are not\nallowed to keep any species of wildlife, regardless of any license they may possess allowing\nthem to keep wildlife that has been legally purchased from a licensed breeder and dealer.\nWildlife found in the wild must be taken to a vet or authorised wildlife shelter operator. Also\nany carer who is not familiar with a species that arrives for care, speak to another carer,\nensure you have the proper information to care for the species you have in care. Never be\nafraid to ask a question, and never think a query you may have is stupid, we all only learn by\ntalking to others and we are always able to learn something new.","Rescue & rehab Centers\nRehabilitation and release of endangered wildlife at rescue centers in northwest Panay:\nconfiscated/donated birds and other endangered wildlife are rehabilitated till being able to fend for themselves when released\nRehabilitation and release of endangered wildlife at rescue centers in NW Panay: confiscated / donated birds and other endangered wildlife are rehabilitated till being able to fend for themselves when released\nWe foster, rehabilitate, and then release injured animals.\nOne of our most obvious and longest activity is the rehabilitation and release of endangered wildlife at rescue centers in northwest Panay.\nThe animals taking into rehabilitation are either found and reported to PhilinCon or seized by PhilinCon’s rangers from illegal wildlife traitor or poacher. After bringing the injured, sick or orphaned animal to one of the three rehab stations (Mag-aba Wildlife Clinic, Pandan, Antique; Bulanao Rescue Facility, Brgy, Antique; Sibaliw Rehabilitation Station, Brgy, Antique) a veterinarian will determine the extent of injury and the probability of successful rehabilitation. If the animal can make a recovery and be released into the wild, it will be nurtured, trained, and medically taken care of. Otherwise the animal will be still nursed but stays in an animal sanctuary.\nIn the first step basic first aid and physical therapy will be applied to the animal. For this purpose the station staff received training explicit for the unique species found on Panay; they understand behavioral issues, nutritional requirements, and have knowledge about species-specific handling. Day-to-day care is applied to the animals; it includes feeding, physical therapy, exercise, medicating and a pre-release conditioning program. In the program, the animals are familiarized with their natural diet to enable them to survive when released back to the wild. To record the recuperation process, regular medical examinations are executed.\nBefore releasing the animals, the reintroduction into the wild has to be carefully planned. To be released, the animal must be healthy, strong, and have intact wild instincts to survive in the wild. The releasing is not only solely dependent on the animal, it also hinge on the right habitat, location, season, sometimes even the weather. When all conditions are met, the animals are released into the wild.\nWe presently maintain several hornbills and raptors at two locations, Bulanao, Libertad, and Santo Rosario, Pandan.\nIn our three rescue centers, injured animals are treated professionally until they can be released.\nThe animals admitted to each of the three rescue centers are checked for their health, treated professionally, if necessary by a vet, fed adequately and kept as long as necessary until they attain top shape in terms of plumage, health (pre-release health check) and locomotor abilities. Then they are released back into the wild. Hornbills pass through a rehabilitation station in the upland forest to accustom to their natural forest environment, larger raptors exercise sustained power flight in large aviaries prior to release from a lowland station.\nThe facilities have been maintained permanently since 1999.\nThe costs for e.g. one hornbill average around 200 USD annually. This includes food, medicine, caretaking, and health monitoring.\nOur MOA (Memorandum of Agreement) authorizes the project to receive and maintain confiscated, donated, and rescued wildlife for rehabilitation, and later release them back into their former habitats. We presently maintain several hornbills and raptors at two locations, Bulanao, Libertad, and Santo Rosario, Pandan.\nA local DVM, Dr. Enrique Sanchez, had been dispatched to Cologne, Germany, for additional training in avian medicine. Likewise, Filipino staff were trained to tend and care for rehabilitated wildlife in our three facilities, namely in Mag-aba Wildlife Clinic, Pandan, Bulanao Rescue Facility in Brgy. Bulanao, Libertad, both in Antique, and the Sibaliw Rehabilitation Facility in Brgy. Tag-osip, Buruanga, Aklan. After proper health checks, the birds are trained and conditioned for release. In the process, the animals are familiarized with their natural diet to enable them to survive when released back to the wild."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:db6120ed-5203-442a-9276-746808e8f544>","<urn:uuid:c109c59d-24f9-4ecb-993a-f1d483b4912c>"],"error":null}
{"question":"What are the key differences between developing fine motor control for handwriting versus developing gross motor control for jumping, and what support strategies are recommended for each?","answer":"Fine motor control for handwriting and gross motor control for jumping require different types of development and support strategies. For handwriting, children need to develop small muscle coordination in their hands, fingers, and wrists, along with hand-eye coordination and dexterity. This typically develops from early scribbling in toddler years to writing letters by age 5, and can be supported through activities like drawing, threading beads, and using scissors. In contrast, jumping development requires large muscle group coordination, balance, and motor planning abilities. It typically emerges around 16-18 months and needs core and leg strength, along with balance skills for stable landing. Support strategies for jumping include outdoor play, climbing activities, obstacle courses, and using equipment like mini trampolines. Both skills can be practiced through progressive difficulty levels - starting with basic movements and gradually advancing to more complex tasks as the child shows improvement.","context":["Fine motor skills refer to the skills we have in making small movements and adjustments using the small muscles in our hands, wrists, fingers, face, feet, and toes.\nThese skills require strength in our smaller muscles, fine motor control, hand-eye coordination, and dexterity. All of these are crucial for a range of everyday but nevertheless complex activities like cutting, carrying, holding, feeding ourselves, drawing, writing, typing, and so on.\nThe acquisition and refinement of these skills are a critical part of a child’s development, but are also used throughout adulthood. While learning the skills, we exercise significant cognitive load and elaborative encoding, but once learned, we can perform many fine motor tasks almost instinctively.\nFine Motor Skills Definition\nFine motor skills can be succinctly defined as the coordination of small muscles, in movements—usually involving the synchronization of hands and fingers—with the eyes.\nSome scholarly definitions include:\n“Fine motor skills involve movements of the small muscles, especially those of the eyes, speech musculature, hands, fingers, feet, and toes.” (Collins & O’Brien, 2003)\nThis definition underscores the complex orchestration of muscular, neurological, and perceptual systems at work during fine motor tasks.\nOther definitions will often define it in contrast to gross motor skills, as with this definition from Biddle and colleagues:\n“Fine motor skills involve the small muscles of the hands and fingers, while gross motor skills involve large muscles of the arms, legs, and torso.” (Biddle et al., 2013)\nKey features of fine motor skills include the use of smaller muscle groups (as opposed to large, whole-body movements), precision in execution, and often a necessary component of visual-motor integration.\nFine motor skills are essential for performing everyday tasks and also underpin the development of self-help skills in children.\nFine Motor Skills Examples\n- Pencil Grasp: When holding a pencil, the thumb, index, and middle fingers coordinate to apply the right amount of pressure. This allows us to control the pencil for drawing or writing. It’s a skill typically developed in early childhood (ages 3-7), and are gradually refined to enable more precise and intricate pencil control.\n- Using Chopsticks: Handling chopsticks requires a lot of dexterity and, in the Western world, is sometimes not developed until adulthood. However, in many Asian cultures, children develop the ability to handle chopsticks at a very young age due to early exposure.\n- Buttoning a Shirt: Buttoning shirts requires the dexterity to hold the button between the thumb and index finger, while also using the other hand to stabilize the fabric and slip the buttonhole over the button itself. This skill involves not only skilled hand-eye coordination, but also a range of muscle memory, and tactile sensitivity to ‘feel your way’.\n- Using Scissors: Handling scissors necessitates a complex set of hand movements and visual guidance. The thumb, index, and middle fingers work in concert to open and close the scissors, while the eyes guide the cutting path. We generally provide young children with ‘safety scissors’ to practice cutting before moving onto scissors that are more heavy duty but dangerous without fine motor skills.\n- Threading Beads: This activity is commonly used in preschools and kindergartens to explicitly encourage the development of fine motor and hand-eye coordination skills. Threading a string through the hole of a bead requires a pincer grasp as well as the complex ability to gauge depth and distance.\n- Tying Shoelaces: Children often use snap-shut or slip-on shoes until they have the dexterity to tie shoelaces. In preschools, they’re often given help in developing this fine motor skill by practicing tying knots in string before proceeding to their own laces.\n- Turning a Key: Inserting a key into a lock and turning it requires precision, hand-eye coordination, and the ability to apply the right amount of force.\n- Playing a Musical Instrument: Fingeroperated musical instruments such as pianos and guitars require refined control over hands and fingers. There’s also an element of size here – where children with small hands won’t be able to use the instruments as intended yet.\n- Sewing: Needlework necessitates careful coordination and control, involving precise movement of the fingers to thread a needle and make stitches. As people improve their dexterity in sewing, they can move to more complex patterns and styles.\n- Using a Computer Mouse: Young children often find it hard at first to navigate computers with a mouse, often opting to use touchscreens until they have the dexterity to operate the mouse.\n- Pouring Liquid from a Jug: We often pour water for children until they develop both the physical strength (tied to gross motor skills) and the fine motor skill of carefully tipping the jug so water comes out at the right speed to successfully transfer water to a cup without spilling.\n- Peeling a Sticker: Successfully peeling a sticker off a sheet calls for a precise pinching motion and the ability to regulate force to avoid tearing. This is a skill often developed early on, when we give children stickers as token rewards for good behavior.\n- Applying Makeup: Applying eyeliner or mascara necessitates very precise hand-eye coordination and a steady hand. This is complicated by the fact that we can’t directly see ourselves when applying makeup, having to view through a mirror where our movements are reversed in front of our own etes.\n- Folding Paper: Folding paper (as in origami) demands precision and accuracy in both the fingers and the eyes. A child might start off with paper planes before moving onto more complex constructions like origami cranes.\n- Brushing Teeth: This involves holding a toothbrush correctly and making precise brushing motions. Children are encouraged to brush gently at first because imprecise and rough movements may cause bleeding of the gums.\n- Building a Tower of Blocks: This requires the hand-eye coordination to stack blocks without toppling them, and the finger control to align them correctly. We see this, for example, in games like Jenga.\n- Manipulating a Smartphone: Using a touchscreen requires precise finger movements and hand-eye coordination to successfully select icons, type text, and perform other tasks. Children may learn this when playing games on their parents’ phones or on a tablet computer.\n- Cutting with a Knife: Whether cutting paper or food, this skill requires a strong grip, hand-eye coordination, and the ability to regulate pressure. Because this is a particularly dangerous activity, children often don’t get practice at it until they are older.\n- Lacing a Thread through a Needle: This requires a high degree of precision, control, and hand-eye coordination that is even difficult for adults to master consistently..\n- Writing on a Chalkboard: Writing on a vertical surface demands wrist movement, hand control, and finger strength. The child will also need to be able to apply the right amount of pressure to get the right results on the board but not break the chalk!\n- Typing on a Keyboard: Touch typing involves all the fingers, each responsible for precise key pressing, coordinated with visual cues from the screen. Because of how common typing is these days, many adults develop the capacity to reflexively touch type, using minimal cognitive load because it becomes a simple matter of automatic processing due to muscle memory.\n- Creating Artwork with a Paintbrush: Holding a paintbrush and creating strokes demands a combination of hand-eye coordination and finger dexterity. Clearly, when we open a child’s coloring book, we can see that it’s a child’s work due to the amount of times they’ve colored outside the lines.\n- Doing a Puzzle: Assembling a puzzle calls for visual perception, hand-eye coordination, and fine finger movements to fit pieces together. In Jigsaws, for example, children will start with larger puzzle pieces until they develop the skills to move down to smaller pieces.\n- Opening a Zipper: Pulling a zipper up or down necessitates a pincer grasp and enough strength to move the zipper. Feeding the zipper pieces together is an added level of complexity on top of this.\n- Cooking: Many aspects of cooking, such as stirring, peeling vegetables, or cracking an egg, call for various fine motor skills. Cooking with parents isn’t just about learning about food and cooking; it’s also a task in learning fine motor skills.\n- Planting Seeds: This activity requires the finger dexterity to pick up and carefully place small seeds, as well as the hand-eye coordination to plant them in the right location.\nFine Motor Skills Milestones for Children\nMost theories of children’s physical development hold that fine motor skills development in children progresses in a generally predictable pattern, albeit with individual differences based on the child’s unique growth and experiences.\nInfants (0-12 Months)\n- Grasping Reflex (0-2 Months): Newborns will instinctively close their hand around a finger or object that strokes their palm.\n- Voluntary Grasping (3-4 Months): Infants start to reach for and grasp objects intentionally, albeit with a rather clumsy, whole-hand “raking” motion.\n- Transfer Object Between Hands (5-6 Months): Babies begin to pass toys or objects from one hand to the other, demonstrating improved hand-eye coordination.\n- Pincer Grasp (9-12 Months): Infants develop the ability to pick up small objects using their thumb and index finger, showcasing improved finger dexterity.\nToddlers (1-3 Years)\n- Self-Feeding (12-18 Months): Toddlers start using their fingers to feed themselves and will eventually begin to use utensils.\n- Stacking Blocks (18-24 Months): They gain the ability to stack a few blocks on top of each other, demonstrating improved hand-eye coordination and control.\n- Drawing and Scribbling (2-3 Years): Toddlers start making marks on paper, initially random scribbles but progressively evolving into more controlled lines and circles.\nPreschoolers (3-5 Years)\n- Cutting with Scissors (3-4 Years): Children begin to use scissors, initially with assistance and eventually independently.\n- Drawing a Person (4-5 Years): Children start to draw recognizable figures, such as a person with a head, body, arms, and legs.\n- Writing Some Letters and Numbers (5 Years): By this age, most children can write some letters and numbers, and may even write their own name.\nIt’s essential to remember that these milestones serve as guidelines rather than rigid expectations. Some children may develop certain skills faster or slower than their peers.\nFine Motor Skills vs Gross Motor Skills\nFine motor skills and gross motor skills are often framed as opposing but equally important skills, where gross motor skills are useful for tasks like running and fine motor for tasks like drawing.\n- Fine motor skills involve the coordination of small muscle groups, primarily in the hands and fingers, often in concert with visual input, to achieve fine manipulation tasks.\n- Gross motor skills are exercised when we engage large muscle groups to perform big movements, often in bursting motions, like when walking, running, jumping, and throwing.\nPsychomotor theorist Rudolf Laban, renowned for his work on movement analysis, posited that gross motor movements (which he categorized as “locomotion,” “manipulation,” and “stabilization”) are foundational to human movement.\nHowever, he also recognized the importance of the intricate, precise movements embodied in fine motor skills.\nGross motor skills typically develop before fine motor skills, in a process that we call proximodistal development.\nFor example, infants will learn first to roll over, sit up, and eventually walk – each being gross motor skills. Later, they will develop more delicate skills such as grasping objects, buttoning clothes, or drawing, which as seen above are examples of fine motor skills.\nDespite the differences, the development of both skill sets are interconnected. The stability provided by gross motor skill development can significantly support the development and execution of fine motor tasks.\nOccupational therapist Anna Jean Ayres contributed significantly to this understanding through her sensory integration theory, which postulates a strong relationship between sensory processing, sensory memory, gross motor function, and fine motor performance.\nEssentially, our ability to make sense of the sensory information around us influences our motor responses, both gross and fine.\n|Fine Motor Skills||Gross Motor Skills|\n|Definition||Coordination of small muscles primarily in the hands and fingers, often in concert with the eyes||Utilization of large muscle groups for big movement activities|\n|Example||Writing with a pencil, threading beads||Walking, jumping, throwing|\n|Key Features||Involves smaller muscle groups, precision in execution, visual-motor integration||Involves larger muscle groups, body coordination, balance|\n|Development||Typically develop after gross motor skills, requiring a higher level of control and precision||Usually develop earlier, foundational for many physical activities and certain fine motor tasks|\n|Theorists/Contributors||Anna Jean Ayres (Sensory Integration Theory)||Rudolf Laban (Laban Movement Analysis)|\nFine motor skills are assessed as developing within a generally normal range for children, although there is variation both between individual children and across cultural groups (e.g. some cultural groups encourage children to perform specific fine motor skills earlier than others, leading to faster acquisition). But overall, children will develop these skills at their own pace, and should be given plenty of play time to practice developing these fine motor skills.\nBiddle, K. A. G., Garcia-Nevarez, A., Henderson, W. J. R., & Valero-Kerrick, A. (2013). Early childhood education: Becoming a professional. London: Sage.\nCollins, J. W., & O’brien, N. P. (2011). The Greenwood dictionary of education. New York: ABC-CLIO.\nDr. Chris Drew is the founder of the Helpful Professor. He holds a PhD in education and has published over 20 articles in scholarly journals. He is the former editor of the Journal of Learning Development in Higher Education. [Image Descriptor: Photo of Chris]","By Deborah Lee MS, OTR/L\n“When should my child start jumping?” is one of the common questions asked by parents. All the more, when parents see their child’s peers jumping up and down, they cannot help but to compare and become concerned. According the research, children should be able to learn how to jump between 16 to 18 months old. This developmental timeframe is also around the time children start learning to run. Therefore, when your child is beginning to pick up their running pace, it is appropriate to support them with activities in order for them to achieve their jumping goal!\nJumping ties into several key areas of development and skills, some but not limited to: motor planning, balance, and muscle strength. Now, some people may not know what motor planning is – so, what is it? Motor planning is “the ability to conceive, plan, and carry out a skilled motor act in the correct sequence from start to end”. In layman terms, your brain is sending signals to the rest of your body in attempt to perform a specific motor act/skill, in this case jumping. In conjunction with motor planning, balance skills and muscle strength are required in order for a child to be able to successfully jump. Without balance skills, a child will not have the postural control as well to stabilize themselves in a standing position when attempting to start the sequencing of jumping and will fall when attempting to land on both of their feet. Similarly to overall body muscle strength, without core and leg strength, your child will most likely not be able to lift themselves up off the ground.\nSo, how can we support our children to meet this specific gross motor developmental milestone? First and foremost, encourage and initiate many opportunities for outdoor play, such as climbing up and down playground apparatus or even running. If outdoor play is not an easy option due to living conditions at home or if there are no parks in close proximity, parents can also create opportunities for children in the home, such as creating a 1-step, 2-step, or even 3-step obstacle courses for your child to climb under, over, etc., have your child step on low leveled stools for them to jump down onto the floor, or have them ascend and descend stairs. You can even purchase a mini trampoline for your child and at times, this is the best equipment to utilize to introduce jumping skills to your child. And of course if purchasing a trampoline is not easy, you can always improvise at home and have them simply attempt to jump or bounce up and down on the bed.\nWhile there are so many strategies to support your child with their jumping skills, it is also important to not overwhelm them since this can intimidate your child and deter them away from wanting to do the act of jumping. Most importantly, start off with baby steps such as having them climb lower leveled equipment first then jump off onto the floor. Once you notice continued progress, you can make the activity or task more difficult for your child in hopes to challenge them by increasing the distance from the ground up and more.\nHere is a case study of a 3-year old boy that came into occupational therapy session with no jumping skills:\nMother expressed concerns that all of his peers in his classroom already know how to jump and she felt stressed that her child was the only one who did not know. Sessions typically started with a 2-step obstacle course that included crawling through a sensory tunnel then walking or running along an uneven surface (couch cushions) to maintain balance skills as well. We also placed 2 to 3 painters tape on the floor that were approximately 5 inches apart to provide visual cues, having the child step on the first painters tape then attempt to jump to the next tape. Because he had no jumping skills yet, the therapist or mother would manually bend his legs in between hers then lift him off the ground in order for him to land on both of his feet thereafter. After several sessions of repeating this activity, the child was able to attempt to jump while alternating his feet at the least. Therapist also encouraged mother to purchase a mini trampoline and model for the child at home while both of them were on it."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:33ab528d-39c3-4d28-a1cc-163331c55a25>","<urn:uuid:0562fc2c-1ec8-428d-910f-fd1dc3f451d7>"],"error":null}
{"question":"What are the display options for data points in line micro charts, and what contrast ratio requirements apply to these graphical elements for accessibility?","answer":"Line micro charts can display or hide data points, with a recommended range of 4-12 data points per chart. Data points can have semantic colors (green for positive, orange for critical, red for negative) with a grey line, or use any color from the qualitative palette for both points and line. For accessibility, these graphical elements must maintain a contrast ratio of at least 3:1 according to WCAG 2.1 level AA requirements for charts, graphs, and other graphical interface components.","context":["Use the line micro chart if:\nYou want to show data points that are above or below a certain threshold.\nNote that the threshold can also be set to zero.\nIn addition to data points, you can use two different colors for the lines. We recommend that you use only semantic colors for this option, such as green for values above the threshold, and red for values below the threshold.\nYou want to show one or two focus points.\nUse this option to display a trend or sparkline to focus on one or two special values, such as the first and/or last value of the chart. By default, the chart is rendered in blue, but application developers can choose another color for the chart line and data points from the qualitative palette.\nYou want to show one or two focus points with semantic colors.\nUse this option to display a trend or sparkline to focus on one or two special values with a semantic meaning, such as the first and/or last value above or below a certain threshold. Note that the threshold can also be set to zero.\nDo not use the line micro chart if:\nYou have scenarios that do not depict time periods.\nLine micro chart in a tile or facet\nNote: The title of the tile or header facet should clearly express the value of the threshold if it is not equal to zero, such as values above and below 50.\nTo present the line micro chart in an appealing way, application developers can set several properties.\nThe line micro chart can display or hide data points. To display data points, we recommend that you use 4 (for quarters) to 12 (for months) data points per chart. The responsive behavior of the chart hides data points if there is not enough space to display them.\nIf data points have semantic colors (such as green for positive, orange for critical, and red for negative values), the line is colored grey.\nIf data points do not have a semantic meaning, application developers can assign any color from the qualitative palette. Note that same color is then also applied to the line.\nAlternatively, you can show a line-only chart, with all the data points hidden. In this case, you can use up to two colors per chart. If the values relate to a threshold, we recommend using semantic colors for the line to highlight values above or below the threshold.\nUnlike data points, focus points highlight specific values, such as the first and/or last value of a time series. Neutral focus points are colored blue. Focus points with a semantic meaning are colored green, orange, or red.\nFocus points are always displayed regardless of the size of the chart.\nYou can set a threshold line. If the threshold isn’t zero, include its value in the title of the tile, header facet, or column in lists and tables to ensure that the chart is not misleading for the user.\nThe labels for the line micro chart are optional. Use the bottom labels to indicate the beginning and the end of the time period. Use the top labels to show the corresponding values for the beginning and the end of the chart. Ensure that the labels for the values are not truncated.\nApplication developers can set the scale manually or automatically. If the threshold is included in the scale, the chart always renders a horizontal reference line.\nUse auto scale for lists and tables if you want to display a trend/sparkline per list/table element independently of other list/table elements.\nUse the manual scale if list/table entries need to be synchronized. Entering the scale manually ensures that all charts within the list/table are comparable.\nWant to dive deeper? Follow the links below to find out more about related controls, the SAPUI5 implementation, and the visual design.","Color Contrast Checker\nThe tool tests the contrast ratio of background and text colors for accessibility. You can use it to visualize different color combinations for your website design that are in compliance with Web Content Accessibility Guidelines (WCAG) and international legislation based on it like the EU Web Accessibility Directive, the Americans with Disabilities Act (ADA), or the Accessibility for Ontarians With Disabilities Act (AODA).\nColor Contrast Tool Guide\nHow to Interpret the Color Contrast Ratios\nWCAG 2.1 Level AA\nLarge text - minimum contrast ratio of 3:1.\nWCAG 2.1 Level AAA\nLarge text: minimum contrast ratio of 4.5:1\nPlease note that incidental text such as images that are purely decorative or part of an inactive user interface component, and logotypes, such as parts of a logo or brand name, have no minimum contrast requirement.\nWhy Use a Color Contrast Checker?\nUse Monsido’s web color contrast checker to quickly check color combinations, and ensuring that all your branded content assets and design elements are accessible to everyone. It can also be used to test color contrast with other legislation, e.g. as an ADA contrast checker.\nNeed additional help?\nSee how we can help you make your website more accessible. Book a free web accessibility scan today.\nFrequently Asked Questions\nColor is such a vital element of web design as it is used to convey personality, attract attention, symbolize action, and indicate importance. As color carries a lot of significance in both being visually pleasing as well as conveying meaning, users must be able to perceive the content of different colors on a webpage. Color accessibility is important as it helps users with visual impairments like low vision or color blindness to properly distinguish content elements and read/view them effectively.\nA color contrast checker is a tool that measures the difference in perceived luminance between two colors to ensure that is perceivable to users with visual impairments or insensitivity.\nThe difference is measured on a ratio scale that starts at 1:1 (for white on white, to 21:2 (black on white). According to the WCAG level AA, the minimum contrast ratio that colors can have for the visual presentation of text and images of text is 4.5:1.\nWCAG 2.1 Level AA requires the visual presentation of text and images of text to have a contrast ratio of at least 4.5:1, except for large text, which should have a minimum contrast ratio of 3:1.\nWCAG 2.1 Level AAA requires a contrast ratio of at least 7:1 for normal text and 4.5:1 for large text (14 point and bold or larger, or 18 point or larger).\nImages must pass the WCAG contrast requirements. Images that contain text must ensure that the contrast between the image background and the text is sufficient, especially if the images are of low quality and if the image needs to be enlarged in any way. Images of text must have a minimum contrast ratio of 4.5:1.\nFor images that do not contain text, but still convey meaning, the image components must still have sufficient contrast to ensure that the overall image is perceivable. WCAG 2.1 level AA specifies that graphical objects and author-customized interface components such as icons, charts, and graphs, buttons, form controls, and focus indicators and outlines, have a contrast ratio of at least 3:1."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:6c55c351-1622-4d32-8fef-ab52baeac5a1>","<urn:uuid:829f0ada-f08f-4987-835c-90a206d14cd8>"],"error":null}
{"question":"What key difference exists between screen-scraping in traditional integration patterns versus modern RPA technology in terms of their business application?","answer":"While both technologies interact with user interfaces, they serve different purposes. Traditional screen-scraping in integration patterns is typically used as part of a broader integration process, where the scraped data needs to be transformed or combined with data from other applications to provide a full solution. In contrast, modern RPA technology is specifically designed as a flexible automation capability that reduces the burden of repetitive tasks on employees, focusing on freeing resources for higher-value activities across multiple applications. RPA has evolved from workflow automation and artificial intelligence, allowing both citizen and expert developers to work with every technology across companies, lines of business, and industries.","context":["In part 1 of this series, “Good Integration Patterns Never Die – You Just Add More”, I described several challenges and identified a set of integration capabilities that solved these challenges. Toward the end of the article, this question was posed:\n…we identified the following types of integration capabilities: screen-scraping, file, EDI, messaging, application integration, Business APIs (with a security gateway), and events. What can we notice about this full list of integration capabilities?\nThe stated answer was that they are all still in use today. But the complete answer should also say that almost always these capabilities are used together to provide complete integration solutions. When looking only at individual integration styles, the expression “can’t see the forest for the trees” is applicable. Are you looking at the trees and not seeing the forest?\nThis article zooms out to see the bigger integration picture. Starting with each integration capability as a focal area we zoom out to see how that integration capability is combined with other integration capabilities to solve the full challenge. At the close of this article I attempt to predict the next set of challenges facing integration and how these might be solved.\nSolutions Use Multiple Integration Styles\nLet’s revisit the previously identified integration styles and see how they are often in combination with one another.\n- Screen-scraping – If a technical interface to an application is available, it will always be more robust and scalable than screen scraping. However, sometimes on older applications there is simply no choice – there is no way into the system other than the user interface, or there is no time to create one. Today robotic process automation (RPA) is used in business processes to invoke these older applications. But, if this were the only application required for the solution, we would simply use its existing interface. Because screen scraping is being used this implies that the intent is to obtain information from this application to be used in a larger context. This information may need to be transformed or combined with data from other applications to provide the full solution. The most common scenario is that this data is used by an overarching business process or application integration. This means the RPA interface is just part of a broader process or integration with many steps, involving use perhaps of application integration, business APIs, and messaging. In an ideal world we would wrap the RPA interface to make it appear like an API, event, or other type of connector to the calling integration flow such that it could be replaced later with a more robust interface without affecting the process.\n- File and EDI – Many businesses continue to exchange files to do business – either directly or through an EDI approach to format and transfer the file. But consider how the file is created before being sent and how is it read when it is received. Application integration may be used to build a file taking multiple records from various sources, format the records, build the file, and then invoke the necessary function to send the file. On the receiving side application integration can read the rows in the file and format these appropriately to feed into the order system, inventory system, CRM or other applications as required.\n- Messaging – messaging provides transaction integrity assuring the successful transfer of a request from one application to another application. The message interaction however is typically just one of application interactions needed to complete an overall scenario. Frequently each application defines formats for data elements differently. Application integration is used to transform the data into the format required for each application and to invoke messaging to communicate these tailored messages to each of the necessary applications. For example, an API call from the web or a mobile app to purchase a product may initiate an order. The API is verified by a security gateway, then the API invokes application integration which may obtain information from the CRM system, use messaging to place the order in the order system, invoke messaging again to update the inventory system, and yet again to invoke the billing and logistics systems while transforming the data appropriately for each invocation and response. Event notifications are sent as the item is shipped. Messaging is fundamentally involved at the transport layer, but there are many other integration patterns at play.\n- Business APIs – The letter “I” in API stands for interface. Business logic is invoked behind this interface, and while on some occasions this may be a single call to a single application (e.g. retrieve a specific data element), in other scenarios (such as the order scenario) what is behind the API is several additional integration capabilities teasing data into or out of multiple applications. Scenarios may include the application integration, messaging, or another capability. Most applications and databases now automatically generate system focused APIs, but these need manipulation by an integration component to become a useful part of a consumer-oriented business API.\n- Events –Events recognize that something happened. Sample events vary from items such as an airplane landing to an updated data record in a back-end system. Applications are being written using events as the trigger to take an appropriate action. In the context of this integration discussion events may themselves be sourced from integration activities that occur in the enterprise back-end systems - for example in our order scenario events are triggered through the application integration and messaging updates. An inventory replenishment application sees these events are decrementing available inventory and requests additional shipments when reorder levels are reached. How is this reorder done? Probably via an API, file, or EDI transaction.\n- Application integration – Application integration provides connectivity between applications transforming the data formats, handling the protocols, and routing the requests appropriately across multiple physical locations as required. Scenarios using messaging, RPA, and files in conjunction with application integration were previously described. Application integration is also frequently invoked via a Business API and it also may act as a source for events.\n- Security gateway – The security gateway is of course part of a larger solution. It is used for securing traffic either entering/leaving the enterprise or between applications in the enterprise – regardless of physical location. Frequent scenarios include use with API traffic, Application integration, and events.\nEach integration capability plays an important role and is optimized for a particular style of integration. Having many capabilities available each playing different roles provides a complete toolbox allowing the right tool and the right combination of tools to be used when needed.\nWhy Do You Care?\nThe scenarios described above probably seem familiar. There are circumstances where a single integration capability solves an issue, but there are many more scenarios where multiple styles of integration are used together. As additional integration needs surface in your environment you may find that adding new styles of integration to some of your existing constructs provides the optimal solution.\nWhen multiple integration styles are used together being able to share assets across the toolset speeds development. The output of one integration style is the input to another style. For example, having application integration or messaging constructs available to an API management solution allow a consumer-oriented API to be created by easily mapping the API inputs to these called application integration and messaging assets. But how easy is it to use these various tools together, and share assets between them?\nPiecing together a toolbox of integration capabilities is challenging. Do the various tools work together? How can assets be shared between tools? Where is the master copy? How many different user interfaces must be learned? How much capacity for each individual tool must be purchased now, and how much is needed in the future as additional projects arise?\nIBM has recognized these challenges are inhibitors to your integration success and has introduced the Cloud Pak for Integration to address these concerns.\nThe Cloud Pak for Integration has combines IBMs market leading integration capabilities for API Management, Application Integration, Messaging, Events, Files, and Security gateway. But this is not a bundle of individual products. The Cloud Pak for Integration is the product. It provides a common user interface that crosses all the integration capabilities and allows for a consistent look and feel - one interface to learn. All the products can be run and managed consistently using a common underlying infrastructure – one set of operations skills. A common asset repository provides sharing of assets between capabilities and is the place to find the integration assets. Furthermore, rather than needing to estimate how much of each individual capability must be purchased, with the Cloud Pak product you simply purchase “integration”. The purchase is a total capacity for integration and allows use of any/all the capabilities in the Pak within this limit. You may repurpose your usage between capabilities as needed. Early needs in your program may require significant development of new application integrations and perhaps messaging infrastructure. Later, once the core integrations are implemented the focus may shift to exposure of some APIs to a broader, perhaps external audience. You can at this point simply use the API management function that is already purchased assuming there is still some unused capacity either from the original purchase, or perhaps from the reduced utilization of the other capabilities. No need to make another separate tool purchase.\nDisclaimer: what follows is a prediction based on the author’s experiences – use at your own risk.\nBusinesses are embarking on a digital transformation bringing new business offerings to market faster, forming ecosystems across industry partners, and expanding their enterprise reach. IT organizations are moving to a hybrid cloud model, implementing agile development methodologies, and using a microservice architecture approach to deliver solutions faster. All of this is driving an increased need for integration as assets are needed to be combined rapidly from disparate locations – even bridging business boundaries. Bringing the integration capabilities together in a single cohesive product – Cloud Pak for Integration - is a tremendous productivity improvement and a base that can grow to support a wide range of integration needs.\nThe number of projects, teams, and integrations is accelerating and relying on a centralized expert team to perform all the integration tasks is not scalable and is a bottleneck to success. Additional developers must be able to perform integrations while maintaining the quality of the deliverables. Using automation to remove potential error prone tasks and make the development of integration assets simpler we can reduce the required level of expertise and enable a larger pool of resources to develop integrations. Adding artificial intelligence (AI) to guide the non-expert developer to do things the right way based on the expertise of the central team also helps scale resources without relying on the central team to do all the work. Prediction: Automation and AI will be the next big integration breakthrough technologies to help move integration passed this current challenge.\nIf you have questions, please let me know. Connect with me through comments here or via twitter @Arglick to continue the discussion.\nGraphics courtesy of Pixabay.com – Mike Goad, Mohamed Hassan, kalhh, Tumisu, and nvodika,","Frequently Asked Questions Around SAP Intelligent Robotic Process Automation\nWhat is RPA technology?\nRobotic process automation (RPA), sometimes called a software robot or bot - but much better called Digital Assistant to avoid any negative connotation associated with the term robot -refers to software that can be configured to do basic tasks across multiple applications, just as human workers do. The robot can be built e.g. to click through a workflow with multiple steps and applications, such as filing a form, uploading data or updating a spreadsheet. RPA software is designed to reduce the burden of repetitive, simple tasks on employees. Its flexible automation capabilities provide business agility, efficiency, and frees resources for higher value activities. It evolved from key technologies: workflow automation and artificial intelligence. The intelligence of RPA allows everyone from citizen developers to expert developers to work with every technology in every company, LoBs and industries. If you want to learn more about RPA, watch the videos on openSAP: SAP Intelligent Robotic Process Automation in a Nutshell.\nWhat is SAP's vision for RPA?\nIntroducing SAP Intelligent RPA was a logical next step for SAP on its way to expand its machine learning and artificial intelligence capabilities and drive customers to become the \"Intelligent Enterprise”. RPA has been a hype topic over years and has begun to reach a level of maturity now. However, current scenarios are underserving RPA's potential. To date, users are challenged with RPA’s low stability, scalability, and increased complexity that comes along with cost intensive bot maintenance. Therefore, they are looking for an integrated offering which serves as a futureproof, cognitive RPA. Additionally, the low-code/no-code approach has proven to be more than a trend. It forces us to create a simple-to-use developer environment to enable business users, citizen developers and expert developers to create bots easily. Currently, customers are adopting RPA rapidly to drive cross-app integration and automation, and therefore, the market is growing fast. RPA Experts share their thoughts on the future of Robotic Process Automation in the Blog. Check also the official product road map.\nWhat is SAP Intelligent RPA's business value for SAP’s customer?\nSAP Intelligent Robotic Process Automation provides traditional RPA capabilities paired with tight process integration capabilities to accelerate digital transformation and automate repetitive tasks in business processes. It enables expert developers, citizen developers and business process experts to build bots that augment human tasks across LoBs and Industries to:\n- Radically enhance efficiency by focusing people on added-value tasks\n- Generate savings by cutting process execution time o Increase service quality to reduce cycle times for revenue generating transactions\n- Increase compliance and analysis capabilities (through e.g. well-documented audit trails)\n- Improve operations to mobilize resources for high-value tasks at lower costs\n- Enhance user satisfaction by removing time-consuming, tedious tasks from the process\n- Reduce human error and gain speed and efficiency\n- Allow parallelization and immediate scalability\n- Introduce new technology without any footprint on the legacy\n- Provide agility and resilience to support strategic initiatives e.g. moving to SAP S/4HANA\nWhat are SAP Intelligent RPA's competitive differentiators?\nOne integrated offering for end-to-end automation\nAs SAP we would like to continue with our pedigree of delivering highly integrated solutions. In the RPA field, it gets more relevant than ever as customers want to automate and easily maintain their landscapes full of different tools, third party applications, heterogenous UIs etc. We would like to play on our strengths here and male bots robust, standardize more and more with solid APIs in the background, lifecycle-manage all the content not letting the automation stack become a \"tool forest\". With SAP Intelligent RPA, you have one full solution providing designtime, configuration/orchestration module and runtime to build and execute bots in attended and unattended mode.\nStay relevant with future-proof core\nWith SAP Intelligent RPA, we invite customers to get on board on our SAP S/4HANA-based core with an ability to flexibly automate in cloud / on-premise / hybrid environments while keeping their options open for adapting new innovations. In this exciting space of AI and automation, it is a great opportunity to stay on the latest platform and gear up for a sustainable future consuming innovations and not getting trapped in legacy systems, dated UIs etc.\nApart from the strategic relevance and an assured future relevance, SAP Intelligent RPA shall empower customers to leverage all those benefits an RPA solution would provide by forming a robust digital workforce across systems & deployments. Customers can gain speed, compliance, efficiency, and do more with their existing resources by focusing on high value activities. SAP provides bot templates for SAP S/4HANA and ECC as cross-LoB best practices pre-packaged content available on SAP Intelligent RPA Bot Store. The packages can be deployed directly from the Cloud Factory and can be adapted using the Cloud Studio. Together with the free trial as part of the SAP BTP trial, this serves a growing partner ecosystem to rapidly get started and become effective.\nSAP Intelligent RPA 2.0 provides an easy-to-use design studio running in the cloud, connected with the Cloud Factory component for orchestration, configuration and monitoring, connected to the bot store, that enables citizen developers and business process experts as well as expert developers to easily and quickly build bots. This low-code experience - and for simple use cases even a no-code experience - paired with a detailed user documentation and an active developer community, offers an intuitive user and developer experience.\nBenefit from SAP AI Portfolio\nB the first version of a cloud-based solution, SAP immediately enables partners and customers to build Digital Assistants which can immediately leverage the entire ML portfolio. There are use cases and demo scenarios with integration of SAP Conversational AI, SAP Workflow Management and much more to come.\nAllow direct use with SAP application at no additional costs\nSAP Intelligent RPA will not require a Indirect Use license when connecting with SAP application portfolio, but our customers, if they want to use other RPA tools, will be required to purchase SAP Indirect Use licenses.\nWhat is SAP Intelligent RPA 2.0?\nSAP Intelligent RPA 2.0 is the latest version of SAP Intelligent RPA with a new cloud-based design studio to build bots. The new Cloud Studio offers a low-code experience, even enabling several use cases to be built with no-code. SAP Intelligent RPA 2.0 is launched at SAP TechEd 2020. This updated product version is targeted not only at expert developers, but also at citizen developers and business process experts. It offers them simple-to-use bot building capabilities and follows the low-code/no-code approach, creating a path to hyperautomation. You can read all about SAP Intelligent RPA 2.0 in this blog series.\nWhat is Hyperautomation?\nEVERYWHERE: we find automation in every company, every line of business, every industry.\nIt is also for EVERYTHING: every technologies are candidates to be automated. And, of course, for EVERYONE: Business Analysts, Citizen developers, or bots expert developers. Everyone will be able to build their own bots.\nTo make it real, we need convergence of tools, and an easy way to do it:\n- Discovering existing repetitive tasks, based on logs, or same screens use, repetitive clicks (e.g. Alt+Tab Keys)\n- Performing and supervising end-to end processes, with Business Management Softwares\n- Using services to enhance the approach, such as Artificial Intelligence, Machine Learning, Natural Language Processing, Chatbots, and of course, Robotic Process Automation, to run the whole system.\nTo do so, low-code/no-code is here to help: involving all stakeholders to create the best of breed and complete automations, in a minimum of time.\nWhat is the Low-Code/No-Code concept?\nLow-Code / No-Code is now a must-have to build bots. It simply means that to build a bot, you don't need to start from scratch every single time. Reusing existing code was already possible before, or drag&droping activities in the designer. Now, these possibilities are enhanced, with even more activities. It is still possible to load pre-built bots, and record part of your activity. But now you can create small scripts and save them. Thus, they will be reusable, for your bot, your next bots, or reusable for colleagues, just as activities.\nHere's the concept of simple to develop, and ready to use elements, that can be shared, within IT landscape: Citizen developers, functional experts have some ideas and are able to build a feature, a set of tasks, and save it. After that, this script will be part of the whole system.\nTechnical experts are not left behind thought. The studio 2.0 is still the perfect technical environment. They can work in teams, to build more complex features, scripts, even bots, including connections, using APIs, involving several tricky applications. You'll be able to create your own library, containing your sets of tasks and ready to use automations.\nWhat are differences in functionalities between versions 1.0 and 2.0 of SAP Intelligent RPA?\nThe major evolution from 1.0 to 2.0 is the new Cloud studio which enables citizen developers and expert developers to speed up bot building to a new level. The Cloud Studio provides a complete cloud design experience without the need to go from desktop tool to other tools. All captures, workflows, tests and delivery, are integrated now in the cloud. And possibilities of co-working between developers themselves and citizen developers, by sharing automations, captures and other artifacts. The new SDKs provide activities for every function for full graphical bot building. SDKs are available for core functionalities including SAP GUI connector, for UI5, Outlook, Excel, Word, and PDF. You can find additional information in our SAP community calls.\nWhat are the key components of the solution?\nThese are the components of SAP Intelligent Robotic Process Automation:\na) Desktop Studio /Cloud Studio: to design the bot workflows, training the robots for the processes that they need to execute, to define interdependencies, conditions for the workflow, landscape integration / target system definitions etc. Learn more about the Desktop Studio here. Learn more about the Cloud Studio here. As of SAP Intelligent RPA 2.0, Cloud Studio enables simple bot building directly in the Cloud. Desktop Studio as an on-premise component further exists for legacy projects.\nb) Cloud Factory: to orchestrate, manage and define triggers and execution modes, and monitor software bots, including direct access to the Bot Store for deployment of pre-built bots. Learn more about the Cloud Factory here.\nc) Desktop Agent: an execution engine to run on user desktop or servers and execute the defined process. Learn more here. If you want to learn more about how SAP Intelligent RPA works, check out our YouTube channel with different playlists of demos and product videos.\nWhat is the difference between attended and unattended RPA, and what does SAP Intelligent RPA offer here?\nAttended bots, also referred to as digital assistants, are built to automate processes that still require interaction with the user (validation, specific actions that the bot can't do). They are deployed on the user's workstation and are typically launched by the users themselves.\nUnattended bots, also referred to as digital workers, automate processes without any human intervention. They are deployed on the server and are typically scheduled or started by a configured trigger mechanism.\nWith SAP Intelligent RPA, you can build both attended and unattended scenarios. For unattended scenarios, the solution offers a set of triggering functionalities. When building attended scenarios, you can enhance the user experience by enriching the user interface. When the robot is helping you to complete your tasks, it can also communicate information through e.g. new buttons, progress bars, or banners.\nHow to accelerate bot building with existing ready-to-use bots?\nIn the SAP Intelligent RPA Store - aka Bot Store - in which prebuilt bots are available for downloading, in different industries or LoBs. This prebuilt content can help developers to quickly build simple bots or combine 2 or more bots to build a new functionally rich one.\nHow to handle triggering and scheduling?\nSAP Intelligent RPA provides triggering and scheduling functionality as part of the Cloud Factory component. See more details here.\nCan you run Artificial Intelligence and Machine Learning features/services as part of RPA? How are AI and ML connected to Intelligent RPA?\nSAP Intelligent RPA is part of the Business Technology Platform (BTP). SAP BTP is also providing AI Business Services to handle unstructured data during the execution of a bot. The services can be integrated into SAP Intelligent RPA via APIs. Of course SAP Intelligent RPA is open to integrate with non-SAP services as long as they publish external APIs.\nWhich programming language is used with the tool?\nIs there compatibility with Chatbots?\nSAP Intelligent RPA and SAP Conversational AI can definitely be interconnected to provide great automations. SAP Conversational AI is a technology which is also accessible from SAP Business Technology Platform. Users can rely on an external Chatbot technology (WS/API calls) or use the integrated SAP technology. Check our dedicated webinars on our SAP community subpage.\nWhat operating system versions are supported by SAP Intelligent RPA?\nCurrently, Windows operating system is supported. Additional requirements such as browser version, system hardware, and software, can be found here.\nCan we extend the validity of trial RPA service subscription ?\nRPA trial account allows you to put your hands on the product and try out all its different components - Agent, Studio, Factory. The Trial landscape is dedicated to have a good taste on the product but cannot be used for productive usage, that's why it contains usage quotas. Currently users are fallowed to run up to 100 active (ready and running) jobs at the same time. Initially trial accounts can be used for 30 days. After that, users can extend the trial period to maximum 12 months. Read information about SAP BTP Trial Accounts and Available Quotas."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:c0708a56-d41a-4635-ae3d-c7ab35181102>","<urn:uuid:9d8c0090-b1a4-4ca8-bfcb-357d7948a9c9>"],"error":null}
{"question":"How do Medicare Part D and Extra Help compare in terms of their coverage of prescription drug costs?","answer":"Medicare Part D provides basic prescription drug coverage with limitations based on the plan's formulary, while Extra Help offers additional financial assistance worth about $5,100 per year for those who qualify. Part D covers a wide variety of prescription drugs but requires regular premiums and may have significant out-of-pocket costs. Extra Help can provide up to 100% coverage of benchmark Part D premiums, reduced or eliminated deductibles, and caps prescription drug copays as low as $1.35 for generics and $4 for brand-name medications. However, to use Extra Help benefits, you must be enrolled in either a Medicare Part D plan or Medicare Advantage plan with prescription drug coverage.","context":["What Drugs Are Covered By Part D Plans?\nMedications are prescribed for all types of medical conditions these days. Many Medicare recipients depend on prescription medications to reduce or eliminate the symptoms of various conditions. Unfortunately, prescription medications can be costly without insurance. Medicare recipients can save money on prescription drugs by enrolling in Part D. Prescription drug coverage does have limitations, however, so it’s important to understand what is and is not included in Medicare Part D.\nMedicare Benefits Solutions\nSep 8, 2020\nMedications covered by Part D plans\nMedicare Part D covers a wide variety of prescription drugs, but there are some limitations and exceptions. In order to know whether a drug is covered by your plan, you will need to check your plan’s formulary. A formulary lists all drugs that are included in a particular plan’s coverage. Even though there are standards set by the federal Medicare program, your plan may include or exclude specific medications. You’ll want to discuss your options with your physician, if possible before having a prescription written. An alternative medication may be available that addresses your condition and works within your plan’s formulary.\nWhat drugs are not covered?\nIf you find that no suitable alternative exists and the drug you are prescribed is not included in your plan’s formulary, you can file for an exemption with Medicare. This is usually done through your physician’s office, but you can also contact your plan manager or local Medicare office to learn more. You will need to provide a valid reason for needing an exemption, and if approved, you may still have limitations on coverage of your exempted medications.\nDoes Medicare Part D cover medications administered in a doctor’s office?Although most prescription medications are purchased from a pharmacy and self-administered at home, there are scenarios in which you may be given a medication outside of the home. A doctor may administer medication in his or her office to monitor its effectiveness or a medication could be used during an inpatient hospital stay. In these cases, Medicare Part D would usually not cover the cost of the medications. Instead, Medicare Parts A and/or B would apply. Medicare Part A provides coverage for care while in a hospital or skilled nursing facility, and Medicare Part B covers the cost of outpatient care administered in a doctor’s office or at a clinic.\nAre supplements covered?\nPart D prescription drug coverage does not usually include supplements and over-the-counter medications. This means that you will be required to cover the full cost of these items, and you will also be ineligible to file a claim for purchases.\nIf, however, you have a Medicare Advantage plan, you may be entitled to additional benefits that are not included in Original Medicare. These benefits may help you pay for supplements or provide reimbursement for purchases of some over-the-counter medications. To be sure that any medication you take is covered, you will need to refer to your plan’s formulary or speak with your Medicare plan manager\nComparing drug plans\nPrescription drug coverage was not originally included in the Medicare program when it was introduced in 1965, but as of 2006, coverage for self-administered drugs is available to Medicare recipients under Part D coverage. This optional coverage provides a benefit for individuals who frequently need medications or who have chronic conditions that require routine treatment through the use of prescription drugs. Review and compare the drug plans available in your area to find the one that meets your individual needs.\nFind a new Medicare plan\nGet a plan recommendation based on what's important to you.","Many or all of the products featured here are from our partners who compensate us. This may influence which products we write about and where and how the product appears on a page. However, this does not influence our evaluations. Our opinions are our own. Here is a list of our partners and here's how we make money.\nExtra Help is a federal program that helps Medicare beneficiaries with low income and limited resources pay for prescription drugs with Medicare Part D.\n“These programs are really important and can save people thousands of dollars per year,” says Erin Guay, paralegal and compliance officer at the Pennsylvania Health Law Project. If you qualify, the government estimates that Extra Help is worth about $5,100 per year.\nIf you’re having trouble affording your prescription drugs, the Medicare Extra Help program — also known as the Part D Low-Income Subsidy — might be able to help. (You must enroll in either a Medicare Part D plan or Medicare Advantage plan with prescription drug coverage to use Extra Help.)\nHere’s what you need to know about the benefits, requirements and application process for Extra Help.\nWhat are the benefits of Extra Help?\nPart D premium subsidies up to 100% of the cost of benchmark Part D premiums in your area. (You will need to pay the difference if you choose a more expensive Part D plan.)\nReduced or eliminated Part D deductibles.\nCaps on prescription drug copays as low as $1.35 for generics and $4 for brand-name medications.\nIn some cases, if you don’t qualify for the full benefits, you can still receive partial benefits on a sliding scale according to your income and resources.\nWho qualifies for Extra Help?\nYou can qualify for Extra Help if you are entitled to Medicare Part A and/or Part B, live in any of the 50 states or Washington, D.C., are not incarcerated and have both limited resources and limited income.\nReceiving assistance from the government programs listed below can also qualify you to automatically receive Extra Help.\nWho automatically receives Extra Help?\nYou are automatically eligible for Extra Help if you have Medicare Part A or Part B and at least one of the following applies:\nYou are entitled to Supplemental Security Income, or SSI.\nYou are eligible for full Medicaid coverage.\nYou are enrolled in a Medicare Savings Program as a Qualified Medicare Beneficiary, Specified Low-Income Medicare Beneficiary or Qualified Individual.\nIf you automatically qualify, you don’t need to apply to receive the benefits.\nIf you don’t automatically qualify, you should still apply for Extra Help if you might meet the income and resource criteria.\nWhat are the resource and income limits for Extra Help?\n“Resources” include your bank accounts, cash at home, investments like stocks and bonds, and real estate other than your primary residence.\nFor 2022, the Extra Help resource limits are:\nLimit for partial subsidies\nLimit for full subsidies\nExtra Help’s resource limits aren't as restrictive as other government programs, Guay says. For example, your home, personal possessions, vehicles, insurance policies, back payments from Social Security or SSI, and burial plots or contracts don’t count toward the resource limits.\nIn addition, if you will use some resources to pay for a funeral or burial, the limits are raised by $1,500 for an individual or $3,000 for a married couple.\nIncome limits for Extra Help are based on the Federal Poverty Level, or FPL. The income limits for those in the contiguous states and the District of Columbia (the numbers are different for those in Alaska and Hawaii) are:\nLimit for partial subsidies\nLimit for full subsidies\nPercentage of the FPL\n$1,699 per month, or $20,385 per year.\n$1,529 per month, or $18,347 per year.\n$2,289 per month, or $27,465 per year.\n$2,060 per month, or $24,719 per year.\nThe latest income thresholds for all states and household sizes can be found on the Department of Health and Human Services’ HHS Poverty Guidelines for 2022.\nAs with resources, there are many exceptions to what counts as income. For example, food, housing and home energy assistance from the government, scholarships, grants and even payments you get from others to help with your household expenses don’t count toward Extra Help’s income limits.\nHow do I apply for Extra Help?\nYou apply for Extra Help through the Social Security Administration:\nOnline: Visit ssa.gov/extrahelp.\nPhone: Call 800-772-1213 (TTY 800-325-0778).\nUsually, you can also apply in person at a Social Security office, but offices are closed to walk-ins during the COVID-19 pandemic. You might be able to arrange a special appointment for in-person help if you call first.\nYou will need to provide information about your household, work, resources and monthly income as part of the application.\n“The application process is actually a lot easier than other benefit programs because you have to fill out an application, but you typically don't have to provide documentation of income and resources the way you often do for Medicaid or the Medicare Savings Programs,\" Guay says. \"Social Security can usually verify the information you’re submitting.”\nYou don't need to be already enrolled in a Part D plan to file for Extra Help. If you qualify, you will receive a letter from the Social Security Administration, and the Extra Help benefits will apply when you're enrolled in a Part D prescription drug plan or Medicare Advantage plan with prescription drug coverage.\nIf you automatically qualify for Extra Help, Medicare will enroll you into a Part D plan. Otherwise, you'll need to enroll in a Part D plan or Medicare Advantage plan with Part D coverage yourself.\nWhat if I don’t qualify?\nEven if you think you might not qualify, Guay encourages applying to be sure.\n“Submit an application and get an actual official decision that would explain how they counted your income,” Guay says. “Then if you’re denied, you can look at the reason you were denied to decide if you want to appeal that decision. Or you might get a notice saying you’re eligible.”\nThere are other programs that might help as well, especially at the state level. “In Pennsylvania, we have a prescription program for seniors,” Guay says. “If people don’t qualify for Extra Help, they may be eligible for some of these other prescription programs that could limit their Medicare costs.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:320f445f-76ab-46f0-a517-338fae0307dd>","<urn:uuid:bc7d0221-0010-4f3a-8969-5d6ea0b066d6>"],"error":null}
{"question":"What materials are used in modern cameo production and how does this relate to current gemstone pricing trends?","answer":"Modern cameos are primarily produced from seashells and layered agates, with helmet shells from Madagascar and South Africa being most common. The main production center for shell cameos is Torre del Greco, Italy. For agate cameos, popular color combinations include white on blue, white on black, and white on red-brown, with layers often dyed for stronger contrast. Regarding pricing, this aligns with current market trends where colored gemstone jewelry generally commands higher profit margins than diamonds - manufacturers see 40% margins with colored stones versus 30% with diamonds, while retailers achieve 65% margins with colored stones compared to 33% with diamonds.","context":["History of Cameos and Cameo Jewelry\nWhat is a Cameo?\nCameo is a method of carving a piece of jewelry or other object with a raised relief, and where the colour of the relief contrasts with that of the background.\nThis is achieved by carving a piece of material with a flat plane where two different colours meet, taking away the entire first colour except for the image and leaving the background as a contrasting colour.\nDuring the course of history cameos have been used to depict rulers and famous leaders. In addition, famous leaders, like Alexander the Great, had them carved portraying themselves as gods.\nMythological and classical themes have also been popular in cameo carving and a lot of the modern ones are images of a beautiful woman in profile.\nNowadays a lot of cameos are carved from seashells, a production method that started in the fifteenth or sixteenth century and made popular in the time of Queen Victoria.\nThese days’ helmet shells from Madagascar and South Africa are the most commonly used shell and the main centre for carving from seashells is Torre del Greco in Italy.\nThe seashells are first marked with a series of ovals and then cut into a series of oval blanks. The highly skilled carver then uses a metal scraping tool called a bulino for the bulk of the carving of the cameo, and then a number of different gravers are used to complete the job.\nA grinding tool is used for faster production and when the cameo has been finished, it is soaked in olive oil, washed with soapy water and then polished with a hand brush.\nModern cameos can also be produced from layered agates. The most popular colour combinations are white on blue, white on black and white on red-brown, and the layers are often dyed to make the colour contrast stronger. Most of the current agate ones are produced with the use of an Ultrasonic Mill.\nA master design is produced by a highly skilled craftsman and the Ultrasonic Mill allows multiple copies of this master design for a cameo to be turned out very rapidly by pressing a master die on the agate blanks. A fine layer of diamond slurry is used to help with the cutting and the die vibrates ultrasonically in a vertical motion.\nCameos have been used as adornments for many things and have even been attached to military accessories such as helmets, sword handles and breastplates. They have also been used to ornament vases, dishes and cups.\nCameos were used as decorations in houses in Roman times and later in Georgian houses in Britain. As well as being carved from shell and hard gemstones such as agate and sardoynx, they have also been carved from the volcanic lava of Mount Vesuvius and in jet from Whitby.\nThe Origins of the Cameo\nThe origins of cameos go back into our distant past and to the development of ancient carving techniques which allowed figures, known as petroglyphs, to be carved into the rocks as long ago as 15,000 BC.\nTrue cameos, which are carvings in relief, were first produced in the Egyptian port of Alexandria in around 300 BC.\nThis development was generated from the introduction of the sardonyx, a multi-layered gemstone, into Egypt and Europe from India and Arabia.\nThese cameos created a picture from the lighter, upper layers of the stone which stood out from the lower, darker layers. The ancient craftsmen used simple belt driven drills and carved the finer details with hand held metal gravers.\nAn amazing example of bas-relief carved in sardoynx is the Farnese Cup, named after the family who acquired it in the fifteenth century. The Farnese Cup was carved around 150 BC and had the head of the gorgon Medusa complete with snakes carved on the underside of the cup.\nIt was believed in ancient times that having Medusa’s head carved on plates and drinking vessels would protect the person using them from being poisoned.\nThe Greek and Roman cameos were commonly carved with images of gods and goddesses, mythological symbols, and portraits of beautiful women. In ancient Greece, young women would use cameos carved with an image of the Greek god of love, Eros, on them to express their desire for a young man.\nThey also became very popular in Roman times. These antique cameos were placed in jewelry or created as portraits to hang on the walls of wealthy Roman’s villas. Many rich Romans owned Greek slaves who were skilled stone carvers who were set to the task of carving intricate cameo’s for their masters.\nAn exquisite example from the Roman era is the Gemma Augusta, which was carved in honour of the Roman Emperor Augustus and another is the Grand Camee de France jewel that was carved by Dioskourides in 25 BC.\nHistory of Cameos from the Renaissance\nThe art of carving cameos was largely lost during the Dark Ages and it wasn’t until the Renaissance brought a renewed interest in classical history and mythology that the lost art began to be relearned.\nIt is believed that seafarers from around the Bay of Naples began this process by carving sea shells into cameos during long sea voyages.\nIn Renaissance Italy, Pope Paul II was an enthusiastic collector and his fascination with cameos might even have played a part in his death as he loved to wear intricately carved ones on his fingers as rings. It is thought that wearing these cameo adornments led to his hands becoming so cold that he caught a chill and died.\nElizabeth I of England was supposed to have given cameos to her favoured loyal subjects as a sign of her regard and esteem. The best craftsmen that were carving them during this period were in Rome and Florence. During the 17th and 18th centuries cameos were collected by the nobility as a sign of wealth and prestige.\nNapoleon was also another lover of these jewels and he even wore a cameo on his wedding day and established a school in Paris that took in apprentices and taught the art of carving them.\nThe Russian Empress, Catherine the Great, was another who had an impressive collection of cameos.\nDuring the 18th century the English potter Josiah Wedgwood, started creating his famous blue pottery and also made Wedgwood cameos in two parts from jasperware ceramics in many different sizes and using lots of different profiles.\nCameos in the Victorian Era\nDuring the Victorian era the popularity of shell cameos grew and eclipsed the popularity of those carved from gemstones like agate and sardonyx.\nThe earliest seashell cameo’s date from the Renaissance and were usually white on a grey background, and these Victorian ones were carved mainly from cowry or mussel shells.\nBy the mid-18th century, shells from the West Indies were also being used, especially helmet shells and queen conch shells. It was also during this 19th century period that profiles of beautiful women on cameos started to predominate.\nDuring the Victorian era the carving of these jewels in the likeness of the owner also became very popular. The Victorian era saw the introduction of mass production and industrialisation, and so cameos could for the very first time be worn as jewelry by ordinary people.\nFinding The Best Cameo Jewelry\nIf you are a fan and wish to choose some jewelry, either for yourself or as a special gift, you will find a huge selection both in stores and online.\nThere are many contrasting colours to choose from, with the white on black, white on red-brown and white on blue colour combinations still being the most popular. The most popular image is still the profile of a beautiful woman.\nThis beautiful jewelry can be found in many different styles and to suit every budget. Traditionally, the pendants were worn on black velvet ribbons around the neck, but you can now get them attached to ribbons or chains.\nChoose from cameos set in gold and silver, and from a huge range of brooches, earrings, rings, bracelets and necklaces. Remember that when you wear your gorgeous jewelry, you are following in the footsteps of many mighty rulers and fashionable beauties from centuries gone by.","Color Stone Market Poised for Major Growth\nGem sales have been steady and rising since 2018.\nMood-boasting, fashion-forward and statement-making, color gemstones are in high demand among consumers in all product categories including fashion, bridal, men’s, self-purchase, and youth jewelry.\nA study conducted by MVI Marketing in the fall of 2020 finds that gemstones capture more attention with younger consumers interested in buying precious stone jewelry, for both lifestyle/fashion and engagement/wedding reasons.\nOver 93% of the more than 1,100 U.S. jewelry consumers polled says they love sapphire, ruby and emerald; more than 40% recently purchased these gems; and nearly 70% likely will buy them in the next two years, cites Liz Chatelain, president of MVI Marketing.\nGem sales have been steady and rising since 2018. The acceptance of color stones as engagement and wedding ring centers and accents has jumped from less than 10% a decade ago to capturing more than 30% of the market, according to recent data from The Wedding Report.\nMoreover, the margins with color stones are higher than those realized with diamonds. Over 92% of manufacturers see margins of 40% with color versus 30% with diamonds; 75% of retailers cited margins of 65% with color versus 33% with diamonds.\nYet, the gem category represents just 9% ($2.4 billion) of U.S. specialty jeweler sales totaling $27 billion, versus diamonds that capture 54% ($14 billion). With over 60% of consumers indicating they want to see more color stone jewelry across product categories in jewelry stores, there’s lots of room to bloom.\nIn a companion survey of 50 jewelry manufacturers and retailers, MVI research finds that targeted educational and sales training for color stones is wanted and needed in the trade to grow sales across categories. Many jewelers surveyed said they plan to spend more time and resources on promoting color. In fact, the research found retailers have increased their color stone jewelry to 15% of their wedding jewelry website posted products. Jewelers say color stones have injected life in their engagement and wedding ring displays.\nFrom fashion and folklore to family and self-actualization, gemstones represent different things to different people. Individual colors, specific stones, and compelling combinations bring people to the gemstone category. There are limitless opportunities for jewelers to merchandise and market color in fine jewelry design, cheers Theresa Namie, merchandise manager for Ostbye.\nKey messages in marketing color stones are their ability to customize any piece, connect with fashion and mood, and tell the story of their journey to market.\nCora Lee Colaizzi, marketing director and senior merchandiser for Quality Gold hails color stones, natural or lab-created, the new way of customizing fashion and wedding jewelry. She finds that color stones have elevated custom design services in fine jewelry.\n“Consumers are searching for things that are unique and express individuality, and color stones provide a wide range of options in gem types, colors and cuts.,” cheers Parag Desai, executive vice president of merchandising and marketing for SDC Creations.\nColor gemstones in general, and birthstones specifically, offer jewelers a profitable, consistent selling product category. Birthstones and colorful gem-set jewelry is preferred by a majority of jewelry consumers, finds The Plumb Club Industry & Market Insights 2021 report.\nSixty percent of the over 1,000 U.S. jewelry consumers surveyed in 2021 say they look for jewelry with a birthstone in the design because of its meaning. The popular fashion portal whowhatwear.com hails birthstones a subtle way of personalizing jewelry, a less obvious symbol that feels more like a secret.\n“Everyone has a birth month, and many consider it part of their identity, so birthstone jewelry makes logical sense to most seeking a meaningful self or gift purchase,” says Colaizzi. Birthstones have become a huge part of the gemstone category, not only in signature jewelry designs, but also added as accents to customize monograms and initials, signet rings, and engagement and wedding rings. In fact, Quality Gold’s entire bridal collection is available for custom design. “Every mounting can be designed with a gem center or accent stones.”\nFashion & Style\nDiscussing gem-set jewelry in terms of the latest fashions resonates with style-conscious, self-purchasing women, who like to stay on trend, especially those looking for true statement pieces, says Namie. She advocates lifestyle imagery as effective on social media, as well as in store to inspire ideas for how women can transform their own wardrobes. Capitalize on broader style conversations by highlighting gemstone designs that work with popular/seasonal fashions, or to transform that little black dress.\nPantone is one way of discussing popular colors and exciting color combinations that elevate the color conversation in style and design, as people, surrounded by lively hues, are paying more attention to what evokes emotions. Siera Burt, graphic designer and marketing coordinator for SDC Creations, also finds nature, especially references to favorite flowers, a way to express color and personality when marketing and merchandising gemstones.\nGiven the strength of blue hues in the spring 2022 palette — soft to saturated, representing both comfort and joy — gems like London and Swiss blue topaz, aquamarine, and blue sapphire will continue to be bestsellers. Red also is expected to be a hot color this year. A symbol of power and strength, red gems like ruby and garnet are forecasted to be important in jewelry design in 2022, as well as raw stones and gems with unique markings.\nIn the color of the year, Very Peri offers a blend of cool and warm tones, the faithfulness and constancy of blue with the energy and excitement of red. Colaizzi says this color, best captured in amethyst, pairs nicely with most of the seasonal shades.\nMood-boasting pieces with uplifting colors in playful designs represent a wave of optimism in trending jewelry. TPC Insights 2021 found that 78% of respondents say colorful gemstone jewelry improves their mood, with 67% wearing jewelry that expresses their mood and personality.\nAnd it’s not just women enamored with color stones, but men as well. A recent Forbes article cites the growing interest in men’s jewelry, with Gen Z and younger millennials the focus of most menswear brands. Men’s jewelry was “abundantly apparent, from bead necklaces to metal elements of nature, even shells and pearls,” Forbes reports.\nA key takeaway from MVI’s gemstone study is that jewelry consumers consider responsible sourcing in their purchase decision-making. More than 40% find it very important to know country of origin for color gemstones, and nearly 50% find it very important to know that workers at a mine or cutting factor are treated well.\nTPC Insights 2021 also finds most consumers care that their jewelry is responsibly sourced, sustainable and ethical in its journey to market. On a scale from one to 10 (10 most important), 20% rated those considerations a 10, with the average ranking 6.5. Nearly three quarters of respondents say they’re willing to pay more for a sustainably sourced product, 26% say a lot more.\nToday’s consumers, driven by millennials and Gen Z, look for sustainability and transparency in their brands, says Valerie Fletcher, vice president of design and product development for Original Design Inc. (ODI). So, as ODI, a Taché Alliance company, expands into color, it was the ideal partnership with Greenland Ruby to bring rubies and pink sapphire from this source for its latest LOVEFIRE collection. Each piece comes with a guarantee of origin, with the gemstones’ journey tracked mine-to-market, and a portion of gem sales supporting environmental initiatives in Greenland through the Pink Polar Bear Foundation.\nThe global gemstones market is expected to grow at a CAGR of 5% through this decade, as a number of recent market research reports forecast. North America is one of the most significant markets for color stones. Growth is attributed to high consumer purchasing power and rising demand for precious stones, as well as the increasing acceptance of lab-created gems, and broader trend for customizing jewelry design, finds TPC Insights."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:10b04dd5-b898-49fe-9253-10956a91c7c6>","<urn:uuid:26567a8b-de32-4525-99f4-60f9966bf0bd>"],"error":null}
{"question":"How do food banks implement sustainable delivery practices while ensuring effective nutrition education, and what are the key logistics and marketing challenges they face?","answer":"Food banks implement sustainable delivery practices through several approaches including packaging optimization to reduce mass and fuel consumption, warehouse impact reduction through improved heating/cooling systems, and logistics optimization through effective scheduling and back-hauling opportunities. They also need to address product damage during transportation through proper packaging and handling. Regarding nutrition education, food banks face challenges in adopting healthy food initiatives including procurement and handling of perishable foods, limited staffing, and other resource constraints. They must also consider that while clients may benefit from nonjudgmental nutrition guidance, some may have practical barriers like lack of cooking equipment or transportation challenges. From a marketing perspective, organizations must carefully evaluate the costs and benefits of pursuing green marketing claims, as these often require significant investment in time and money to demonstrate conformity with eco-label requirements or perform detailed assessments.","context":["Food pantry and food bank healthy food initiatives combine hunger relief efforts with nutrition information and healthy eating opportunities for individuals and families with low incomes. Such initiatives offer clients healthy foods such as fruits, vegetables, whole grains, low-fat dairy products, and lean proteins, and can implement client choice models. Initiatives can include fruit and vegetable gleaning programs, farm Plant-a-Row efforts, and garden donations. Healthy food initiatives can also modify the food environment via efforts such as on-site cooking demonstrations and recipe tastings, healthy meal kits, featured product placements, produce display stands, or point-of-decision prompts. Some food pantries and food banks establish partnerships with health and nutrition professionals to offer screening for food insecurity and medical conditions (e.g., diabetes), provide nutrition and health education, and health care support services as part of their healthy food initiatives (FA-Nutrition initiative, Bush-Kaufman 2019*).\nExpected Beneficial Outcomes (Rated)\n- Increased healthy food consumption\n- Increased food security\nOther Potential Beneficial Outcomes\n- Improved nutrition\n- Improved weight status\n- Reduced emissions\nEvidence of Effectiveness\nThere is some evidence that food pantries and food banks that use healthy food initiatives increase fruit and vegetable consumption, improve diet quality, and increase food security for clients more than traditional food pantries and banks (An 2019, Grabow 2020, Cochrane-Martin 2018). Available evidence associates such initiatives with a significant increase in fresh fruit and vegetable distribution through food pantries (Long 2019), and when implemented as intended, with improved quality of foods available and selected by pantry clients (Caspi 2019). However, additional evidence is needed to confirm effects especially over the long-term (An 2019).\nFood pantry initiatives that provide nutrition education and recipe demonstrations can increase the variety of fruits and vegetables and the amount of produce clients consume, and can improve nutrition knowledge, home cooking skills and habits, and food security status (An 2019, Flynn 2013*, Keller-Olaman 2005*). In a Rhode Island-based study, food pantry clients participating in a plant-based cooking and nutrition education program and preparing these meatless recipes at home 2-3 meals per week, improved weight status and reduced total food costs (Flynn 2013*). In a rural Missouri case study, healthy food initiatives at food pantries are associated with improved healthy food access and food choices, as well as increased gardening knowledge, support, and resources (Chapman 2017). Program evaluations of comprehensive healthy food initiatives that include a client choice model with a variety of featured healthy food options, connections to community resources and services, a respectful and welcoming environment, and on site opportunities for clients to build skills are associated with significant improvements in food security, self-sufficiency, and diet quality over time (Cochrane-Martin 2018).\nFood banks and pantries with healthy food initiatives that use client choice models for food selection and tailor messaging, recipes, and food tips for their clients appear to have greater effects on healthy eating decisions and vegetable use than generic messaging and food tips (Clarke 2011*). One Utah-based study suggests food pantry healthy food nudge programs may improve dietary choices, with the largest effects reported among Hispanic survey respondents (Coombs 2020*). A Connecticut-based study suggests ingredient bundles with recipe tastings may increase the likelihood that food pantry clients select healthier food options more than tastings alone (Stein 2019). Healthy food display interventions can help food pantry clients select healthier food items (An 2019) and may improve diets among people who are food insecure (Grabow 2020).\nHealthy food initiatives in food pantries that include gleaning, farm, or garden donations can reduce waste and increase the availability of local, seasonal produce for participants, which may reduce emissions from fossil fuels used to produce, process, and transport food (Ringling 2020, FAO-Food waste, Hic 2016, SSSA-McIvor 2017, CCAFS-Campbell 2012). Healthy food initiatives in food pantries may also reduce the energy intensity of an individual’s diet if more plant-based foods are consumed in place of animal products (Ringling 2020).\nSuccessful healthy food initiatives are most often found in food pantries or food banks that have written nutrition guidelines, client choice distribution models, and adequate refrigerator storage (Long 2020). Building a healthier food inventory, enhancing partner agency access, storage, and distribution capacity for fresh foods, developing nutrition education, and expanding community partnerships and settings for healthy food distribution are key components for healthy food initiatives at food banks (Wetherill 2019*). Establishing strong nutrition policies at food banks is a suggested strategy to improve the nutritional quality of food distributed; however, such changes have the potential to alter relationships with existing donors, possibly reducing the total amount of food available for distribution (Handforth 2013*). Interviews with food bank and food pantry personnel suggest that other challenges to adopting healthy food initiatives include the procurement, handling, and monitoring of large quantities of perishable foods (Campbell 2013*), as well as limited staffing and other resources (An 2019). Such interviews also suggest food pantry clients may benefit from nonjudgmental nutrition guidance; however, some clients may have other reasons not to choose healthy foods, for example, lack of cooking equipment or transportation challenges (Cooksey-Stowers 2018*).\nThe cost of healthy food initiatives varies; some are very low cost and relatively straightforward to implement, such as healthy food display interventions (Grabow 2020).\nImpact on Disparities\nFeeding America, a hunger-relief organization with over 200 member food banks and 60,000 food pantries and meal programs nationwide, is one organization working nationally to increase healthy foods distributed through food banks (FA-Who we are). As of 2020, 69% of foods distributed through Feeding America member food banks were considered healthy and align with USDA nutritional guidelines. Feeding America also operates mobile food pantries to bring healthy foods to families in underserved areas and the Produce Matchmaker program that connects growers directly with food banks to donate surplus, fresh produce (FA-Nutrition initiative). In response to the COVID-19 pandemic, Feeding America’s network of food banks has implemented many new drive-through and outdoor food distribution locations, delivered emergency food boxes, and worked with many schools and school districts to provide grab-and-go cold meals for children and families while schools are closed (FA-Who we are).\nFarm to Food Bank, Farm to Food Pantry, and Farmers Ending Hunger programs are in place in many states, including Connecticut (CFB-Farm donations), Georgia (GFBA-FTFB), Kentucky (KAFB-FTFB), Maryland (MFB-FTFB), Montana (CFC-MT FTFB), Oregon (FEH-Oregon), and Rhode Island (RI Food Bank-Community farms). Some states offer growers a tax credit for donations of excess produce to state-sponsored food banks, as in Arizona, California, Colorado, Oregon (NRDC-Gunders 2012), Iowa (IA DOR-FTFD tax credit), and Kentucky (KAFB-FTFB).\nMany food banks use farming, gardening, gleaning, and other healthy food programs to procure fresh produce and support healthy eating for their clients, as in Community Food Bank of Southern Arizona in Tucson (CFBSA-Programs), Chester County Food Bank in Pennsylvania (CCFB-Programs), Inter-Faith Food Shuttle in Raleigh, North Carolina (IFFS-Food bank), and Santa Barbara County Food Bank (SBCFB-Programs). Ample Harvest, a non-profit organization, helps home and community gardeners donate their excess produce to over 8,700 food pantries in all 50 states (Ample Harvest-Garden donations). The Missouri Department of Agriculture supports the Grow & Give program part of the 10,000 Garden Challenge, which encourages home and community gardens to donate a portion of their produce to local food banks and pantries (MDA-Grow and give 2011).\nMany food pantries have implemented more comprehensive healthy food initiatives. For example, the More Than Food initiative builds capacity for food banks and food pantries to offer client choice, connections with community services, supports to reach their goals, and a welcoming environment; this initiative was developed through evaluations of the Freshplace food pantry program in Hartford, CT (Foodshare-More than food, Foodshare-Freshplace). The Fresh Start program at Kelly Memorial Food Pantry in El Paso, Texas also provides healthy food assistance, health and wellbeing classes, and case management services to address the root causes of hunger and overcome barriers to personal and financial independence (Fresh Start). The Whole Body Approach to Wellness (WBA) program, developed through a collaboration between Northern Illinois Food Bank and Northern Illinois University, is a health promotion, nondiet program to help food pantry clients develop a healthy relationship with food and fitness, while respecting body, shape, and size diversity and focusing on a holistic approach to wellness and behavioral outcomes. Participants who are at risk for or already diagnosed with Type 2 diabetes also receive additional fresh produce as part of the program (NIFB-WBA).\nTo increase the amount of healthy foods available, statewide food policy councils can require food banks to spend a portion of funds on fresh fruits and vegetables and low-fat milk, for example, the New York State Council on Food Policy (CDC DNPAO-FPC).\nAmple Harvest-Garden donations - Ample Harvest. Find a local pantry if you have garden surplus you want to donate.\nChangeLab-Banking on health - ChangeLab Solutions. Banking on health: Improving healthy beverage & nutrition standards in food banks.\nCHD-Healthy food planning 2005 - Columbus Health Department (CHD). Improving access to healthy food: A community planning tool. 2005.\nSRTSNP-Safe routes to healthy foods - Safe Routes to School National Partnership (SRTSNP). Healthy communities: Safe routes to healthy foods.\nISU-Food and sustainability resources - Iowa State University (ISU), Sustainable Food Processing Alliance. Online resources for food and sustainability.\nCitations - Evidence\n* Journal subscription may be required for access.\nAn 2019 - An R, Wang J, Liu J, et al. A systematic review of food pantry-based interventions in the USA. Public Health Nutrition. 2019;22(9):1704-1716.\nGrabow 2020 - Grabow KN, Schumacher J, Banning J, Barnes JL. Highlighting healthy options in a food pantry setting: A pilot study. Family and Consumer Sciences Research Journal. 2020;48(3):263-275.\nCochrane-Martin 2018 - Martin A, Booth JN, Laird Y, et al. Physical activity, diet and other behavioural interventions for improving cognition and school achievement in children and adolescents with obesity or overweight. Cochrane Database of Systematic Reviews. 2018;(3):CD009728.\nLong 2019 - Long CR, Rowland B, McElfish PA. Intervention to improve access to fresh fruits and vegetables among Arkansas food pantry clients. Preventing Chronic Disease. 2019;16(1):1-7.\nCaspi 2019 - Caspi CE, Canterbury M, Carlson S, et al. A behavioural economics approach to improving healthy food selection among food pantry clients. Public Health Nutrition. 2019;22(12):2303-2313.\nFlynn 2013* - Flynn MM, Reinert S, Schiff AR. A six-week cooking program of plant-based recipes improves food security, body weight, and food purchases for food pantry clients. Journal of Hunger & Environmental Nutrition. 2013;8(1):73-84.\nKeller-Olaman 2005* - Keller-Olaman SJ, Edwards V, Elliott SJ. Evaluating a food bank recipe-tasting program. Canadian Journal of Diabetic Practice and Research. 2005;66(3):183-186.\nChapman 2017 - Chapman D. Healthy food access in Missouri food pantries through evidence-based intervention. Journal of Human Sciences and Extension. 2017;5(1):141-157.\nClarke 2011* - Clarke P, Evans SH, Hovy EH. Indigenous message tailoring increases consumption of fresh vegetables by clients of community pantries. Health Communication. 2011;26(6):571-582.\nCoombs 2020* - Coombs C, Savoie-Roskos MR, LeBlanc H, Gast J, Hendrickson J. Nudging urban food pantry users in Utah toward healthier choices. Health Promotion Practice. 2020:1-7.\nStein 2019 - Stein EC, Stowers KC, McCabe ML, White MA, Schwartz MB. Ingredient bundles and recipe tastings in food pantries: A pilot study to increase the selection of healthy foods. Public Health Nutrition. 2019;22(9):1717-1722.\nRingling 2020 - Ringling KM, Marquart LF. Intersection of diet, health, and environment: Land grant universities’ role in creating platforms for sustainable food systems. Frontiers in Sustainable Food Systems. 2020;4(70).\nFAO-Food waste - Food and Agriculture Organization of the United Nations (FAO). Food wastage footprint & climate change.\nHic 2016 - Hic C, Pradhan P, Rybski D, Kropp JP. Food surplus and its climate burdens. Environmental Science and Technology. 2016;50(8):4269-4277.\nSSSA-McIvor 2017 - McIvor K. Soils in the city: Community gardens. Soil Science Society of America (SSSA). 2017.\nCCAFS-Campbell 2012 - Campbell B. Is eating local good for the climate? Thinking beyond food miles. Research Program on Climate Change, Agriculture and Food Security (CCAFS), CGIAR Research Programs. 2012.\nLong 2020 - Long CR, Narcisse MR, Rowland B, et al. Written nutrition guidelines, client choice distribution, and adequate refrigerator storage are positively associated with increased offerings of Feeding America’s detailed Foods to Encourage (F2E) in a large sample of Arkansas food pantries. Journal of the Academy of Nutrition and Dietetics. 2020;120(5):792-803.e5.\nWetherill 2019* - Wetherill MS, White KC, Seligman HK. Nutrition-focused food banking in the United States: A qualitative study of healthy food distribution initiatives. Journal of the Academy of Nutrition and Dietetics. 2019;119(10):1653-1665.\nHandforth 2013* - Handforth B, Hennik M, Schwartz MB. A qualitative study of nutrition-based initiatives at selected food banks in the Feeding America network. Journal of the Academy of Nutrition and Dietetics. 2013;113(3):411-415.\nCampbell 2013* - Campbell EC, Ross M, Webb KL. Improving the nutritional quality of emergency food: A study of food bank organizational culture, capacity, and practices. Journal of Hunger & Environmental Nutrition. 2013;8(3):261-280.\nCooksey-Stowers 2018* - Cooksey-Stowers K, Read M, Wolff M, et al. Food pantry staff attitudes about using a nutrition rating system to guide client choice. Journal of Hunger & Environmental Nutrition. 2018;14(1-2).\nCitations - Implementation Examples\n* Journal subscription may be required for access.\nFA-Who we are - Feeding America (FA), Hunger and Health. Who we are and our COVID 19 response.\nFA-Nutrition initiative - Feeding America (FA). Healthy communities need healthy foods.\nCFB-Farm donations - Connecticut Food Bank (CFB). Farm donations.\nGFBA-FTFB - Georgia Food Bank Association (GFBA). Farm to food bank program (FTFB): Georgia farmers feeding Georgia families.\nKAFB-FTFB - Kentucky Association of Food Banks (KAFB). Farms to food banks (FTFB).\nMFB-FTFB - Maryland Food Bank (MFB). Farm to food bank (FTFB).\nCFC-MT FTFB - Community Food Co-op (CFC). Southwest Montana Farm to Food Bank program (MT FTFB): Support Farm to Food Bank.\nFEH-Oregon - Farmers Ending Hunger (FEH). How Farmers Ending Hunger works and the Oregon Food Bank partnership.\nRI Food Bank-Community farms - Rhode Island Community Food Bank. Community farms growing food for the food bank.\nNRDC-Gunders 2012 - Gunders D. Wasted: How America is losing up to 40 percent of its food from farm to fork to landfill. New York City: National Resources Defense Council; 2012.\nIA DOR-FTFD tax credit - Iowa Department of Revenue (IA DOR). Farm to food donation (FTFD) tax credit.\nCFBSA-Programs - Community Food Bank of Southern Arizona (CFBSA). Programs and services include home and community gardening, farmers' markets, gleaning, education, and advocacy.\nCCFB-Programs - Chester County Food Bank (CCFB). Food bank programs include gardening kits, gleaning, farming, nutrition education, and healthy cooking classes.\nIFFS-Food bank - Inter-Faith Food Shuttle (IFFS). A food bank pioneering innovative, transformative solutions to end hunger: We feed, we teach, we grow.\nSBCFB-Programs - Santa Barbara County Food Bank (SBCFB). Programs moving the community from hunger into health.\nAmple Harvest-Garden donations - Ample Harvest. Find a local pantry if you have garden surplus you want to donate.\nMDA-Grow and give 2011 - Missouri Department of Agriculture (MDA). 10,000 Garden Challenge joins forces with Missouri's food banks to 'Grow & Give.' News Release. 2011.\nFoodshare-More than food - Foodshare. More than food: Because it takes more than food to end hunger.\nFoodshare-Freshplace - Foodshare. More than food: History of the Freshplace program.\nFresh Start - Fresh Start. Fresh Start program at Kelly Memorial Food Pantry in El Paso, Texas.\nNIFB-WBA - Northern Illinois Food Bank (NIFB), Feeding America. Health & nutrition blog: Whole Body Approach to Wellness (WBA) program.\nCDC DNPAO-FPC - Centers for Disease Control and Prevention (CDC), Division of Nutrition Physical Activity and Obesity. DNPAO state program highlights: Food policy councils (FPC).\nRelated What Works for Health Strategies\nTo see citations and implementation resources for this strategy, visit:\nTo see all strategies:","HOW TO GO ABOUT IT\nThe marketing function plays an important role in deciding how to market and sell the product. This is particularly important for eco- innovation because product marketing benefits can often be a key part of the business case for eco-innovation, through eco-labelling for example. However, capitalizing on these potential benefits can be tricky due to the challenge of quantifying sustainability benefits and the proliferation of eco-labels and green marketing claims, which have led to consumer scepticism in some markets. Also, making green marketing claims will often require a significant investment of time and money in order to demonstrate conformity with the requirements of an eco-label or to perform a detailed Life Cycle Assessment in order to obtain an Environmental Product Declaration. It is therefore important to establish the likely costs and benefits of pursuing green marketing claims before committing to specific marketing activities and campaigns.\nKey questions to discuss with the company to support innovation in marketing are:\n- Are your customers interested in sustainability performance? Or are they simply interested in the potential financial or functional benefits of eco-innovative products such as reduced energy consumption?\n- If claims are made about the sustainability benefits of our products, can we back them up with solid (preferably quantitative) evidence?\n- Are there recognized eco-labels or sustainability standards that are relevant for our markets?\n- What are our competitors saying about the sustainability performance of their products?\n- Would there be business benefits from communicating our sustainability message to other stakeholders such as possible financiers, local governments or environmental lobby groups?\nIt is critical whenever making marketing claims about the environmental performance of a product to avoid ‘greenwash’ – confusing or misleading claims that attempt to highlight certain environmental aspects of a product whilst glossing over less flattering aspects. A variety of good sources of information now exists about eco-labels and the requirements for making a green marketing claim. These include the ITC Standards Map for eco-labels, an ISO standard (ISO 14020:2000) on ‘Environmental labels and declarations’ as well as information specifically on how to avoid greenwash.\nThese documents, and other sources of information that provide guidance on making green marketing claims listed in the ‘Background information’, can help you to avoid the mistake of greenwashing.\nFor most manufacturing companies the sales activity will not make a significant contribution to the company’s overall sustainability impact. It can of course have a significant impact in terms of the economic and social sustainability of the company. The main issue to consider is the opportunities for partnerships to build new sales channels in order to access markets that were previously inaccessible. For example, the Tasty Tuna Company could partner with charities that promote sustainable fishing, such as the Marine Conservation Society, in order to gain introductions to large retailers in Europe that are interested in sourcing more sustainable fish products.\nThe delivery of physical goods can have a significant environmental impact and economic cost. These issues are often particularly important for relatively low value, high volume products such as food or construction materials. Opportunities for innovation may exist in the following areas:\nPackaging – Reducing the mass of packaging reduces resource consumption and fuel consumed in transportation. The design of tertiary packaging for reuse vs single use (and recycling) is often a significant issue to be considered. A good example of packaging innovation for sustainability is provided by the Eco2Distrib case study described in the publication The Business Case for Eco- innovation (UN Environment, 2014).\nWarehouse impacts – Heating or cooling systems and lighting at warehouse facilities can be a major source of energy use with significant scope for improvement.\nLogistics optimization – Effective scheduling can reduce the distance that goods are transported leading to fuel savings. Opportunities for back-hauling, whereby the vehicle that has delivered a load from A to B is used to transport a different load back from B to A should also be investigated.\nProduct damage in transportation – Product damage or loss during transportation is sometimes accepted as a necessary overhead, but this need not be the case. Causes might include poor packaging, poor handling or poor temperature control (particularly for food products)."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:1f67ed53-eeff-413d-9f3d-09f7e0fb8f57>","<urn:uuid:588a6c2e-19a4-4057-828d-1e80c4482622>"],"error":null}
{"question":"Can you provide a comprehensive breakdown of vaccine types (live vs. inactivated) and explain both their safety profiles for rheumatologic patients and their compatibility with JIA medications?","answer":"There are two types of vaccines: inactivated and live vaccines. Inactivated vaccines do not contain living virus or bacteria and cannot cause infection. Live vaccines contain weakened but living forms of virus or bacteria. For JIA medications: NSAIDs, sulfasalazine, and hydroxychloroquine are safe with all vaccines. However, medications like corticosteroids, methotrexate, leflunomide, cyclophosphamide, azathioprine, cyclosporine and biologics affect the immune system and have restrictions. With these medications, inactivated vaccines (like diphtheria, tetanus, pertussis, inactivated polio, pneumococcus, meningococcus, Hib, injectable flu vaccine, hepatitis vaccines, and HPV) are safe. Live vaccines (including measles, mumps, rubella, oral polio, varicella, nasal flu vaccine, BCG, and yellow fever) should not be given until patients have stopped these medications for 1-2 months. Rheumatologic diseases themselves don't make patients ineligible for any vaccines - it's only the immunosuppressive medications that create restrictions due to their effects on the immune system.","context":["|Other questions about JIA medications||2601.00000000000||Other questions about JIA medications||Other questions about JIA medications-CAN||O||English||Rheumatology;Adolescent;Pharmacy||Pre-teen (9-12 years);Teen (13-15 years);Late Teen (16-18 years)||Body||Skeletal system||Drug treatment||Pre-teen (9-12 years)\nTeen (13-15 years)\nLate Teen (16-18 years)||NA||2017-01-31T05:00:00Z||Jennifer Stinson RN-EC, PhD, CPNPLori Tucker, MDAdam Huber, MSc, MD, FRCPCMichael Rapoff, PhDShirley Tse, MD, FRCPCLynn Spiegel, MD, FRCPC||0||0||0||Flat Content||Health A-Z||<p>You may still have other questions about JIA medications. This section will answer some of the questions you may have. Talk to your doctor for more information.</p>||<h2>What about vaccinations?</h2><p>We need vaccines to help prevent infections. JIA itself does not cause problems with the vaccines. However, JIA medications can reduce the protection you get from vaccines. Some JIA medications can limit the type of vaccine that you can safely receive. It may be risky to take certain vaccines. </p><p>There are two different types of vaccines: inactivated vaccines and live vaccines. The live vaccines should not be given when you are on certain types of medications. </p><h2>Drugs that are safe with ALL vaccines</h2><p>NSAIDs, sulfasalazine, and hydroxychloroquine do not change your immune system.\n<strong>ALL ROUTINE VACCINES ARE SAFE.</strong></p><h2>Drugs that are safe with SOME vaccines</h2><p>Corticosteroids, methotrexate, leflunomide, cyclophosphamide, azathioprine, cyclosporine and biologics DO change your immune system. </p><p>\n<strong>THE FOLLOWING VACCINES ARE SAFE AND RECOMMENDED:</strong></p><ul><li>Diphtheria, tetanus, pertussis</li><li>Inactivated polio </li><li>Pneumococcus (Prevnar or Pneumovax) </li><li>Meningococcus (Menjugate, Menjutec, Menactra) </li><li>Hemophilus influenza type B </li><li>Influenza (flu) vaccine (injection)</li><li>Hepatitis B vaccine and hepatitis A vaccine </li><li>Gardasil: the vaccine for human papillomavirus (HPV).</li></ul><p>\n<strong>THE FOLLOWING ARE LIVE VACCINES.</strong> They are considered\n<strong>UNSAFE</strong> with these drugs.\n<strong>THESE VACCINES SHOULD NOT BE GIVEN</strong> until you have stopped these medications for at least one to two months: </p><ul><li>Measles, mumps, rubella </li><li>Oral polio vaccine </li><li>Varicella (chickenpox) vaccine </li><li>Influenza (flu) vaccine given as a spray into your nose</li><li>BCG (the vaccine for tuberculosis) </li><li>Yellow fever.<br></li></ul><p>If you have any questions about the safety of a vaccine, ask your doctor or nurse.</p><h2>What about complementary and alternative medicine?</h2><p>\n<a href=\"/Article?contentid=2613&language=English\">Complementary and alternative medicines are therapies</a> that are not prescribed by your regular health-care team. These include prayer, meditation, therapeutic touch, acupuncture, vitamins, minerals, supplements, or naturopathic or homeopathic therapies. There are many reasons why some people use complementary and alternative medicine. </p><p>The therapies prescribed by your regular health-care team are called conventional medicine. Complementary medicine is used together with conventional medicine. Alternative medicine is used in place of conventional medicine. </p><h3>Important safety points for complementary and alternative medicine</h3><ul><li>Talk to your doctor about any complementary or alternative medicines that you are considering. These can interact with JIA medications. They can affect how your body responds to your medications. They may lead to more side effects.</li><li>Remember that the label may not reflect what is in the bottle. Some of these treatments have ingredients not listed on the label. They may have much more or much less of the featured ingredient than their label states. </li><li>Even though they may be “natural,” these treatments can have side effects too. </li><li>The claims for many of these treatments can be attractive. They can claim to help improve, or even cure, JIA. They can promise to make you feel better. Much of the information about these treatments is based on personal stories from patients or alternative health care providers. Although some of these treatments may help, many have not been scientifically studied. </li><li>Check with your doctor to make sure these treatments do not interact with your medication. In many cases, your doctor cannot say for sure that there will be no interaction. Many of these treatments have never been studied. </li><li>Do not stop your prescribed medications. Instead, use these other treatments in addition to your regular medicines. Be sure your doctors or nurses know what you are taking. </li><li>Some of these treatments may ask you to restrict certain foods in your diet. Make sure you are not cutting out nutrients your body needs. You still need many nutrients to grow and develop properly. For example, if you are reducing dairy products, you still need the calcium for healthy growing bones and teeth. </li></ul>||https://assets.aboutkidshealth.ca/AKHAssets/other_questions_about_JIA_medications_JIA_US.jpg|","Featured in the April, 2013 Scleroderma, Vasculitis & Myositis eNewsletter\nThere are currently 17 diseases for which there are vaccines available. The Centers for Disease Control makes specific recommendations regarding when and to whom these vaccines should be given.\nA vaccine-preventable infection is an infection for which there is a vaccination available that can help keep a patient from developing the infection. In some cases, as with the influenza vaccine, the vaccine can at least reduce the symptoms and severity of an illness, even if the disease itself cannot always be fully prevented.\nThe following is a list of commonly administered vaccines, and the infections they protect against:\n- Influenza: Since the influenza vaccines changes every year, the influenza vaccine needs to be given every year.\n- Pneumovax: This vaccine prevents infections caused by Streptococcus pneumonia, which is a common cause of pneumonia and meningitis in adults.\n- TdaP: This protects again three infections: Tetanus, diphtheria, and pertussis (the bacteria known to cause Whooping Cough).\n- Over the past decade, doctors have seen a sharp rise in the number of children and adults diagnosed with Whooping Cough in the United States.\n- Zostavax: This vaccine prevents shingles, a painful rash that is caused by the same virus that causes chicken pox.\n- MMR: This vaccine protects against measles, mumps, and rubella (German measles). MMR is often administered during childhood. However, as an adult, you may need a booster. Your physician can determine this by sending blood work.\nVaccinations in Adults with Rheumatologic Diseases\nPatients with rheumatologic disease have an increased risk for vaccine-preventable infections. This includes patients with scleroderma, myositis, and vasculitis. This is in part due to the effect of the rheumatologic disease on the immune system. Mostly, however, this risk is a result of the immunosuppressive or immunomodulary medications used to treat these illnesses.\nImmunosuppressive and immunomodulary medications treat rheumatologic disease by altering or damping the patientís immune system. This weakening of the immune system can be highly beneficial in treating many rheumatologic diseases. However, it can increase the risk of developing infections, many of which are vaccine-preventable.\nThe following is a list of immunosuppressive medications that are commonly used to treat rheumatologic illness:\n- High dose Prednisone\n- Humira (brand), Adalimumab (generic)\n- Orencia (brand), Abadacept (generic)\n- High dose Methotrexate\n- Rituxan (brand), Rituximab (generic)\n- Remicade (brand), Infliximab (generic)\n- Cellcept (brand), Mycophenolate mofetil (generic)\n- Imuran (brand), Azathioprine (generic)\n- Arava (brand), Leflunomide (generic)\n- Enbrel (brand), Etanercept (generic)\n- Cytoxan (brand), Cyclosphosphamide (generic)\n- Kineret (brand), Anakinra (generic)\nThe effectiveness of a vaccine depends greatly on the recipientís ability to develop an immune response to the vaccine. If the immune system is not properly functioning, the body cannot develop an immune response to the vaccine, and all benefits are lost.\nTypes of Vaccines:\nVaccines can be divided into two basic groups: Inactivated vaccines and Live vaccines.\n- An Inactivated vaccine does NOT contain any living virus or bacteria. A person receiving an inactivated vaccine cannot become infected by the vaccine.\n- A Live vaccine contains a living, but weakened, form of a virus or bacteria. The risk of developing an actual infection from a live vaccine is extremely low. However, this risk does increase in patients who are taking medications which suppress the immune system. In some instances, a live vaccine will need to be withheld because of this risk. This can be an important consideration in patients being treated for rheumatologic disease.\nIt is important to remember that the vast majority of vaccines given to adults are Inactivated vaccines, and therefore cannot cause infection.\nIt is also important to remember that rheumatologic illness by themselves do not make anyone ineligible to receive a vaccine. This includes live vaccines. All currently available vaccines are safe in patients with rheumatologic disease. It is only in cases where patients with rheumatologic disease are taking immunosuppressive medications, that there is cause for concern.\nWho Gets Which Vaccines and When?\nThe types of vaccine you will need will be based on the following elements:\n- prior medical history, especially your history of prior infections\n- current pregnancy status\n- current medications, especially if you are taking immunosuppressive or immunomodulatory medications\nYour doctor may wish to vaccinate before immunosuppressive treatment is started, or they may wish to wait until the treatment is completed. Choosing the right time to vaccinate will both reduce the potential of infection from a live vaccine and increase the likelihood that you will develop a good response to both live and inactivated vaccines.\nBe sure to review which vaccines you should be receiving with your doctor and when they should be administered. A considerable amount information regarding vaccinations is available on the CDCís website.\nImportant Facts About Vaccines to Remember\n- Each vaccine has a schedule. Some vaccines, like the influenza vaccine, need to be administered every year. Other vaccines may only need to be given once, and yet others will require boosters, or additional doses, periodically.\n- You can receive a vaccination even if you are taking an antibiotic.\n- It is safe to receive more than one vaccine in a single visit.\n- If you are planning to travel abroad, consult your physician regarding any pre-travel vaccines you might need to take. Different countries may recommend different vaccines before visiting. The CDCís website provides very comprehensive information regarding what vaccines are recommended depending on the countries you are planning to visit.\n^ Back to Top"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:943dda33-2774-4d1c-84d0-4278325a2307>","<urn:uuid:1757d4bf-1be9-498d-86f5-ec6660fa33ea>"],"error":null}
{"question":"What are the key differences between the history and purpose of the Cave Spring Methodist Church organ and the Katherine Esterly Organ at SsAM, focusing particularly on their musical roles and symbolic significance?","answer":"The Cave Spring Methodist Church organ's history was relatively simple - its bell was moved to Verona when the church split around 1880, and it served a basic worship function. In contrast, the Katherine Esterly Organ at SsAM was a much more complex and purposeful project - it incorporated pipes from three former parishes (St. Andrew's, St. Matthew's, and Cathedral Church of Saint John's) and was specifically designed as both a worship and concert instrument. While the Cave Spring/Verona organ mainly supported traditional church services, the Katherine Esterly Organ was designed to support diverse musical styles, choral repertoire, improvisations, and public concerts, serving as a cultural force in downtown Wilmington. Additionally, the Katherine Esterly Organ carried deep symbolic significance as it represented the successful union of multiple historic parishes, while the Cave Spring organ's transfer was simply part of a church relocation.","context":["Marshall County was established by an act of the Legislature passed February 26, 1836 from parts of Bedford, Lincoln and Maury Counties. Verona is located on the Verona-Caney Spring Road about five miles north of Lewisburg. It is situated on 1,125 acres granted to David Ross by the State of North Carolina. By 1810 Allen Leeper had acquired the land and on December 31, 1815 he gave it to his daughter and son-in-law, Mary and James L. Ewing. Mr. Ewing built a mill where Rock Creek passed through his land in the early 1800's and in some old records the place was referred to as \"Ewing's Mill\". Around 1844-1845, Jessee Stegall bought the mill, rebuilt the dam, opened a store, and the community was then known as Stegall's Mill. With the establishment of the post office, the community was known as Tyrone, but the name was officially changed to Verona on December 29, 1868.\nThe only church in the vicinity in the very early days was the Bethbirei Presbyterian church, established in 1810.\nVerona Methodist Church had its beginning when the Cave Spring Methodist Church was closed and two new churches built to replace it. We do not know exactly when the Cave Spring Methodist Church was started but the following quote from LIFE ON THE COMMON LEVEL gives us a picture of its beginning:\n\"He (Thomas Burr Fisher, 1844-1922) grew up within sight of the locus of the community -- Cave Spring School and church, of which he said: 'Here my life began to unfold and to appropriate what was offered from without. When I reached the age of five years, I insisted on going to school. 'There were three men living within one mile or less of Cave Spring, who had large families, Uncle Elihu Hunter, Uncle Jonathan Thomas, and my father. We used to have subscription schools. When a teacher would come into the community to make up a school, he would be told to see these men and if they agree, the school would be assured. The school house built, it was not long until a Sunday School was organized, then appointments for preaching were made and a church was born, sharing the same building with the school. This was the community center, the greatest, the dearest place in the world. The church grew, numbering at one time 275 members.\n'The school house had been built near an ancient fracture in the earth's crust, Cave Spring, an idyllic spot. This (Cave Spring) was a bold stream of water about 25' below the surface, which had been disclosed by some natural catastrophe in unknown ages past, by the caving in of the earth and rocks on one side, leaving a large irregular opening or sink down the side of which a rugged path led to the water.\"\nTrustees of Cave Spring Church were N. B. Wiggs, James Adams, George Yarbrough, James Pylant and George W. Fisher. A deed was issued to N. B. Wiggs and other trustees of the M. E. Church, south for a Meeting House lot by John Fisher and James Ewing. It was dated 1856 and recorded October 23, 1857. The deed was for two acres of land and clearly stated that there was a building on the land. This leads me to believe that the building was erected sometime in the 1840's. The church/school building stood across the road from the house now owned and occupied by Mr. and Mrs. Waldo Rutherford, formerly known as the O'Neal farm. A small cemetery enclosed in an iron fence is all that remains there today. The story is told that Nancy Jane Thomas, daughter of Jonathan and Susan Thomas, who died January 23, 1858 at the age of 24, requested that she be \"allowed to sleep close beside the church\" she loved. This request was granted and the little plot is the last resting place for Nancy Jane and her parents.\nThe Cave Spring Church was still in existence in 1878 when Allen Tribble was appointed to the Cave Spring Circuit, Franklin District, Tennessee Conference. About 1880 the church was divided with part of the membership establishing a church in Farmington (October 1, 1880) and the other part establishing a church in Verona. The Cave Spring Church was torn down and part of the material was used in building the new church. The pews (which had been made by Mr. N. B. Wiggs) and the church bell were moved to Verona. The building was erected on land deeded to the Trustees by W. A. and Hattie N. McCurdy. They had paid $25 for the lot. The deed was dated July 28, 1880 and was recorded July 29, 1880, in Marshall County, Tennessee, Book D-2, page 350. Trustees were: J. M. Patterson, Sr., Berry S. Haggard, J. H. Word, W. A. McCurdy, H. T. Drake, James H. McCurdy and M. V. Cathey. Helping to move the church to Verona were Marshall G. Brown, L. B. Collins, L. M. Fowler, Dr. J. M. Patterson and Capt. Wm. M. Robinson.\nMarshall Gabard Brown and Sarah Pinkston Drake were married December 17, 1879 in the old Cave Spring Church by the Rev. T. B. Fisher. Both were charter members of the church at Verona, Mr. Brown being received into membership of the Cave Spring Church by baptism in 1876 and Miss Drake in 1878.\nOther charter members of the Verona Church were: Pink Boren, L. M. Fowler, Lebert Patterson, Mr. and Mrs. Joe Regen, Mr. and Mrs. M. V. Cathey, Mr. and Mrs. J. M. Cathey, Eli Wilson, Mr. and Mrs. L. B. Collins, Addison Hunter, Jessie McCurdy, Mrs. Alice Adair, and Dr. and Mrs. James M. Patterson. The church is very proud of all members -- farmer, banker, business and professional men and women, housewife and mother -- who have served their church with faithfulness and devotion. Age and death has taken their toll, but with the faith of our forefathers, with the strength of that great heritage handed down to us, we who worship at Verona United Methodist Church, have continued to move forward until due to hard economic times have had to make the difficult decision to close the doors to this faithful church.\nAfter many discussions and many prayers, the decision to close was made so that the remaining faithful members could use their talents and abilities in one of our sister churches. Our heritage shows us that we are to go out and make disciples and that is our plan. This was the hardest decision to make but we feel that God is leading the current members forward into the future with this decision.\nThe list of current members as of February 2011 are: Robert Lee Cathey, III, Ruth Daniel, Wayne and June Daniel, Grace Ewing, Mickey Ewing, Tony Ewing, Sherry Hunter Haney, Pamela Jo Hunter, Richard Hunter, Tom Hunter, Carol Spence, Christopher Wakham, Joe and Sandy Wakham.\nSpecial recognition must be paid to some who have served our church long and well. Mr. W. B. Long was Superintendent of the Sunday School prior to 1914. He was followed by Mr. Clayton Long who served from 1915 to 1955. Others that served were Howard Cochran, Joe Mac Hickerson, Henry White, Tommy Jordan, Walter Fowler, and Wayne Daniel.\nMrs. W. B. (Ella) Long was the first organist and served for more than fifty years. Mrs. Martha Frances Liggett served for many years until her passing in 1998. Mrs. Sandy Jordan Wakham, who was a piano student of Mrs. Liggett, served until the closing of the church.\nMiss Zula Collins, a public school teacher, was a teacher in the Sunday School for about fifty years. Buford Ellington, who later served as Governor of the State of Tennessee, was a member of the Board of Stewards and taught the Men's Bible Class. He, his wife Catherine, and their children John Earl and Ann, were active members of the church.\nThe same old bell that called people to worship as it did more than 100 years ago still remains at the church today. The beautiful cedar cross which adorns the church was carved from the cedar door-steps of the Cave Spring Church by the late Mr. R. L. Johnson.\nA register of the pastors that served the Verona United Methodist church are:\n|T. B. Fisher||1880 -1881|\n|W. A. Turner||1881 -- 1885|\n|T. B. Fisher||1885 -- 1886|\n|J. W. Hensley||1886 -- 1891|\n|W. T. Goodloe||1891 -- 1893|\n|C. E. Heriges||1893 -- 1894|\n|S. G. Thompson||1894 -- 1896|\n|D. B. Coleman||1896 -- 1899|\n|C. S. Gabard||1899 -- 1900|\n|B. F. Isom||1900 -- 1904|\n|A. P. Walker||1904 -- 1906|\n|A. J. Ewing||1906 -- 1907|\n|G. W. Taylor||1907 -- 1909|\n|J. L. Kellum||1909 -- 1911|\n|Allen Miller||1911 -- 1912|\n|J. B. Cheek||1912 -- 1914|\n|O. C. Haley||1914 -- 1915|\n|M. K. Harwell||1915 -- 1919|\n|J. D. Hewgley||1919 -- 1920|\n|J. B. Estes||1920 -- 1923|\n|G. M. Davenport||1923 -- 1924|\n|G. L. Hensley||1924 -- 1925|\n|J. M. Madewell||1925 -- 1928|\n|C. R. Wade||1928 -- 1930|\n|B. J. Staggs||1930 -- 1934|\n|R. D. Davis||1934 -- 1937|\n|T. E. Hillard||1937 -- 1938|\n|W. A. Bass||1938 -- 1940|\n|J. E. Woodward||1940 -- 1942|\n|H. L. Smith||1942 -- 1947|\n|W. C. Moorehead||1947 -- 1948|\n|E. G. Goodwin||1948 -- 1951|\n|Dan Toline||1951 -- 1953|\n|William Berninger||1953 -- 1955|\n|George Jones||1956 -- 1960|\n|James Denny||1960 -- 1962|\n|J. C. Eilotte||1964|\n|J. J. Mabry||1966 -- 1970|\n|Paul Ford||1970 -- 1971|\n|Rudolph James||1971 -- 1973|\n|Allen Knight||1973 -- 1976|\n|H. Seeman||1976 -- 1978|\n|Eugene Gibson||1978 -- 1982|\n|Larry Helton||1982 -- 1984|\n|Edwin Beard||1984 -- 1989|\n|William Pearce||1989 -- 1995|\n|Tom Herring||1995 -- 1997|\n|Larry Pedigo||1997 -- 2002 Senior Pastor|\n|B. T. Thomas||1997 -- 1999 Associate Pastor||Matt Charlton||1999 -- 2000 Associate Pastor|\n|Kaye Harvey||2000 -- 2003 Associate Pastor||Katherine Paisley||2002 -- 2003 Senior Pastor||Don Dunlap||2003 -- 2005|\n|Don Noble||2005 -- 2008|\n|Steven Christopher||2008 -- present|\nThrough 1996 preaching services had been held every other Sunday and in 1997 services were held every Sunday with the help of an associate pastor. Verona had been a part of a four-point charge that included: Berlin UMC, Caney Spring UMC, and Farmington UMC. In June 2003 Farmington made the decision to become a station church so therefore, the Berlin Circuit became a three-point charge and continued until the present time. The pastor at that time began delivering three sermons every Sunday starting at Verona -- 8:30 am, Caney Spring -- 9:45 am and then Berlin -- 11:15 am.\nMany improvements have been made during the 131 years since the church was moved to its present location. Sunday School rooms were added in the 1930's and still more in the 1950's. The memorial pews and carpet were added in 1968. In 1999 more improvements began with the help of money left to the church by Martha F. Liggett -- central heating and cooling, covering on the front of the church and new concrete steps and pad.\nA list of memorials to the church follows:\nPews were given in memory of -- M. V. Cathey by grandchildren and great-grandchildren; Mrs. Ella W. McCurdy and Eric Wiles by nephew, Charles Wiles; Gilbert and Sallie Hunter, Neola, Will and Gill Hunter; in memory of their father and mother, Mr. and Mrs. Marshall Brown, 1968, by Mrs. Homer Doggett, C. C. Brown, Mrs. Clayton Long, and Noble Brown; in memory of Mr. and Mrs. T. J. Hickerson, by Mr. and Mrs. Joe Mac Hickerson and Mr. and Mrs. Jack Liggett; in memory of Hiram Tennison, by Rev. and Mrs. J. L. Perry and mr. and Mrs. C. F. Rich; in memory of Hiram Tennison, by Mr. and Mrs. Sam Tennison; by Mr. and Mrs. Glendon Daniel and family; in memory of Mr. and mrs. J. P. Mc Curdy, by granddaughter, Martha F. Liggett; in memory of Mrs. Ella McCurdy, by nephew, Charles Wiles; in memory of Eric Wiles, by niece, Martha F. Liggett; in memory of Mr. and Mrs. W. T. McCurdy, by daughter, Martha F. Liggett; in memory of Mrs. and Mrs. Ben Adair; in loving memory of Mr. and Mrs. Clayton Long, by the Cecil C. Brown family and Mr. and Mrs. Homer Doggett; in honor of Mr. and Mrs. O. N. Hunter, by Mr. and Mrs. Will Norris Hunter and Mr. and Mrs. Edward Hunter; in honor of Mr. and Mrs. Frank Hunter, by \"The Children\"; in memory of Mack and Sallie Fowler by \"The Children\".\nThe alter cross given in loving memory of C. C. Long, Superintendent of Church School, 1915-1955 by Mrs. C. C. Long, Mr. and Mrs. Charles Long, Col. And Mrs. Ross B. Young, Mrs. Louise Young, and Mrs. Lois Hobson.\nBrass vases given in memory of Joe C. Hunter 1963.\nFlower bowl given in memory of Claiborne Hunter.\nTwo offering plates in memory of Mr. and Mrs. Orville Hunter\nBible Stand given and made by Will Norris Hunter.\nBible given by Nub and Rachel Johns in memory of Mrs. Lena Bigham.\nFlower arrangement picture given by Mr. and Mrs. Carroll Wiggins.\nOrgan donated by Dr. Charles Hopper in memory of Minnie Lee Hopper.\nOak Cabinet made and donated by Will Norris Hunter.\nAngel stature donated by Carroll and Ruth Wiggins in memory of Toni Hunter.\nStain glass windows in front of church building (replacing where air conditioners were) donated by Carroll and Ruth Wiggins, Wayne and June Daniel, and Joe and Sandy Wakham.\n$25,000 was left to Verona United Methodist Church by Martha F. McCurdy Liggett.\nAmerican flag and Christian flag were donated by Joe and Sandy Wakham in honor of their children, Joey, Chris and Robin, and in memory of Cynthia Wakham.\nStand given in memory of Martha F. Liggett by Joe and Sandy Wakham.","INSTALLATION OF THE\nKATHERINE ESTERLY ORGAN\nThe installation of the Katherine Esterly Organ began on Monday, March 7, 2016.\nWHY A NEW ORGAN?\nAs many have experienced, the music at the Episcopal Church of Saints Andrew and Matthew, during the services and at special events, is well known to lift the spirits and nourish the souls of those in attendance. However, the organ reached the end of its useful life cycle several years ago; in its place currently is a temporary electronic organ.\nAfter a lengthy in-depth process of research and evaluation, the Organ Committee at SsAM developed a plan for rebuilding and improving this 60-year-old organ. Quimby Pipe Organs, Inc., specializing in organ designs closest to the unique musical needs of parishes, was selected. They were also chosen because they best understood SsAM’s philosophy as well as our desire to incorporate pipes from the three former parishes that have consolidated into one… St. Andrew’s, St. Matthew’s and the Cathedral Church of Saint John’s.\nThis renovation has improved the organ’s sound and will maintain the instrument for years to come. An enhanced organ honors the excellence of our choir and looks forward to the blending of young voices of the Cathedral Choir School of Delaware.\nBecause of SsAM’s location in downtown Wilmington and its commitment to serving as a spiritual, cultural and social force in the community, the new organ will have an impact on music and culture far beyond the context of worship. Being particularly well designed as a bold concert instrument, it will also support the many groups that perform regularly at SsAM as well as attract additional musical groups.\nThe new organ has been named after Dr. Katherine (Kitty) L. Esterly, a revered and honored pediatrician, a beloved and respected member of our congregation and of our state. She has been a major proponent of the new organ, understanding its importance to the congregation and the community. The organ was dedicated on February 19, 2017.\nWAS THIS A RENOVATION OR A NEW ORGAN?\nThe SsAM organ project is a fairly radical renovation project that utilizes some pipe work from the organs of old St. Matthews and from old St. Andrews, which were both built by the M. P. Moller Co. in the mid-1940’s. Essentially, a new organ is being built that meets all design objectives while incorporating historic elements of the two instruments: approximately 8 ranks of pipes from the St. Andrews organ and 4 ranks from the St. Matthews organ.\nThere are a number of challenges in building an ideal organ for SsAM. Aside from the issue of finding the right design philosophy that can produce an instrument suitable for the wide range of musical styles suggested by the SsAM liturgies, the limited space requires that this be done with a limited number of pipes. With the current organ at 34 ranks, a new instrument housed in the same confined space would have difficulty providing the tonal and musical options that our parish needs. Although many congregations are now effectively solving this issue with digital instruments or by adding digital divisions, the Organ Task Force, as well as the SsAM Vestry unanimously agreed that our preference is for a custom built pipe organ. Another challenge was to find a builder who could embrace our unique musical customs and create a quality instrument that suits our space, musical requirements and budget limitations.\nUNIQUE DESIGN CONCEPT\nConsidering the space limitations as well as challenges with regard to design strategy, some creative solutions were needed to fill such a tall order. Many excellent designs were submitted by some of the leading organ builders, but Quimby Pipe Organs, Inc. of Warrensburg, Missouri was chosen as the builder. Michael Quimby and his outstanding staff in collaboration with SsAM’s music director David Christopher have developed what we believe to be the perfect instrument for the church. With the addition of a new organ chamber, increasing the size of the new instrument to 45 stops and using the most modern technology, David and Michael have devised a visionary instrument that we believe will meet all of the parish’s musical and liturgical needs. The design strategy has at it’s core a classic instrument with well developed choruses of sound. However, the addition of a few orchestral stops as well as placing the three main divisions under expression (Great, Swell and Choir) will expand the musical options beyond the scope of most traditional mid-size organs. The new instrument with be particularly strong at supporting congregational singing, accompanying a wide range of choral repertoire, facilitating inspiring improvisations within the context of liturgy, providing continuo for early music, and serving as a dynamic and inspiring recital instrument. In other words, the new SsAM organ will “step out of the box” much the same way that our parish in known for stepping out of the box spiritually, socially and liturgically.\nClick here to see a stop list for the new instrument. TONAL SPECIFICATIONS\nClick here to learn more about QUIMBY PIPE ORGANS, INC.\nIMPORTANT SYMBOLISM OF THIS PROJECT\nPerhaps the most important aspect of the new organ is that it serves as a symbol and critical milestone in the evolution of the parish. By taking the strongest elements from the organs of the two historic parishes of SsAM (St. Matthews and St. Andrews) and weaving them into a new and visionary entity, we give credibility and a sense of permanency to the successful union of these two churches. The project is enhanced with the addition of pipes from the Cathedral Church of St. John’s. Every time the new instrument is used in worship or in concert, our hope is that it will serve as a reminder that God’s will can be realized against all odds. In our case, this means defying all of the statistics that surround the viability of creating a parish such as SsAM; one that is radically welcoming, yet intentionally diverse. It also means finding a new way to stay faithful to the principles behind traditional values, but in a way that allows the Holy Spirit to help us evolve into a community that is alive and relevant to the world today.\nThe most significant change to the architecture of the sanctuary was the addition of a new chamber in the front of the church directly behind the where the choir sits on the pulpit side of the nave. As a result, the front of the church has a stronger sense of balance with matching pipe facades on both side of the room. According to SsAM’s architect, Lee Sparks (Design Collaborative, Inc.), the addition of the second organ chamber is not only an inevitable improvement because of its beauty coupled with functionality, but it completes the architectural balance of the room in a way that stays true to the spirit of original (historic) architectural concept. In the rear gallery of the church, attractive pipe work around the central round window replaces the former facade, but the basic layout (of the antiphonal organ) is very similar to the original design."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:b6a797e5-de9d-4ac7-a0c1-2b3f6f4b3f93>","<urn:uuid:5c54a424-59d7-4267-91fc-7c84ff839e58>"],"error":null}
{"question":"What are the data integration capabilities of STEP compared to its implementation in astronomical FITS files?","answer":"STEP and FITS handle data integration differently. STEP's unique feature is that it integrates product data throughout a product's entire lifecycle, eliminating data redundancy across systems by providing a single product data storage standard. In contrast, FITS implements data integration through Header Data Units (HDUs), where related data components are organized as 'imsets' - logically related extension images. For example, in FITS, a science image, error image, and data quality image are stored as related extensions in the same imset, but this integration is primarily focused on astronomical data components rather than full product lifecycle integration.","context":["The STandard for the Exchange of Product model data (STEP) is currently being developed by the International Organization for Standardization (ISO). It is intended to contain the unambiguous representation of product data throughout the entire life cycle of a product.\nThe unique feature of STEP is that it integrates product data. In today's environment, the data for a product is managed in many different systems with little integration and with data redundancy. The data redundancy is primarily caused by the lack of integration. Data must be replicated from system to system in order to maintain a correlation between the systems. STEP proposes to alleviate this problem by providing a single product data storage standard which integrates the data.\nThe STEP standard does not intend to standardize how an enterprise gets its data into the standard representation. The intention is to provide a migration path. The migration path is described by the levels of implementation of STEP. These levels are:\nLevel 1- Passive File\nLevel 2- Active File\nLevel 3- Database\nLevel 4- Knowledge Base\nThe passive file exchange is a flat ASCII file which contains the STEP data. This level is basically translator based. The active file is a memory resident image of the STEP data which can be used interactively by applications. The database implementation level provides additional facilities whereby applications can be constructed that take views of the data which are different than the native storage format, and support relational, network, hierarchical and object oriented types of queries. The knowledge base implementation stores the data as objects complete with attached constraint rules from the STEP standard. The first three levels of implementation can not attach rules to entities and attributes without the use of application code even though these rules can be and are defined in the standard.\nSTEP can be somewhat directly implemented. The standard is written in the EXPRESS language that was developed for the purpose of allowing the standard to be human readable and computer interpretable. This means that programs can be written to use the standard as input. The standard defines the data structure and the rules for data storage. A program can read the standard and determine the data storage structure. The program can also be provided with mappings between STEP entities and attributes to an application's entities and attributes. This structure allows for complete restructure with a minimum of programming effort. This is especially true when entities and attributes are added, modified or deleted from the standard.\nEXPRESS is designed to be implementation independent and thus support the four levels of implementation defined for STEP to date. It defines a common interpretation of the standard in terms of data and rules. This common interpretation can be used to drive computer processes. In order to do so, both the data definitions and the rules must be implemented. In levels 1 through 3, the data definitions can be used to define mappings to other applications or between versions of the standard. In level 4, this can be extended to where not only data can be mapped, but processing requirements (in the form of EXPRESS local and global rules) and complex relationships can be checked and enforced without additional programming effort.\nEXPRESS has a number of dialects which are:\nThe overall structure of the standard is summarized here which is also known as STEP On A Page. In words, the basic breakdown is:\nFurther information on all these parts and their current status can be found here.\nThe piece of STEP which actually gets implemented, exchanged, or shared is the Application Protocols. Here is some various information on some of these:\nApplication protocols can be implemented via flat/ physical file translators via STEP Part 21 or via the Standard Data Access Interface (SDAI).\nAn important question to ask when considering the implementation of STEP is: Why should my company move to STEP rather than continue using existing accepted standards?\nThis is an important question. Existing standards, their implementation and use can provide some interesting bench marks with which to compare STEP. There is an important difference between STEP and most other standards. The difference is that most other standards deal only with particular application areas or particular deliverables or products. STEP is intended to store all data for a product throughout its life-cycle with out regard to discipline or application area.\nSTEP is an international standard. This designation gives STEP a distinct advantage over company, industry, and national standards for companies in today's economy. STEP data can be exchanged and shared across international boundaries. This means that products designed in one country could be produced anywhere in the world.\nSTEP is intended to deal with all types of products and all data related to those products. It also forces the integration of product data. This integration is a very distinguishing feature of STEP. No other standard currently offers this capability. This feature is very important to consider when evaluating STEP. Standards which do not provide this integration provide benefits only if data is reused. The integration provided in STEP can be an important advantage even if data is never exchanged. Integration of data and the systems that deal with that data eliminates or at least severely reduces redundancy. This eliminates the need for tasks required to support the redundancy.","3.2 FITS File Format\n3.2.1 Multi-Extension FITS Files\nFITS1 is a standard format for exchanging astronomical data, independent of the hardware platform and software environment. A file in FITS format consists of a series of Header Data Units (HDUs), each containing two components: an ASCII text header and the binary data. The header contains a series of keywords that describe the data in a particular HDU and the data component may immediately follow the header.\nFor HST FITS files, the first HDU, or primary header, contains no data. The primary header may be followed by one or more HDUs called extensions. Extensions may take the form of images, binary tables, or ASCII text tables. The data type for each extension is recorded in the\nXTENSION header keyword. Figure 3.1 schematically illustrates the structure of a FITS file and its extensions.\nEach FITS extension header contains the required keyword\nXTENSION, which specifies the extension type and has one of the following values: IMAGE, BINTABLE, and TABLE, corresponding to an image, binary table, and ASCII table, respectively.\nTable 3.2: HST Header Keyword Descriptions\nData type for extension\nExtension names that describe the type of data component\nSCI (science image)\nERR (error image)\nDQ (data quality image)\nSAMP (number of sample)\nTIME1 (exposure time)\nEVENTS2 (photon event list)\nGTI2 (good time interval)\nWHT (weight image)\nCTX (context image)\nAllowed for any extension except SCI, and used for an image with uniform value for all pixels\n- Only found in NICMOS and WFC3 data.\n- Only found in COS and STIS data.\n- When an image has the same value at all pixels (e.g., data quality value), the extension has no data component. Instead, the constant pixel value is stored in the header keyword\nA set of FITS extension images which are logically related to one another is called an imset. For example, the error image and the data quality image are in the same imset as the science image itself. The keyword\nEXTNAME is used to specify the extension names of different images in the same imset. For example, the science data from the two ACS/WFC CCDs are stored in extensions 1 and 4 of a MEF file, but may also be referred to as ['sci', 1] and ['sci', 2], where 'SCI' is the extension name, and the integers 1 and 2 refer to the imset. Similarly, one may access the science data, error array, and data quality array of only the first imset located in extensions 1, 2, and 3, respectively, as ['sci', 1], ['err', 1], and ['dq', 1]. The data format chapters of each of the instrument-specific data handbooks provide specific information on extension names and imset numbers.\n3.2.3 Waivered FITS Format\nFile formats for the first and second generation HST instruments (FGS, FOC, FOS, HSP, WF/PC-1, GHRS, and WFPC2) were developed before the standardization of MEF format. The waivered FITS format was developed in response to the need for a machine independent storage format for these data and was based on the idea of stacking multi-group data as a new dimension in a FITS image.\nFor example, a WFPC2 science data file containing data from four cameras (groups) has four 800 x 800 pixel images in its data file. The waivered FITS file containing these data has an image with dimensions 800 x 800 x 4 pixels in its primary HDU. Similarly, a FOS file may contain a two-dimensional spectrum stored as 2064 x 40 pixels in the primary HDU constructed from 40 groups of 2064 pixel one-dimensional spectra.\nThe first extension of a waivered FITS file will contain the header information as a binary table with each group represented as a row in the table. Each column in the table is a header keyword. For example, In the case of a WFPC2 image consisting of four groups, the first extension of the waivered FITS file is a table containing four rows.\n1A description of FITS format and various supporting documents can be found at the website https://fits.gsfc.nasa.gov/fits_home.html.\nIntroduction to the Hubble Space Telescope Data Handbooks\n- 1. Obtaining HST Data\n- 2. HST File Names\n- 3. HST File Formats\n- 4. HST Data Analysis\n- 5. Observation Logs"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:70620729-4a3f-454f-9bc9-e2e1779daa2f>","<urn:uuid:fe8ee8fe-5057-4061-bcf0-dce2e7a3da6a>"],"error":null}
{"question":"How do current laws define death in relation to organ donation, and what changes have been proposed for anencephalic infants?","answer":"The Uniform Determination of Death Act (UDDA), adopted by all states, defines death as either irreversible cessation of cardiorespiratory activity or irreversible cessation of all brain activity, including brain stem activity. Some supporters of anencephalic organ donation have proposed modifying the UDDA to include anencephalic infants as a third category of legally dead, allowing organ removal before vital functions cease. However, critics argue this would merely create a legal fiction that doesn't change their biological status as living human infants.","context":["Address correspondence to Mary Z. Pelias, PhD, JD, Dept. of Biometry and Genetics, Louisiana State University, Medical Center, 1901 Perdido Street, New Orleans, LA 70112-1393. Ph: 504-568-6151, Fax: 504-568-8500.\nRecent advances in medical and surgical technologies now permit successful organ transplantations for persons whose lives are in peril because of failure of one or more of their own vital organs. While organ transplantations have brought dramatic benefits to many grateful persons, the use of these procedures has also generated intense debate about ethical and legal propriety of removing organs from individuals who still have some vital functions or who are being maintained on machines. One very troublesome area of vital organ transplantation is the use of anencephalic infants as sources of these organs. Although both the proponents and opponents of this sad but potentially life-saving application of organ transplantation argue their positions with sincerity, their arguments sometimes lack clarity.\nAnencephalics lack recognizable cerebral hemispheres as well as overlying cranium and scalp11. The vital functions of live-born anencephalic infants are controlled by their brain stems, and they do show some reflexive responses during their brief lives5. They usually have adequate cardiorespiratory activity to permit survival for at least a short time. Because anencephaly is usually lethal within hours or days after birth, anencephalics fall into a class of infants who are born dying; they are permanently unconscious, with no hope of growth, mental development, cognition, or interaction with others.\nThe Uniform Determination of Death Act\nA cornerstone in the debate about anencephalics is the Uniform Determination of Death Act (UDDA)20, which has been adopted in every state in response to the dilemmas generated by organ transplantation technology2. This legislation defines death as irreversible cessation of cardiorespiratory activity or irreversible cessation of all brain activity, including activity of the brain stem. Authors who oppose the use of anencephalics as organ donors note that anencephalics do have brain stem activity and are, indeed,biologically alive. On the other hand, authors who support the use of anencephalics as organ donors have suggested that the UDDA could be modified to include anencephalic infants as a third category of the legally dead, so that healthy vital organs may be removed from these infants before the decline or cessation of vital functions18. Rewriting the UDDA to define anencephalics as legally dead would not, however, alter the fact that these infants are alive. No change of language would change the biological status of these infants or the fact that anencephalics are human infants, born alive, but born dying6. To create such a legal fiction would be nothing more than an error in semantics and, as Mearns et al 9 note, would carry the ominous tones of legalized social experiments that have occurred in the past.\nThe critical concerns of diagnosis\nThe suggestion to modify the UDDA has elicited heated debate, including the comments by Mearns et al in this journal. These authors, as well as most others, acknowledge that most anencephalics are aborted, or are stillborn, or are otherwise unsuitable for consideration as organ donors2,10. However, the fact that very few anencephalics could be considered as potential organ donors does not nullify their potential value in organ transplantation. Nor does the paucity of potential recipients nullify the value of anencephalics as potential donors. Numbers should not be an issue. Concerns about the accuracy of diagnosis are, however, especially relevant, because of the broad spectrum of neural tube defects and the association of anencephaly with syndromes that preclude transplant donation. To avoid the possible perils of the slippery slope, careful and conservative diagnosis of isolated anencephaly must adhere to a strict medical standard10,11,14,16. Mearns et al. also urge caution because of our meager knowledge of the dying process, although this lack of knowledge does not change the consistent observation of the deaths of anencephalics within a short time after birth.\nNew aplications of old values\nMearns et al argue further that allowing donations of vital organs from anencephalics would undermine regulations that have emerged from time-honored cultural values and priorities. These values and priorities presumably include respect for persons and an abhorrence of killing. Among physicians, of paramount value is the Hippocratic admonition to avoid doing harm. In recent times, however, we have found a need to examine our new technological powers in the light of older values as we try to reconcile the discrepancies between the two. Indeed, the UDDA was written in response to a need to define precisely what we believed we always knew about death. We are not abandoning our time-honored values as much as we are still trying to find a harmony between older ethics and newly developed powers in the medical arts. The question of using anencephalics as vital organ donors focuses on the specific ethical conflict between the utilitarian concept of using one person to benefit another and the Kantian concept of valuing each person as an end in himself, not as a means to another end, such as the well-being of someone else2. Thus far, no one has formulated a satisfactory philosophical compromise that would settle this basic ethical conflict.\nA classification based on brain function\nAnother fundamental conundrum with anencephalics is the value of the higher brain as a measure of personhood. Some authors have suggested that anencephalics be assigned the legal status of non-persons, on the theory that severe neurological impairment is incompatible with personhood7,16,17. As non-persons, these infants would then be available for organ removalømuch as one would remove porcine or primate organs, with no moral obligation to consider the implications of shortening the life of the donor. This suggestion, undoubtedly well intended, also side-steps the fact that anencephalics are living human infants, who become persons in the eyes of the law at the moment of live birth. The use of neurological impairment as a measure of personhood creates the difficult dilemma of basing a categorical social classification on the anatomic continuum of anterior neural tube defectsøa dilemma that so far has defied resolution.\nThe parents\" decision\nWhat is remarkably absent from the debate about anencephalics as organ donors is thoughtful consideration of the parents of these dying infants. Perhaps we should first acknowledge the tragedy that strikes the family when an anencephalic infant is born. How the parents cope with the situation may range from requests for aggressive life support to requests for minimal, if any, intervention. Now that some anencephalics may become suitable organ donors, some parents are faced with another option, and for some parents the prospect of donating the organs of their dying infant offers solace in a futile and bleak circumstanceøa chance, as some parents have stated, to give some import to the brief life of their infant8. In these situations, the grief of the parents is profound, and their decisions are made in an atmosphere that is charged with complex emotions. Yet our society has consistently stood behind parents as the primary decision makers for their own children3,12,15,21. With the expanding power of medical technology to manipulate death, parents are increasingly faced with the terrible decisions of letting their dying children dieødecisions that only the parents will have to live with for the rest of their lives. What may be appropriate in the arduous circumstances of the birth of an anencephalic infant would be thoughtful and careful counseling with the parents about their options in coping with an event that is, at best, immensely difficult. This counseling should acknowledge the grief of the parents but should also include careful discussion about the implications of organ donation, including the possibility that the parents could experience residual guilt long after the event16. If the parents elect to donate their dying infant\"s organs, and if this decision means that the infant\"s short life is shortened by hours, the parents may find some comfort and peace in knowing that their infant\"s life was not utterly futile. If the parents are firm in their decision to donate their infant\"s organs, then physicians will be in a position to assist in realizing some good from the hopeless circumstances that surround the anencephalic infant. We would do well to listen carefully to the parents of Theresa Ann8.\nDefinition of eligible donors\nArticles on anencephalic organ donations, in both the medical and the legal literature, indicate clearly that a consensus on these issues is unlikely. The Supreme Court of Florida speculated about whether any right or wrong would ever be found8. One study noted astonishing disparities in the definition of death from one state to the next, even though all states have adopted the UDDA4. Other studies have tentatively considered amending the Uniform Anatomical Gift Act19 to permit the parents of anencephalics to donate the vital organs of their infants before cessation of all vital functions, but with great caution about the slippery slope16. Such an amendment would require careful construction, with provisions for protecting infants with other neural tube defects, individuals in persistent vegetative states, inmates on death row, and any other persons who might be considered as potential organ donors. Ultimately, we must deal with anencephalics as human infants, not as legal concepts1. We must honor the values of the parents and the value of the infants as we try to realize some good from their short, sad lives.\n1. Annas GJ: From Canada with love: Anencephalic newborns as organ donors? Hastings Cent Rep 17:36-38 1987.\n2. Botkin JR: Anencephalic infants as organ donors. Pediatrics 82:250-259 1988.\n3. Bowen v. American Hospital Association, 476 U.S. 610 (1986).\n4. Bryne PA, Evers JC, Nilges RG: Anencephaly - organ transplantation? Iss Law Med 9:23-33 1993.\n5. Bryne PA, Nilges RG: The brain stem in brain death: A critical review. Iss Law Med 9:3-21 1993.\n6. Capron AM: Anencephalic donors: separate the dead from the dying. Hastings Cent Rep 17:5-9 1987.\n7. Fletcher J: Indicators of humanhood: a tentative profile of man. Hastings Cent Rep 2(5):1-4 1972.\n8. In re T.A.C.P., 609 So.2d 588 (Fla. 1992).\n9. Mearns EA, Lebel RR, Gold RL, et al: Anencephalics as organ donors. The Fetus 8500-19-20, 1994.\n10. Medearis DN, Holmes LB: On the use of anencephalic infants as organ donors. Ne Engl J Med 321:391-393, 1989.\n11. Medical Task Force on Anencephaly The infant with anencephaly. Ne Engl J Med 322:669-74 1990.\n12. Meyer v. Nebraska, 262 U.S. 390, 1923.\n13. Moore KL, Persaud TV: The Developing Human: Clinically Oriented Embryology, 2nd edition, Philadelphia: WB Saunders Company,\n14. Peabody JL, Emery JR, Aswal S: Experience with anencephalic infants as prospective organ donors. Ne Engl J Med 321:344-350, 1989.\n15. Pierce v. Society of Sisters, 268 U.S. 510, 1925.\n16. Shewmon DA, Capron AM, Peacock WJ, et al: The use of anencephalic infants as organ sources: A critique. JAMA 261:1773-81, 1989.\n17. Singer P: Sanctity of life or quality of life? Pediatrics 72:128-129, 1983.\n18. Truog RD, Fletcher JC: Anencephalic newborns: Can organs be transplanted before brain death? Ne Engl J Med 321:388-91, 1989.\n19. Unif. Anatomical Gift Act §1, 8A U.L.A. 5 Suppl. 1987.\n20. Unif. Determination of Death Act §1, 12 U.L.A. 340 Suppl. 1991.\n21. Wisconsin v. Yoder, 406 U.S. 205, 1972."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:69b20d18-6b10-426c-862c-031a8ec185e1>"],"error":null}
{"question":"How can I adapt traditional camp songs for different age groups while preserving their educational and entertainment value?","answer":"For younger children, use songs like 'The Green Grass Grew All Around' which allows them to simply repeat lyrics after an adult and encourages them to observe their natural surroundings. Songs like 'Wheels on the Bus' are great for specific situations like bus rides to camp. For mixed age groups, use versatile songs like 'Kookaburra' which is easy to learn and suitable for all ages. For older participants, incorporate traditional folk songs like 'Camptown Races' or 'Oh My Darling, Clementine' which have historical significance and memorable choruses. You can also add educational value by teaching them songs with cultural significance, like 'This Land Is Your Land,' which Bruce Springsteen called the greatest song ever written about America.","context":["Summer Camp Songs\nSpending time around the campfire telling scary stories, reminiscing, and eating marshmallows is one of the best parts of camping. It’s also the perfect time to have some fun by singing Summer Camp Songs.\nSinging Summer camp songs is a great activity for many reasons. Songs can build camp spirit, give people a chance to express themselves, and help the group create some great memories. They also give you a chance to share your favorite songs with the group and show off your magnificent voice!\nHowever, to really get the most out of singing around the campfire, you need to find some songs that everyone knows and enjoys. In this post, I’ll be sharing some classic Summer Camp Songs that will make your next outing much more fun.\nIntroducing new songs to the group\nIf you have children in the group, it’s often a good idea to incorporate some actions into your songs. This will help them memorize the lyrics faster and can make singing even more enjoyable.\nSo, when you sing a song like “This Land Is Your Land” and have lyrics like:\nAs I was walking that ribbon of highway,\nI saw above me that endless skyway:\nI saw below me that golden valley:\nThis land was made for you and me.\nYou can use hand gestures that indicate driving for the ribbon of highway, point at the sky for the endless skyway, make a valley gesture with your hands for golden valley, then point at each other for the last line. It really brings some songs to life.\nIf a song has a lot of verses, like the original version of This Land Is Your Land, it sometimes helps to just learn 3 or 4 verses initially. You can then repeat the verses they know to memorize them before gradually introduce new verses.\nAnother great option is sing songs in a round, so the first person starts the song, then after the first line, the second person starts the song. It can be quite fun once everyone gets the hang of it. Here is a video of some girls singing Row, Row, Row Your Boat using this technique.\nThe Best Summer Camp Songs\n#1 – This Land Is Your Land\nThis is one of the most famous folk songs ever written. It was penned by Woodie Guthrie in 1940, using an existing melody from the song “When the World’s on Fire”. Bruce Springsteen one called This Land Is Your Land The greatest song ever written about America. Listen to the original song and check out the lyrics.\n#2 – There’s a Hole In the Bucket\nThis is a classic children’s song that was developed from a German folk song written in the 17th Century. The song is actually a discussion that occurs between two characters, Liza and Henry, about how to fix a hole in a bucket.\nYou could make this song more fun by having all of the males in your group sing the Henry part and having the females sing the Liza part. This Wikipedia page features the lyrics and a recording of the melody. You can also check out a great version of this song recorded by the Sesame Street.\n#3 – The Llama Song\nAlthough The Llama Song is one of the strangest Summer camp songs every created, it is definitely worth learning! It is strange because of the bizarre lyrics about llamas and other animals. The are several versions of this song, but the most common uses these lyrics and hand gestures. This video shows another version which sings about other animals as well.\n#4 – Camp Granada (Hello Muddah, Hello Fuddah)\nCamp Granada is a classic song that has been enjoyed by Summer campers for decades. It is a parody song written by Allan Sherman and Lou Busch in the 1960’s. The lyrics are based on the complaint letters that Allan received from his child while at Summer camp. The letters made him laugh so much that he made this hilarious song.\n#5 – Wheels On the Bus Song\nThis is a great song to sing if the kids are riding the bus to camp. Put this over the radio and encourage kids to sing along to the the lyrics.\n#6 – The Green Grass Grew All Around\nThe Green Grass Grew All Around is a wonderful song to sing along with younger children because they can simply repeat the lyrics as they are sung by an adult. As the song progresses, it becomes more complex and interesting. The song is a great choice for camping because it also encourages children to look at their surroundings and think about the many exciting things in nature. Listen to The Green Grass Grew All Around.\n#8 – Kookaburra\nAlthough “Kookaburra” being about an Australian animal, it is often enjoyed by campers in the United States and Canada. The song was written by Marion Sinclair in 1932, a primary school teacher living in Victoria. The song is easy to learn and perfect for kids of all ages. Check it out.\n#9 – Home On The Range\nThis classic song will be well known to older Americans, but there are many younger people who haven’t been introduced to it yet. If you can strum a few chords on the guitar, it is the perfect song to sing while sitting around a campfire. Listen to Home On The Range.\n#10 – Reese’s Peanut Butter Cup\nThis is a simple repeat-after-me song that kids love. The lyrics are easy to learn and the whole camp will be singing (and dancing) along in no time. Listen to Reese’s Peanut Butter Cup.\n#11 – Oh What A Beautiful Morning\nOh What A Beautiful Morning is the perfect track to be sung in the morning upon waking. It really gets the day off to an exciting start and gets everyone into a great mood. View the lyrics here and listen to Hugh Jackman performing it here.\n#12 – Bed Bug Song\nBoth kids and adults will have a great time singing the bed bug song. The song first became popular in Beech Bluff, Tennessee in the 1920s and quickly spread across the rest of the United States. There are a few alternate lyrics available and you can even add some custom verses yourself! Here is one set of lyrics and a recording of the song.\n#13 – Herman The Worm\nThis slightly strange song remains very popular in Summer camps in the United States. The song is about Herman, a worm who apparently eats other worms (or other animals), depending on the lyrics you use. You can see the lyrics here or watch a performance here.\n#14 – Camptown Races\nEveryone has sung this song at some point in their life! It remains one of the most popular Summer camp songs thanks to its catchy melody and simple lyrics. It is actually a very old song that was published in 1850 by an American singer named Stephen Foster. Foster was one of the most famous songwriters of his time and the quality of this tune says a lot about his skill. Listen to Camptown Races.\n#15 – If You’re Happy\nThis is a classic sing along song for kids. It’s been around since the 1950’s and is dangerously catchy. Listen to the full version of If You’re Happy here.\n#16 – Down By The Bay\nAlthough Down By The Bay was written as a children’s song, it is so enjoyable to listen to that many adults also love it. It’s unknown who wrote the tun, but it was famously performed by Raffi on his album Singable Songs for the Very Young which was released in 1976. Here is a video of Raffi performing his version of the song.\n#17 – She’ll Be Coming Around The Mountain\nThis is a slightly dangerous Summer camp song because of how addictive the melody is. You will struggle to get the tune out of your head after singing it a few times! It is actually a traditional folk song that was first published in 1927 with a melody adapted from the African-American gospel song “When the Chariot Comes”. Listen to She’ll Be Coming Around The Mountain.\n#18 – Take Me Home, Country Roads\nTake Me Home, Country Roads is a classic American song written by Bill Danoff, Taffy Nivert, and John Denver. The song is about the state of West Virginia and is considered an anthem in most of the country.\n#19 – Boom-Shika-Boom\nThis is a hilarious call-and-repeat song from The Learning Station. It can even be made better by incorporating some percussion and playing a funky beat as the group sings along. Check out this video to see Boom-Shika-Boom in action.\n#20 – Oh My Darling, Clementine\nOh My Darling, Clementine is a an American folk ballad that was written by Percy Montrose in 1884. It is a very catchy song that has an easy-to-remember chorus that children enjoy singing. Listening to Oh My Darling, Clementine.\nI hope you enjoyed our article on Summer Camp Songs! For more fun camp activities, bookmark the site."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:3f0043d9-23dc-4a1b-9512-e99f7672efb2>"],"error":null}
{"question":"What similar types of physical evidence of World War II attacks remain visible at Schofield Barracks and in UK cities today?","answer":"Both locations retain visible evidence of wartime attacks. At Schofield Barracks in Hawaii, bullet holes from Japanese strafing during the Pearl Harbor attack are still visible today, serving as a grim reminder of the 1941 assault. Similarly, in the UK, physical reminders of the German bombing campaign remain, particularly exemplified by Coventry's 14th Century cathedral, which was severely damaged during the November 1940 raid and whose ruins still stand as evidence of the devastation caused by the Luftwaffe's attacks.","context":["On the 80th anniversary of Japan’s surprise attack on Pearl Harbor, Oahu, Hawaii (with its many memorials) is once again in our sights—this time, for remembrance. But just as Pearl Harbor extended beyond Hawaii during World War II, it does so today as the world recognizes the red, white and blue sacrifice of “the greatest generation” of valiant troops who defended the USA. As Pearl Harbor’s echoes of war are honored in peace, here’s a look at some relevant monuments then and now.\nCurrent Pearl Harbor battleships pay tribute to those lost on December 7, 1941.\nWorld War II Valor in the Pacific National Monument\nWorld War II Peace Memorial on Attu Island.\nAmerican troops during the Battle of Attu attempt to reclaim the Aleutian island from Japanese invaders.\nFor the USA, World War II began at Pearl Harbor. But World War II’s Pacific valor memorials extend beyond Hawaii’s military outpost. The “World War II Valor in the Pacific National Monument” actually comprises nine memorials in three western states—California, Alaska and Hawaii. One little-known memorial in Alaska commemorates the Battle of Attu in the Aleutian Islands—the USA’s only land combat against Japan on American soil. These battlefields and military installations were declared a National Historic Landmark in 1985. Two years later, the U.S. Department of the Interior and the Japanese government collaborated and officially placed the World War II Peace Memorial on Engineer Hill. Pearl Harbor, however, is home to five of the famous World War II valor memorials—the USS Arizona Memorial, the USS Oklahoma Memorial, the USS Utah Memorial, parts of Ford Island and Battleship Row. All nine memorials are run by the National Park Service, but Pearl Harbor’s sunken battleships remain under the U.S. Navy’s jurisdiction.\nUSS Arizona Memorial\nPearl Harbor’s USS Arizona was sunk 80 years ago. Architect Alfred Preis’s USS Arizona Memorial design represents the December 7, 1941 setback as well as the USA’s ultimate victory in World War II.\nThe USS Arizona Memorial is Hawaii’s biggest tourist attraction.\nThe USS Arizona Memorial was partially funded by an Elvis Presley benefit concert and dedicated by President John F. Kennedy in 1962.\nPearl Harbor’s most famous memorial salutes 1,177 sailors and Marines who perished in the December 7, 1941 attack (most are still entombed in the sunken battleship wreckage which is a gravesite). This National Historic Landmark was designed by Alfred Preis, an Austrian-born Jew and converted Catholic who escaped Nazi persecution only to become a later victim of the USA’s internment camp policy which detained and relocated thousands of Japanese, German and Italian-American citizens after the Pearl Harbor attack. Preis (who also conceived the Honolulu Zoo), designed the memorial’s low-slung bow shape to represent an initial battle defeat, perseverance, and ultimate war victory. Elvis Presley performed a benefit concert to raise funds for the memorial, which was dedicated by President John F. Kennedy in 1962. With more than 1.8 million visitors per year, the USS Arizona (which still leaks 9 quarts of fuel per day) is Hawaii’s biggest tourist attraction.\nSchofield Barracks suffered collateral damage during Japan’s attack on Pearl Harbor.\nTwo-thirds of the USA’s airfield fleet at Pearl Harbor was incapacitated by Japan’s surprise attack on December 7, 1941.\nOriginally built in 1909, Schofield Barracks is an army base designed to protect Pearl Harbor and the island of Oahu from attack. That barely happened 75 years ago as only a handful of U.S. counterattack airplanes took off. Imperial Japan’s December 1941 two-wave assault wiped out two-thirds of U.S. air defenses at adjacent Wheeler Airfield and inflicted collateral damage to soldier housing. Bullet holes from Japanese strafing are still visible, a grim reminder of the sacrifice and bravery of American forces. Today Schofield Barracks serves as command headquarters for United States Army Hawaii. Known today as the Tropic Lightning Division, it’s home to the 8th Theater Sustainment Command, 7,000 housing units, training facilities, firing ranges, and helicopter landing zones. It was also a prominent locale in the film From Here To Eternity.\nThe 441-acre Ford Island was front and center during the Pearl Harbor attack.\nThis island (originally called Mokuʻumeʻume by native Hawaiians) at Pearl Harbor could be called Flip Flop Island. The 441-acres of land has changed ownership hands faster than a hot luau torch (even King Kamehameha gifted it away)—serving as an ancient Hawaiian fertility ritual site, sugar plantation and eventually, a dredged military battleship station isle that became the epicenter of the Pearl Harbor attack. Among other duties, it currently serves as a tsunami warning station and it was featured in the films Tora Tora Tora! and Pearl Harbor.\nFord Island’s Chief Petty Officer Bungalows\nFord Island petty officer bungalow at Pearl Harbor.\nIn the 1920s and 1930s, the Navy built single-story wooden bungalows on Ford Island to house chief petty officers who worked on nearby battleships. Some were damaged during the Pearl Harbor attack, but ultimately, they survived (and housed officers until the 1990s). One historic bungalow succumbed to the wrecking ball of an overzealous contractor who inadvertently demolished it in 2015 as part of a renovation project. Oops. The National Park Service admitted its mistake—the demolition violated procedure as Hawaiian preservation authorities weren’t consulted beforehand as required. The bungalow was replaced with a replica residence.","In early World War Two - from autumn 1940 to spring 1941 - German bombs killed 43,000 people across the UK. As the 75th anniversary of the start of the Blitz approaches, Imperial War Museum North has a new exhibition.\nHorrible Histories: Blitzed Brits looks at how the bombing came about - and the devastation and death that was caused in the nightly raids.\nIn the quiet early months of the war people in the UK prepared for air attacks by Nazi Germany, says Ian Kikuchi from the Imperial War Museum. But domestically there were few signs that Hitler's bombers would soon be based just across the English Channel.\nThen in spring 1940 the Germans invaded France and the Low Countries. Later in the summer the German air force, the Luftwaffe, tried to gain air superiority in the skies over the UK in the Battle of Britain.\nBut it was the resilience of the Royal Air Force during that \"Spitfire summer\" that caused the Germans to revise their plans, says Kikuchi - and on 7 September 1940 they embarked on a sustained eight-month bombing campaign, targeting all the UK's major cities.\nThe Germans tried to disrupt imports of food - and docks in places like Plymouth, Liverpool, and Belfast were targeted.\nOn the ground, people would have heard the drone of the engines of German aircraft - waiting for high explosive bombs to drop, some weighing up to 2000kg.\nLondon in particular, says Kikuchi, was bombed for 57 nights in a row between September and November 1940.\nUp to 10% of bombs did not explode on impact. The small hammer pictured below was used by bomb disposal teams to dislodge fuses.\nUnlike iron or steel, its brass head was less likely to create sparks or a magnetic field.\nAs well as RAF sorties, the British military used gas-filled barrage balloons to deter the Luftwaffe, with searchlights piercing the night sky, and ground-based anti-aircraft weapons - like the Swedish-designed Bofors gun - firing up at German bombers.\nThe anti-aircraft weapons caused other issues though. Kikuchi says there is an account of one London council calling for the guns to stay silent - as the vibrations they caused when fired were blamed for breaking toilet bowls in council-owned homes.\nTo make it more difficult for the Germans to spot cities at night, strict blackout restrictions were imposed. Homes covered their windows to stop light from seeping out, streetlights were dimmed and car headlights were reduced to small slits.\nBut in the night sky, with moonlight shimmering on river estuaries, certain cities like London, Liverpool and Hull were easier to spot.\nThe colour photo above shows an air raid warden surveying bomb damage in central London. Kikuchi says the wardens were a key figures civil defence service - each given a few streets to look after.\nThey reported damage, educated local residents and kept detailed records of where families usually slept at night - and what kind of air raid shelters they had.\nDepictions of air raid wardens as pompous and power-crazed - like Warden Hodges in the BBC sitcom Dads Army - are not necessarily fair, says Kikuchi. He says many of them were public-spirited individuals wanting to do their bit for the war effort.\nThe job also offered steady employment, he says. By the summer of 1940, he says, more than 600,000 people were employed in air raid precautions work.\nTo escape the bombs, more than a million children, thousands of young and expectant mothers, and some disabled people were evacuated to the countryside.\nKikuchi says for the city evacuees - and those in the country who took them in - there was often a culture shock, as two worlds collided.\nLarge scale evacuation began on the eve of the outbreak of war in 1939 - but when the expected air raids on cities did not happen, many evacuees returned home.\nOnce the bombing began in earnest a year later, there was a second wave - and some children ended up being sent overseas to North America, South Africa, Australia and New Zealand.\nWhen war broke out in 1939, there was a real fear that poisonous gases, like those used on the battlefields of World War One, would be inflicted on the civilian population during air raids.\nGas masks were issued to everyone - but some manufacturers saw a commercial opportunity to protect family pets. This next image shows a gas-proof dog kennel.\nBut despite everything that was done to protect people and to confuse the Luftwaffe - from mass evacuations to the blackout - the German bombing campaign on the UK from late 1940 into 1941 was devastating. Buildings and homes were destroyed. Thousands of people lost their lives.\nThis image - from 23 December 1940 - shows fire crews tackling a blaze in a bombed warehouse in Manchester. The city experienced two nights of heavy bombing just before Christmas.\nNightly scenes like this were repeated across the country - and when morning came, ruins were on show for all to see.\nThis next photo shows piles of rubble in Coventry on 16 November 1940, the day after the bombing raid which also knocked the heart out of the city's 14th Century cathedral.\nIn May 1941, Hitler turned his attention to the Eastern Front - and domestically, says Kikuchi, there was a sense that Britain had weathered the worst of what the Luftwaffe was able to inflict.\nBut although the Blitz accounts for more than two-thirds of the 60,000 or so civilian WW2 deaths - the deadly attacks continued sporadically.\nIn summer 1944, after the D-Day landings in Normandy, the Germans launched their vengeance weapons - V1 flying bombs and then V2 rockets.\nThis final image shows a police officer comforting a man whose home in south London was destroyed in a V1 bomb attack.\nThe man had taken his dog for a walk on a Saturday afternoon. He returned to find his house destroyed, and his wife - who had been inside making dinner - killed.\nHorrible Histories: Blitzed Brits can be seen at IWM North, part of Imperial War Museums, in Manchester until 10 April 2016.\nAll images and video subject to copyright.\nSubscribe to the BBC News Magazine's email newsletter to get articles sent to your inbox."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:7835bbbc-2422-4105-91ed-dcc5a64e5973>","<urn:uuid:715f9b3b-45c1-46ee-8f66-6d842c3975ad>"],"error":null}
{"question":"How do the complexity limitations differ between describing systems using wavefunctions versus Fermi-Dirac statistics?","answer":"Both approaches face different complexity challenges when describing quantum systems. Wavefunctions become exceedingly complicated as system size increases, making it practical to write out full wavefunctions only for very simple systems, even though it's theoretically possible to write them for any system. Meanwhile, Fermi-Dirac statistics becomes particularly challenging when dealing with non-ideal Fermi gases, where calculating thermodynamical functions cannot be reduced to simple combinatorial analysis and requires using the Gibbs method with anti-symmetric wave functions. Both approaches require working with increasingly complex mathematical representations as the number of particles or states in the system grows.","context":["Quantum statistics applied to systems of identical particles with half-integral spin ( in units ). It was proposed by E. Fermi (1926), and its quantum-mechanical meaning was elucidated by P. Dirac (1926). According to Fermi–Dirac statistics there can be at most one particle in each quantum state (the Pauli principle). For a system of particles subject to Fermi–Dirac statistics, the states are described by wave functions that are anti-symmetric relative to permutations of particles (that is, of their coordinates and spins), while for Bose–Einstein statistics they are symmetric.\nThe quantum state of an ideal gas is defined by giving the totality of occupation numbers of the levels of the system in the space of momenta and spins , , where each indicates the number of particles with momentum and spin . In the case of Fermi–Dirac statistics can be equal to zero or one.\nA gas is a system of a very large number of particles, and therefore its quantum levels are very densely distributed and tend to a continuous spectrum as the volume tends to infinity. It is convenient to group the energy levels in small cells containing levels in a cell. To each cell corresponds an average energy , and the number is assumed to be very large. The quantum-mechanical state of the system is defined by the collection , where is the number of particles in a cell, that is, the sum of the for the levels in the cell. The number of different distributions over the cells (that is, the statistical weight of the state of the ideal Fermi–Dirac gas) is equal to\nand determines the probability of that distribution of the particles over the cells characterized by the occupation numbers . The statistical weight is calculated by means of combinatorial analysis, taking account of the indistinguishability of the particles and the fact that there can be at most one particle in each state.\nThe most-probable distribution of the particles over the quantum states, corresponding to a given energy and number of particles ,\nis found from the extremum of the statistical weight (1) under the additional conditions (2). The corresponding average occupation numbers are equal to\nwhere is the chemical potential, , is the Boltzmann constant (a universal constant, ), and is the absolute temperature. The magnitudes and are found from the conditions (2).\nThe entropy of an ideal Fermi gas is defined as the logarithm of the statistical weight (1) for the most-probable distribution (3)\nwhere the summation is over all cells. The entropy can be used to calculate the free energy and other thermodynamical functions.\nIn the case of a non-ideal Fermi gas, the calculation of the thermodynamical functions is a tricky problem, and cannot be reduced to a simple problem in combinatorial analysis. Their calculation is based on the Gibbs method, taking account of Fermi–Dirac statistics. If the Hamilton operator of the system is known, then the free energy is equal to\nwhere the trace operator is taken over states satisfying the requirements of Fermi–Dirac statistics, that is, over anti-symmetric wave functions. This can be achieved if one uses a representation for in which its action is defined on the space of wave functions and occupation numbers, that is, if one passes to the second quantization representation.\nFor references see Bose–Einstein statistics.\nFermi-Dirac statistics. Encyclopedia of Mathematics. URL: http://www.encyclopediaofmath.org/index.php?title=Fermi-Dirac_statistics&oldid=22423","The wave-particle duality of matter is dealt with in quantum mechanics by considering that, rather than a particle traveling along a definite path, it is distributed through space like a wave. The classical idea of a trajectory is thus replaced in quantum mechanics by a wave, which is defined by a wavefunctionrepresented by ψ.\ni.e. the spatial distribution of a particle is represented by a wave, and the wavefunction is the mathematical function that describes this wave. The wavefunction contains all the information that would be available from the trajectory.\nFrom this, it follows that it must be possible to write a function that describes the complete state of a system (as it is possible to write a wavefunction for all the individual particles that make up the system). A function that completely describes a system is also called a wavefunction. (By “completely describes” we mean that the function contains information about all the properties of the system that may be experimentally determined, for example at the simplest level the position and momentum of particles.)\nIt is a fundamental principle in quantum mechanics that the wavefunction of a system contains all the information about measurable physical properties of the system.\nThe wavefunction is the function ψ described as “a function that precisely describes the system of interest ” in the section on principles of quantum mechanics on the previous page.\nIn general, a wavefunction is a function of the spatial coordinates of the particles that make up the system and of the time. If the way the system changes with time is not of interest, then this part of the wavefunction may usually be neglected. (It commonly proves possible to write the wavefunction as a product of a solely time-dependent part and a time-independent part.)\nThe wavefunction may also depend upon a property of the particles themselves known as their spin, but discussion of this will be left till later.\nAlthough, as we have said, it is theoretically possible to write a wavefunction for any system, the function rapidly becomes exceedingly complicated. It will thus normally be the case that a fully written out wavefunction will only be encountered for very simple systems.\nOne very important use of the wavefunction comes from the Born interpretation, which derives information about the location of a particle from its wavefunction.\nQuite simply, it states that the square modulus of the wavefunction, |Ψ|2 , at any given point is proportional to the probability of finding the particle at that point. (The quantity |ψ|2 is thus a probability density.)\nThis is simply illustrated in a 1-dimensional situation, when we can state that if the wavefunction of a particle has a value ψ at the point x, then the probability of finding the particle between x and x + dx is proportional to |ψ|2 dx . Thus the probability of finding the particle between two points a and b is proportional to the integral of the square modulus of the wavefunction, evaluated between limits of a and b:\nThe square modulus of a wavefunction (or indeed any function in general) is given by ψ ψ*, i.e. by the function multiplied by its complex conjugate. This means that |ψ|2 must always be real and positive.\nSince physical properties are directly related only to the square modulus of the wavefunction, it follows that we need not be concerned about the effect on these properties of the wavefunction being complex or negative at a particular point in space. (Note however that the sign of a wavefunction can indirectly be of great significance, as it gives rise to the possibilities of constructive or destructive overlap between different wavefunctions, which is for example crucial in theories of molecular bonding.)\nIt may be shown that if a wavefunction is an accurate description of a system, then it can be multiplied by any constant factor and still remain an accurate wavefunction for the system. i.e. if Ψ is a wavefunction, then so is NΨ, where N is a constant.\n(This is because any wavefunction must satisfy a particular equation known as the Schrodinger equation. The wavefunction occurs on both sides of the equation, so any constant factor may be cancelled. This is covered in more detail under a discussion of the Schrodinger equation, here.)\nThis allows us to find a constant factor, N, called a normalisation constant, that makes the constant of proportionality in the Born interpretation of the wavefunction equal to one. i.e. it makes the proportionality into an equality.\nNow, for a wavefunction NΨ, the probability of finding the particle anywhere in space is proportional to the integral of the square modulus of the wavefunction, N2|Ψ|2 , over all space. However, we also know that the probability of finding the particle somewhere in space must be equal to one (it is certain the particle must be somewhere in space), so we may write:\nEvaluation of the integral allows us to calculate the constant N required to normalise the wavefunction. Note that the integral is over all the space that the electron may occupy, eg from + ∞ to -∞ .\nFrom here on, unless otherwise specified, it should be assumed that the wavefunctions used are normalised. i.e. ψ includes a numerical factor that means in one dimension:\nIn three dimensions, the requirement for a wavefunction to be normalised is:\nwhere dτ is shorthand for dxdydz."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:200ae7d3-3788-4be3-93b8-49c196069c4f>","<urn:uuid:1f396825-d268-443f-966d-10af262778da>"],"error":null}
{"question":"What are the key environmental impacts of landfill waste disposal, and what modern technological solutions are being implemented to mitigate these effects?","answer":"Landfills pose several environmental impacts: they leak and pollute groundwater, emit unsafe gases, and can pose physical dangers as evidenced by the Ghazipur landfill collapse that killed 2 people. The leachate from landfills can contaminate underground water tables, potentially leading to water scarcity. However, modern technological solutions are being implemented to reduce these impacts. Municipal sanitary landfills represent an improvement over traditional landfills by incorporating careful site location, day-to-day management of waste spreading, soil covering, bottom liners, and leachate collection systems. Additionally, waste compaction technology is being used to compress materials like cans and plastic bottles into blocks, which prevents metal oxidation and reduces required air space. These technological interventions, while not completely eliminating environmental impacts, significantly reduce the negative effects of landfill waste disposal.","context":["In her race to be the champion among other countries , the waste that our country leaves behind totals to about sixty-two million tons a year. Waste generation is hence a major crisis faced by our country and the world at large. The integrated effect of population, development and social changes along with improper waste management and lack of awareness has paved the way for this problem. Waste generation with its impact on every aspect of our lives thus deserves prime attention.\nThe increasing amount of garbage and its impacts can be traced back to many reasons. The growing population and the subsequent increase in production for meeting the demands contribute to increasing waste. Along with this, the corporate tactics for the financial gains of low durability products and the introduction of a mentality of ‘new is prestige’ made people opt for new products and has consequently increased waste. Urbanisation without effective urban planning and changes in living patterns add to this problem. The change in consumption patterns and the lack of responsible waste management in households, industries and government alike contribute to the mounting pile of garbage. Instead of encouraging students to use old books, the schools make it compulsory to buy the new ones for their own monetary gains. The households, instead of using kitchen waste for compost, often prefer to throw it somewhere, land or water. All of these small actions make each individual a deliberate sponsor for this ever-growing mountain of garbage.\nThe impact of waste is tremendous! Its effect on people is multidimensional; social, economical and environmental. Organic or inorganic, hazardous or non-hazardous, if not treated properly has its own impacts. Organic waste is easy to handle while others are not. E-wastes, hospital wastes and hazardous wastes are of grave concern and needs special attention.\nIndia is a country which tried to remove its concept of ‘purity-pollution principle’ for centuries but ironically , the problem of waste management made it possible for this discriminatory principle to again reassure its place back in society. Waste generation and its inefficient management affect the lower strata more than the upper strata of our society. It can be easily drawn from the fact that most of the landfills are found near the places where people of low incomes reside. Also, the waste management that exists is done by the poor and the people of lower caste without any supportive networks and protection; they are more prone to health risks and yet, the voice for this section of our society is never echoed.\nThe stray dogs feed on the meat waste thrown into the open and this becomes one of many factors for the aggressive behaviour of the dogs. Increased waste helps in the outgrowth of rodent population which in turn may act as vectors for malaria, cholera etc and has the potential to affect food security as well. Recalling Ghazipur landfill incident, where the collapse of sixty-five-metre-high landfill killed 2 people, makes us aware of different kind of danger landfill poses to people.\nIncreased waste also means inefficient use of resources. Inefficient waste management leads to increased diseases which in turn increases the expenditure on health and loss of the manpower ,consequently increasing poverty and reducing immunity. Huge cost is incurred to revive the water bodies that are affected by waste dumps. When the state steps in to face the challenges of the health and waste sector, it might result in lower spending on other sectors .\nWaste not only increases expenditure but also decreases revenue . Who would want to spend their money only to see smelly landfills and waste-filled water bodies ? Waste, thus affects the natural beauty of our country and could lead to potential loss in the tourist sector .\nAs a disease prone society with polluted environment, our ranking could lower in the global indices such as Environment Performance Index and thus hurt the pride and the prestige our country holds in international level.\nGreat pacific garbage patch and marine debris failed to catch the necessary attention until the harrowing sight of bird skeletons with stomach full of plastics became viral! It makes us remember how the irresponsible acts of self-declared ‘dominant species of Homo sapiens’ affect other innocent species as well.\nIn the paper Marine Pollution Bulletin , one of the reasons stated for the upstream movement of mangrove forests is the sewage sludge that causes pollution in water bodies. Sewage sludge is also known to cause about twenty-three types of diseases! Sewage sludge and waste polluted water bodies have negative impacts on the species in the water as well. When the contaminated water is used for irrigation, it too affects soil fertility or when consumed, may endanger some species leading to the interruption in the food chain. The wastes may obstruct the flow of our natural drainage system and add to the possibility of floods.\nThe landfill fires and the burning of plastics and other wastes which emit greenhouse and other gases causes air pollution and contributes to greenhouse effect , global warming and climate change . The leachate from landfills and waste may pollute the underground water table which may cause water scarcity. The recently debated scenario of agricultural waste , stubble and its burning needs mentioning too. Plastics and other wastes also affect the fertility of the soil. Hospital waste may cause release of pathogens and formation of drug-resistant microorganisms. Toxic materials from waste mainly E-waste causes serious ailments such as cancer , respiratory problems etc.\nThe picture after the floods in Kerala of the piled up wastes that the floods left behind makes us think that the nature itself had become the judge, jury and executioner for its suo moto case against humans. Before we become victims of our own greed , this menace of waste has to be tackled. Multifaceted problem of waste needs multi-pronged strategy of command and control instruments, economic schemes ,R & D , technology , awareness generation help in reducing and managing wastes at individual and industrial level. Hence , an integrated approach of academia, intellectuals and stakeholders is needed for the effective management of waste.\nThe primary solution is, of course, the three R’s; Reduce, Reuse, Recycle with source reduction given prime importance. Awareness generation at the grassroots level helps in creating a more responsible society. Every one of us has the responsibility to do our part. Biodegradable waste can be used as compost through methods such as vermicomposting, bio composting etc.\nEncouraging the use of more durable things is appreciable. Instead of plastics and paper glasses or plates, let’s switch to glass tumblers at weddings etc. Using cloth bags in lieu for plastic bags can be made compulsory. Even such small actions can contribute to a larger cause of managing waste. Participation of NGOs are an effective way for awareness generation and managing waste. Successful examples that can be quoted are Saahas, No Dumping .\nStringent legislations are to be implemented. Irresponsible handling of waste must be monitored with standard in residential and industrial areas. Segregation of waste at home and industries must be made compulsory and must be fined if not done properly. The model used in Germany of refunding for the plastic bottles when returned can be introduced. Incentives must be in place for industries for proper waste management. Imposing waste tax should be implemented effectively. Mechanisms must be initiated to make sure that valuable metals are recovered from e-waste. Along with control instruments , monitoring must also be done such as placing cameras and managing data on waste generation .\nTechnological interventions help in easy waste disposal. Municipal sanitary landfills are better when compared to traditional landfills. With careful site location , day-to-day management of evenly spreading and covering with soil and bottom liners and leachate collection systems, landfills can reduce the impact of waste. Incinerators can also be used for waste reduction if it is accompanied with air pollution control equipment, technical supervision and segregation of wastes which can be harmful under high temperature like heavy metals or toxic products . Though such measures are not devoid of complete impact on the environment, it reduces the impact multi-fold.\nAlong with this , more importance must be given to R & D for technology-driven and sustainable ideas. Bitumen roads using plastics can be used more. Enzyme cocktails of PETase and MHETase or enzyme PETase which can digest plastic are more innovative ideas. Generating energy, fuels from wastes including refused-derived fuel helps substantially.\nAll our achievements as a country cannot be heralded when there is a pile of unaccounted waste, revealing the need for a circular economy rather than a linear one. Waste is a cancer that slowly deteriorates our Mother Earth. Until and unless we, the sons and daughters, understand the responsibility we hold towards nature and our fellow beings, there may not be an earth but only one large planet of garbage!","The world has been facing the problem of waste disposal and the rapid industrial and technological development has only accentuated it. The increasing variety of waste has become one of the biggest challenges pertaining to environmental protection.\nIn olden times, waste generated was generally organic in nature which would dissolve into the ground, but alongside modern development, the proportion of hazardous substances in the garbage has increased manifold. It poses grave threat to humans, animals and plants.\nEarlier bags used to be made of harmless substances like cloth and jute. Pottery was used to carry things like milk, tea or curd. The plastic has changed the situation for the worse because the plastic never degrades. Its recycling is possible, but there is generally a lack of safe, quality and proper management for plastic disposal.\nIn India, the problem of waste disposal can be witnessed everywhere; from overflowing garbage to improper disposal of e-waste this is a very serious issue which needs to be recognized and tackled on a war footing. Improper drainage system, mountains of garbage, urinating in public, throwing waste on the streets, etc are all very usual sights in our country. Therefore, we need to develop effective methods of disposal of waste to sustain life and conserve environment.\nMeaning of Waste Disposal\nWaste disposal implies removal, destruction or storing of damaged, used or other unwanted domestic, agricultural or industrial products. Burning, burial at landfill sites or sea, and recycling are part of the waste disposal process.\nBroadly speaking, waste disposal involves collection, transportation, dumping, recycling, sewage treatment along with monitoring and regulation measures for waste products. In the course of these various processes, there are a lot of problems that tend to get associated with waste disposal.\nProblems due to Improper Waste Disposal\n- Excessive Quantity – The world witnesses huge generation of waste on a regular basis. About 220 million tonnes of waste is produced annually in the United States alone. We can imagine how much waste might be produced globally.\nAccording to a World Bank report, the average global municipal solid waste (MSW) generation per person on daily basis is about 1.2 kg and the figure is expected to rise up to 1.5 kg by 2025. Effective waste disposal is a major requirement of every state and local authority. Without prioritizing on reuse, recycling or the use of environmentally friendly materials, a number of one-time use products are produced globally, leading to increase in the amount of waste generated.\n- Toxic nature of waste – The ever-expanding manufacturing industries produce very hazardous toxic products that end up getting thrown away after use. For example, packaging is one of the biggest and rapidly enlarging categories of solid waste in which approximately 40% of the waste is plastic which is not biodegradable.\n- Mess at landfills – Lack of proper on-site waste management of landfills contributes to serious threats to the environment. In the long-term, landfills leak and pollute ground water and other neighboring environmental habitats, making waste management very difficult. They also emit potentially unsafe gases.\n- Prevalence of vested interests – Waste disposal and management has become a lucrative business. All aspects of the market from operating landfills, sewer systems and incinerators to recycling facilities are dictated by the large enterprises that operate in the waste disposal business. The aim of such businesses is simply to make profits regardless of the waste reduction requirements or the resultant destructive impacts on environment.\n- Outdated waste disposal technologies – Instead of developing effective recycling and waste reduction programmes, short-term solutions are relied upon for waste disposal and management facilities. As a result, outdated technologies are used to deal with waste disposal. Lack of novel technologies for reducing the toxicity and volume of waste or enhancing recycling, especially of solid waste is a major issue. Some of the technologies that are marked as “green” are not so in the true sense.\nTypes of Waste\n1. Liquid Waste – This type of waste is generally found both in households as well as in industries. Dirty water, organic liquids, wash water, waste detergents and even rainwater are certain types of such waste. Liquid waste can be classified into point and non-point source waste. Point source waste is all the manufactured liquid waste. Natural liquid waste is classified as non-point source waste.\n2. Solid Waste – This type of rubbish includes a variety of items found in your household along with commercial and industrial locations. Solid waste has the following types:\n- Plastic waste: Many products that can be found in our household such as bags, jars, etc. come under this category. Many types of plastic can be recycled though it is non-biodegradable. Plastic should be sorted and placed in the recycling bin.\n- Paper/card waste: Packaging materials, newspapers, cardboards and other products are included in this category of waste.\n- Tins and metals: Various items such as metals that exist in our households come under this category.\n- Ceramics and glass: These are easily recyclable items.\n3. Organic Waste – This type of common household waste includes all food waste, garden waste, manure and rotten meat. Organic waste is turned into manure by microorganisms over time but this does not mean that it can be disposed anywhere. It must never be simply discarded with general waste as it causes the production of methane in landfills.\n4. Recyclable Waste – All waste items that can be converted into products to be used again are the ones that come under this category. Paper, metals, furniture and organic waste are the type of waste that can be recycled.\n5. Hazardous Waste – All flammable, toxic, corrosive and reactive waste come under this category. These items can cause harm to the environment as well as to the people and so must be disposed off carefully.\nMethods of Waste Disposal\n- Landfill – The waste that cannot be reused or recycled is spread out in a landfill in some low-lying areas across the city. After each layer of garbage, a layer of soil is added. After this process, the area is declared unfit for construction for next 20 years. This area can only be used as a playground or park.\n- Incineration -The process of controlled combustion of garbage to reduce it to incombustible matter (ash, waste gas, and heat) is called i The waste gasses generated in the process are treated and released into the environment. This process reduces the volume of waste by 90 percent. It is considered as one of the most hygienic methods of waste disposal. The heat generated is used to produce electric power in some cases.\n- Waste Compaction –Waste materials such as cans and plastic bottles are compacted and made into blocks and are sent for recycling in the process. Transportation and positioning is made easy as this process prevents oxidation of metals and reduces air space need.\n- Biogas Generation – Food items, animal waste or industrial waste from food packaging industries are sent to bio-degradation plants. These plants convert the waste to bio-gas by degradation with the help of bacteria, fungi, and other biological means. The micro-organisms use up the organic matter as food. The degradation can occur either aerobically (with oxygen) or anaerobically (without oxygen). Bio-gas generated as a result is used as fuel, and the residue is used as manure.\n- Composting – Organic materials tend to decompose with time. Majority of wastes are made by food scraps, yard waste, etc. Nutrient rich manure is formed by organic materials which are buried under beds of soil and which decay under the action of microorganisms. This is the process of composting. This process increases the soil’s water retention capacity and enriches it.\n- Vermicomposting – The process of using worms for the degradation of organic matter into nutrient-rich manure is known as vermicomposting. Organic matter is taken up by worms and they feed and digest it. Excretory materials or the by-products of digestion given out by worms makes the soil nutrient rich which enhances the growth of bacteria and fungi.\n- Recycling – Wastes are transformed into products of their own genre using industrial processing and this process is known as recycling. Commonly recycled products include paper, glass, aluminium and plastics. This process allows us to reuse the items in an environmentally-friendly manner.\n- Disposal in Ocean/Sea – Radioactive wastes are usually dumped into oceans away from human habitats but this is thought to deprive ocean of its inherent nutrients and also harm the aquatic life.\nWaste Disposal Management/Solutions\nNatural resources are decreasing. Raw material is becoming expensive and the cost of extracting it is also increasing. In this context, recycling metal like aluminum saves 95 percent energy and 60 percent of the time.\n- Government Initiatives – In collaboration with NGOs in Indonesia, biological portion of waste is allowed to rot and the chemical part is separated from it. Help is sought from tourists too to avoid mindlessly piling of waste, and generate as far as possible organic waste, which has the potential for the development of urban mineral resources.\nOne advantage of such initiatives is that people become aware of waste as well as the difficulties in disposing it off. Teens are taught that they should try to reduce garbage and avoid chemical waste.\nThe Government of India has launched Swachh Bharat Abhiyan (SBA) and Smart City Mission (SCM) to improve standards of sanitation, hygiene and living across the country. In preparing a three-year work agenda (2017-18 to 2019-20), the Niti Aayog has prepared a comprehensive framework to tackle the problem of Municipal Solid Waste (MSW).\nAs per the 2011 Census, 77 million residents living in 7,935 urban areas, generated 170,000 tonnes of solid waste per year in India. In view of this fact, the Aayog has been developing the agenda to complete the work in time, because by 2030, due to the expansion of cities boundaries, 590 million inhabitants would be living in cities, making management of waste all the more difficult.\nThe solutions suggested in this agenda are of two types: Generation of energy from waste material for large municipalities and disposal of waste for small towns and semi-urban areas by setting up a plant. In it, it has been suggested to set up a new Waste to Energy Corporation of India (WECI) under the public-private partnership model. After the establishment, the proposed corporation will play an important role in the generation of energy from waste in 100 smart cities by 2019.\nThe government should be given credit for starting discussion on this social and environmental problem. The agenda proposed by the Niti Aayog to make the country clean and green is a step in the direction of proper waste management.\n- 3 R’s – Management of waste and movement in the right direction to achieve zero waste can be achieved with the implementation and consistent practice of the three R’s mantra of Re-use, Reduce, and Recycle. More efforts should be put towards the education of waste management by local communities, authorities and states. Proper segregation of waste, awareness regarding proper disposal and making people practice 3 R’s can help us tackle the waste disposal issue.\n- Effective waste disposal and management – Improved solutions for various problems associated with waste materials can be offered by an effective strategy for municipal waste disposal and management. Gradual improvement of new and cost-effective facilities which aim to encourage higher environmental protection standards is an essential part of this strategy.\n- Controlled and Monitored Land filling activities – Various local construction industries generate tonnes of construction and demolition materials. Construction and demolition materials can be resourcefully reclaimed, reused or recycled in other projects such as landscaping, village houses, recreation facilities or car parks, or roads with the control and monitoring of land filling and fly-tipping activities in the area of public works.\n- Hygienic and efficient waste disposal management –Solution to waste problems can be offered by a multifaceted approach on waste transfer and diversion in terms of more hygienic and efficient waste disposal management. The local authorities and state waste management facilities need to formulate waste disposal plans, with an objective of making certain that there is convenient and proper waste disposal at landfills and waste transfer facilities to address most of the waste problems. Mandating equipment standards and rerouting of refuse collection/transfer are measures that can enhance the environmental performance of waste disposal operations.\n- Thermal waste treatment – It has been proved that thermal waste treatments are not 100% green as they are usually said to be. States researchers as well as green groups and academicians can explore the possible developments regarding advanced thermal waste treatment techniques. For dealing with the environmental concerns, appropriate and improved thermal waste treatment technology is a significant strategy.\n- Extra charges for polluters – Polluters who generate waste need to pay for the suitable disposal of non-reclaimable materials when it comes to waste management. Law requires the polluter to pay for the impact caused to the environment. Charging schemes should be incorporated on all waste disposal aspects including construction waste and domestic waste through public fill reception facilities for the pay principle to be effective. It is part of Eco-product responsibility policy, which is a great tool for waste reduction, recovery and recycling. Producers, wholesalers, importers and retailers are required to share responsibility for the collection, treatment, disposal and recycling of used products with an aim of cutting back and steering clear of the environmental impacts caused by such products.\nImportance of Waste Disposal\n- Public Health Preservation – It’s important for proprietors to remove potentially dangerous chemicals as environmental waste has a significant impact on public health. For example; due to the presence of dangerous elements in dead car batteries, they should be disposed very carefully.\n- Economic Advantages – Employment of proper waste removal system provides a lot of societal economic benefits. For example, ensuring that a product is used as a part of an area (garden or local farm) for compost can help replenish nutrients in the ground and help municipalities save money on landfill areas.\n- Environmental Protection – Improper disposal of a plastic product or one containing dangerous chemicals can help the waste make its way into local water sources and soil, thus affecting local wildlife as well as natural growth processes. Apart from NGOs, governments are also engaged in the protection of wildlife across the world. Special efforts are also being made to save the living organisms facing extinction due to improper methods of waste disposal.\n- Saving Energy – Organizations can benefit from usage of proper waste management and waste disposal strategies. Significant amount of resources is spent on incineration systems by these organizations which use up energy without providing any significant gain. Technologies which help capture energy used by these incinerators have been crafted which allow cities to generate electricity through the process of incineration.\n- Climate Change – Due to the improper disposal of waste, greenhouse gases have seen a significant increase. These trap a lot of heat which harm all living organisms.\nFaulty waste disposal gives rise to a number or issues or problems which can ultimately cause harm to the Planet Earth. By following simple techniques such as the 3 R’s can help us bring a significant decrease in the waste produced and also help us dispose it properly. Employing various waste disposal methods, management techniques and solutions can help us bring a significant decrease in the number of problems that are caused globally due to improper waste disposal."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:03f3e168-fcce-4749-b898-ad54cd744705>","<urn:uuid:77df8bf8-aa0e-4a61-8cb6-a9f6b119bb15>"],"error":null}
{"question":"Please outline the practical applications of enzyme kinetics in human metabolism and describe how gene therapy technology has progressed in treating genetic diseases.","answer":"Enzyme kinetics has practical applications in understanding human metabolism through kinetic constants like Vmax and Km, which help predict enzyme behavior in living organisms and estimate enzyme concentrations. The study reveals how enzymes function in diverse body reactions and their role in metabolism. Regarding gene therapy progress, while trials have not had significant success historically, the science is advancing and the risk-benefit ratio is improving. Gene therapy aims to address genetic abnormalities that cause or predispose individuals to diseases by modifying malfunctioning genes. However, this advancement requires careful consideration due to uncertain risks, particularly in germ-line modifications that could affect future generations.","context":["What are the types of enzyme kinetics?\nA select few examples include kinetics of self-catalytic enzymes, cooperative and allosteric enzymes, interfacial and intracellular enzymes, processive enzymes and so forth. Some enzymes produce a sigmoid v by [S] plot, which often indicates cooperative binding of substrate to the active site.\nHow is enzyme kinetics measured?\nThe rates of these reactions can be accurately measured using a UV-Visible spectrophotometer. When an enzyme (E) binds with a substrate (S), an intermediate or enzyme/substrate complex (ES) is produced, which can further react and yield a by-product (P), shown in Scheme 1.\nWhat are enzyme kinetics used for?\nEnzyme kinetics is the study of the rates of chemical reactions that are catalysed by enzymes. The study of an enzyme’s kinetics provides insights into the catalytic mechanism of this enzyme, its role in metabolism, how its activity is controlled in the cell and how drugs and poisons can inhibit its activity.\nWhy is understanding enzyme kinetics important?\nEnzymes are essential for life and are one of the most important types of protein in the human body. Studying enzyme kinetics provides information about the diverse range of reactions in the human body, which we can use to understand and predict the metabolism of all living things.\nWhat is Vmax and KM?\nVmax is the maximum rate of an enzyme catalysed reaction i.e. when the enzyme is saturated by the substrate. Km is measure of how easily the enzyme can be saturated by the substrate. Km and Vmax are constant for a given temperature and pH and are used to characterise enzymes.\nWhat is MM plot?\nThis is a plot of the Michaelis-Menten equation’s predicted reaction velocity as a function of substrate concentration, with the significance of the kinetic parameters Vmax and KM graphically depicted.\nWhat is a kinetic enzyme assay?\nWhat is a kinetic enzyme assay? In the kinetic assay method, the progress of the reaction is continuously measured as substrates are converted into products. Changes in concentration of both substrate and product cause shifts in measurements.\nWhat is the kinetic assay?\nAn enzyme-based assay that measures the amount of substrate present by correlation of the rate of reaction with the known dependence of the rate on substrate concentration, usually under first-order conditions. ( see also end-point assay)\nWhat is enzyme kinetics study?\nEnzyme kinetics is the study of the binding affinities of substrates and inhibitors and the maximal catalytic rates that can be achieved.\nWhat are the two basic observations to study enzyme kinetics?\nWhat are the two basic observations made in the laboratory to study enzyme kinetics? The velocity is directly proportional to enzyme concentration and hyperbolic with respect to the substate concentration.\nHow is enzyme kinetics used in real life?\nThere are many practical uses of enzyme kinetics. For example, the kinetic constants can help explain how enzymes work and assist in the prediction of the behavior of enzymes in living organisms. Vmax and Km both play a key role in understanding the metabolism of the human body.\nWhat is Vmax enzyme kinetics?\nIn enzyme kinetics, Vmax is the maximum velocity of an enzymatically catalyzed reaction when the enzyme is saturated with its substrate. Since the maximum velocity is described to be directly proportional to the enzyme concentration, it can therefore be used to estimate enzyme concentration.","Gene therapy and enhancement\nStephen Napier says the Vatican is concerned about the use of gene therapy technology . . .\nDespite scientific advances in gene therapy and enhancement, the 2008 Vatican document Dignitas Personae (DP) expresses concern for the use of these technologies. First, let’s look at what these terms mean. Numerous diseases have a genetic base, either causing or pre-disposing one to disease. Gene therapy aims to address such abnormalities by modifying the gene/genetic complement that is functioning abnormally.\nAlthough gene therapy trials have not had significant success to date, science is making progress, and the risk-benefit ratio for this research is slowly changing. DP recognizes that gene therapy may progress more rapidly and, with cautious foresight, considers that gene research might not be used only to cure disease, but to enhance our capacities or abilities.\nTo consider adequately the ethical issues, DP makes some needed distinctions. First, there are two kinds of gene-therapy: somatic cell therapy which “seeks to eliminate or reduce genetic defects on the level of … cells which make up the tissue and organs of the body,” and germ-line therapy which “aims instead at correcting genetic defects present in [sperm and eggs] with the purpose of transmitting the therapeutic effects to the offspring of the individual” (# 25). Second, there is an intuitive distinction between therapy and enhancement. As is commonly understood, therapy refers to procedures or practices that aim to cure a pathology or disease. Enhancement (in this context) refers to procedures that aim to improve the capacities or abilities of the person by modifying his or her genome.\nThe ethical issues, then, are that gene therapy seeks to adjust our genetic complement, and the risks of doing so are as yet uncertain. Furthermore, such research on the embryo requires the embryo be engendered through IVF, itself an immoral practice. And lastly, seeking enhancements manifests an attitude of domination of man over man.\nRegarding body-cell therapy, DP renders a positive judgment, saying that “such actions seek to restore the normal genetic configuration of the patient or to counter damage caused by genetic anomalies or those related to other pathologies” (#26). The document gives germline (sex cell) therapy a more cautious judgment because the modifications affect one’s offspring. These risks are unknown. “Because the risks connected to any genetic manipulation are considerable … it is not morally permissible to act in a way that may cause possible harm to the resulting progeny” (# 26).\nGerm-line modification would be done for the sake of future children. For example, the decision to eliminate the gene that causes sickle cell anemia would theoretically produce future children free of this disease. But it might also have serious side-effects. The gene for controlling this type of anemia also protects children against malaria. Efforts at germ-line engineering seek to take control of our genetic patrimony, but we do not yet have the wisdom to do this successfully. Once changes are made in the germ line, they may be very difficult to reverse.\nWith regard to genetic enhancement technologies, one would modify a gene (or have one added) that would confer greater abilities or capacities to a particular individual. Parents who enjoy athletics might wish to have a child who is a speedy runner.\nSuch a plan is highly unlikely to succeed because many of our capacities are poly-genetic (they involve a group of genes manifesting complex interactions with one another) and our most meaningful traits and abilities are not genetically determined but are learned and formed through practice, upbringing, formal learning and environment.\nWhen the President’s Council on Bioethics discussed enhancement technologies, it did not address genetic enhancement, saying that in such cases the relationships and interactions among these genes (and between one’s genes and the environment) are certain to be enormously complex. Isolating all the relevant genetic variants and knowing how to work with them to produce the desired result will therefore prove immensely difficult.\nNevertheless, in the event that such advances are made, DP renders the following judgment: “Such manipulation would promote a eugenic mentality and would lead to indirect social stigma with regard to people who lack certain qualities, while privileging qualities that happen to be appreciated by a certain culture or society; such qualities do not constitute what is specifically human” (# 27).\nThe document goes on to point out that seeking enhancements violates both the principles of justice and charity. Justice is violated because the equality of all humans would be compromised in a culture that had an enhanced and unenhanced class — or one in which certain capacities were favored and lacking them meant lacking full moral status.\nSeeking enhancement violates charity in that at some level seeking to modify oneself manifests a disgust with oneself, a rejection of the finite nature of man. Implicitly, man takes on a domineering role, seeking to rule and control his nature and life. Such is the height of hubris and is contrary to the self-love required to respect every human being, even ourselves.\nIn rendering a negative judgment on these kinds of interventions, DP says that they imply “an unjust domination of man over man” and urges us to “an attitude of care for people and of education in accepting human life in its concrete historical finite nature” (# 27).\nStephen Napier, Ph.D., is a staff ethicist at the National Catholic Bioethics Center. He serves on the University of Pennsylvania’s Institutional Review Board."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:f003dc95-dc14-41c7-a79f-5306ff1bc0f8>","<urn:uuid:60d01886-2aa2-40cf-b24c-de93edc6ffe5>"],"error":null}
{"question":"How do the concepts of real particles and position coordinates compare in terms of their measurement and representation?","answer":"Real particles and position coordinates represent different aspects of physical measurement. Real particles are dense excitations that emerge from virtual particle interactions and can be measured as approximations of all possible virtual states, with their mass being measurable in stable states. Position coordinates, on the other hand, are represented through algebraic scalars measured in meters, with their sense defined by algebraic signs (positive to the right of origin, negative to the left). Both concepts involve measurement and representation, but while real particles emerge from quantum vacuum fluctuations and field interactions, position coordinates are geometric representations used in kinematics to specify a particle's location along a single coordinate axis.","context":["TL;DR – Matter is seen as a result of energy field interactions in the Universe and in order to get rid of matter, fields need to stop existing first.\nIf someone would ask you to describe matter, you’d probably say that anything which occupies space and has some volume is called matter.\nBut what if you were asked to define mass? Well, the most general answer found in every school’s textbook is that mass is the matter contained in an object.\nIt’s funny how we describe mass as ‘matter’ and matter as ‘something’. Ever wondered what actually that ‘something’ is?\nHiggs Boson: The Internet’s Darling\nThere are a lot of fields swarming around us. Every particle we know so far has its own field. So we have both types of particles: with mass (electrons etc.) and without mass (photons etc.).\nIf you know how relative velocity works and how friction plays its role, you might get a hang of it. See, every field exists even when we cannot feel its presence. That’s because whenever we experience a field, we are actually interacting with its respective particle which is nothing but an excitation or concentrated energy in that field.\nFrom the recent observations, we know that the Higgs Field is never null, it is always excited with a positive value. So it exists everywhere and always has its effect on everything.\nTo know how things get their mass, we need to understand:\nHow Fields Interact\nOne field can penetrate and interact with the other with the help of its excitation. Basically, their respective excitations are their means of communication. Ever noticed why atoms weigh more than the masses of their subatomic particles combined? That’s because not all the energy is coming from the particles that we notice in there: the protons, neutrons and the electrons.\nComing back to our concept of relative velocity, whenever a particle passes through another field, it experiences a sort of a drag (like friction) in its way. This slowing down in its speed is what the particle perceives as mass. The original speed would have, otherwise, been the speed of light of the particle that now has a mass. And more specifically, a rest mass (because we can measure it). But we know that particles like photons have no rest mass and they travel at the speed of light. That’s probably because they don’t feel any drag from other fields. And we can’t measure their mass at such a high speed of light.\nVirtual and Real Particles\nVirtual particles are the excitations in the fields which we can’t notice because they interact with each other very swiftly to give rise to noticeable dense excitations called real particles. Thus, the real particles are actually the best approximations of all the possible virtual states of that particle. And we usually measure the mass of the real particle. This doesn’t mean that the virtual particles don’t contribute to the actual overall mass of any system. A real particle still spends some time in its virtual states (for a very very short time though). So, we can measure the approximated effect of these virtual particles time-to-time as an additional mass.\nThe Quantum Vacuum\nThe Quantum Vacuum is the most stable state or maybe the ground state of the Universe. It is the place where virtual particles are in action, appearing and disappearing in fractions of fractions of seconds. The Quantum Vacuum is what gives rise to the real particles with the most stable and long approximations of its virtual particles. The Quantum Vacuum has still some measurable energy due to the existence of Quantum Vacuum Fluctuations. In order to get rid of all the energy and matter, we need to clear up the fields in the Quantum Vacuum which is ultimately next to impossible.","Chapter Objectives. To introduce the concepts of position, displacement, velocity, and acceleration. To study particle motion along a straight line and represent this motion graphically. To investigate particle motion along a curved path using different coordinate systems. To present an analysis of dependent motion of two particles. To examine the principles of relative motion of two particles using translating axes..\nDownload Policy: Content on the Website is provided to you AS IS for your information and personal use and may not be sold / licensed / shared on other websites without getting consent from its author.While downloading, if for some reason you are not able to download a presentation, the publisher may have deleted the file from their server.\n5. Introduction Mechanics – the state of rest of motion of bodies subjected to the action of forces\nStatic – equilibrium of a body that is either at rest or moves with constant velocity\nDynamics – deals with accelerated motion of a body\n1) Kinematics – treats with geometric aspects of the motion\n2) Kinetics – analysis of the forces causing the motion\n6. Rectilinear Kinematics: Continuous Motion Rectilinear Kinematics – specifying at any instant, the particle’s position, velocity, and acceleration\n1) Single coordinate axis, s\n2) Origin, O\n3) Position vector r – specific location of particle P at any instant\n7. 4) Algebraic Scalar s in metres\nNote : - Magnitude of s = Dist from O to P\n- The sense (arrowhead dir of r) is defined by algebraic sign on s\n=> +ve = right of origin, -ve = left of origin\n8. Displacement – change in its position, vector quantity\n9. If particle moves from P to P’\nis +ve if particle’s position is right of its initial position\nis -ve if particle’s position is left of its initial position\nInstantaneous velocity is defined as\n11. Representing as an algebraic scalar,\nVelocity is +ve = particle moving to the right\nVelocity is –ve = Particle moving to the left\nMagnitude of velocity is the speed (m/s)\n12. Average speed is defined as total distance traveled by a particle, sT, divided by the elapsed time .\nThe particle travels along\nthe path of length sT in time\n13. Acceleration – velocity of particle is known at points P and P’ during time interval ?t, average acceleration is\n?v represents difference in the velocity during the time interval ?t, ie\n14. Instantaneous acceleration at time t is found by taking smaller and smaller values of ?t and corresponding smaller and smaller values of ?v,\n15. Particle is slowing down, its speed is decreasing => decelerating => will be negative.\nConsequently, a will also be negative, therefore it will act to the left, in the opposite sense to v\nIf velocity is constant,\nacceleration is zero\n16. Velocity as a Function of Time\nIntegrate ac = dv/dt, assuming that initially v = v0 when t = 0.\n17. Position as a Function of Time\nIntegrate v = ds/dt = v0 + act, assuming that initially s = s0 when t = 0\n18. Velocity as a Function of Position\nIntegrate v dv = ac ds, assuming that initially v = v0 at s = s0\n19. PROCEDURE FOR ANALYSIS\nEstablish a position coordinate s along the path and specify its fixed origin and positive direction.\nThe particle’s position, velocity, and acceleration, can be represented as s, v and a respectively and their sense is then determined from their algebraic signs.\n20. The positive sense for each scalar can be indicated by an arrow shown alongside each kinematics eqn as it is applied\n21. 2) Kinematic Equation\nIf a relationship is known between any two of the four variables a, v, s and t, then a third variable can be obtained by using one of the three the kinematic equations\nWhen integration is performed, it is important that position and velocity be known at a given instant in order to evaluate either the constant of integration if an indefinite integral is used, or the limits of integration if a definite integral is used\n22. Remember that the three kinematics equations can only be applied to situation where the acceleration of the particle is constant.\n27. A small projectile is forced downward into a\nfluid medium with an initial velocity of 60m/s.\nDue to the resistance of the fluid the\nprojectile experiences a deceleration equal to a =\n(-0.4v3)m/s2, where v is in m/s2.\nDetermine the projectile’s\nvelocity and position 4s\nafter it is fired.\nCoordinate System. Since the motion is\ndownward, the position coordinate is downwards\npositive, with the origin located at O.\nVelocity. Here a = f(v), velocity is a function of\ntime using a = dv/dt, since this equation relates v,\na and t."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:736740aa-f62d-49ea-ac23-8bd4e8b227a9>","<urn:uuid:541a64ff-a215-47b4-8fb4-a2cbe4034ec5>"],"error":null}
{"question":"Why is Kuk significant for plant domestication history, and what current farming practices help preserve soil quality like those ancient farmers did?","answer":"Kuk is significant as it provides the earliest evidence of plant domestication in Oceania, showing systematic cultivation of taro, yam and bananas from 7,000-10,000 years ago through careful land management. Modern soil preservation techniques that follow similar principles include crop rotation (changing crops grown in a field regularly), crop residue management (leaving previous crop residue on soil), and contour farming (performing farming practices along land contours rather than up/down hills).","context":["Kuk Early Agricultural Site covers 116ha of swamp land that has been systematically cultivated since 7,000 and possibly 10,000 years ago. This makes it the earliest evidence for the domestication of plants in Oceania.\nCrops grown include taro, yam and bananas. Stone tools were found, as well as ditches and drainage channels.\nThe traditional agricultural practice continued until the 1930s when Europeans arrived in the region.\nMap of Kuk\n- ●● Cultural\nMichael Novins United States 30-Jul-16\nIn July 2016, I spent a few days in Mount Hagen, where I stayed at the Highlander Hotel. (link). Mount Hagen is a short drive from the Kuk Early Agricultural Site, Papua New Guinea's only World Heritage Site, which demonstrates a technological leap that transformed plant exploitation to agriculture around 7,000 to 6,400 years ago, based on vegetative propagation of bananas, taro and yam. Kuk, however, is probably only interesting to agricultural historians and wasn't that interesting to a more casual visitor; in fact, the core area, at least to me, was indistinguishable from the buffer zone, which looked similar to most of the agricultural areas surrounding Mount Hagen. Prospective visitors should contact Nicholas Namba (email: nnamba77 (at) gmail.com), who acts in some type of official capacity and has lived at the site since the early Australian excavations; he's very familiar with the site and was, in fact, the first guide that I've had at any WHS to point out the core area as distinct from the buffer zone. It would be difficult, if not impossible, to rely on the WHS map to identify the core area, so a visitor should seek guidance from Mr. Namba. Perhaps the most surprising thing about my visit to Kuk was that the weather app on my iPhone recognized that I was at Kuk Agricultural Site.\nPapau New Guinea is best known for its avifauna, so my main objective was to see at least one species of bird-of-paradise in the wild. I went bird watching at Kumul Lodge, which is located at high altitude about an hour from Mount Hagen. During my afternoon at Kumul, I saw two birds-of-paradise, a male ribbon-tailed astrapia, one of the iconic, flamboyant birds-of-paradise, and a female brown sicklebill, which is more restrained. I also visited two markets in Mount Hagen, the central market, which is largely devoted to agricultural products, and the betel nut market, which is on the road to Kumul Lodge but is not as frequently visited. Deep in the betel nut market you can find the vendors selling marijuana, which they refer to as PNG gold; contrary to my expectations, the marijuana vendors were not reserved and actively sought to be photographed.\nI arranged my flights so that I would not have to overnight in Port Moresby, the very uninteresting capital of one of the most interesting countries, but I did coordinate to allow for a several hour stopover. During my time in Port Moresby, I visited Adventure Park PNG, which is more of an aviary and zoo than amusement park. The aviary displays 11 of the 42 species of bird-of-paradise, including Raggiana bird-of-paradise, the national bird of Papua New Guinea, and the zoo displays several species of tree kangaroo, the country's iconic mammal.\nI made all of my arrangements with Aaron Hayes at Ecotourism Melanesia. PNG was the 164th country that I have visited and was, by far, the most expensive for independent travel. The value-to-cost ratio of my visit to Kuk was probably the lowest of any of the WHS that I have visited, but, it was worth the high price to visit other parts of PNG.\nIn 1977, it was my great good fortune to be part of professor Jack Golson's multi-disciplinary team during the field season at Kuk Station. My husband, Art Rohn (North American anthropologist/prehistorian) and myself (cartographer) were visiting Australia for a year's sabbatical where we became friends with Jack Golson and faculty members at Australia National University and Sydney University.\nI left cartography and archaeology years ago to pursue other interests. But Kuk left such an indelible stamp on me that for years I searched the Internet for mentions of the site. There were only a few papers on Kuk, one citing our work there. However, since Kuk's inclusion as a World Heritage Site in 2008, all manner of Kuk publications abound.\nKuk remains one of the most happy and vibrant memories in a life thus far liberally sprinkled with adventure. Imagine how exciting it was for a young woman of 34 to contribute to what would become one of the most important continuously cultivated agrarian sites in the world. I remember vividly how my husband at the time, Art Rohn, offered his vast expertise to help interpret what Kuk was all about.\nThe excavations, of course, were paramount to this multi-disciplinary interpretation. At first I used the previous mapping techniques, increasingly aware of their inability to represent sometimes amorphous features without diffinitive boundaries. One evening we sat around the dinner table, in one of the 2 stilt houses we called camp, hashing over better ways to map the mounds and valleys which were part of the intricate gardening system. I believe (though time may have dulled my memory) that it was in fact I who suggested we use a rather primitive mapping instrument called a \"leaf alidade\", a very simple brass siting device, to fashion a series of contour maps. All agreed this was a good idea and so Jack sent for such an instrument, delivered by the next visitor from ANU.\nI would go out daily with my helper, one of the indigenous highlanders perhaps all of five feet tall, before the rains came to drive us back to camp. I stood at the map table sighting a string attached to a plumbob which Ku held over the features. I drew a line and then extended a tape to measure distance, marking each with a dot on the map. Height was determined by sighting a measuring rod. From the field maps with thousands of points and elevations, Art looked over the actual features, turned to the map and connect the dots of similar elevation.\nAt the end of the season (10-12 weeks?) we had some 15-20? field maps, most with a contour interval of 2cm. Yes, 2cm. I received permission to keep copies of some of the originals which are still in my possession.\nIt is these maps which appear in Jack Golson's original monograph and several subsequent publications. I believe they capture the essence of these features as well as could be done at the time, prior to sophisticated computer techniques now in practice.\nExperiencing Kuk, Mt. Hagan, the Upper Waghi Valley, with Mt. Okka an active volcano looming in the distance, was like being part of a National Geographic special. The natives were so startled by my fair hair and light skin, I felt like the Queen of England! I loved one occasion when I was entrusted with driving our native workers home in our Land Rover. They sang and chewed beetlenut the entire way, perched on the bonnet and running boards. Even the acrid smell of their sweat seemed like perfume to this romantic American.\nOne day on the dig Jack was approached by a Big Man (tribal leader) who told Jack that the native workers would not be able to work the next morning on account of the fact that they had to avenge a sorcery killing of one of their pigs. All turned out well. No one was hurt when the local constabulary broke things up and the workers returned to the dig that afternoon. But Payback was regularly practiced as was Bride Price Exchange. Keener is PNG currency based on shells of the same name. But a bloke could still sell a couple of pigs to buy a ticket and fly to Port Moresby.\nOnly wish I could post some of the photos from my time at Kuk and in the Western Highlands taken by Ed Harris, a member of our team, and Paul Goretski(sp?)a visiting student from Sydney University.\nI remain most grateful to Jack Golson and others who made my experience possible.\nSolivagant UK 28-Jun-08\nIf Kuk does get inscribed in 2008 it will be another case of a “near miss” for us I am afraid! We may well have flown over it when we took a light plane along the Waghi valley from Goroka to Mt Hagen (the site is a short distance NE of Mt Hagen airfield) but I wouldn’t claim that as “seen”! However, I wonder if it is really “seeable” by anyone other than archaeologists connected with the government or university research.\nBut we did achieve some “connection” with the site as we visited the JK McCarthy Museum in Goroka (This is the main \"Highland Museum\"). Its exterior is painted with a mural (Photo) depicting the early agriculturalists of the area (described in the museum as the “world’s first gardeners” or similar words! PNG is clearly, and rightly, proud of this historical development by its peoples). A section of the museum was devoted to the discovery at Kuk Swamp (which is situated further west near the town of Mt Hagen) of the remains of these early agricultural practices. Excavations have identified plant residues, stone implements and indications of ditch/mound building which show that, as long ago as 10000 years BP, there had been “agriculture” in the area based initially on taro and then, around 6500 years BP, involving bananas, sugar cane and yams. The dates are very significant because they indicate that these people in the Highlands of PNG were not recipients of the knowledge and skills for sedentary agricultural practices but must have developed them independently.\nAt the museum I was able to have an interesting conversation with one of the curators about the problems for a country like PNG in gaining inscription for one of its sites. They were getting help from ICOMOS but, what I call the “managerially based, Western oriented”, approach set out by UNESCO/ICOMOS for assessing sites had clearly caused many problems to PNG where such concepts are not well established and, in some cases, are not acceptable to the country’s culture. Land ownership and use was clearly a major problem – you only have to spend a few days in the highlands of PNG to realise just how important this is as a cultural issue. Even our progress around the area was impacted by several land-based disputes which prevented us from travelling as planned (though we did still visit a traditional village in the countryside outside Mt Hagen) and, on one occasion involved us requiring police protection – not for anything we had done but because of a dispute between the hotel we were staying at and the traditional “owners” of the land on which it was built. Three of our guides during the trip had family members or friends who had been involved in arson and even “murder” arising from Clan disputes – you got the impression that it was just a “way of life”. One guide said he would have to try to kill the driver with whom he was friends but who came from another clan if matters between the clans ever came to that!(A fascinating aspect of visiting tribal PNG is the way it makes one examine the assumptions of one's own society. Were the guide's \"beliefs\" on this matter much different from those of soldiers in the army of a \"Nation State\" who will kill for their country?). The Central Government and its western-based law system is often not brought into such matters which are solved instead in traditional ways with “Payback”. You can imagine the problems when, into this cultural milieu, are dropped Western heritage conservation concepts such as “buffer zones” and “management plans”! The local Kawelka people are not going to take kindly to being told what (and what not) to do with their land. I understand that a Research Station was closed down in the early 90s and the excavations covered over. The locals then moved in and there is concern about destruction of the historical remains. If you are interested you can get an idea of some of the issues and how the PNG government and UNESCO are handling them by searching for “Kuk” within this document from a 2007 UNITAR workshop on the \"Management and Conservation of WHS\"\nIt would be interesting to get a post on this site from an archaeologist who was involved in the excavations and/or the preparation of the inscription documentation – is it possible to visit the site (and under what conditions)? Is there anything to see? Are there any plans for improving visitor access? Etc, Etc. Or does this join the list of sites (see my review of the Omo Valley) which are archaeologically significant but where there is nothing to see!!\nShare your experiences!\nHave you been to Kuk? Add your own review!\nFull name: Kuk Early Agricultural Site\nUnesco ID: 887\n- Papua New Guinea\nCriteria: 3 4\n- 2008 - Inscribed\n- 1999 - Deferred Bureau - Needs new Management plan and better visitor handling facilities etc\nThe site has 13 connections. Show all\n- Peat: \"The remains of former agricultural practices are mostly buried and preserved underneath deposits that have accumulated in the wetland through ...... in situ peat accumulation\" and \"comparison of the records and finds from archaeological excavations in the 1970s (following drainage) and in the late 1990s (25 years after drainage for the Station), indicate that only the upper peat layer that accumulated in the last 100 years has been severely degraded\" (Nom File)\n- Irrigation and drainage\n- Tea: The site has been used for growing tea. AB review In \"the late 1960s ... the landscape was drained and converted into a tea plantation\"\n- Innovations in Agriculture: \"It contains well-preserved archaeological remains demonstrating the technological leap which transformed plant exploitation to agriculture around 6,500 years ago. It is an excellent example of transformation of agricultural practices over time, from cultivation mounds to draining the wetlands through the digging of ditches with wooden tools. Kuk is one of the few places in the world where archaeological evidence suggests independent agricultural development and changes in agricultural practice over such a long period of time.\" (UNESCO)\nWorld Heritage Process\n- Only WHS in their country\n- First sites filling gaps cited by ICOMOS: non Australian cultural sites of the Pacific 2008\n- Relict Cultural Landscapes: The property is nominated as both an evolving cultural landscape and a relict one. ICOMOS considers that its outstanding universal value is associated with archaeological evidence and hence it is appropriate to consider this a relict landscape (AB ev)\n- Slow Starters: 1997-2008 : 11 years","EROSION & SEDIMENT CONTROL\nClick on picture for more information\nA small grain or legume crop planted in the fall to recover unused plant nutrients from the root zone, control soil erosion and improve the soil.\nCritical Area Planting\nPlanting grasses or other vegetation to protect a severly eroding area from soil erosion.\nPlanting grasses or legumes to improve forage production, improve livestock nutrition, protect the soil from erosion and improve water quality.\nTrees, shrubs or grasses planted next to waterways including rivers, streams and drainage ditches to filter runoff, improve water quality, protect the soil from erosion and provide wildlife habitat.\nWindbreaks For Poultry Houses\nTrees and shrubs planted near poultry houses to improve air quality, create visual screens, protect against winter winds and provide shade.\nDead Bird Composting Facility\nA roofed structure designed for composting the normal daily accumulation of dead birds from a poultry operation.\nA long earthen embankment built across the slope to direct runoff water from a specific area.\nGrade Control Structure\nEarthen, wooden, concrete or other structure built across a drainageway or gully to control and reduce water flow.\nShaping and establishing grass in a natural drainageway to prevent gullies from forming and control soil erosion.\nHeavy Usea Area Protection\nStabilizing areas that are disturbed because of frequent and intensive use by livestock or farm equipment.\nLivestock Watering System\nA system of troughs and water lines to provide livestock with water from a spring, pond, well or other source.\nManure Storage Structure\nThese structures are used to store manure produced by poultry or livestock until conditions are right for field application or transport can be arranged.\nRoof Runoff System\nA system for collecting, controlling and disposing of runoff water from non-residential farm buildings.\nA method of treating sinkholes to protect water quality by eliminating a direct channel for pollutants to enter groundwater.\nProtecting a stream by exluding livestock, stabilizing the stream channel and establishing vegetative buffer zones.\nAn earthen ridge around a hillside that stops water flow and stores or guides water safely off a field.\nWater Control Structure\nA structure in a water management system or drainage channel that conveys water, controls the direction or rate of flow, maintains a desired water surface elevation or measures water.\nRestoring the water and plant community in a former or degraded wetland to improve water quality and provide wildlife habitat.\nTillage, planting and other farming practices performed on or near the contour of the field--not up and down the hill.\nCrop Residue Management\nLeaving residue from the previous crop on the soil surface for a specific period of time by reducing tillage.\nChanging the crops grown in a field on a regular basis.\nIntegrated Pest Management\nEvaluating and using a tailored pest management system to reduce crop and environmental damages.\nPoultry and livestock producers transport excess manure off their farms as part of an animal waste management system and nutrient mangement plan.\nApplying the correct amount and form of plant nutrients to achieve realistic crop yield goals while minimizing the movement of nutrients into surface waters and groundwater.\nManaging pasture grazing by moving livestock from one area to another at the proper time to maintain high quality forage.\nChanging farming practices near the farmstead in order to reduce the risk of contaminating water sources, particularly domestic water supplies.\nThis guide features 26 conservation practices - best management practices (BMPs) that farmers can use to\nmaintain farm production, control soil erosion, manage nutrients and\nsafeguard water quality.\n1114 Shawan Road, Suite 4\nCockeysville, Maryland 21030\nPhone (410) 527-5920"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:3098b35d-2053-4351-a1e9-3c712ff8d537>","<urn:uuid:beb2ac19-d0a0-4b91-9e45-39f409f7c356>"],"error":null}
{"question":"Can you compare the chemical composition and properties of commercial Red Devil Lye with the mixed lye solution used in hot process shaving soap making?","answer":"While Red Devil Lye was a single-type lye product used for various purposes like making soaps, cleaners, and food processing, hot process shaving soap specifically uses a dual-lye combination of both sodium hydroxide (40%) and potassium hydroxide (60%). This specialized mixture makes the shaving soap softer than regular bar soap and allows it to build up lather faster. The shaving soap recipe also includes specific ratios of water to lye (6:1) and has an 8% superfat content, making it more complex than the general-purpose Red Devil Lye that was once available in stores.","context":["Buying lye can be a problem these days. For all its uses, properties and various advantages (and caustic effects), lye is sometimes not readily available for users now. This is because, Red Devil Lye, the sole lye brand died years ago, forcing soap makers to look for other options.\nWhy buying lye is all important: You cannot just do without lye if you plan to make soaps, cleaners or even bath salts. Lye is required to make biodiesel too. Food grade lye has a lot of food processing uses – it is used to cure olives, make pretzels, Chinese noodles, hominy corn, bagels etc. Lye is also used in glass products to give a high-end frosted look and to adjust pH in several chemical processes. Above all, lye is a must for making domestic lye cleaners, pool cleaners etc. Lye is now used even in hair relaxers and other hair care products. With such varied uses, one cannot but need lye. If not from the retailers, customers need to find other sources for buying lye.\nCrackdown on selling lye: When Red Devil Lye was available on the ‘cleaners’ section of our stores, we never had issues with regard to buying or transporting lye. It could be used for every other use of lye – in soap making, biodiesel preparation, glass manufacture and cleaning. With the demise of Red Devil, you have two responsibilities before you buy lye – 1. ensure that the concentration of lye is useful for your purpose, 2. ensure that you buy lye with legal proof to avoid being labeled as a miscreant. Lye disappeared from the cleaning isle of stores for a reason – the law makers wanted to stop criminals from making illegal use lye. To level the pH of methamphetamine, illegal drug makers used lye. The Federal government, in an attempt to put an end to all illegal meth operations, stopped selling lye across the country.\nHow and where to buy lye: To buy lye, you can approach only manufacturers and not retailers. You may have to sign a statement of attesting why you want lye and how you plan to use it. This process is to circumvent bulk buying of lye for meth operations. Companies, soap making suppliers and manufacturers who sell lye often keep a documentation of who buys lye from them. There are some online depots which sell lye. The below tips will give you a clear idea on where to buy and where not to buy lye:\n1. Online Depots: You online at Essential Depot.\n2. Soap making suppliers: Some soap making suppliers (online and offline) sell lye. If you have registered for a local soap making course, you can probably ask your soap making trainer on where to find quality soap making supplies and go ahead and buy them. But here again, the quantity of lye given to you will be limited and you may not be able to use it for large scale production.\n3. Try online sellers: If you are the type who cannot just forget Red Devil brand of lye, try to find an old, unused bottle on eBay or other auction sites. However, in this case, you may have to foot the bill for packaging, shipping and other charges. You may also have to state your reasons for using lye.\n4. Manufacturers: If you can make friends with a manufacturer of lye, you can buy lye from him. Even when you buy from manufacturers, ensure that you buy from an authentic manufacturer using proper documentation. This would avoid trouble for you and the manufacturer in future.","How to Make Hot Process Shaving Soap\nThis shaving soap, made with lots of moisturizing shea and cocoa butter, works up into a thick, creamy lather almost instantly, giving a smooth, comfortable shave. Shaving soaps differ from regular soaps in a couple of ways. This soap is made with both sodium hydroxide and potassium hydroxide, making it softer than bar soap (40% and 60%, respectively). This softness makes it faster to build up lather. Shaving soaps have an interesting fatty acid profile compared to a traditional bar soap. This soap has 40% stearic acid in it from shea butter, cocoa butter, palm oil and pure stearic acid. This makes the lather of this soap thicker, more stable and creamier. Perfect for shaving!\nThis soap is scented with a cool combination of peppermint and patchouli essential oils. You can substitute an equal weight of any essential oil or your favorite fragrance oil. We carry many Masculine Fragrance Oils that the men in your life are sure to love!\nAfter the cook, this shaving soap has glycerin and yogurt stirred into it. Glycerin makes the soap softer and is a wonderful humectant for the skin. In addition to being a good skin conditioner, yogurt contains lactic acid which makes the soap more fluid and easier to pour. You can substitute 1¼ teaspoons of 60% sodium lactate solution, if you prefer.\nThis soap is made in a 6 quart slow cooker. You can comfortably double or triple this batch, but do not go beyond that unless you have a larger cooker. If you want to halve this batch, you should also use a smaller cooker because the level of soap will be too low to easily stick blend.\nThis soap has a 6:1 ratio of water to lye and 8% superfat.\n- Difficulty: Advanced\n- Yield: 6 x 8 oz jars, each with 4.4 oz / 125 g product, by weight\n- Prep Time / Clean Up: 25 Minutes\n- Perform Time: 1 Hour, 45 Minutes\n- Total Time: 2 Hours, 10 Minutes\n- Digital Scale\n- 6 qt Slow Cooker\n- Immersion Blender\n- Funnel Pitcher\n- Latex or Nitrile Gloves\n- Digital Thermometer\n- Silicone Spatula\n- Pipettes (for essential oil)\n- Paper Towels\n- Small glass bowl or cup\n- 6 x 8 oz Plastic Jars\n- 6 x Black Dome Lids\n- 5 oz / 141.7 g Stearic Acid (25% of oils)\n- 5 oz / 141.7 g Shea Butter (25% of oils)\n- 4 oz / 113.4 g Palm Oil (20% of oils)\n- 3 oz / 85 g Coconut Oil (15% of oils)\n- 2 oz / 56.7 g Cocoa Butter (10% of oils)\n- 1 oz / 28.3 g Castor Oil (5% of oils)\n- 1.05 oz / 29.9 g Sodium Hydroxide (NaOH)\n- 2.47 oz / 69.9 g Potassium Hydroxide (KOH), 90% Purity\n- 17.6 oz / 499 g Distilled or Deionized Water\n- 1 oz / 28.3 g Glycerin (Available from Pharmacies)\n- 0.75 oz / 21.26 g Plain Yogurt, Any Fat Percentage (Greek Yogurt or Regular)\n- 0.5 oz / 14.17 g Peppermint Essential Oil\n- 0.1 oz / 2.83 g Patchouli Essential Oil\nBefore starting this tutorial please make sure to read all instructions. Wear safety goggles, closed-toed shoes, long sleeves, long pants and gloves when working with lye or raw soap. Make sure there are no small children or pets in the room and that you are undistracted. Make your lye solution in a stainless steel or polypropylene container, such as a Funnel Pitcher. Do not use glass or another kind of plastic. Use caution when handling heated oils. Use disposable pipets when dispensing essential oils. Do not place undiluted essential oils in plastic ware. Carefully weigh all ingredients and gather your equipment before you start working.\nIt’s always a good idea to run a new soap recipe through a lye calculator. You will need to use one that accommodates dual lye such as the one found at http://soapcalc.net/calc/SoapCalcWP.asp.\nStep 1 – Melt the Butters and Oils\nCombine the stearic acid, shea butter, palm oil, coconut oil, cocoa butter, and castor oil in your slow cooker. Turn the cooker on to high and allow your oils to melt completely. This will take about 30-40 minutes depending on how hot your slow cooker is. Stearic acid takes a long time to melt, so be patient!\nWhile your oils melt, add the potassium hydroxide and sodium hydroxide into the distilled water in a funnel pitcher. Work in a well-ventilated area. Stir until dissolved. (Note: add the lye to the water, not the water to the lye. The mixture will get quite hot!)\nCombine the essential oils, glycerin, and yogurt in a small glass bowl or cup.\nStep 2 – Bring the Soap to Trace\nPour the lye solution into the oils.\nStick blend on high. The mixture will seize up into an applesauce consistency.\nAfter a couple minutes, the mixture will have the texture of soft mashed potatoes.\nAfter about 10 minutes of stick blending, the mixture will smooth out and have the appearance of cake mix batter. The soap base is now at trace.\nStep 3 – Cook the Soap Base\nCover the soap and cook on high for 45-60 minutes, stirring every 20 minutes. To stir the soap, use the spatula to fold the soap to stir it. The soap will be a very thick paste, so just do your best to stir it.\nWhen the soap takes on a glossy, slightly translucent condition, test the soap for doneness. The easiest way to test the soap is to take a tiny bit of the soap on a small spoon and place it on your tongue. Undercooked soap will “zap” your tongue like a battery. Cooked soap will just taste slightly bitter. Alternatively, you can test the soap with phenolphthalein drops or a pH Meter by taking 1g of the soap and dissolving it in 99g of distilled water and inserting the probe. The soap will have a pH at or below 10.\nStep 4 – Finish the Soap\nTurn off the slow cooker. Allow the soap to cool to 175o, or below.\nStir in the yogurt mixture. The soap will loosen up a little.\nWorking quickly, spoon the soap into each of the jars.\nUse your hand to firmly press the soap into each of the jars, removing as many air bubbles as you can.\nSmooth out the top of each jar using a spatula.\nClean off the outside of each jar with a damp paper towel. Keep the soaps uncovered for 48 hours to allow them to dry out. The soaps are safe to use right away, but will last longer if you let them cure for a couple of weeks.\nTo use, wet a shaving brush with warm water. Swirl the brush across the top of the soap to build a thick lather. You are now ready to shave!\nPDF DOWNLOAD FOR YOUR CONVENIENCE\nSHARE ON SOCIAL MEDIA"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:b988bef4-827c-484e-8d1e-cc1cbd2508e2>","<urn:uuid:b71e832e-01c7-40ac-b009-492b665793dc>"],"error":null}
{"question":"How do the diagnostic accuracy challenges differ between biosensor-based prostate cancer detection and wearable-based COVID-19 monitoring?","answer":"In prostate cancer detection using biosensors, the main accuracy challenge relates to the molecular organization and ionization of the self-assembled monolayers, where proper carboxylate group ionization is crucial for accurate detection of the PCA3 gene. For wearable-based COVID-19 monitoring, the accuracy challenges include requiring sufficient stable baseline data prior to infection, difficulty distinguishing COVID-19 from other respiratory viral infections, and potential interference from other medical conditions or medications. The wearable method shows 85% detection rate for COVID-19 cases, while the traditional PSA test for prostate cancer produces a large proportion of false positives. Both technologies aim to improve upon current diagnostic methods, with biosensors working to replace invasive testing and wearables providing continuous, real-time monitoring.","context":["Study could help develop biosensors for non-invasive diagnosis of diseases\nMarch 24, 2021\nBy José Tadeu Arantes | Agência FAPESP – The efficacy of biosensors used in clinical tests depends critically on the surface of the device on which the biorecognition molecules are immobilized. This surface can be adjusted and sometimes controlled using self-assembled molecular monolayers as matrices. The monolayers are films made up of organic molecules that under the right conditions assemble spontaneously on metal surfaces via chemical bonds between the sulfur atoms and the metal.\nA study conducted at the University of São Paulo’s São Carlos Physics Institute (IFSC-USP) in Brazil compared the performances of two types of self-assembled monolayers, one consisting of mercaptoacetic acid (MAA) in water and ethanol, and another of 11-mercaptoundecanoic acid (11-MUA) in ethanol. The respective films were evaluated in terms of their capacity to produce sensors for detection of the gene PCA3, which is specific to prostate cancer cells.\n“We showed that efficient immobilization of a simple DNA strip to detect the gene PCA3 can be achieved even in less organized monolayers, provided the terminal groups are ionized,” Paulo Augusto Raymundo Pereira, lead author of the study, told Agência FAPESP.\nAn article reporting the findings is published in The Journal of Physical Chemistry C.\nThe study was supported by FAPESP via a postdoctoral fellowship awarded to Raymundo-Pereira, and a scholarship and a Regular Research Grant awarded to other participants. Another source of funding was the Thematic Project “Toward a convergence of technologies: from sensing and biosensing to information visualization and machine learning for data analysis in clinical diagnosis”, led by Osvaldo Novais de Oliveira Junior, Raymundo-Pereira’s research advisor.\n“The study showed that the differences in performance between biosensors made with MAA film and 11-MUA film are not due solely to monolayer organization. Carboxylate group ionization is important. For this reason, it’s necessary to know the right conditions for formation of the film with these characteristics,” Raymundo-Pereira said.\nBecause MAA has proved promising for biosensor matrices, the comparison of the different preparation conditions investigated in the study can contribute to the production of high-quality films. “This knowledge can help construct other types of matrix prepared with monolayers,” Raymundo-Pereira said. “It’s now available to any researcher. As a side-effect, our own group has created another biosensor to detect the novel coronavirus.”\nHe stressed the importance of constructing non-invasive biosensors, especially in light of the growing use of telemedicine during the pandemic because of social distancing. “In the diagnosis and monitoring of prostate cancer, which was our proof of concept, the standard procedure is to quantify the level of prostate-specific antigen or PSA,” he said. “This entails taking a blood sample from the patient, which is an invasive procedure. Moreover, the result isn’t always conclusive as a large proportion of false-positives result from high levels of PSA associated with inflammation of the prostate, for example. In this case, the medical recommendation is biopsy, which is even more invasive. The antigen expressed by the gene PCA3 can be detected in urine by users of a biosensor that could be sold by drugstores.”\n“Both contributions of the study relate to more accurate diagnosis of prostate cancer and the possibility of replacing such detection methods as PCR (polymerase chain reaction), which is essential to diagnose not just cancer but also other diseases, including COVID-19,” Oliveira Junior said.\nBesides the IFSC-USP group, the study involved researchers at the Brazilian National Nanotechnology Laboratory (LNNano), run by the Brazilian Center for Research in Energy and Materials (CNPEM) in Campinas, in the state of São Paulo; Hospital de Amor in Barretos, also in São Paulo; and Instituto de Pesquisa Pelé Pequeno Príncipe in Curitiba, Paraná state.\nThe article “Influence of the molecular orientation and ionization of self-assembled monolayers in biosensors: application to genosensors of prostate cancer antigen 3” is at: pubs.acs.org/doi/10.1021/acs.jpcc.0c09055.\nAgência FAPESP licenses news reports under Creative Commons license CC-BY-NC-ND so that they can be republished free of charge and in a straightforward manner by other digital media or by print media. The name of the author or reporter (when applied) must be cited, as must the source (Agência FAPESP). Using the button HTML below ensures compliance with the rules described in Agência FAPESP’s Digital Content Republication Policy.","Early Detection of COVID-19 at Scale Using Wearables\nPresymptomatic detection of COVID-19 has been a challenge in containing the ongoing pandemic. Wearable monitoring devices provide an exciting opportunity for early detection for real-time infectious disease monitoring at scale.\nA new model for early disease detection\nA hallmark of COVID-19 is transmission in the absence of symptoms: a long presymptomatic period, as well as individuals who are asymptomatic (1). Approaches for early, pre-symptomatic and asymptomatic detection of COVID-19 are expected to have a major impact in the direction of the ongoing pandemic.\nThe current state of testing technology is inadequate. Traditional testing methods, including qPCR tests, are expensive, slow, typically performed infrequently and often ineffective at early detection (qPCR has over 50% false-negative rate within the first five days of exposure (2)). Thus, new methods are needed.\nWearable monitoring devices have the potential to fill this void. In 2017, our team demonstrated that early detection of infectious disease (Lyme, respiratory viral infection) using wearable devices is possible even days before self-reported symptoms and in asymptomatic cases (3). We wrote an algorithm, Change of Heart, to implement this concept. Our new study shows that measurements from commercially available smartwatches can be used to track and even predict respiratory viral infections (RVI) (4).\nWearable devices are an ideal tool for early RVI detection for a variety of reasons. Firstly, these devices provide continuous, real-time, 24/7 diagnostic data. Secondly, wearable devices are relatively low-cost and have already reached a significant level of widespread adoption. Recent data suggests that 20% of Americans use a wearable device (5) and they are only becoming more popular. Globally, predictions forecast wearables shipment volume to increase to total 637.1 million units by 2024, demonstrating a 5-year compound annual growth rate (CAGR) of 12.4% (6). Transforming existing wearable devices into effective health monitoring tools would provide millions of individuals with immediate access to early COVID-19 detection through something as simple as downloading a mobile application.\nMobilizing the Research Infrastructure\nScaling early detection methods poses many challenges, including cost, speed, and the handling and processing of copious amounts of data. To address these challenges, we created the Personal Health Dashboard, a secure and scalable system for analyzing and visualizing participants’ data. In February, 2020, at an early point in the pandemic, we adapted our disease detection algorithms and Personal Health Dashboard to focus on SARS-CoV-2 wearable detection methods with the ability to implement them at scale.\nThis project quickly escalated into an interdisciplinary army composed of eight key working groups borne from the Stanford Healthcare Innovation Lab, the Stanford Center for Genomics and Personalized Medicine, and the Snyder Lab at the Stanford University School of Medicine.\n- Outreach and Partnerships\n- Front-end and Customer Service\n- Security and Privacy\n- Mobile Programming (iOS/Android)\nOn April 14th, we announced a collaboration with Fitbit and the Scripps Research Institute (7-8), which greatly accelerated our work. Almost overnight, our enrollment increased by over 3,000%. We were also very fortunate to collaborate with Survivor Corps, the largest COVID-19 patient advocacy group in the country. Diana Berrent, COVID-19 survivor and founder of Survivor Corps, is an enthusiastic fitness tracker and immediately understood the purpose and impact of our work.\nEarly results were very encouraging. In our very first case in April, 2020, our algorithm was effective in detecting COVID-19 9.5 days before symptom onset.\nWith proof-of-concept in hand, our subsequent study yielded great insight into the body’s response to SARS-CoV-2 infection and our ability to detect it. Altered resting heart rate, sleep, and step count were observed in 80% of COVID-19 infection cases, and we could detect these alterations in over 85% of positive cases prior to, or at, symptom onset. Two separate algorithms were developed:\n1) In the RHR Difference (RHR- Diff) method, we detect and identify periods of elevated resting heart rate (RHR) compared to a healthy baseline constructed from a 28-day sliding window.\n2) In the anomaly detection method (HROS-AD), we created a new feature known as HROS (Heart Rate Over Steps) by dividing heart rate with step count in hourly intervals. We then compared HROS of each period with all other intervals using Gaussian density estimation to find anomalous periods.\nIn regards to false positives/negatives, we found that our method relied heavily on having sufficient stable baseline data prior to infection. We encountered difficulty detecting abnormal RHR in individuals where this was lacking. Data for some of these participants were also presumably influenced by other medical conditions and/or medication.\nWe have proceeded to modify these algorithms for online, real-time detection methods. We note that these detection methods currently have limitations: 1) COVID-19 is difficult to distinguish from other RVIs (although COVID-19 does have a longer pre-symptomatic period than, for example, influenza B infection). Nonetheless, we believe these alerts are still useful, given that early detection of infectious illnesses could mitigate future contagions. Furthermore, even if the algorithm cannot distinguish the specific identity of the infection, subsequent tests can be administered to identify the illness. 2) Since additional stressors can trigger our algorithm’s detection alarms, the incoming data needs to be contextualized. The use of additional data types (e.g. respiration rate and skin temperature), and new analysis approaches should enable us to distinguish between many different types of stressors and, perhaps, different types of RVIs.\nWe are all very excited about the promise of this work which has just been published (4). Having adapted our wearable detection system to multiple, independent diseases, we are now in the process of extending these algorithms across a variety of diseases for which early detection is key, including sepsis and Lyme disease, chronic conditions such as heart disease, diabetes/insulin resistance, and even mental health crises. We envision a future in which these wearable devices will be simultaneously protecting us from a variety of diseases.\nUltimately, we see this work as directly interfacing with our healthcare system to provide individuals more personalized and precise care.\nWe are currently in the process of launching phase 2 of this early COVID-19 wearables project, where we will be alerting individuals to our algorithm’s detection results in real time. If you are interested in signing up or supporting Phase 2 of our study, please go to www.innovations.stanford.edu/wearables.\n- He, X., Lau, E.H.Y., Wu, P. et al. Temporal dynamics in viral shedding and transmissibility of COVID-19. Nat Med 26, 672–675 (2020). https://doi.org/10.1038/s41591-020-0869-5\n- Kucirka LM, Lauer SA, Laeyendecker O, Boon D, Lessler J. Variation in False-Negative Rate of Reverse Transcriptase Polymerase Chain Reaction-Based SARS-CoV-2 Tests by Time Since Exposure. Ann Intern Med. 2020;173(4):262-267. doi:10.7326/M20-1495\n- Li X, Dunn J, Salins D, Zhou G, Zhou W, et al. (2017) Digital Health: Tracking Physiomes and Activity Using Wearable Biosensors Reveals Useful Health-Related Information. PLOS Biology 15(1): e2001402. https://doi.org/10.1371/journal.pbio.2001402\n- Mishra, T., et al. Pre-symptomatic detection of COVID-19 from smartwatch data. Nat Biomed Eng (2020). https://doi.org/10.1038/s41551-020-00640-6\n- Vogels E. About one-in-five Americans use a smartwatch or fitness tracker. Pew Research Center. 9 Jan 2020. Accessed 5 Nov 2020. https://www.pewresearch.org/fact-tank/2020/01/09/about-one-in-five-americans-use-a-smart-watch-or-fitness-tracker/\n- International Data Corporation (IDC). Worldwide Wearables Market Forecast to Maintain Double-Digit Growth in 2020 and Through 2024. Accessed 5 Dec 2020. https://www.idc.com/getdoc.jsp?containerId=prUS46885820\n- Armitage H. Stanford Medicine scientists hope to use data from wearable devices to predict illness, including COVID-19. Stanford School of Medicine. 14 April 2020. Accessed 5 Nov 2020. https://med.stanford.edu/news/all-news/2020/04/wearable-devices-for-predicting-illness-.html\n- Fitbit Collaborates with Scripps Research and Stanford Medicine to Study the Role of Wearables to Detect, Track and Contain Infectious Diseases like COVID-19. Business Wire. 14 April 2020. Accessed 5 Nov 2020. https://www.businesswire.com/news/home/20200414005330/en/"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:5f2cac5d-38ae-40c4-a9f5-d8fba9bac09a>","<urn:uuid:15c25417-cac7-4c8f-812c-665f16217a1a>"],"error":null}
{"question":"How do internal bank audits and category B inspections compare in terms of their independence and objectivity when reviewing financial operations?","answer":"Both internal bank audits and category B inspections serve similar roles within their organizations but have limitations in terms of independence. Internal bank audits are part of ongoing monitoring of a bank's internal controls and may be compromised by auditors having other job responsibilities. Similarly, category B inspections are performed by inspection departments within large-scale companies that are involved in design, procurement, and construction, meaning they're not independent since they provide inspection services to their own company. Both types of reviews are considered less objective than external audits or category A inspections, which are performed by independent third parties with no direct ties to the organization being evaluated.","context":["An audit is a formal review of a person's, company's or organization's financial accounts and statements. A bank audit is a routine procedure that reviews a financial institution to make sure it is in compliance with all regulations and industry best practices.\nThe 2008 financial crisis revealed many weaknesses in financial institutions' risk management, governance and controls. As a result, the use of quality bank audits has been prioritized to protect American consumers.\nThe Purpose of the Audit\nOne of the primary reasons for performing bank audits is to provide an objective evaluation of a bank's business activities, information systems and controls. A financial institution's key areas vary based on the services it offers, the risk of fraud and the complexity of the systems it has in place.\nA bank audit should reveal whether a bank has sound, ethical practices that abide by regulations put in place to protect consumers.\nTypes of Bank Audits\nAudits fall into one of two categories: internal or external.\nInternal bank audits play the role of risk management and work as part of the ongoing monitoring of a bank's internal controls and financial assessment procedures. These audits assess the competence of a bank's policies and procedures, as well as its compliance with them.\nSpecific tasks may include reviewing the accuracy of financial statements, evaluating the amount of risk the company is taking on and analyzing how it assesses its capital.\nGenerally speaking, external audits are preferred for bank audits because it is more likely that the auditors can be objective in observations and reporting if they do not have any direct ties to the organization being audited.\nBoth internal and external auditors serve the same purpose in that they perform their duties to be assured that the bank's financial statements and operations are free of mistakes and inefficiencies.\nHowever, external audits may be more thorough because the external audit is the sole job of the auditor, whereas employees doing internal audits may have other job responsibilities that get in the way.\nCompliance With Regulations\nOne of the more important parts of the bank audit process involves checking on the compliance of financial institutions. Auditors need to make sure banks are following federal, state and local regulations, as well as their own policies. These regulations are put in place to protect consumers, and violations could endanger people.\nIf problems are found, bank auditors usually make recommendations to improve the procedures.\nUsing Substantive Analytics\nAuditors use substantive analytics to check a company's financial statements for errors and material misstatements, which is information so incorrect that it misleads individuals who use the data for decision-making. These procedures focus on plausible relationships between financial data and nonfinancial data.\nFor example, if the number of loans a bank distributed decreases, an auditor expects to see a decrease in interest earned. If this information does not reasonably correlate, it is the auditor's job to identify the reason behind it.\nAn audit test of controls goes hand in hand with substantive analytics because it tests the controls put in place to prevent material misstatements from happening. If a financial institution's controls are strong, they not only prevent mishaps from taking place, but also detect current inefficiencies.\nAn auditor who finds an error during a test of controls usually conducts further testing with a larger sample size to see if the issue is systemwide.\n- American Institute of CPAs: Performing Audit Procedures in Response to Assessed Risks and Evaluating the Audit Evidence Obtained\n- Association of Chartered Certified Accountants: Analytical Procedures\n- The Principles and Practice of Auditing, George Puttick, et al.\n- Federal Deposit Insurance Corporation: The FDIC’s Examination Process for Small Community Banks\nStefon Walters earned a bachelor's degree in Economics from the University of North Carolina at Chapel Hill. After college, he went on to work sales and finance roles for a Fortune 200 company before founding two tech companies. He is also the author of Finessin' Finances, a full-length book on personal finances.\nKeith Brofsky/Photodisc/Getty Images","Industrial Third Party Inspection refers to independent inspection services that are provided by inspection agencies. This article supplies you with information about TPI definition, TPI companies and also supplies you with third party independent inspection procedures for different fix, rotary, electrical and instrumental equipment.\nIn purchasing and procurement projects the sellers and vendors are first parties. The second parties are purchasers or buyers.\nFirst party inspection refers to quality control activities that are done by equipment vendors or sellers.\nSecond party inspection refers to inspection activities that are done by equipment buyers or purchasers.\nIn fact first party is seller and second party is buyer. So when you say second party inspection, it means that an inspection and quality control activities that are done by a buyer and when you say first party inspection, it means that an inspection and quality check that are done by a vendor in its construction shop.\nThese quality controls and inspections are done based on Proforma Invoice, Purchase Order and International standards.\nThe Third Party Inspection refers to independent inspection activities that are done by a Third Party Inspection Agency either hired by a buyer or seller.\nThe ISO 1720; - Requirements for the operation of various types of bodies performing inspection- specifies characteristic of these kinds of inspection agencies.\nBased on these requirements, these agencies must be impartial and independent in their inspection activities and in their inspection reports and results.\nAll interested companies such as traders, buyers, sellers, engineering companies, construction companies must have access to the services of these inspection agencies.\nThe impartial and independence conditions are very important factors for third party Inspection agencies. In this direction ISO 17020 does not allow these companies to be participated or involved in any procurement, purchasing, construction, installation and even design activities.\nThey must only provide third party inspection services and not anything else.\nBased definition of the ISO 17020, these inspection companies are category A inspection organization. Based on this standard there are two more inspection organizations that are not independent.\nA category B inspection organization is an inspection department in a large-scale company that is involved in the design, procurement, construction, installation, etc. and this inspection department provides inspection services to its own company.\nNormally a category B inspection organization is a quality control department in an engineering or EPC company. This department assists company in procurement and purchasing projects and makes inspections on the equipment and materials that are purchased by own company.\nSuggest an EPC company has a contract with an oil company for design, procurement, and construction of a desalting plant. Then the quality control department of this EPC Company makes inspection on the equipment and material that have been ordered by EPC contractor for this desalting plant. So this is what we name second party inspection and based the ISO 17020 is category B type inspection.\nThis inspection will not be enough by end-user or in this example oil company. The oil company will hire an independent inspection agency or in other word a category A inspection company to conduct inspection on the procured equipment and material by EPC company.\nSometimes the end-user or “oil company in this example” ignores to hire an independent inspection company by itself but mandates that the EPC Company to hire an independent inspection agency that is in their approved list.\nDifferent projects have different procedures for handling inspection activities in their procurement projects. Sometimes the procured equipment is inspected by both parties i.e. by EPC contractor quality control as well as by end-user third party inspection agency. In other word equipment are inspected by category A and category B inspection organizations.\nBased on ISO 17020 definition, there is also category C inspection organizations. These inspection organizations are not too much. It refer to an inspection department of engineering company that provide services to its own procurement and purchasing projects as well as to other engineering companies. Similar to the category B inspection organizations these companies also cannot be independent.\nThere are lots of scopes of work for Category A inspection agencies or in other word in third party inspection companies. One of them is Third Party Quality Inspection that refers to quality part of work. In this part an inspection agency checks and makes sure that the quality of the commodity is as the same of purchase order specification.\nThe scope of work is determined by buyers and purchasers. Some of them require only a pre-shipment inspection and some other may mandate more stringent scope and ask their third party inspection agency to witness important tests and inspections such as material certificate review, material identification, welding inspection, NDE test review, dimensional control, hydro-static testing, mechanical running testing, performance testing, painting inspection, packing and marking inspection and loading inspection.\nThe scope of work for inspection and testing is defined through inspection and test plan (ITP). This is very important document in purchasing and procurement projects. This document identifies duty and task of each parties regarding inspection during manufacturing and construction process.\nIn giant project it is necessary that an inspection company or a category A inspection organization to expedite inspection activities. It is duty of inspection agency to check that to see if all inspections and tests are done with satisfactory results before issuing of inspection release notes. But in no case an inspection agency must expedite procurement work such as reporting to client about percentage of manufacturing progress or etc. These are not task of inspection agency; this must be done by EPC contractor expeditor engineer or technician.\nThis checking must be done based approved inspection and test plan. The inspection agency can use the ITP as check list to see if all inspection completed and their inspection visit reports are available.\nIt is best practice that an inspection releases note to be referred to the purchase order number as well as to the inspection and test plan document number. The quantity also must be stated in release note if the commodity is going to be shipped partially.\nIt is responsibility of the category A inspection organization or inspection agency to issue an inspection visit report after each visit in manufacturer shop. The inspection date must be notified to inspection agency normally 7 working days in advance. It is responsibility of the vendor to notify the inspector agency. The quality control team of a manufacturer must check inspection and test plan frequently and when they see they have reached to specific stage of construction that need an inspection, then must notify inspector for its visit.\nThe communication channel between inspection agency, vendor and client must be set-up. This normally is done in Pre-Inspection meeting or Pre-Production meeting. In this meeting that is conducted before commencement of project, all parties will be attended in the meeting and discuss about project coordination and communication.\nAll parties are responsible to act and communicate based what is agreed and confirmed in the meeting and Minute of Meeting (MOM) will be based for coming actions.\nRaw material inspection must be done based purchase order specification. The third party material stamp inspection is another important task of inspection agencies. They check material conformity based on BSEN 10204 requirements for 3.1 and 3.2 certifications.\nA 3.1 material certificate refers to the certificate that is issued by a material manufacturer but a 3.2 certificate is material certificate that is issued by material manufacturer but confirmed and approved by independent inspection agency. It is necessary that inspection agency to witness important test during of manufacturing process for confirming such certificates.\nThird Party Inspection Instruction for Fix Equipment:\nThird Party Inspection Instruction for Rotary Equipment:\nReciprocating Compressor, Centrifugal Pump, Gas Turbine, Steam Turbine, Fan and Blower, Centrifugal Compressor, Screw Compressor Testing, Centrifugal Pump Performance Test, Centrifugal Compressor Testing, Reciprocating Compressor Testing, Fan Mechanical Running test, Fan Performance Test\nTPI Inspection Instruction for Electrical Equipment:\nDid you find this article useful? Click on below Like and G+1 buttons!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:5dc7b141-4d55-483c-91a2-e063aaf5b2d2>","<urn:uuid:ac7c9132-02c7-4843-9a64-67ee1359bdac>"],"error":null}
{"question":"How do light conditions affect both phone photography of diamonds and outdoor air quality, and what are the recommended lighting setups for each context?","answer":"For diamond photography with phones, natural light from windows is ideal, while fluorescent diamond lamps should be avoided as they create moving black stripes due to shutter speed limitations. Direct yellow halogen light creates excessive sparkle and should also be avoided - instead, standing with your back to a window or between spotlights near a white wall provides scattered, even lighting. For air quality, sunlight plays a crucial role as it reacts with volatile organic compounds (VOCs) and nitrogen dioxide to form ground-level ozone, which is a major component of photochemical smog, particularly in summer. This ground-level ozone can irritate eyes, nose and throat and decrease lung function.","context":["In this article, we give you important photography tips when using your mobile phone camera, including what to avoid, to create the most authentic image of your fancy color diamond.\nSince casual photography took over our lives, whether taken with semi-professional cameras or our mobile phones, some kind of digital image will accompany any diamond report we propose to a client. The client then presents this image to the end buyer to induce the selling process.\nThe quality of the image becomes a crucial element to set the selling process in motion. As we have certain visual expectations from a diamond – especially a fancy color diamond, the color factor being the main motivation for the purchase – by saying quality, we actually mean authenticity.\nThe vast majority of all fancy color diamond images we encounter in our everyday work – such as websites that exhibit inventory, mobile phone images, or even auction catalogs – do not faithfully represent the actual look of the diamond. These images, in most cases, are over processed. This creates disappointment for the buyer once the stone is presented. In any case, low quality images that do not do justice to the stone will not motivate the buyer to even ask to see it.\nIn this article, we give you important photography tips when using your mobile phone camera, including what to avoid, to create the most authentic image of your fancy color diamond. With enough practice, you will be able to come up with amazing images and videos that will match the look of your diamond in real life.\nAll images in this article were taken with an iPhone 7 during regular daytime office conditions, with a window facing north. No filters were applied.\nMobile phones always create better diamond images using the video mode. The diamond will appear closer, showing the diamond’s sparkle and its real, face-up color dispersion.\nMoving the diamond slightly closer or farther from the camera will help you find the right focal point as well.\nStill images, on the other hand, are less forgiving in this sense, as the focal point in most cases will be on the center, leaving the crown blurry.\nThe downsides for the video mode are:\n- File size that is too large to send via WhatsApp. Keep the video under 7 seconds to avoid\n- Background noises. This can be addressed quite easily by muting the sound in any\nRegardless which mode you use, it is important to clean the lens (front or rear) prior to taking any shot. A dirty lens will create a smothered, hazy image. Make sure you clean the lens area (see image below) with a diamond cloth.\nMost mobile phone cameras have a color-balancing algorithm. Simply put, it means that the camera’s phone is trying to average exposure between two extreme axes, especially when the subject is small and the background dark. This configuration will lighten the diamond’s image dramatically. In most cases, a black background will make the diamond appear much lighter.\nIn the images below, we see a 1.02-carat deep pink radiant. On the left side, the image is with a white background that shows the real color of the diamond. On the right side, a black background, which results in a much lighter diamond color. In general, a light, neutral background will show most diamond’s real colors.\nThe two ideal accessories to create a proper white background would be:\n- Diamond color paper to place the diamond inside of (see image).\n- A white diamond box to place the stone in. It is important to remove the plastic cover to receive the full luster of the stone (see image).\nVideo on white background, real diamond color\nThe light under which the photo is taken is also an important factor. When buyers and sellers attempt to take a picture under a standard diamond lamp, moving black horizontal stripes are seen on the phone’s screen. This happens for the simple reason that the shutter speed of the phone’s camera doesn’t move as fast as the light waves of a florescent diamond lamp, especially in the video mode. So try to avoid using a diamond light when taking a picture.\nVideo under fluorescent light\nAnother thing to avoid is taking a picture under direct yellow halogen light. This creates excessive sparkle on the diamond, preventing a clear picture. To avoid this, stand with your back to a window and let the natural light shine on the stone. In case you are in a windowless space or it is at night, try standing in between spotlights next to a white wall to receive a scattered and even light reflecting from the large white surface.\nVideo under direct yellow halogen light\nVideo under direct yellow halogen light\nUsing the on-screen exposure features\nWhether you are in video or still mode, it is very important to use the exposure sensor on the screen. Using this feature creates very accurate results and could easily compensate for poor lighting conditions.\nOn screen exposure\nSize and proportion\nWhen the recipient is a nonprofessional, in most cases he or she won’t have a real sense of the diamond’s actual physical size. For that reason, it is important to send an extra picture of the stone as it lies on the back of a woman’s hand. Although a skin tone might enhance the color of the stone, it will give a true feeling of the stone’s size.\nPlacing on the hand for real proportion\nThe selfie-loupe technique\nThis technique produces the most authentic results if done correctly. You will need a diamond loupe and tweezers. You might need someone to hold up a white background.\n- Clean the phone camera lens.\n- Place the phone on a table with the screen facing up.\n- Activate the selfie camera.\n- Place the diamond loupe on top of the phone lens (making sure the loupe lens is clean as well).\n- Hold the diamond with the tweezers above the loupe.\n- Make sure the lighting fixture on the ceiling is not visible in the frame. If unavoidable, ask someone to hold up a white background 40 centimeters above the diamond.\n- Fix the on-screen exposure to reach optimal look, as needed.\n- Select video mode, preferably, although still images will result in an authentic image as well (the tweezers will look distorted due to the diamond loupe effect).\n- Always select a white background, preferably.\n- Use video mode for a closer view and a more realistic feel of the stone.\n- Create an extra video showing the stone on the back of a hand to demonstrate actual size.\n- Correct lighting with the on-screen exposure slider.\n- Use the selfie-loupe technique to create best results.\nThis article was free of charge. For full access Click here","The sources of outdoor or ambient air pollution are varied and may be either natural or produced by human activities. Air pollutants have been linked to a range of adverse health effects, including respiratory infections, cardiovascular diseases and lung cancer. Reducing air pollution will decrease the global burden of disease from these illnesses.\nHuman beings need good-quality air for their health and well-being. Air is a mixture of nitrogen (78 percent), oxygen (21 percent), carbon dioxide and some inert gases (1 percent). It also contains varying quantities of water vapour as well as numerous harmful substances: natural pollutants such as dust and volcanic ash, and pollutants that are produced by human activities.\nAir quality is simply the state of the air around us. Clean, unpolluted air would be considered good air quality. The presence of pollutants in the air, whether due to natural or human causes, in sufficiently high concentrations, results in poor air quality. Air quality thus describes the healthiness and safety of the atmosphere.\nThe sources of outdoor or ambient air pollution are varied and may be either natural or human-made. Natural outdoor air pollution includes oxides of sulphur and nitrogen from volcanoes, inorganic biological decay, lightning strikes and forest fires, and pollen from plants, grasses and trees, and particulate matter from dust storms. Natural pollution is all around us all the time. However, concentrations can sometimes increase dramatically, for example after a volcanic eruption or at the beginning of the growing season.\nPerhaps of more concern, given our ability to have greater control over its release into the atmosphere, is anthropogenic pollution, which can also have a detrimental impact on ambient air quality. The most common source of anthropogenic air pollution outdoors is the burning of fossil fuels such as coal, oil and gas, in power stations, industrial facilities, homes and vehicles. Depending on the nature of the fuel and the type of combustion process, pollutants released into the atmosphere from the burning of fossil fuels include nitrogen oxides, sulphur dioxide, carbon monoxide, particulate matter, lead and volatile organic compounds (VOCs). Other sources of these pollutants include forest fires, the manufacturing of chemicals, fertilisers and paper, and waste incineration. These pollutants are all referred to as primary pollutants, as they have direct sources.\nUnlike the ozone layer in the stratosphere, which makes our planet habitable by absorbing harmful solar ultraviolet radiation, ground-level or tropospheric ozone is a pollutant that is formed indirectly by the action of sunlight on VOCs in the presence of nitrogen dioxide. Ozone is referred to as a secondary ambient air pollutant.\nBoth primary and secondary pollutants are, to a greater or lesser extent, detrimental to human health, depending on their concentration in the air and the sensitivity of the individual. As a consequence, national and international legislation exists to regulate and control the amount of pollution emitted into the atmosphere, and to ensure that objectives for improving ambient air quality are achieved.\nGenerally, pollution is unseen or invisible. However, at times the pollutants in a particular airshed can reach such high concentrations that the air actually appears polluted. In larger urban centres, particularly in the summer, pollution may form smog — a brownish yellow or greyish white haze hovering over the skyline. Smog consists of particulate matter and ground-level ozone.\nParticulate matter (PM) is defined as either PM10 (1/8 the thickeness of a human hair) or PM2.5 (1/20 the thickness of a human hair). These minute particles are released into the air in liquid or solid form and can include dust, dirt, soot and smoke. The sources of PM are vehicles, factories, construction activities, fires, naturally occurring windblown dust and vegetation.\nOther hazardous air pollutants may adhere to PM and increase its toxicity. Particulate matter can also be formed in the air as a result of chemical reactions between gases such as nitrogen oxides, sulphur dioxide and carbon monoxide. Particulate matter, and PM2.5 in particular, can penetrate deep into the lungs, damaging lung tissue and reducing lung function.\nGround-level ozone (O3) is the main component of photochemical smog or summer smog. Ground-level ozone is a compound that forms in the lower atmosphere as the result of a reaction between nitrogen dioxide (NO2) and other airborne substances in the presence of ultraviolet light. Ground-level ozone is the same as the ozone in the upper atmosphere: the only difference is elevation. Ground-level ozone in low concentrations can irritate the eyes, nose and throat and can decrease lung function and affect physical performance.\nYour lungs take in the air around you. As PM2.5 comprises a mixture of microscopic solids, combustion products, metals and other toxins, it is a cause of serious health concern. The small particles that we breathe deep into our lungs become lodged in the alveoli, eventually damaging lung function. Even low concentrations of PM2.5 can cause an array of both short- and long-term health effects. Individuals who are sensitive to high levels of pollution may experience a variety of symptoms that range from uncomfortable to life threatening. People with a pre-existing illness such as diabetes, lung disease (e.g. asthma, chronic bronchitis, emphysema and lung cancer) or heart disease will be more sensitive to air pollution than the average population.\nOther high-risk groups include the elderly, children and athletes. With age, the heart, lungs and immune system are weakened, and health problems such as cardiovascular diseases or respiratory illnesses increase.\nIndoor air pollution from solid fuel use, along with urban outdoor air pollution, are estimated to be responsible for 3.1 million premature deaths worldwide every year and 3.2 percent of the global burden of disease. More than half of the health burden from air pollution is borne by people in developing countries.\nAir pollutants have been linked to a range of adverse health effects, including respiratory infections, cardiovascular diseases and lung cancer. Reducing air pollution levels will decrease the global burden of disease from these illnesses.\nPollution prevention requires policies on air quality and transport, air pollution control regulations in cities, emission controls in industry, and the promotion of clean, renewable energy sources. Interventions to reduce indoor air pollution include switching from the domestic use of solid fuel to cleaner fuels; introducing efficient technology and ventilation in homes, schools and the working environment; and stopping smoking. Efforts to significantly reduce air pollutants will also help to reduce greenhouse gas emissions and mitigate the effects of global warming.\nTo find out more about the proposals of scientific experts, visit the search.rec.org website."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:d9e4b2f8-fa14-4967-8598-d74b04e4cbf9>","<urn:uuid:a3fd6928-b155-4141-b22b-abdb44f8c5c3>"],"error":null}
{"question":"How do sensor sizes compare between DSLRs and camcorders in terms of their depth of field capabilities?","answer":"Full-frame and APS-sized sensors found in DSLRs create a shallower depth of field compared to the smaller sensors in camcorders. With smaller sensors (like those in amateur and prosumer camcorders), depth of field increases inversely to the size of the format. This is because shorter focal length lenses can be used with smaller formats to achieve the same picture size. For example, at small apertures and focal lengths shorter than 50mm equivalent, it's difficult to be out of focus using a camera with a small sensor. The bigger the sensor, the shallower the depth of field - this is why DSLRs, with their larger sensors, require much more attention to focusing, especially when shooting video.","context":["The useful article The Definitive Camera Guide in the new magazine, Digital FilmMaker, together with a discussion in Amateur Photographer on whether to buy separate video and still cameras, prompted me to remind myself, in the headlong rush to use full-frame and APS-sized cameras for video, why a small sensor, as found in most amateur, 'prosumer' and some professional camcorders, is preferable for the type of video photography that I do – travel, wildlife and documentary. Film makers which I define as those setting out to make a movie with actors or a staged documentary, appear to be fixated on getting a film-like appearance. Since the appearance of film can be more easily achieved with the depth of field range of a larger sensor (as with 35 mm cine cameras) some follow the fashion and use a large-sensored DSLR. The fact that we see in such videos endless focus shifts that the dramatic effect is expected and therefore ruined is neither here nor there. Such is the world of arty film making.\nFor travel, wildlife, family and documentary videos there is one great disadvantage in using a DSLR for video. The depth of field is simply too shallow. I have seen some awful efforts. Even if the camera can be focused, the slightest movements lead to the autofocus hunting to regain focus; objects slightly in front or slightly behind the point of focus that are needed to be sharp to provide context are fuzzy. DSLR video is fine for controlled shooting. The many rigs now available can be used; focus can be pulled manually after rehearsing the shot.\nCamcorders are not thought cool any more. But they do work and work very well, especially those in the prosumer ranges of the major manufacturers (even though the gimmicks, like the projectors now added by Sony for example are a real pain and very silly). The quality of the full HD video is superb. I have sold wildlife footage from my Sony camcorder.\nSo what is the advantage of using a small sensor for video? For stills, the bigger the sensor the better, particularly for the prevention of noise. The advantages of a smaller sensor in terms of depth of field are very great for travel and wildlife. In general, with the same final image size, depth of field increases inversely to the size of the format. That is because for the same picture size, a shorter focal length lens can be used with the smaller format. Shorter focal length equals greater depth of filed. Even though the circle of confusion has to be reduced as the format size decreases, the depth of field is still increased.\nThe first graph illustrates the point. I compare five formats: 1. Full-frame 35 mm; 2. APS (I know the various sensors vary in size a bit but I have taken one for comparison); 4/3 format as used on many modern non-DSLR cameras; 2/3 sensor as used on many professional camcorders; 1/2.88 sensor as used on many amateur, prosumer and some professional camcorders. I calculated for each the equivalent focal length of the lens and, using the circle of confusion appropriate to each format, read off the hyperfocal distance at f/11 and f/4. The graph shows the near point of acceptable focus (i.e. half the hyperfocal distance), the far point of course being infinity.\nThe difference in depth of field is clear and dramatic. At a small aperture and at focal lengths shorter than the equivalent of about 50 mm, it is difficult to be out of focus using a camera with a small sensor.\nI used the same data to calculate the depth of field with the camera focused at 3 metres, again at f/11 and f/4.","Original Thread DJI Forum|X5-how to get the most out of it --------------------------- The X5 brings the Inspire 1 further into the world of professional photography, where you can no longer point and shoot. With this additional power there are now more ways to create a terrible image than a good one so you need to know what you're working with. This is frustrating because when you spend a lot of money you expect all your results will be superior but this isn’t the case. You will take many shots that look horrible compared to the X3, making some people feel they received defective cameras. The most significant improvement is interchangable superior lenses. A better lens gives you much more power which does allow for much much better results, but just as it’s relatively easy to drive a car but impossible for most of us to drive a 747, you need to know how to wield the new power or you will crash and burn. The X3 is pretty easy to use as you just point it in the right direction and as long as the image isn’t grossly over or underexposed you are going to get a decent image This is because three of the main adjustable components, focal length, focus and aperture, are fixed. So what are focal lengths? Easiest way to understand them is how “zoomed in” or “zoomed out” the lens is. Measured in millimetres, it determines the field of view the lens sees. Wide lenses, such as the X3, show a great deal in the frame where long lenses are quite tight. The focal length also determines the character of the image. Wide lenses create an almost bending quality where you feel like you are close to an object but can somehow see more than you would expect. Long lenses are the opposite, they feel more intimate and straight, making you feel like your subject is the only thing in the world. This goes far beyond what is physically shown in the frame, it affects how you perceive it as well. Another thing different focal lengths affect is depth of field, which is basically how much of the image is in focus. With a shallow depth of field your subject will be in focus but everything in front or behind it will be increasingly out of focus. The longer your lens, the shallower the depth of field. Take the X3 for instance. It has a very wide focal length which makes everything sharp, regardless of where it is in the frame and is so wide it completely removes the requirement to focus at all. But with the X5, longer focal lengths will require much more attention to these aspects, as focus will become a very real issue. So choosing the right focal length for a shot and shooting conditions (single or dual control) is crucial. While speaking of depth of field, the second aspect the X5 allows for is variable aperture. Aperture refers to the iris of the lens itself. This works the same way as your eye. If there is too much light the iris of your eye will close, and in low light it opens very wide. But light isn’t the only thing the aperture controls, it also affects depth of field. When you open the aperture by reducing the number (f/1.7) it also reduces the depth of field. When you close the aperture (f/8 or f/16) it expands the depth of field, sometimes to infinity. Think of squinting your eyes to make things clearer, it’s the exact same thing. Okay, so we’re getting there. Focal length determines how tight or wide the image is, along with adding character, and aperture controls how much light comes in, and both affect depth of field, which is how much of your image is in focus. So let’s review and add it to what we already know. Shutter speed is of course how many times per second the shutter opens and closes per second. This is another way to control light but the shutter has a big effect on the final image. Put the shutter to high and you will get a stuttered effect where every frame is ultra sharp. A high shutter will also exaggerate rolling shutter known as the jello effect. Too slow a shutter will give a lot of motion blur and aesthetically look like cheap video. In feature films, the shutter speed of choice is exactly double the frame rate. This is known as a 180° shutter as film cameras used a shutter that was literally measured in the circular degrees the shutter obstructed. So if you are shooting 24 frames per second the shutter speed would be 48. With the X3, since we couldn’t control aperture, it’s fixed at f/2.8, if you were already at 100IS) (lowest light sensitivity) the only other way to control light without changing the shutter was to add ND filters. The ND filters act like sunglasses and reduce the light, allowing you to get the shutter close to the correct number. But with the X5 we can now control the aperture, so we can just raise it up to balance the shutter. So instead of f/2.8 we can raise it f/16 or even f/22 depending on the lens. But as you read above, this will also affect your depth of field so you need to be careful you are getting the right effect. It might be that you are shooting on a bright day but still want a shallow depth of field. In this case you would still put an ND filter on, set the shutter to double your frame rate, and set the aperture to a low number like f/2.8 or f/1.7. So as you can start to see, your perfect image is a well thought out combination of all these things. Focus. Okay, focus can be a nightmare. It’s the easiest way to ruin a beautiful shot. For this reason someone just starting out is best to use the highest aperture possible to keep the iris closed. In extreme conditions with a low (open) aperture, the depth of field can be as shallow as 1/2 inch. This means if you were filming someone’s face their eyes would be sharp in focus but the tip of their nose would out of focus. Imagine trying to determine what’s in focus using an iPhone screen while your Inspire is 1/2 mile away! The challenges are massive and expect to make some big mistakes in the early days. You will quickly learn how the lens reacts and where infinity sits. But in general, the larger the screen you are using to determine focus the better. Finally, I just wanted to touch on the difference in sensor size. Many are surprised to hear and baffled at the concept that sensor size also affects depth of field. Crazy right? The bigger the sensor you have the shallower the depth of field. To illustrate this let’s compare sensor sizes of the X3 and X5. The X3 uses a 6.17 x 4.55mm sensor and the X5 is 3x larger, 17.3x13.00mm so you will need a much longer lens to cover the same field of view. The longer the lens the shallower the depth of field and voilà! Feel free to ask for clarification for any of this. Again, it's more general than X5 specific."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:9747e165-061b-4f5d-9f62-3a6633bc5d6a>","<urn:uuid:49c42521-15ac-4d68-a920-0f380562c249>"],"error":null}
{"question":"How do L'Anse aux Meadows and Red Bay compare as early European settlements in Canada, considering their historical significance and time periods?","answer":"L'Anse aux Meadows and Red Bay represent two distinct periods of early European presence in Canada. L'Anse aux Meadows contains timber-framed turf dwellings built by Norse expeditions over 1,000 years ago (around 1000 CE) and represents the earliest known European presence in North America. In contrast, Red Bay Basque Whaling Station, located on the south coast of Labrador, dates to the mid-16th century and stands as the most complete example of a 16th-century Basque whaling station in North America. Both sites are located in Newfoundland and Labrador, but they represent different waves of European contact - Norse exploration versus Basque commercial whaling operations.","context":["A broad definition of architecture - one that calls for a balance between artistic and technical factors as well as between folk and high-art traditions - is not new.\nClassic definitions of architecture hold that it is \"the mother of the arts.\" Today architecture is more likely to be defined, not as an art, but in technical terms, or as an expression of a given society's values. What we refer to as architecture can include structures that are modest and ephemeral as well as monumental buildings; often the distinction between architecture as a high art and architecture as a reflection of simple vernacular traditions seems irrelevant to the appreciation and experience of built form. The late American architectural historian Spiro Kostof called for \"a more inclusive definition of architecture and, consequently, a more democratic view of architectural history.\" This democratic approach to defining the term architecture seems to suit the Canadian tradition, where the story of our built environment has been written by artists, craftspeople and builders whose origins, backgrounds and reasons for building have been very different.\nA broad definition of architecture - one that calls for a balance between artistic and technical factors as well as between folk and high-art traditions - is not new. The Roman architect and theorist Vitruvius (active 46-30 BCE) wrote that architecture needed to possess three qualities, usually rendered in English as commodity, firmness, and delight: good planning, sound construction, and a sense for beauty. Vitruvius required all three elements to be present for a building to be \"architecture.\" But each of these characteristics has links to the cultural norms and technological processes of a given society. All buildings, from tipis to churches, can tell us about the societies that created them - what they valued, how they marshalled construction teams, what technology they could call on, and what abstract values they espoused through the buildings they commissioned. Neither function nor art is sufficient to explain what architecture is; a building is more than the sum of its parts. Architecture is a social statement rendered in three dimensions.\nBy leaving the definition of architecture somewhat open and inclusive, we are free to explore the subject in a number of ways at the same time. If we look at it historically, over a long stretch of time, we can trace the evolution of style and the multitude of influences which come together in the creation of the built environment. An historical approach exposes key relationships between architecture and other disciplines - sculpture, drawing, engineering and urban design, to name a few. Most of us, however, experience architecture in relation to certain generic building types. We live in a house, worship in a religious building, go to work in skyscrapers, spend our money in shopping centres, feed our cars at gasoline stations, stay overnight in a hotel, go to hospitals when we are sick or seek recreational outlets in one of a variety of sports facilities. Barns and grain elevators, places of work for some Canadians, are cultural landmarks to many others. Designers of buildings, from the untrained (who simply want to design and build their own lakeside cottage) to the highly trained and experimental designers of major public monuments (such as Douglas Cardinal, who designed the Canadian Museum of Civilization), are more than practitioners of architecture. They are adding to the record of our built environment and to our understanding of what architecture is and what role it plays in our lives.\nArchitecture in Canada\nCanadian architectural history is most easily understood by linking it with political developments. Indigenous architecture in Canada was created during the period before European contact and took on a different character following contact. First Nations peoples in this country produced unique architecture, both temporary and permanent. In the former category are included such ephemeral structures such as the igloo, wigwam and tipi; in the latter are included pit-houses and wooden longhouses of the northwest coast. These buildings are not only technically ingenious and original, but reflect deeply held cultural belief systems that make the buildings rich repositories of spiritual and religious significance. Contemporary indigenous architecture continues to be practised, and embraces both traditional and contemporary materials and forms (see Architectural History, Early Architecture of the First Nations).\nIf Canada's aboriginal people have created an enduring architectural legacy, European traditions have marked Canada's architectural history with its buildings and its ways of categorizing and describing them. Many of the terms used to describe architectural styles in Canada have been adopted from a European critical tradition. The first European visitors, however, did not establish permanent settlements. Canada has some fragmentary archaeological remnants of Viking contact, specifically the Newfoundland site of l'Anse aux Meadows, which dates to c 1000 CE. Basque sailors, attracted to the rich fishing banks off the east coast of Atlantic Canada, also left traces of their sojourns here, such as Red Bay, Labrador, which dates to the middle of the 16th century.\nEuropean architecture of permanent English and French settlers is first seen during the 16th century. Thereafter, until well into the 19th century, we continue to categorize Canadian architecture with reference to its cultural origins in either French or English Canada. During the French Regime, from 1608 to 1759, many significant stone buildings were erected, for ecclesiastical, public and military purposes. French military Fortification and urban planning traditions were imported. Royal architects from France ensured that a high level of craftsmanship and planning was maintained. The fort of Louisbourg (destroyed 1758 and rebuilt from the 1960s) is the most significant example of French military planning in North America.\nBritish influence is seen first in the fortifications it constructed against the French presence in North America - and then the American presence. Modest block houses were built, and then more elaborate forts. Urban planning and domestic architecture soon followed. Comparing the geometric plan of Halifax (established 1749) with the more organic plan of Quebec City shows the very different ways that the French and English laid out cities. After the Conquest of Québec (1759), British traditions gradually came to dominate. Québec and other cities laid out during the French Regime took on a new and decidedly English character (see Architectural History, Architecture under the French Colonial Regime).\nArchitecture from 1759 to 1867 is referred to by reference to British royal traditions: the Georgian and Victorian periods. The Georgian period is associated with the development of refined domestic architecture. American Loyalists brought their own traditions into Canada, particularly in Atlantic Canada and present-day Ontario. The Victorian period is associated with the development of many public buildings, especially from the 1840s through the 1860s. These were often built much larger than the cities that created them could afford, because the cities that sponsored them were vying for settlers and government commissions. Examples include Montréal's Bonsecours Market and the Kingston City Hall. Universities were built, such as University College, University of Toronto (see Architectural History, 1759-1867).\nFollowing Confederation in 1867, until World War I broke out in 1914, peoples from countries other than England and France immigrated to Canada, bringing their architectural traditions with them. American Loyalists and German immigrants during the 18th century had already made their presence felt particularly in Atlantic Canada towns such as Lunenburg; during the 19th century, Scottish masons brought excellent skills to the Ottawa Valley, in such towns as Perth. Central Europeans, particularly from the Ukraine, arrived, interested in exploiting the agricultural hinterland in the west made newly accessible by the railway. They left an enduring architectural legacy. An example is the Ukrainian church formerly at Smoky Lake, Alberta, and now in the Canadian Museum of Civilization.\nPerhaps in response to the growing cultural diversity of Canada during the period from 1867 to WWI, architects and government officials sought a national idiom. The Parliament Buildings, Ottawa, made the British-inspired Civic Gothic Style influential for a time. Against this residual British influence was the increasing influence of the Americans, who pioneered skyscraper technology during the latter part of the 19th century. Canadian architects struggled under the dominant shadow of the USA, whose architects, some Canadians felt, were given an undue share of available work in Canada.\nThere was increasing pressure throughout this period for architects to professionalize (to create self-regulating organizations responsible for setting and maintaining standards of practice). During the latter part of the 19th century, architecture, surveying, and engineering became separate professions. Architects debated whether their training should take place via apprenticeships or in universities (see Architectural History, 1867-1914).\nFollowing WWI, Canadian architects continued to seek an appropriate national idiom and gingerly experimented with European modernist traditions. Architects such as John Lyle sought a national expression through the use of regional ornament. But architects such as Ernest Cormier denied that national themes were appropriate and called for the development of an international style in Canadian architecture. During these years, and not without considerable difficulty, the first women began to enter the profession of architecture.\nFollowing World War II, Canadian society embarked on a feverish period of city-building, in response to returned veterans and the resulting baby-boom. Cars became affordable for many people, and suburbs transformed the way Canadians lived. Architecture was given a boost by specific recommendations made in the report of the Royal Commission on National Development in the Arts, Letters & Sciences (commonly known as the Massey Report, 1951), which recommended that Canadians develop a Canadian idiom in architecture, and proposed steps to ensure this. The Modern Movement in architecture became popular, particularly in Vancouver, Winnipeg and Toronto (see Architectural History, 1914-1967).\nBy 1967, when Canada celebrated its centennial with Expo 67, architects such as Arthur Erickson were acclaimed internationally. Following 1967, Canadian architecture developed in many different directions. Individual architects pursued personal visions. Some, such as Alberta's Douglas Cardinal, brought aboriginal sensibilities into the Canadian mainstream. Others, such as Eberhard Zeidler, brought European sensibilities into Canada, and gained fame internationally. Canadian cities grew rapidly, and many older buildings were demolished. The Heritage Conservation movement, which upheld the retention and restoration of our architectural history, began (see Architectural History, Canadian Architecture 1967-1997).\nWhile the search for a Canadian idiom in architecture is ongoing, it is generally recognized that regional differences will contribute to a diversification of architectural expression, rather than a single dominant national idiom. Canadian architecture will thus become as diverse as the rest of Canadian geography and society.\nHazel Conway and Rowan Roenisch, Understanding Architecture: An Introduction to Architecture and Architectural History (1994); Harold Kalman, A History of Canadian Architecture (1994); Spiro Kostof, A History of Architecture: Settings and Rituals, Second Edition (1995); Carol Herselle Krinsky, Contemporary Native American Architecture: Cultural Regeneration and Creativity (1996); Hanno-Walter Kruft, A History of Architectural Theory from Vitruvius to the Present, trans Ronald Taylor et al. (1994); Rhodri Windsor Liscombe and Michelangelo Sabatino, Canada: Modern Architectures in History (2016); David Watkin, The Rise of Architectural History (1983).","Canada Post has issued another five-stamp set as part of its ongoing series celebrating Canada’s UNESCO World Heritage Sites.\nWith this fourth issue of the UNESCO series, Canadians are being invited to discover the country’s early historic settlements while envisioning the origins of complex organisms on this planet.\n“These sites are treasures for Canada and the world—and we hope that these stamps instill pride and arouse a sense of wonder in every Canadian,” said Canada Post President and CEO Deepak Chopra.\nThree of the stamps feature sites that are new to the multi-year series, which has celebrated all 18 sites:\n- Mistaken Point, N.L., at the southeastern tip of Newfoundland, gives us some sense of what life looked like when organisms began to get larger than microbes and complex. Embedded along this gorgeous coastline are groups of the oldest known fossils of ancient soft-bodied life forms, dating back 560 to 580 million years. These strange multi-celled organisms ranged in size from as small as a fingernail to as long as a metre and had no legs or eyes. They are believed to be the planet’s first large life forms.\n- The Historic District of Old Québec, Qué., founded in 1608 by Samuel de Champlain, is still bustling with activity, yet has maintained the integrity of essential historical buildings and spaces over more than four centuries. Fortified with walls, gates and bastions, this is the only colonial city north of Mexico to have preserved its ramparts.\n- L’Anse aux Meadows National Historic Site, N.L., at the tip of Newfoundland’s Great Northern Peninsula, contains timber-framed turf dwellings built by a Norse expedition more than 1,000 years ago. The settlement is the earliest known European presence in North America.\nThe remaining two stamps bear images of locations that previously appeared on U.S.-rate stamps issued in 2015. That year, Canada Post recalled a stamp for the first time ever after it was discovered the hoodoos shown on the 2015 $1.20 Dinosaur Provincial Park stamp were actually located in Drumheller, Alta., almost 200 kilometres away from Dinosaur Provincial Park.\n- Dinosaur Provincial Park, Alta., was declared a World Heritage site for its exceptional fossil specimens of Cretaceous dinosaurs, as well as undisturbed badlands and riverside habitat.\n- Red Bay Basque Whaling Station, N.L., on the south coast of Labrador, is the most complete and extensive example of a 16th-century Basque whaling station in North America.\nWorld Heritage Sites are chosen by UNESCO (the United Nations Educational, Scientific and Cultural Organization), which “seeks to encourage the identification, protection and preservation of cultural and natural heritage around the world considered to be of outstanding value to humanity.”\nDesigned by Lara Minja, of Lime Design, the 24 mm by 20 mm self-adhesive stamps are sold in booklets of 10 and 30. A gummed souvenir sheet of the five stamps, an official first-day cover cancelled in Trepassey, N.L., and postage-paid postcards of the three new stamp images are also available. Postage-paid postcards for Dinosaur Provincial Park and Red Bay Basque Whaling Station are also available."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:80dfda2e-7e47-4239-898a-9a7e5aa8704e>","<urn:uuid:e3575e48-5441-41d3-bfec-8e82a2297ad0>"],"error":null}
{"question":"What rights do children have in EU judicial proceedings, and what is the role of child protection officers in ensuring these rights?","answer":"In EU judicial proceedings, children have the right to be heard, access adapted proceedings, receive information and advice, have legal representation, and protection of privacy. The EU Agency for Fundamental Rights notes that while respect for children's right to be heard has improved in criminal proceedings, children are not always heard in civil cases. Child protection officers play a crucial role in protecting these rights by monitoring cases from arrest through rehabilitation, preparing reports on the child's personal and social status, and supervising the execution of court dispositions. They must attend court sessions, as juvenile courts cannot convene without their presence, and they have the power to request modifications to disciplinary measures or early release after one-third of a term is served.","context":["Written by Anna Dimitrova-Stull, Ulla Jurviste and Irene Penas Dendariena,\nEvery year hundreds of thousands of children across the EU are involved in judicial proceedings. They may enter into contact with justice systems in many different ways: for family issues such as divorce or adoption, in administrative justice for immigration or nationality matters or in criminal justice, as victims, witnesses or suspects/offenders. It is essential that they are met with a system that respects both their particular vulnerability and their rights. To that purpose, a set of international standards has been developed to ensure that children are heard and protected in the process.\nThe EU Agency for Fundamental Rights (FRA) reveals that child participation practices in the EU vary not just across but also within Member States, and these practices are not always child-friendly. It points out that in recent years respect of the child’s right to be heard has improved particularly in criminal proceedings but in civil ones children are not always heard. There is a need to ensure that children’s access to justice and their treatment in legal proceedings is effectively monitored to prevent any discrimination, says FRA.\nIn January 2016, the European Parliament Committee on Civil Liberties, Justice and Home Affairs (LIBE) endorsed the agreement reached by the Parliament and the Council last December to set new rules designed to make legal protection of children who are suspected or accused of a crime more consistent within the EU, providing a set of rights that meet their specific needs. The agreement still needs to be approved by the plenary in March.\nThis Keysource brings together a selection of information materials describing the situation of children in judicial proceedings.\nStanding up for children? The Directive on procedural safeguards for children suspected or accused in criminal proceedings / Debbie Sayers, EU Law Analysis Blog, 22 December 2015.\nThis article analyses the proposed Directive in order to examine whether the aim to protect the rights of children in criminal proceedings is achieved.\nJustice needs to be more child-friendly / FRA, press release, 5 May 2015.\nJustice systems need to do more to cater to the needs of these children. Making justice more child-friendly is in the best interests of the child, improves child protection and children’s meaningful participation in judicial proceedings, says FRA.\nUne justice adaptée aux enfants. Points de vue et expériences des professionnels / Journal du droit des jeunes 5/2015 (n° 345 – 346), pp. 26-34.\n“Les enfants ne sont pas suffisamment soutenus lorsqu’ils participent à des procédures pénales ou civiles ; des environnements qui peuvent être intimidants pour des enfants ne sont pas toujours adaptés à leurs besoins.”\nJust how child-friendly should justice be? / Killian O’Brien, Law Society Gazette, Jan/Feb 2013, pp. 14-15.\nIn this article, the author stresses the need for a well-informed, proactive approach to the involvement of children in the judicial process and argues that it is essential that children’s rights are promoted at an early stage.\nChild-Friendly Justice: turning law into reality / Ankie Vandekerckhove & Killian O’Brien, ERA Forum, December 2013, Volume 14, Issue 4, 523-541 pp.\nFrom the angle of the Council of Europe’s guidelines about justice for children, this article comments on what children can do whenever they get involved in legal proceedings where their rights and interests are at stake.\nThe role of the EU legal and policy framework in strengthening child friendly justice / Rebecca O’Donnell, ERA Forum, December 2013, Volume 14, Issue 4, pp. 507-521.\nThis article examines the growing recognition in legislation and jurisprudence of the need for specially tailored processes for children and the practical challenges in achieving them.\nThe way forward: the implementation of the EU Agenda for the rights of the child / Margaret Tuite, ERA Forum, December 2013, Volume 14, Issue 4, pp. 543-556.\nThis is an overview of EU activities on child-friendly justice. It draws attention to a need to focus on implementation aspects: on the collection of data to underpin, measure and monitor implementation at regular intervals, and on the value and benefits of interdisciplinary and multidisciplinary cooperation and coordination, including training.\nChildren’s Involvement in Criminal, Civil and Administrative Judicial Proceedings in the 28 Member States of the EU / European Commission, Policy brief, June 2015, 54 p.\nThe Commission has released several reports (see below), completing its study on children in judicial proceedings. Its aim was to gather all available data and statistics in Member States (MS) on children’s involvement in justice and to describe the law and policy in place. This policy brief presents the findings of the study in an accessible manner. It focuses on the implementation by MS of 10 key safeguards: access to adapted proceedings; right to information and advice; right to be heard; right to representation; right to protection of privacy; the best interests of the child; multidisciplinary cooperation; training of professionals; monitoring mechanisms; and access to remedies.\nDetailed information in the following reports:\n- Children’s involvement in administrative judicial proceedings (EU summary and country reports);\n- Children’s involvement in civil judicial proceedings (EU summary and country reports)\n- Children’s involvement in criminal judicial proceedings (EU summary and country reports).\nChild-friendly justice – Perspectives and experiences of professionals on children’s participation in civil and criminal judicial proceedings in 10 EU Member States / FRA, May 2015, 129 p.\n“Practices of child participation in criminal and civil judicial proceedings vary considerably not just across, but also within Member States, pointing to a need for clear and consistent standards and guidelines and the systematic monitoring of their implementation. Children are not sufficiently supported when participating in criminal or civil proceedings, court settings that can be intimidating for children are not always adjusted to their needs”.\n- Annex 2 “Indicator overview tables and national legislation and policies” which provides an overview of the findings by country, showing the structural and process indicators used in the analysis of the evidence collected by FRA.\n- Country reports for the comparative report on Children and Justice.\nHandbook on European Law relating to the rights of the child / FRA, 2015, 254 p.\nThe handbook is designed for non-specialist legal professionals, judges, public prosecutors, child protection authorities, and other practitioners and organisations responsible for ensuring the legal protection of the rights of the child.\nResearch and Selection of the Most Effective Juvenile Restorative Justice Practices in Europe: Snapshots from 28 EU Member States / European Council for Juvenile Justice (EVJJ) & International Juvenile Justice Observatory (IJJO), 2015, 251 p.\nAccording to this study, the benefits of restorative justice for children and young people are numerous. Despite these benefits, however, restorative justice still plays a marginal role and far too few children and young people in Europe benefit from restorative justice processes.\nSave Money, Protect Society and Realise Youth Potential: Improving Youth Justice Systems in a Time of Economic Crisis / EVJJ & IJJO, 2013, 86 p.\nThis paper argues that it has been shown that times of economic restraint provide a good incentive to really think about what works in youth justice. It claims that youth justice systems need to be better at saving money and protecting the community, but above all, they need to provide real outcomes for the children experiencing them.\nEU Institutions’ views\nProposal for a Directive of the European Parliament and of the Council on procedural safeguards for children suspected or accused in criminal proceedings / COM (2013) 822 final, 27 November 2013.\nThe aim of the directive is to establish specific legal protection for children, as they are particularly vulnerable. It sets out minimum rules that meet their needs. Among the priorities is ensuring that children are able to understand and follow the proceedings, preventing re-offending and fostering their social integration.\nEU Agenda for the Rights of the Child / COM (2011) 60 final, 15 February 2011.\nMaking the justice system more child-friendly in Europe is a key action under the Agenda. It stresses that children can be treated as adults without always being afforded specific safeguards in accordance with their needs and vulnerability. Effective access to justice and participation in administrative and court proceedings are basic requirements to ensure a high level of protection of children’s interests.\nInitial appraisal of a European Commission Impact Assessment: European Commission proposal on procedural safeguards for children in criminal proceedings / Alison Davis, European Parliament Research Service, February, 2014.\nThis document seeks to provide an initial analysis of the strengths and weaknesses of the European Commission’s Impact Assessment (accompanying the above proposal) which was transmitted on 28 November 2013.\nProtection of Children in Proceedings / Directorate-General for Internal Policies, 2010, 15 p.\n“Children’s rights are heard and protected in proceedings in all European Member States. Although a child’s rights are heard in all Member States, there are substantial differences in the provisions governing how these rights are heard”.\nA justice system adapted to children / 20 May 2015.\nCouncil of Europe\nChild-friendly juvenile justice: from rhetoric to reality / Parliamentary Assembly report, 19 May 2014.\n“Despite the panoply of international and regional standards providing a well-established framework for modelling juvenile justice, there is a considerable and continuing dissonance between the rhetoric of human rights discourse and the reality of juvenile justice interventions, in particular juvenile detention, for many children.”\nGuidelines on child-friendly justice / Committee of Ministers, 17 November 2010.\nThese guidelines set out basic rules for European states to follow when adapting their justice systems to the specific needs of children. They apply to all circumstances in which children are likely, on any ground and in any capacity, to be in contact with the criminal, civil or administrative justice system.\nGeneral Comment N° 12: The Right of the Child to be Heard / Committee of the Rights of the Child (CRC), 2009.\nThe objective of the Comment is to support States parties in the effective implementation of article 12 of the UN Convention on the Rights of the Child. This article establishes the right of every child to freely express her or his views, in all matters affecting her or him. This right imposes a clear legal obligation on States parties to recognize this right and ensure its implementation by listening to the views of the child and according them due weight.\nGeneral Comment N° 10: Children’s Rights in Juvenile Justice / CRC, 2007.\nIn this document, the CRC provides guidance and recommendations to States parties in dealing with children in conflict with the law. It places special emphasis on prevention and on alternative measures to criminal justice.\nRights, Remedies & Representation: Global Report on Access to Justice for Children / Child Right International Network, January 2016, 43 p.\nThis report represents a snapshot of how the world has tried to develop mechanisms to protect children’s rights and ensure that there are remedies for violations of children’s rights. It represents an overview of the findings of 197 country specific reports.\nJoint paper on the proposal for a Directive of the European Parliament and the Council on procedural safeguards for children suspected or accused in criminal proceedings / Amnesty International & Save the Children, December 2014, 15 p.\n“In the administration of juvenile justice, states must systematically ensure respect for the best interests of the child, the child’s rights to life, survival and development, to dignity, to be heard and to be free from discrimination”.\nJoint position paper on the proposed directive on procedural safeguards for children suspected or accused in criminal proceedings / Fair Trials International & Children’s Rights Alliance for England, September 2014, 26 p.\n“The Children’s Directive therefore stands to significantly improve both the situation of children’s rights in the EU, and to contribute substantially to the strengthening of mutual trust between Member States”.\nCompendium of international instruments applicable to juvenile justice / Terre des Hommes, 2014, 477 p.\nThis document aims to provide a tool that will gather together all the texts guaranteeing children’s rights in the systems of justice.\nProtecting children’s rights in criminal justice systems / Penal Reform International, 2013, 190 p.\nThis is a training manual for professionals and policymakers. It covers a variety of topics and issues including: child protection, crime prevention, law enforcement, trial procedures, sentencing and rehabilitation.\nChild-friendly justice guidelines: recent judgments of the European Court of Human Rights / Council of Europe, December 2014, 8 p.\nThis note refers to recent judgments of the Court in which the child-friendly justice guidelines are cited, since their adoption by the Committee of Ministers on 17 November 2010.\nSee also :\nTorture and ill-treatment: Dushka v. Ukraine / 3 February 2011.\n- Child‘s ability to participate in the proceedings: Adamkiewicz v. Poland / 2 March 2010.\n- Detention: Nart v. Turkey / 6 May 2008.\nFor more information on existing case law of the European Court of Human Right and the Court of Justice of the EU, see: Handbook on European Law relating to the rights of the child , Op. cit.\nData on Children in Judicial Proceedings in EU28 / DG Justice, European Commission.\nThis site presents an online database containing the results of the study commissioned by DG Justice to collect data on children’s involvement in judicial proceedings in the EU (see: Criminal Justice ; Civil and Administrative Justice ).\nEU programmes and projects\nJustice Programme (2014-2020).\nRights, Equality and Citizenship Programme (2014-2020).\nAction grants to support transnational projects aiming to build capacity for professionals in child protection systems and legal professionals representing children in legal proceedings / Open call for proposals – deadline: 04/05/2016.\nThis call will fund activities for three priorities: Capacity-building for practitioners/professionals working with or for children in alternative care or detention; Capacity-building for lawyers/legal advisers representing children in criminal, administrative and civil justice; Capacity-building for legal and other practitioners such as social and health workers, youth workers and the police to pilot and roll out multi-disciplinary evidence-based child-friendly practices.","Home Law International Handbook of Juvenile Justice\nChild Protection Officers\nThe “Child Protection Officer” is the new name that the 2016 Juvenile Protection Law adopted for the Ministry of Social Development’s personnel in charge of dealing with juvenile justice issues. Previous names were “probation officers” and sometimes called “social supervisors.” Child protection officers are assigned with a role to perform across all stages of the juvenile justice chain: arrest and detention by the police, investigation and indictment by prosecutors, trial by courts, rehabilitation in social institutions, and aftercare programs. Probation could also mean placing the juvenile under the supervision of a child protection officer instead of incarceration.\nThe Law attached a great deal of significance to probation, as an alternative to incarceration. The court might order placing the child under the supervision of a child protection officer for a term ranging from 1 to 5 years. The order can stop anytime during this period upon the prosecution’s request, the child protection officer, or the juvenile himself or his guardian; and after reviewing the child protection officer’s report on the progress that has been achieved. If the child fails to follow the probation’s term, the court might consider other dispositions, including deprivation of liberty. If the juvenile commits another offence during the probation period, the probation would come into an end and the child might be retried on the original offence, besides the new one. The Ministry of Social Development, by administrative decisions and capacity building programs, should give a greater attention to the probation as a referral disposition by strengthening the system and its staff.\nChild protection officers work under the direction of the Child Protection Unit, headed by the supervisor at the headquarters of the Ministry of Social Development in the city of Ramallah. Such officers work at the district level under the daily administrative management of the head of Social Directorate of the Ministry of Social Development within each district. As in other countries, the work of these social officers, as they are not supposed to be technically juvenile lawyers, is quite informal. They assist courts and law enforcement officials in determining the personal and social context of the child who violates the law on a case-by-case basis.\nThe role of child protection officers starts upon the arrest of the child by the police. Once detained and transferred to prosecution, the child officer should continue to monitor the juvenile’s case in each interrogation session. At the court hearings, the judge obtains a report from a child officer on the juvenile’s personal and social status. The report of the child officer, provided in practice upon formal request from the court, might comprise “general information about the child, his environment, performance in school and his heath.” The Ministry has designed a form to be filled out by child protection officers and be used before courts.\nChild protection officers in practice provide the required information after interviewing the juvenile and his family. To complete the report, some officers visit the child at home, police station, and in detention. Despite the efforts that some child officers take in its preparation, judges often ignore such a report and include it to file merely as part of a routine procedure. Most judges do not even bother reading the officer’s report. Unfortunately, Article 30 of the 2016 Juvenile Protection Law (on court sessions) uses a language that may imply that the courts only “listen” to, but not under an obligation to build its findings based on the report of the child protection officer. However, the report might have significant weight in the judgment in case of no objection to its content by the parties or the juvenile judge.\nOnce the court makes a disciplinary measure, particularly if the measure is to place the child in a care institution, the role of child officer will not end. In this connection, Article 48(1) of the Law stated that: “The child protection officer shall supervise the execution of the dispositions set forth in this Law. He shall monitor the convicted child and present recommendations to the juvenile and to those in charge of his education. The officer shall prepare periodical reports to the court and prosecution every three months on the child’s situation.” The court, based on such reports, might modify or end the measure. In practice, however, a few child officers do visit care institutions or detention centers, mainly on their own personal initiative. The child protection officers have also the power to bring a child prior to the completion of his disciplinary measure to the court to request an extension to that measure. The officers, at the same time, have the right to request the court to release the juvenile after spending one-third of his term at the social care institution.\nAs noted earlier, child protection officers are overwhelmed with daily work and have no time to work on all cases of children in conflict with the law. With about 10-30 juvenile cases a month in each district, coupled with the lack of transportation to police stations, courts, or rehabilitation institutions, “it is next to impossible for probation officers to perform well in studying all cases of juveniles. The time of the probation officer is barely enough to prepare the report to the court.”\nUnder such circumstances, child protection officers perform on ad hoc basis. Sometimes they respond to phone calls from police stations having cases of children in conflict with the law. It is difficult to respond positively when there are a number of police stations in the district in which the child officer functions. In other instances, child officers coordinate between the family and prosecutor. Such officers cannot attend all juvenile hearings, but rather observe selected court sessions.\nIn order to advance their efficiency, it is estimated that each district needs minimum three child protection officers. At least one officer should work on fulltime basis on juvenile justice issues. Now as the juvenile court can no longer be convened without the presence of the child protection officer (Article 25(2) of the Law), hiring more officers cannot be avoided. The Ministry should make transportation available for the officers to visit children at home, school, police stations, detentions, courts, and social care institutions. The work of these officers does not lack legal basis, but it rather requires executive orders and appointing adequate number of qualified staff.\n|< Prev||CONTENTS||Next >|"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:f746432f-3d44-409d-b630-34371ff2192a>","<urn:uuid:e8bed487-d8dd-4ac5-89b2-1cb0dfd1df1d>"],"error":null}
{"question":"What are the differences between the ceremonial practices for group military burials at Arlington in 1899 versus the Catholic Church's current requirements for cremation ceremonies?","answer":"The 1899 Arlington military burial was a large-scale coordinated ceremony with 336 bodies, featuring presidential attendance, military honors, cannon salutes, and both Protestant and Catholic religious elements, with bodies displayed under tents before burial. In contrast, modern Catholic cremation ceremonies follow strict liturgical guidelines where cremated remains must be in a worthy vessel placed on a suitable table during the Funeral Liturgy, cannot be covered with a pall, and must follow specific procedural steps including the Rite of Committal at the burial site. The military ceremony allowed for more flexibility and public display, while Catholic guidelines emphasize structured liturgical elements and more private, controlled ceremonial aspects.","context":["The following article is about the burial of the members of the American military forces who had been temporarily interred in Cuba and Puerto Rico and then returned to the United States.\nThe bow of the former Allan Lines liner ROUMANIAN, now a U.S. Army Transport and recently renamed USAT CROOK, plied the waves of the Caribbean bound for the United States. In the past, it carried excited passengers, untried regiments of soldier bound for Cuba, and regiments of disease-ridden men, sick but happy to be heading home. This trip was different. The four hundred foot long vessel had only a few passengers, among them Brigadier General Ezra Ewers and his family, a Captain Carnahan and his wife, a Lieutenant Frazer and his wife, and a contingent of undertakers. The ship’s flag was at half mast. The remainder of the “passengers” did not notice of the roll of the sea. They, 667 in number, were dead.\nThe ship was charged with bringing home the bodies of those soldiers who had died in Cuba and Puerto Rico during the Spanish American War. Of the 667 bodies (though this number does vary in different accounts), 438 were from Cuba, and 119 were from Puerto Rico. Of the bodies, 110 were not identified, though often their regiment was known. The men were disinterred and brought aboard, being attended to by the large group of undertakers. The bodies were placed in metal coffins and then inside of crates.\nThe vessel arrived in New York, her bow and rigging draped in black, reminiscent of a hearse. The Secretary of War, Russel Alger, had ordered Maj. Gen. Wesley Merritt to meet the ship. The orders read:\n“By direction of the President, you will, upon the arrival of ROUMANIAN with the remains of the soldiers who were killed or died at Santiago and Porto Rico, fire a fitting salute, order all flags half-staff and detail a sufficient guard of honor to see the caskets taken off the ship and expressed to their former homes…”\nFamilies were given the option of claiming the bodies of loved ones, so long as they paid for the shipping of the body to their place of interment. Many bodies were reclaimed and found their way home to their loved ones. The bodies that were not claimed by their family, or not identified, were to be taken to Arlington National Cemetery for interment.\nFrom New York, those bodies to be interred at Arlington were placed aboard a funeral train, suitably draped in black. The bodies were offloaded at Rosslyn, Virginia and transported to the place of the their burial. The number of bodies were so great that the train had to make two runs to deliver all of the bodies.\nThe men were not buried in a mass burial, but in a coordinated burial of over three hundred soldiers in adjacent graves on a two acre plot overlooking the Potomac River. On Thursday, April 6, 1899 all was in place and an interment ceremony was to be held. The coffins were still sealed within wooden boxes and would remain so. Each box bore a statement that read:\n“For sanitary reasons the within casket, which is hermetically sealed, should not be opened or removed from the wooden box.”\nThe 336 boxes were placed under tents in groups of about twenty. Row of graves were previously dug and cross beams placed across their openings. When all was ready, the boxes were carried out of each tent and individually placed on a set of cross beams. Soon the tents disappeared and a sea of boxed coffins filled the field. The boxes were flanked by mounds of earth. Each box was covered with an American flag.\nThe boxes were not placed in any specific order, however, the boxes containing the remains of the few officers were placed at the front, closest to where the president would be. These officers included Capt. Edgar Hubert (8th U.S. Infantry), Lt. William Wood (12th U.S. Infantry), Lt. R. S. Turman (6th U.S. Infantry). Lt. Francis Creighton (Volunteer Signal Corps)\nThe dignitaries and the populous of the nation’s capital city came out for the event, as many federal offices were closed to allow the employees to take part.. The official delegation included President McKinley, members of the cabinet, Maj. Gen. Nelson Miles and various members of the military. Also present were the British and German ambassadors. The dignitaries arrived accompanied by the tune of the funeral march from “Saul.” A military contingent was present from the District of Columbia National Guard, the Naval Militia and Fourth and Fifth Artillery. Joining them were about 15,000 members of the public. The crush to get into the cemetery was so great that the president and his entourage were caught in a traffic jam, and the police had to be called to clear the way.\nAs the president’s party approached, the parents of John O'Dowd of the 7th U.S. Infantry broke through the crowd to place flowers on the box containing their son’s body, followed by the family of Lt. William Wood doing the same for their son.\nThe field was surrounded the by ranks of soldiers on three sides. The band played “Nearer My God to Thee.” The post chaplain of Fort Monroe, Rev. C. W. Freeland, performed the military committal service, and when he got to the portion that included the words “…dust to dust, earth to earth,” a soldier placed at each grave crumbled a handful of earth onto each box. The entire assemblage said the Lord’s Prayer. Afterwards, Father McGee of St. Patrick’s Church stepped forth to consecrate the ground for the Catholic soldiers included. During the entire event, a cannon was fired every half hour from nearby Fort Myer. As the service reached its conclusion, the Fourth and Fifth Artillery fired three salutes and “Taps” was played.\nThe service over, the dignitaries withdrew. The ground was left to those charged with the actual burials, expected to take two or three days. The soldiers were now interred in their native land.","The traditional teaching of the Catholic Church with regard to the proper burial of the sacred remains of the deceased and the resurrection of the body on the last day requires periodic catechesis. This is especially so today with the ever-increasing number of Catholics choosing cremation.\nWhile interment of the body remains the preference of the Church, after the manner of the burial of the Lord Jesus, the use of cremation is allowed according to the following norms. These norms promote the faith and practice of the Church with regard to the burial of a Christian. For the most part, these are already in force according to current liturgical law and the liturgical books.\nThus, respect for the remains of the cremated body, as befits the dignity of a baptized person, is ensured. It is the duty of the pastor, with other priests and deacons, to communicate these norms to parishioners, bereavement ministers and funeral directors as part of a periodic catechesis on the reverent and proper burial of the dead.\nNORMS FOR FUNERAL RITES WITH CREMATION:\nA. If a body is to be cremated, it is always preferable that cremation take place after the Funeral Liturgy.\nWhen cremation takes place after the Funeral Liturgy, the Rite of Committal occurs with the burial/disposition of the cremated remains. The Rite of Committal does not take place in the church after the Rite of Final Commendation at the conclusion of the Funeral Liturgy.\nB. If a body is cremated prior to the Funeral Liturgy and the burial/disposition follows, then:\n1) The cremated remains are to be brought to the Church in a worthy vessel, that is, in a solid and durable container, which may appropriately be marked with the name of the deceased;\n2) The vessel may be carried in the entrance procession or it may be put in place before the Funeral Liturgy begins (cf. OCF, no. 427);\n3) The vessel is to be positioned on a suitable table in the same place where the coffin is usually positioned, and not in the sanctuary (cf. OCF, no. 427);\n4) The covering of the vessel with the pall is to be omitted (cf. OCF, no. 434);\n5) The Funeral Liturgy is to be celebrated in accord with the Roman Missal, the Order of Christian Funerals and “Appendix 2” of the Order of Christian Funerals (cf. OCF, no. 428);\n6) Texts should be chosen in view of the fact that the body of the deceased is not present but has been cremated (cf. OCF, nos. 428-429);\n7) In the Funeral Mass with cremated remains, the Rite of Final Commendation is to take place following the Prayer after Communion; in the Funeral Liturgy outside of Mass with cremated remains, the Rite of Final Commendation takes place following the Lord’s Prayer;\n8) The alternate form of the dismissal is to be used (OCF, no. 437);\n9) The Rite of Committal is to be conducted at the cemetery, mausoleum or columbarium as soon as possible following the Funeral Liturgy, using the alternate form (OCF, no. 438);\nIt is most appropriate that the burial/disposition of the cremated remains immediately follow the Funeral Liturgy.\nIn any case, the length of time between the Funeral Liturgy and the burial of cremated remains is not to exceed thirty days.\n10) The cremated remains are to be buried in a cemetery or entombed in a mausoleum or columbarium (cf. OCF, no. 417);\nA mausoleum or columbarium can only be erected where there is already a cemetery (CCL, c. 1242).\nIt is not permitted to scatter cremated remains.\nLikewise, it is not permitted to delay the burial/disposition of the cremated remains in anticipation of the eventual burial of another person. The permanent storage of cremated remains in a private home, funeral home or any other place is prohibited.\nThe integrity of the cremated remains is always to be respected. The cremated remains of one deceased person may not be mixed with the cremated remains of another person. It is not permitted to spanide the cremated remains and retain, inter or entomb them in more than one place.\nIt is also not permitted to spanide the cremated remains in such a way that they are contained in lockets or jewelry. Any other practice which violates the integrity of the cremated remains and impedes reverent and proper burial/disposition is prohibited.\nIf burial takes place at sea, the cremated remains are to be in a solid and durable container, and not scattered.\n11) The place of burial or entombment may be memorialized appropriately.\nC. If cremation and burial/disposition takes place prior to the Funeral Liturgy, then:\n1) The funeral rites are to be adapted according to the prescriptions of “Appendix 2” of the Order of Christian Funerals (cf. nos. 422-425).\n2) The Rite of Committal with the Final Commendation takes place with the burial/disposition of the cremated remains.\n3) The Funeral Liturgy may follow but without the Rite of Final Commendation and the Rite of Committal, since these have already taken place.\nCardinal Justin Rigali\nArchbishop of Philadelphia\nOctober 1, 2010\nOCF refers to the Order of Christian Funerals\nCCL refers to the Code of Canon Law"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:1f79df60-b18c-4c42-b840-aa180acf8847>","<urn:uuid:19e8626f-755d-447e-980a-2f8d6e6aeee6>"],"error":null}
{"question":"Could you explain the origins of specialized lifting techniques and vertical farming, and how have both evolved to address space and efficiency challenges?","answer":"The Jefferson Lift, also known as the Straddle Deadlift, developed as a specialized lifting technique with two main variations focusing on shoulder position relative to the bar, aiming to optimize lifting efficiency in limited space. Meanwhile, vertical farming's evolution traces back to 1915 when Gilbert Ellis Bailey first coined the term, followed by hydroponics development in the 1930s by William Frederick Gerick. The concept was further developed by Ike Olsson in the 1980s and Professor Dickson Despommier in 1999, who envisioned it as a solution for urban food production. Both practices have evolved to maximize efficiency - the Jefferson Lift through specific grip techniques and stance positions, while vertical farming has advanced to include controlled environments and soil-less growing methods, now covering approximately 30 hectares globally as of 2020.","context":["by Al Myers\nI received an email the other day asking a few questions regarding technique for the Jefferson Lift. I thought this was a very appropriate question since the Jefferson Lift will be a big part of our USAWA competitions this year. This lift will be contested in both Nationals and Worlds. The IAWA official name for the Jefferson Lift is the Straddle Deadlift – so these two names are interchangeable. I have heard in the past this lift also called the Kennedy Lift, but that is not entirely correct. The Kennedy Lift is a straddle lift where the bar starts at a higher position than floor level. First, lets go over the USAWA rules for the Jefferson Lift:\n18. Jefferson Lift\nThis lift is also known as the Straddle Deadlift. The rules of the Deadlift apply except that the bar will be lifted between the legs, with a leg on each side of the bar. The lifter may face any direction and feet placement is optional. One hand will grip the bar in front of the lifter while the other hand will grip the bar behind the lifter. The bar may touch the insides of either leg during the lift. The heels are allowed to rise as the bar is lifted, but the feet must not change position. The bar is allowed to change directions or rotate during the lift.\nI have seen two techniques for the Jefferson Lift used in competition. I will go over both of these techniques.\n1. Shoulders Perpendicular to the Bar\nIn this technique, the lifter straddles the bar with a foot on each side of the bar with feet in line with the bar. As the bar is lifted, the bar will rotate to some degree at the finish position.\n2. Shoulders Parallel to the Bar\nIn this technique, the lifter sets up for the pull with the shoulders in line with the bar. The feet are slightly off-set as they straddle the bar. The bar comes straight up with very little rotation.\nThere are advantages to both styles, but I prefer technique number two for several reasons. I feel because it takes the rotation out of the bar it allows a more direct line of pull, and an easier lockout. Technique number one will help with the initial pull from the floor because both legs can be more involved at the start. A problem with tech #2 is that the lead leg will be overloaded at the start, and more strain will be felt in the hamstring of the lead leg. I have pulled a hamstring in this leg before doing the Jefferson. Another important thing is the proper feet placement with tech #2. The toe of the lead leg should be turned slightly in. The back foot should be almost parallel to the bar. Doing this “blocks” any bar rotation as the weight comes up. The width of stance should be of comfortable width – not too wide or too narrow. This is important in order not to hit the inner thighs with the bar as the lift is completed. The back hand (the one behind the lead leg) should have the knuckles facing forward, while the front hand should have knuckles facing away, using an alternate grip. Try to keep the grip as close as comfortable as this will shorten the height the bar has to be lifted. If done correctly with technique #2, there should be very little twisting of the body as the lift is completed. At the end of the pull drive the shoulders up like with a deadlift.\nBody mechanics play a big part in the Jefferson Lift. Obviously, having long arms help. I have seen lifters with short arms have serious problems at lockout (OUCH!). You are a natural at the Jefferson Lift if you can match or exceed your best deadlift. I have seen lifters where this is the case. The line of pull is more centered under the body with the Jefferson than a conventional deadlift. Also, the Jefferson is a great training lift. I add it into my “pulling rotation” at least once every 6 weeks.","As urban populations continue to grow, entrepreneurs are going beyond traditional farming to find new ways to feed everyone while minimising the effect on our land and water resources. Vertical farming is one such method that has been used all around the world. Food crops may be conveniently farmed in urban settings using Vertical Farming by planting in vertically stacked layers to conserve space and require little energy and water for irrigation.\nVertical farming is the process of producing crops in layers that are vertically stacked. Controlled-environment agriculture, which tries to maximise plant development, and soil-less farming techniques such as hydroponics, aquaponics, and aeroponics, are frequently used.\nBuildings, shipping containers, tunnels, and abandoned mine shafts are among popular structures used to host vertical farming systems. There are approximately 30 hectares (74 acres) of functioning vertical farms around the globe as of 2020. Vertical farming, in conjunction with other cutting-edge technology such as customised LED lighting, has resulted in crop yields that are more than ten times greater than those obtained by standard agricultural methods.\nVertical farming is still in its early stages in India, but there are a few entrepreneurs and agri-tech enterprises aiming to revolutionise the area.\nVertical Farming Background and Concept\nGilbert Ellis Bailey originated the phrase “vertical farming” and published a book named “Vertical Farming” in 1915. William Frederick Gerick pioneered hydroponics at the University of California, Berkeley, in the early 1930s.\nke Olsson, a Swedish ecological farmer, devised a spiral-shaped rail system for growing plants in the 1980s and proposed vertical farming as a method of raising vegetables in cities.\nProfessor Dickson Despommier invented the concept of vertical farming in 1999. His idea was to grow food in urban areas, utilising less distance and saving time in transporting food produced in rural regions to cities.\nHe aimed to produce food in urban areas in order to have fresher goods available sooner and at a reduced cost. As a result, vertical farming is defined as the cultivation and production of crops/plants in vertically stacked layers and vertically inclined surfaces.\nThe plants are vertically piled in a tower-like form in the physical arrangement. This reduces the amount of space needed to cultivate plants. Following that, a combination of natural and artificial lighting is employed to ensure an ideal atmosphere for the plants’ effective growth. The third component is the plant’s growth medium. Aeroponic, hydroponic, or aquaponic growth media are employed instead of soil as the growing medium.\nAs the methodology gets more scientific, the process’s efficiency grows, and as a result, vertical farming becomes more sustainable, consuming 95 percent less water than previous agricultural methods.\nAlso Read, Oxagon: The World’s First Floating City in the World\nVertical Farming Techniques\nIt is a method of producing food in water without the use of soil by employing mineral fertiliser solutions.\nThe primary benefit of this strategy is that it lowers soil-related cultivation issues such as soil-borne insects, pests, and illnesses.\nAeroponics was inspired by NASA’s (National Aeronautical and Space Administration, USA) endeavour in the 1990s to develop an effective technique to grow plants in space. There is no growth medium in aeroponics, hence there are no containers for growing crops. Instead of water, mist or nutrient solutions are utilised in aeroponics. Because the plants are attached to a support and the roots are sprayed with nutritional solution, there is very little space, very little water, and no soil required.\nThe name aquaponics is derived from the combination of two words: aquaculture (fish farming) and hydroponics (the process of growing plants without soil in order to develop symbiotic interactions between the plants and the fish). The symbiosis is established by feeding nutrient-rich waste from fish tanks to hydroponic production beds called “fertigate.”\nIn turn, the hydroponic beds act as biofilters, removing gases, acids, and chemicals from the water, such as ammonia, nitrates, and phosphates. Furthermore, the gravel beds serve as a home for nitrifying bacteria, which aid in nutrient cycling and water filtering. As a result, the newly cleansed water may be recirculated back into the fish tanks.\nThe Benefits of Vertical Farming\nVertical farming offers various advantages, making it promising for agriculture’s future. The land need is fairly minimal, water usage is 80% less, water is recycled and stored, pesticides are not used, and in the case of high-tech farms, there is no true reliance on the weather.\nA vertical farm makes farming possible within the constraints of a metropolis. When the farms are close by, the food is delivered swiftly and is always fresh, as opposed to the chilled stuff commonly seen in stores. Transportation reduction minimises the cost of fossil fuels and the accompanying emissions, as well as transportation spoilage. Vertical farming, like anything else, has its limitations. The biggest issue is the initial capital expenses for building the vertical farming system.\nThere are further expenditures associated with building the structures as well as their automation, such as computerised and monitoring systems, remote control systems and software, automated racking and stacking systems, programmable LED lighting systems, temperature control systems, and so on."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:7601f478-4df8-4750-8606-e4fd3be0b019>","<urn:uuid:38abc93f-c15d-442d-abe3-4bcfbd141419>"],"error":null}
{"question":"Looking for nutrient comparison between chia seeds and quinoa - protein content & nutritional value for health benefits","answer":"Chia seeds contain 4.69g of protein per 28.35g (one ounce), along with high levels of omega-3, omega-6, fiber, and folic acid. They're rich in phosphorus (244mg per ounce), calcium, iron, and other minerals. Quinoa is a complete protein containing all nine essential amino acids and provides 6g of protein per cooked cup. It's also high in manganese, magnesium, folate, phosphorus, and lysine. Both foods are particularly nutritious, with chia seeds supporting heart health, bone formation, and blood sugar control, while quinoa's nutrients may help with migraines, diabetes, and atherosclerosis.","context":["|Chia seeds Quick Facts|\n|Scientific Name:||Salvia hispanica|\n|Origin||Native to Central & Southern Mexico and Guatemala|\n|Colors||Black, brown, white (Seed)|\n|Shapes||Tiny, oval; Diameter: 1 mm (Seed)|\n|Major nutrients||Phosphorus (34.86%)\n|Health benefits||Heart health, Form bones, Level of sugar, Treat arthritis, Form hemoglobin|\n|More facts about Chia seeds|\nIt is an erect, low growing or branched herb which grows upto 1.75 meters (5.7 feet) in height. Leaves are green, pointed, ovate, opposite; 4-8 cm (1.6-3.1 inches) long and 3-5 cm (1.2-2 inches) wide. Flowers are hermaphrodite and purple or white in color. Seeds are tiny, oval, smooth and 1 mm in diameter.\nChia is native to Central & Southern Mexico and Guatemala. In early 3500 B.C., it was used by Aztecs and became a cash crop of Mexico in between 1500-900 B.C. It was harvested by Toltec and Teotihuacan civilizations in between 2600-2000 B.C in the Mexico Valley. Chia was used by pre Columbian civilizations as raw material to produce medicines. Aztecs use chia as a food or is mixed with other foods, water, ground into flour, beverage and pressed to make oil. In Mexico, Chia survived for 500 years in regional areas. Australia is regarded as the high producer of Chia in the world. Today, chia is cultivated by The Chia company in Australia.\nIn 16th century, it was cultivated in pre Columbian times by the Aztec. The economic historians consider it as a vital a food crop. In 21 Aztec provincial states, it was provided as annual tribute to the rulers. The chia seed (whole or ground) is used in Bolivia, Mexico, Paraguay, Argentina and Guatemala for food and nutritious drinks.\n28.35 grams of dried seeds covers 138 calories, 244 mg of phosphorus, 0.772 of manganese, 0.262 mg of copper, 15.6 µg of selenium, 0.124 g of tryptophan, 2.19 mg of iron, 9.8 g of dietary fiber, 8.71 g of total fat, 95 mg of manganese, 179 mg of calcium, 2.503 mg of niacin, 0.176 mg of thiamin, 0.227 g of isoleucine, 0.269 g of valine, 0.151 g of histidine, 1.3 mg of zinc, 0.201 g threonine, 0.389 g of leucine, 4.69 g of protein, 11.94 g of carbohydrate and 0.275 g of lysine.\nHealth Benefits of Chia seeds\nChia seeds are consumed by adding it to protein shakes, cereal, smoothies, yogurt and salads. It is slightly nutty and mild flavor. It is mixed with water which forms likes a clear gel. Moreover, in nutrition it has high content of amino acids and protein. In addition, the chia seeds are used to make breads and are rich in omega-3, protein, fiber, omega-6 and folic acid. It promotes the skin health, heart health, lowers signs of aging, digestive system and builds strong bones. The studies have also shown that it has the ability to heal diabetes.\n- Heart health\nChia seeds are rich in omega-3 fatty acids which help to maintain the heart health as well as cognitive health. The research conducted on mice shows that the supplements of omega-3 polyunsaturated fatty acid help to have better, localizatory and spatial memory. (1)\n- Form bones\nPhosphorus has a vital role in maintaining the teeth and bone health. It assists calcium for forming strong bones. It enhances the gum health and the enamel of tooth. It alleviate the ailments such a loss of mineral density and bone loss which is also called osteoporosis. It promotes the overall health and the functions. The adequate intake of phosphorus is associated with healthy heart and cardiovascular diseases. (2)\n- Level of sugar\nManganese helps to control the level of blood sugar in the blood of humans. It also prevents the chances of diabetes. It controls the sugar level in the blood, normalize secretion and synthesis of insulin as well as the uncertain drops of sugar. (3)\n- Treat arthritis\nCopper is associated with anti-inflammatory properties that helps to reduce the arthritis symptoms. The arthritis patients use the copper bracelets with a belief that it will help in curing this ailments. Copper is stored in the water overnight and should consume in the morning after waking up. It raise the metabolism for the proper functions. (4)\n- Form hemoglobin\nIron provides the shade of dark red to the blood and assists to transport oxygen to the body cells. The body requires extra hemoglobin because the blood is lost due to internal and external injuries. Mostly women lose blood during menstruation due to which they are at high chances of suffering from anemia. (5) (6)\n- Treat cramps\nMagnesium treats the kidney stress, back muscles and muscular tension. It assists in the calcium absorption that assists the bone healing faster. Its deficiency leads to the symptoms of leg cramps and general fatigue. The adequate intake of magnesium helps to treat the chronic leg cramps. (7)\n- Skin health\nThe studies show that zinc aids acne and pimples. Zinc helps to promote the function of white blood cells which assist the healing process and prevent the body from infections such as ulcers, canker sores, surgical incisions, burns and wounds. It is essential for collagen production that is essential for the regrowth and repair of skin. (8)\n- Cure eczema\nEczema is a chronic skin disorder which is caused due to the zinc deficiency in body. Zinc is vital for the treating the infections and supports the restoring ability for healing properly. The balance of zinc in the blood helps to alleviate the irritation. (9)\n- Prostate health\nZinc helps to maintain the prostate health. The deficiency of zinc enlarges the prostate gland and sensitive to cancer. The prostate disorder patients should take 15 mg of Zinc daily under the medical observation. The studies show that the adequate presence of zinc in the blood helps to lower the growth of tumor. (10)\n- Pregnant women\nZinc is vital for repairing of DNA and its functions. It is vital for the cell growth and builds the cell constituents during pregnancy. During pregnancy various enzymatic and development activity so zinc is essential for the mothers and infants. (11)\n- Blood Thinning\nIt contains high content of Omega-3 fatty acids due to which it acts as a blood thinner. Those who take the blood thinners or are scheduled for surgery should not consume Chia seeds. The excessive use of blood thinners result in excessive bleeding which might be harmful to the health.\n- Drop in Blood Pressure\nThe high blood pressure patients should consult doctor before using chia seeds. It possess the properties which helps to lower the diastolic blood pressure.\n- Allergic Reactions\nChia seeds might cause an allergic reaction such as rashes, watery eyes and hives. It also results in severe symptoms such as problem in breathing, vomiting, tongue swelling and diarrhea.\nTips for serving Chia Seeds\nChia seeds possess mild nutty taste and usually used for garnishing dishes. Some tips are provided below:\n1. Chia pudding\nChia seeds and coconut milk are great for making pudding for breakfast. Combine ingredients a night before and put it in fridge. The next day it becomes a special treat.\n2. Debloating beverage\nChia seeds should be mixed with water and consume it by chasing it with water.\n3. Blueberry chia muffins\nBlueberry muffins is the chia filled recipe which is baked into batter.\n4. Antioxidant berry smoothie\nIt could be tossed into the morning smoothie. It contains high content of Vitamin C and fiber.\n5. Cherry Chia Seed Pudding\n6. Chocolate Pistachio Chia Shake\n7. Banana Chia Seed Cake\nUse tapioca flour, almond flour and flaxseed meal as a base for cake. Eggs and banana helps to bind everything whereas coconut milk, vanilla extract, coconut oil and honey provides a sweet flavor. It could be topped with butter or add cream cheese for frosting.\nFresh ginger and Serrano chile provides spicy kick to this smoothie. The mixture of pineapple juice and chia seeds helps to thicken mixture. It could be put in freezer for about 10 minutes before serving .\n9. Healthy Gluten-Free Chia Chicken Tenders\nGround flaxseed, chia seeds, herbs and Parmesan cheese helps to bread the gluten free chicken. Then, it should be baked.\n10. Gluten-Free Macaroons with Raspberry Chia Seed Jam\n11. Chia Breakfast Pudding\nChia seeds should be soaked in the vanilla almond milk the whole night. Then combine it and use the fresh nuts and fruits as toppings. Keep it in fridge for about five days.\n12. Sticky Chia Brown Rice with Fried Egg\nAdd chia seeds to the fried egg dish and simple sticky rice.\n13. Chia Fresca Drink\nThis tropical beverage is loaded with omega-3 fatty acids as well as potassium. Just use coconut water, chia seeds and pineapple juice to keep it simple. But a drizzle of maple syrup could be added for sweet.\n14. Chocolate and Coconut Chia Seed Mousse\nGrind Chia seeds for a smoother and mousse texture. Mix ground chia seeds, coconut butter, coconut milk, sweeteners and cacao powder. Then leve it for twenty four hours to get thicken.\n- The Chia seeds oil has high amount of Omega-3, 9 and 6 fatty acids.\n- The seeds are used as a thirst quencher or survival food.\n- It helps to reduce LDL cholesterol, triglycerides and raise HDL cholesterol.\n- It also possesses anti-cancer properties.\n- Seed are considered as digestive, febrifuge, disinfectant and ophthalmic.\n- The infusion made from Chia seeds helps to treat fevers.\n- A seed poultice is applied to infections.\n- The chewing of seeds provides strength in long journeys.\n- Seeds help to clean eyes and eliminate foreign particles from eyes.\n- The allergic reactions such as rash, facial swelling, watery eyes, hives, rashes, dizziness and shortness of breath might be experienced.\n- One should be conscious of the side effects before consuming it.\n- The people allergic to Chia seeds should stay away from Chia seeds.\n- The excessive use of Chia seeds provokes side effects.\nHow to Eat\n- The seeds are soaked in water and are flavored with cooling drink and fruit juices.\n- The seeds (gelled) are used to make pudding or gruel.\n- The seeds are sprouted consumed by adding it in salads, soups, sandwiches, stews etc.\n- The grounded seeds are added to meals, biscuits, bread, cakes etc.\n- Chia seeds are also consumed raw or added to dishes.\n- Grounded chia seeds are used in pinole.\n- The gelled seeds are used in pudding or porridge.\n- The leaves are used as teas or nutritious drink.\n- Chia seeds are sprinkled over yogurt, smoothies, cereal and oatmeal.\n- It is added to juices, drinks, milk shakes and protein shakes.\n- It is added to omelets and salads.\n- Chia plant is considered as the member of mint family which is originated from Central Valley of Mexico and is a member of the mint family.\n- Before 3500 B.C, Chia seeds were used as a source of food.\n- Chia fresca is a Mexican drink which is made by soaking Chia seeds in water till gelatinous and adds sugar as well as lime or lemon juice.\n- It helps to hydrate body.\n- Chia seeds are rich in antioxidants and could be stored for long time periods as it does not get spoil quickly.","Quinoa On A Renal Diet\nLearning more about the variety of starches out on the market is invaluable. Quinoa, pronounced (Keen – wah) is a versatile product that is actually a seed. Looking at it for a renal diet, you can see that it has many of the desired properties we want in a meal.\n1 cup dry quinoa, has 626 calories, 24 gm protein, 109.1 gm of carbohydrate, and 12 gm fiber. You should realize that it expands to 4 times it’s size when cooked! So, 1 cup dry = 4 cups cooked quinoa! Wow, that really makes for a full stomach on just a little bit of food. You could eat 1 cup quinoa for 160 calories, 6 gm protein, 28 gm carbohydrate and 3 gm of fiber. That is a healthy side dish, and you don’t even need to add meat! As a person on a renal diet, if you need to restrict protein, you can easily eat this and add some low potassium vegetables to the meal and you are set for a filling dish that didn’t go over your needs but filled you up. And you can feed this to your whole family.\nA Simple Introduction To Quinoa\nThis is a food that is both old and new; as an ancient staple and a rediscovered foodie favorite. If you have noticed quinoa at all, you know what everyone is saying about it. Can it be that quinoa is the perfect food for you and your family? Could quinoa be the one new ingredient you try this year that changes the way you cook? Quinoa is all that and a powerhouse of nutrition, flavor and texture. Let’s take a brief look at this super-food and why it is quickly becoming all the rage.\nWhat is Quinoa?\nThose who are not yet familiar with quinoa, may think of it as a grain, or at least a replacement for grains. The little tiny disc is actually a seed of a plant in the same family as beets, chard, and spinach. These nutritious, amino acid rich seeds are light and fluffy when cooked, with a little snap to it. You’ll also find quinoa in a variety of beautiful colors such as gold, red, and even black.\nWhile relatively new to the US market, quinoa has been cultivated in Peru, Chile, and Bolivia for over 50 centuries (that’s right, I said centuries!) and is a staple food in their diets. The Incas considered quinoa a sacred food and referred to it as the “mother seed,” which is why we often refer to it as “Gold of the Incas.”\nWhen Spanish conquistadors were trying to gain control of the South American indigenous people, they destroyed the fields in which quinoa was grown, and outlawed the farming and sale of quinoa. In1980, two Americans re-discovered the health and nutrition potential of quinoa and started cultivation in Colorado. Today, quinoa is finding its way into homes and restaurants all over the map.\nFor being such a tiny little seed, quinoa is a complete protein, containing all nine essential amino acids. Quinoa is also a great source of manganese, magnesium, folate, phosphorus, and lysine, which is essential for tissue growth and repair. The minerals contained in relatively high amounts may also be especially helpful for those people who have consistent migraines, diabetes, and atherosclerosis.\nOne of the most highly valued aspects of quinoa for many people is it is gluten free. Those looking for alternatives for wheat and other gluten foods can turn to quinoa in several forms to replace the gluten in their diets.\nHow to Eat Quinoa\nThe quinoa seeds are naturally covered by a saponin residue that is bitter to the palate. This is one defense mechanism the plant has to fend off the occasional passing critter that wants a snack. While commercial cultivation processes remove much of the saponin that coats the seed, it is still a good idea to rinse the seeds in cold water to make sure the process is complete. However, there are many brands that are pre-rinsed. You can use your own judgment.\nOnce you have the quinoa rinsed, bring a pot of one part quinoa and two parts water to a boil, cover, and simmer slowly for fifteen minutes, or until the the water is absorbed and the quinoa is tender. If you want to keep more of the natural nutty flavor, you can dry roast the seeds before cooking them. Put the quinoa in a skillet over medium heat and toss, just until the quinoa becomes fragrant.\nNow you have a multitude of options for preparing your quinoa. Whether you like it hot or cold, you can put it into a salad or in a soup. You can also form your cooked quinoa into patties with a variety of ingredients. Go ahead and add cooked quinoa to your favorite pancake or muffin recipe for a brand-new take on healthy eating.\nNo matter if you are looking for a gluten free alternative to grains, or you are a person on a renal diet who is just looking for something new to experiment with, or you want to give your nutrition a real boost, give quinoa a try. With all the possibilities, you could easily create a quinoa creation for every day of the year!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:264cec69-b49a-4c3a-955a-bc65ed04342a>","<urn:uuid:acf9ef1f-2532-450c-94b2-682d3d88094c>"],"error":null}
{"question":"How can someone avoid being negatively affected by others' emotions in support groups?","answer":"In a well-run support group, negative emotional contagion can be prevented by having an attentive and careful facilitator. Additionally, research suggests that with proper training, people can learn to tune how much they let others' emotions affect them, allowing them to care without being consumed by others' feelings.","context":["Sharing others Feelings\nOne reason sometimes given for not being interested in attending a support group or even in talking with others who have cancer is that their worries and feelings will be contagious. That is, people sometimes worry that they will leave a group feeling worse than they did before they came. In a well-run group, that won't happen as the facilitator will be attentive and careful.\nThanks to Allison for sending on this article that confirms that this is real worry and gives some excellent tips to avoid that outcome:\nHow sharing other people’s feelings can make you sick\nEveryone says we need more empathy – but too much of it can burn you out. Mind training lessons from monks and psychopaths could help us care without crumbling\nBy Emma Young\nTANIA SINGER wasn’t the first person to put a Buddhist monk in an fMRI machine. But the neuroscientists who had scanned supposedly caring, sharing brains before did it to find out where empathy comes from. Singer was looking for ways to avoid it.\nFew people would argue that the world is cursed with an excess of empathy. But we are starting to discover that our capacity to share other’s emotions and take their perspective comes with a sting in its tail. Overdosing on the misfortunes of others is not just a problem for those in high-exposure professions such as nursing. All of us are vulnerable to catching the pain of others, making us angrier, unhappier, and possibly even sicker.\nFortunately, work on locating the root of empathy in the brain has also led to the discovery that with the right training, we might be able to tune how much we let others’ emotions affect us. This could allow us the best of both worlds – to care, without letting it consume us.\nEmpathy is undeniably a good thing. Understanding how others are feeling is a bonding mechanism that we are finding in an increasing number of animals, including dolphins and rats. In humans, primatologist Frans de Waal of Emory University in Atlanta, Georgia, has suggested that being affected by another’s emotional state was the earliest step in our evolution as a collaborative species.\nBut the pitfalls will be apparent to anyone who has been in a room full of babies. If one starts crying, pretty soon, they’re all at it. Babies don’t understand the difference between their own emotions and those being felt by others, and so what one feels, they all feel. Negative and positive emotions alike spread like a virus. As our sense of self develops, we learn to distinguish other people’s emotions from our own, although a variety of experiments, most recently studying our behaviour in online social networks, indicate we are not entirely free of the risk of emotional contagion (see “Socially contagious“).\nThat’s because the distinction between what we and others feel isn’t terribly clear to our brains. Singer, then at University College London (UCL), and her colleagues demonstrated this in 2004 when they put 16 romantic couples into an MRI scanner. When they gave the volunteers a painful electrical shock, this elicited activity in brain regions known to respond to physical pain and also in regions tuned to emotional pain. But when volunteers saw their loved one get a shock, no activity registered in their physical pain centres – while the emotion regions lit up like fireworks. Notable among these was the anterior insula, where a lot of the coordination between brain and body takes place.\nSince then, many other studies have confirmed that this “empathy for pain” network exists, and that it doesn’t distinguish whether the pain you’re observing is physical or psychological. “The basic principle is the same,” says Singer, who is now at the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany.\nWhat’s more, over the past few years it has become apparent that we don’t just catch pain from those we are intimate with. The first hints came from people in care-giving professions who often see the stress and pain of others, such as hospice staff, nurses, psychotherapists and paediatricians. Since the early 1990s, a kind of empathy burnout has increasingly been documented – given names including “secondary traumatic stress” and “vicarious traumatisation”. Symptoms include lowered ability to feel empathy and sympathy, increased anger and anxiety, and more absenteeism (see “The hurt locker”). Various studies link these symptoms with an indifferent attitude to patients, depersonalisation and poorer care.\nIt’s perhaps unsurprising that empathy burnout can affect people frequently surrounded by other people’s pain. But a recent spate of experiments suggests that the dark side of empathy spells trouble for everyone. You can “catch” stress any time you understand someone else’s pain and share in it, activating your empathy for pain network.\nRead more: https://www.newscientist.com/article/mg23030732-900-how-sharing-other-peoples-feelings-can-make-you-sick/"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:9b27ca0b-e3fa-42f1-b2c7-6917e3661a4f>"],"error":null}
{"question":"How do cold weather outdoor activities impact safety considerations compared to wet weather camping preparations?","answer":"For cold weather outdoor activities, specific safety protocols include dressing in layers with wool, silk or polypropylene materials, avoiding overexertion which can strain the heart, and covering exposed skin with hats, gloves, and scarves. When engaging in recreation like hiking or skiing, it's crucial to notify others of your location and carry emergency supplies including dry clothing, two-wave radio, and fire starters. For wet weather camping, preparation focuses on keeping gear and shelter dry through proper tent setup with tarps, using ground sheets, and maintaining separation between tent fly and inner tent. Both scenarios require vigilant monitoring - in cold weather, watch for signs of frostbite and hypothermia like shivering and pale skin, while in wet conditions, monitor for water pooling and condensation that could compromise shelter and gear.","context":["When you are in the process of planning an epic camping trip, do you envisage dark skies and rainy days? Not likely.\nSometimes, like it or not, you end up camping in the rain. That doesn’t mean it has to be a miserable experience. Experienced campers accept the fact that the weather can change in the blink of an eye, and that the eager promises of sun kissed days made by charismatic weather reporters are sometimes broken. Experienced campers are prepared for the weather to turn, so that they can turn what might be a horrible experience, into one that doesn't suck.\nThis article aims to give you a few tips to help you on your next not-so-sunny camping adventure.\n1. Setting up your tent in the rain\nTarps are worth their weight\nIf you are unlucky enough to arrive at your campsite when it’s raining, setting up your tent is going to be a lot easier if you first build some temporary shelter. If you have arrived by car you can use that as one anchor point for your tarpaulin. Don’t forget to bring rope and a couple of extra poles for anchoring the corners. You can use trees or even your extendable hiking poles to secure the tarp as you lay out your tent keeping it dry. Make sure that the roof of your tarp slopes toward either side of your tent allowing water to run off gently without pooling on top. Putting in the work now will cause you less stress in the future.\nPick your camping spot carefully\nIf it’s already raining, sometimes all you want to do is start getting pegs in the ground and be done with it. Stop. Take time to first think about where you are going to position your tent. Are you near a water source? How close to other tents are you, and what does their set up look like? Are you uphill or downhill from other campers? Ask yourself questions such as these to determine how likely it is that your tent will end up bearing the brunt of a downpour.\nDiscover more: advice on how to care for your wetsuit\nLay a ground sheet underneath and in front of your tent\nWhile modern tent floors are designed to keep the water out, a bit of extra material between your sleeping space and the floor can only help. Just make sure that the edge of the ground sheet is not protruding around the tent as it can funnel water beneath your tent making things worse.\nSet up your tent fly\n…and make it tight, but not too tight. Keep your guy-ropes evenly spaced so as to keep tension uniform around the tent. Ensure that they are tight enough to create space between your tent fly and the inner tent body. If the two touch, moisture will wick on the tent body and slowly drip, drip onto you and your gear. But be careful not to make your guy-ropes too tight as this can pull the fly down and create same effect. And remember to angle those tent pegs away from the tent so that if the wind picks up, it doesn't take your fly with it.\nLearn more: the contacts you need during bushfire season\nIf your tent has vents, open them. Windows won’t work for obvious reasons, but some tents have raised vents that allow water to run away freely while improving ventilation. We all know how humid it can get when it rains here in Australia – condensation can be your worst enemy. Air it out and stay drier for it.\n2. Keeping your gear dry while camping in the rain\nEven if it rains, you shouldn’t have to worry about wet camping gear. Not only is wet gear a damper on the spirits (cough) it could be bad for your health, and end your camping trip early.\nStoring your camping gear\nThe key to a dry tent is making sure that the inner tent body doesn’t touch the walls of your tent fly. In the same way that a tent fly pulled too tight can cause the walls to touch, gear rested against the inside walls of your tent is going to create the same effect.\nConsider less clothing\nIf it’s warm enough, you might even consider removing any clothes that you want to remain dry for later. Throw on a pair of board shorts, thongs and a singlet while setting up and packing down your tent, and get to work knowing that you can always get dry and warm up later in a fresh set of clothes.\nThere are a number of ways to keep your gear dry while camping in the rain. For the ultimate protection, take a specially designed dry bag with you on your adventures to store valuables and a spare set of dry clothes. A hard plastic protective case is a great choice for keeping your electronic equipment such as cameras, phones, and laptops safe from the rain.\nInvaluable on any trip. Use them to store anything from food and clothes to your entire pack if things get really bad.\nQuick dry towels\nModern synthetic fibres are excellent at absorbing moisture and drying later. Take a few quick dry towels with you so that you can not only dry yourself after washing, but can also mop up any puddles that form inside your tent. If your tent has an awning / storage area at the front door lined with tarpaulin, it’s important to keep this area dry too: another job for the quick dry towel.\n3. Cooking on your wet camping trip\nUsing a portable camping stove indoors is an accident waiting to happen. If it’s raining, those snags might have to wait. Instead, have a backup meal planned that doesn’t require cooking: think cold meats, salads, and cheeses. However, if you have the time and space, you can transform your camping area into a stunning al fresco dining space .\nThat’s where your tarp and poles come in. To camp and dine in style and comfort, you’re going to need to bring some extra gear. Ideally you will need two tarps: one to create a roof over your tent, and another to create dining space out front. Make sure your tarp is positioned with plenty of head room, ideally enough space to stand upright without slouching. Position your cooking equipment as far from your tent as possible, and if you are careful, you can still enjoy those snags.\n4. Packing down your wet tent\nWorst case scenario? It’s still raining. In that case you are going to need to carefully disassemble your tent in order prevent it getting wetter than it already is. If your tent clips to the poles (rather than using fabric loops) it’s a good idea to break it down from the inside, out.\nOnce you have stored your gear in plastic bags and you've moved it all to a dry place out of the way, collapse the inner tent body before removing the fly. This way the fly is still held up by the tent poles, doing its job the whole time. Make sure all windows and doors are well sealed at this stage.\nRoll the inner body up loosely and move to a dry place or strap to your pack to dry out later.\nRemove your tent poles quickly and store them in your tent's storage bag. Roll up your tent fly gently and store alongside your tent's body.\nDon’t forget to check your site for tent pegs, mallets, tarps – basically anything that you might have left behind frantically packing down your wet tent.\nIf your tent uses loops rather than clips, you can collapse the whole thing in one go, and still keep it reasonably dry – but the process is a bit different.\nRemove your guy-rope pegs and store in your tent's bag.\nDisconnect the poles at each corner, carefully laying the tent flat before removing them, all the while making sure that the fly is still covering the tent body.\nUsing the fly as protection, you can now either:\na) get underneath the fly and roll up the inner tent body to store them separately;\nb) roll the two together, and separate them later to dry out when the rain stops.\n5. Drying your tent and your tent fly\nAt the first opportunity you need to either hang your tent to dry, or pitch it and allow it to air out. If you don’t dry your tent completely, the next time you take it out and set up you’ll suffer for your laziness. Camping in the rain is one thing; camping in a mouldy, smelly tent sucks.\nCamping in the Rain Checklist\nBefore you head off, make sure you've packed the essentials. We've created a handy checklist saving you the hassle.","Precautions for Cold Weather Health and SafetyContact: Beth Perrine (517) 241-2112Agency: Community Health, Department of\nJanuary 20, 2005\nWinter in Michigan is a celebrated season despite the extreme drops in temperature posing serious risks and hazards. To combat these potential dangers, there are specific guidelines citizens can follow to stay safe and healthy throughout the cold weather months.\nBe extremely careful if you use a wood stove, fireplace or space heater in your home. Always keep a multipurpose, dry chemical fire extinguisher near the area you are heating. Do not burn paper in your fireplace or wood stove and do not leak flue gas indoors. If you are using an indoor gas heater, be sure it is located in a well-ventilated space and only use the type of fuel recommended by the manufacturer.\nRegardless of the type of heating device you are using, be sure that it is up to date and meets all safety standards. Toxic fumes, such as carbon monoxide, from old or faulty heaters can cause unconsciousness or death from lack of oxygen.\nWhile inside, monitor the indoor temperature carefully. Because they lose body heat much faster than adults, infants should never sleep in a cold room. It is also necessary for older adults to take extra home heating precautions, as they tend to have slower metabolisms and therefore make and retain less heat than other adults. If you are caring for an infant or senior citizen, be sure to frequently check that their homes are adequately heated. If heating is not at a safe level, making alternative housing arrangements is recommended.\nWhen the weather is extremely cold, and especially if there are high winds, try to stay indoors. Making trips outside as brief as possible can help to reduce the potential dangers associated with cold weather. To remain healthy and safe this winter, please follow these cold-weather tips while outdoors:\n- Dress warmly and stay dry: Be sure to dress in layers in wind resistant clothing. Wool, silk or polypropylene inner layers will hold more body heat than cotton. If your clothing is wet, go inside as soon as possible. When inside, remove the wet clothing as soon as possible.\n- Avoid exertion: Cold weather can put extra strain on the heart. If you have heart disease or high blood pressure, follow your doctor’s advice about shoveling snow or other hard work in the cold. The body is already working hard to stay warm, so extra work can cause an overload.\n- Cover exposed skin: Always wear a warm hat that covers ears, gloves or mittens that cover the full wrist, and a scarf or ski mask to protect face and neck.\n- Be Safe During Recreation: Notify friends and family where you will be before you go hiking, camping, or skiing. Avoid perspiring or becoming overtired. Be prepared to take emergency shelter. Pack dry clothing, a two-wave radio, waterproof matches and paraffin fire starters with you. Do not use alcohol and other mood altering substances, and avoid caffeinated beverages. Carefully watch for signs of cold-weather health problems. It is important to be aware of any changes in exposed skin during cold weather periods.\nFrostbite and hypothermia are very serious conditions that can be lessoned by early recognition and treatment. Shivering can be a good indicator that it’s time to go in, as it is the first sign that the body is losing heat.\nFrostbitten skin is hard, pale, cold and has no feeling. When the frostbitten skin is in warm air, it will become red and painful. Very severe frostbite can cause blisters, gangrene (blackened dead tissue), and deep tissue damage in tendons, muscles, nerves and bones.\nHypothermia is a life-threatening condition that is caused by short exposure to extreme cold or long exposure to mild cold. Symptoms of hypothermia include trembling, stiffness of muscles, puffiness in the face, poor coordination, confusion, and low consciousness and reactivity.\nIf you suspect frostbite, hypothermia or other complications surrounding extreme weather, seek emergency medical care immediately."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:594a064d-2d6e-4414-90f4-10494a78be94>","<urn:uuid:17dbe6de-62e6-4e03-9ee7-2fc52740350b>"],"error":null}
{"question":"How do the Special Areas of Alberta and Yellowstone National Park compare in terms of their historical establishment reasons?","answer":"Both areas were established for different reasons in different time periods. The Special Areas in Alberta were created in 1938 under the Special Areas Act as a response to hardship brought upon southeastern Alberta during the drought of the 1930s. In contrast, Yellowstone National Park was established much earlier, in 1872, when President Ulysses S. Grant signed it into law as a public pleasure ground and the first national park in the world.","context":["Special Areas Board\nThe Special Areas Board is the governing body of Alberta's special areas. Special areas are designated rural municipalities similar to a municipal district, however, the elected advisory councils are overseen by three representatives appointed by the province, under the direct authority of Alberta Municipal Affairs.\nThe three special areas were created in 1938 under the authority of the Special Areas Act as a result of hardship brought upon a particular area in southeastern Alberta during the drought of the 1930s. A special area is not to be confused with a specialized municipality, which is a completely different municipal status.\nThe Special Areas Act of 1938 created the six special areas of Tilley East, Berry Creek, Sullivan Lake, Sounding Creek, Neutral Hills, and Bow West, which had previously been special municipal areas. In 1939, these six special areas were consolidated into the four special areas listed below. The original six special areas included 3.2 million hectares, while the current three only include 2.1 million hectares.\n- Tilley East Special Area, No. 1: The northern part of this special area was withdrawn and added to Berry Creek-Sullivan Lake Special Area in 1941, and now forms the portion of Special Area No. 2 that is south of the Red Deer River. Tilley East was still a special area in 1955, but was not by 1959. This area is now part of Cypress County, formerly the Municipal District of Cypress No. 1.\n- Berry Creek-Sullivan Lake Special Area, No. 2: The eastern portion of this special area was withdrawn and added to the Sounding Creek-Neutral Hills Special Area in 1939. The northern part of Tilley East Special Area was added to this special area in 1941. It was renamed Special Area No. 2 in 1959.\n- Sounding Creek-Neutral Hills Special Area, No. 3: The eastern portion of the Berry Creek-Sullivan Lake Special Area was added to this special area in 1939. It was renamed Special Area No. 3 in 1959. In 1969, the northern portion of Special Area No. 3 became Special Area No. 4.\n- Bow West Special Area, No. 4: This area was still a special area in 1955, but was not by 1959. It is now part of Vulcan County and the Municipal District of Taber.\nList of special areas\nAlberta's three special areas had a combined population totalling 4,499 in 2011.\n|Change (%)||Area (km²)||Population\n|Special Area No. 2||April 7, 1959||2,025||2,074||−2.4||9,342.42||0.22|\n|Special Area No. 3||April 7, 1959||1,122||1,266||−11.4||6,623.96||0.17|\n|Special Area No. 4||January 1, 1969||1,352||1,389||−2.7||4,403.03||0.31|\n|Total special areas||—||4,499||4,729||−4.9||20,369.41||0.22|\n- List of census divisions of Alberta\n- List of communities in Alberta\n- List of municipal districts in Alberta\n- List of municipalities in Alberta\n- Special Areas Act 1934, a British act also imposed due to hardships\n- Specialized municipalities of Alberta\n- \"Types of Municipalities in Alberta\". Alberta Municipal Affairs. 2008-05-16. Retrieved 2008-12-18.\n- \"About The Special Areas\". Special Areas Board. Retrieved 2009-12-27.\n- \"Special Areas Act\". Alberta Queen's Printer. Retrieved 2009-12-06.\n- Alberta Queens Printer - Special Areas Act\n- The Special Areas Act, 1938. Statutes of the Province of Alberta passed in the fourth session of the eighth legislative assembly. p. 439.\n- The Special Areas Act, 1939. Statutes of the Province of Alberta passed in the seventh session of the eighth legislative assembly. p. 179.\n- Andison, R. A. (July 15, 1941). \"Certain provincial lands withdrawn from the Tilley East Special Area and added to the Berry Creek-Sullivan Lake Special Area\". The Alberta Gazette. Retrieved April 21, 2011.\n- The revised statutes of Alberta, 1955. Volume IV. Chapter 317. An Act respecting Special Areas. p. 467.\n- Andison, R. A. (April 7, 1959). \"An act to amend the Special Areas Act\". The Alberta Gazette. Retrieved April 21, 2011.\n- Andison, R. A. (April 29, 1939). \"The areas of the Berry Creek-Sullivan Lake Special Area and the Sounding Creek-Neutral Hills Special Area, amended\". The Alberta Gazette. Retrieved April 21, 2011.\n- Strom, H. E. (December 31, 1968). \"Lands constituted as Special Area No. 4\". The Alberta Gazette. Retrieved April 21, 2011.\n- \"Population and dwelling counts, for Canada, provinces and territories, and census subdivisions (municipalities), 2011 and 2006 censuses (Alberta)\". Statistics Canada. 2012-02-08. Retrieved 2012-02-14.\n- \"Municipal Profiles (Special Areas)\". Alberta Municipal Affairs. May 31, 2013. Retrieved June 5, 2013.","Yellowstone National Park(noun)\nthe first national park in the United States; located in the border area between Wyoming and Montana and Idaho; spectacular wilderness; famous for Old Faithful geyser and for buffalo and bears\nYellowstone National Park\nYellowstone National Park is a national park located primarily in the U.S. state of Wyoming, although it also extends into Montana and Idaho. It was established by the U.S. Congress and signed into law by President Ulysses S. Grant on March 1, 1872. Yellowstone, widely held to be the first national park in the world, is known for its wildlife and its many geothermal features, especially Old Faithful Geyser, one of the most popular features in the park. It has many types of ecosystems, but the subalpine forest is dominant. Native Americans have lived in the Yellowstone region for at least 11,000 years. The region was bypassed during the Lewis and Clark Expedition in the early 19th century. Aside from visits by mountain men during the early-to-mid-19th century, organized exploration did not begin until the late 1860s. The U.S. Army was commissioned to oversee the park just after its establishment. In 1917, administration of the park was transferred to the National Park Service, which had been created the previous year. Hundreds of structures have been built and are protected for their architectural and historical significance, and researchers have examined more than 1,000 archaeological sites.²\nThe Nuttall Encyclopedia\nYellowstone National Park\na high-lying tract of land in the State of Wyoming (q. v.) traversed by the Yellowstone, about the size of Kent, being a square about 75 m. in diameter; is set apart by Congress as a great pleasure ground in perpetuity for the enjoyment of the people; it abounds in springs and geysers, and care is taken that it be preserved for the public benefit, to the exclusion of all private right or liberty.\nyellowstone national park\nQuotes by yellowstone national park -- Explore a large variety of famous quotes made by yellowstone national park on the Quotes.net website.\nThe numerical value of yellowstone national park in Chaldean Numerology is: 8\nThe numerical value of yellowstone national park in Pythagorean Numerology is: 9\nSample Sentences & Example Usage\nWe still have a bunch of trips scheduled for the next few weeks, so we’re either going to have to cancel or take clients up to Yellowstone National Park and do what we can, we rely on this part of the year to provide the money to keep us going through the winter.\nAs managers of Yellowstone National Park, we balance the preservation of park resources with public safety, our decision takes into account the facts of the case, the goals of the bear management program and the long-term viability of the grizzly bear population as a whole, rather than an individual bear.\nAs managers of Yellowstone National Park, we balance the preservation of park resources with public safety, our decision takes into account the facts of the case, the goals of the bear management program, and the long-term viability of the grizzly bear population as a whole, rather than an individual bear.\nImages & Illustrations of yellowstone national park\nFind a translation for the yellowstone national park definition in other languages:\nSelect another language:\nDiscuss these yellowstone national park definitions with the community:\nWord of the Day\nWould you like us to send you a FREE new word definition delivered to your inbox daily?\nUse the citation below to add this definition to your bibliography:\n\"yellowstone national park.\" Definitions.net. STANDS4 LLC, 2017. Web. 17 Oct. 2017. <http://www.definitions.net/definition/yellowstone national park>."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:29972f67-e63d-4bb3-a03a-53abcde349dd>","<urn:uuid:5679a32f-41f3-4a10-938b-4d1f69fca14a>"],"error":null}
{"question":"What are the key differences between infection-based and allergy-triggered respiratory conditions when it comes to symptom relief patterns and duration?","answer":"In infection-based respiratory conditions like acute bronchitis, symptoms typically develop after having a cold or flu and persist for 10-20 days regardless of environmental factors. However, with allergy-triggered conditions, symptom relief occurs when the person is no longer exposed to the specific trigger (such as pollen, pets, or mold). Both types of conditions can share similar symptoms like congestion and breathing difficulties, but the key distinction lies in their response to environmental changes. Additionally, infection-based conditions may involve fever and typically resolve on their own, while allergy-related symptoms will continue to recur with exposure to triggers unless properly treated through measures like immunotherapy.","context":["Overview of Sinus Drainage\nSinus drainage is also frequently known as acute sinusitis. This occurs when the cavities that is around the nasal passages become swollen and inflamed. This area is also called the sinuses. When this occurs, then there is a problem with drainage and mucus will begin to build up in this area. Sinus drainage is usually caused by a cold or an allergy. The sickness usually goes away within 10 days.\nSymptoms of Sinus Drainage\nThere are a variety of other symptoms that come along with sinus drainage. Some of these symptoms include: fever, headache, pressure in the ear, reduced sense of taste and smell, coughing, bad breath, and fatigue.\nSymptoms that are closely related to sinus drainage are most commonly a thick drainage from the nose or down the throat. This drainage is typically yellow or green. You may also feel congested and have difficulty breathing through your nose. It also seems that there is a lot of pressure and pain in the face. The most common spots for this are in the eyes, cheeks, forehead, or nose. These symptoms will typically be worse if you bend down.\nWhen to See a Doctor with Sinus Drainage\nMost of the time, if sinus drainage is directly related to a cold, then you will not really need to see your physician. If you feel like you struggle with sinus drainage regularly, do not have any improvement in your symptoms, or running a serious fever.\nCauses of Sinus Drainage\nSinus drainage can be classified into two categories. Sinus drainage can be a result of a cold or from allergies. In a cold, a bacterial infection is the case. If the reason is allergies, then the sinuses will swell because they are trying to get rid of the allergen that is present. An allergen can be anything that you have an allergy to, such as pets, foods, pollen, mold, etc. If a cold or allergies cause the sinuses to be inflamed for a long period of time, then there may be a risk of infection.\nWhat does Sinus Drainage Feel Like?\nAllergies and sinusitis can often feel very similar – stuffy nose, runny nose, wheezing, watery eyes, etc. If your sinus drainage is the cause of the cold, then you will not feel relief from this for days. If allergies are to blame for the sinus drainage, then you will experience relief when you are no longer around the trigger.\nRisk Factors for Sinus Drainage\nThere are a variety of risk factors for sinus drainage. People that are more likely to experience sinus drainage are people with a medical condition, such as cystic fibrosis or HIV/AIDS, a deviated septum, or an allergic condition. Sinus drainage is one of the main symptoms that is related to allergies.\nComplications of Sinus Drainage\nSinus drainage is typically not anything serious, unless it lasts for a long time. When sinus drainage is present for more than a week, there is a high risk for infection. Chronic sinusitis is the formal name for sinus drainage that lasts for longer than twelve weeks. Meningitis is the infection that occurs because of the inflammation and fluid around the spinal cord and brain. Since you are not able to properly breathe, then you may notice that you are not able to breathe or taste things very well. In extreme cases, there many infections that spreads to the bones or skin. If infection occurs around the eye, then blindness can occur. Blindness in these cases may be temporary or they may be permanent.\nHow to Prevent Sinus Drainage\nMost people believe that to get rid of or prevent sinus drainage, you must breathe in only dry air. The opposite is true. If you are at risk or currently have sinus drainage, then you will want to be breathing in humid air. This can be done through having a humidifier. If you do not have a humidifier, then you can take a steamy shower or go into the bathroom with the shower running and breathe in the humid air.\nSince sinus drainage is a direct result from allergies and upper respiratory infections, then it is important to stay healthy. If you have been diagnosed with certain allergies, then you will want to follow your physician’s directions on how to minimize symptoms. If you know what triggers your allergies, then avoid them completely. If you are unsure of what your allergies are to, then schedule an appointment with one of our allergists to discover what they are. This is the key step in prevention. If your sinus drainage is typically a result of a respiratory infection, then you will want to minimize your time around people that are sick and wash your hands frequently.\nTobacco smoke is one of the main causes to sinus drainage. If you are currently a smoker, then you will need to quit smoking immediately. If you are constantly around tobacco smoke or air pollution, then limit your time outdoors or around these irritants. These pollutions will irritate and inflame the nasal passages.\nDiagnosis of Sinus Drainage\nYour physician will begin by asking what your symptoms are and what seems to trigger them. The physician will also feel around for any tenderness and look inside of your nose. If symptoms tend to be brought on by allergies, then your physician will recommend allergy testing. Allergy testing will allow you to know what to avoid. Imaging tests, nasal endoscopy, or sinus cultures will all help to see any other underlying conditions that may be the cause of sinus drainage.\nTreatment for Sinus Drainage\nIf your sinus drainage is the result of allergies and your physician is able to pinpoint which allergen causes it, then your physician may recommend immunotherapy for treatment. This will increase your exposure to the allergen, with the goal of getting your body used to the allergen and overtime, will decrease the reaction. There are also many medications that your physician may recommend, such as: pain killers, decongestants, saline spray, antibiotics, or corticosteroids.","Symptoms And Causes Of Bronchitis: What Are the Signs and Symptoms of Bronchitis?\nAfter you already have the flu or a cold acute bronchitis due to an illness generally develops. The main symptom of acute bronchitis is a constant cough, that might last 10 to 20 days. Other symptoms of acute bronchitis include wheezing (a whistling or squeaky sound when you breathe), low fever, and chest tightness or pain. If your acute bronchitis is serious, in addition you may have shortness of breath, particularly with physical action. The signs or symptoms of chronic bronchitis include coughing, wheezing, and chest discomfort.\nChronic Bronchitis Symptoms, Treatment and Contagious\nBronchitis is considered chronic when a cough with mucus continues for at least two years in a row, and at least three months, for most days of the month. Bronchitis occurs when the trachea (windpipe) and the big and small bronchi (airways) within the lungs become inflamed due to disease or annoyance from other causes. Chronic bronchitis and emphysema are types of a condition defined by progressive lung disorder termed chronic obstructive pulmonary disease (COPD).\nBronchitis and asthma are two inflammatory airway illnesses. The illness is called asthmatic bronchitis when and acute bronchitis happen together. Common asthmatic bronchitis triggers include: The symptoms of asthmatic bronchitis are a blend of the symptoms of asthma and bronchitis. You may experience some or all the following symptoms: You might wonder, is asthmatic bronchitis contagious? Yet, persistent asthmatic bronchitis commonly is just not contagious.\nBronchitis is a Familiar Infection Causing Irritation and Inflammation\nIf you suffer from chronic bronchitis, you might be vulnerable to developing more serious lung disorders as well as heart problems and infections, so you should be tracked by a physician. Acute bronchitis is generally due to lung infections, 90% of which are viral in origin. Repeated attacks of acute bronchitis, which weaken and irritate bronchial airways can lead to chronic bronchitis.\nBoth adults and children can get acute bronchitis. Most healthy individuals who get acute bronchitis get better without any issues. After having an upper respiratory tract illness like the flu or a cold frequently someone gets acute bronchitis a day or two. Breathing in things that irritate the bronchial tubes, including smoke can also causes acute bronchitis. The most common symptom of acute bronchitis is a cough that generally is not wet and hacking at first.\nBronchitis (Acute) Symptoms, Treatment, Causes\nWhat's, and what are the causes of acute bronchitis? Acute bronchitis is inflammation of the bronchial tubes, and acute bronchitis is suggested by a cough lasting 5 or more days as a cause. Chronic bronchitis may be developed by people who have persistent acute bronchitis. The most common causes of acute bronchitis are viruses. Bacterial causes of the disorder contain: Other irritants (for instance, tobacco smoking, chemicals, etc.) may irritate the bronchi and cause acute bronchitis.\nCoughing Up Green Mucus Contrary to what many think, mucus secretion is important for the body. This sticky secretion lubricates our respiratory organs and protects their membranes against infectious bacteria, fungi, and other environmental pollutants. An average human...\nYou can Find Two Types of Bronchitis: Acute (Short-Term) and Chronic (Long-Term)\nWhile smokers and people over 45 years of age are most likely to develop chronic bronchitis, babies, young kids, and the elderly have a heightened risk of developing acute bronchitis. Smoking is the most common cause of chronic bronchitis and may also result in acute bronchitis. Treatment for chronic bronchitis includes bronchodilators, anti-inflammatory drugs, and chest physical therapy for loosening mucus in the lungs. Seek prompt medical care if you're being treated for moderate although bronchitis symptoms recur or are persistent.\nBronchitis Symptoms, Treatments & Causes Merck Manuals\nInfectious bronchitis typically begins with the symptoms of a common cold: runny nose, sore throat, fatigue, and chilliness. When bronchitis is intense, fever may be slightly higher at 101 to 102 F (38 to 39 C) and may last for 3 to 5 days, but higher fevers are unusual unless bronchitis is brought on by influenza. Airway hyperreactivity, which is a short term narrowing of the airways with restriction or damage of the quantity of air flowing into and out of the lungs, is common in acute bronchitis. The damage of airflow may be activated by common exposures, such as inhaling light irritants (for instance, cologne, strong scents, or exhaust fumes) or cold atmosphere. Older individuals may have unusual bronchits symptoms, like confusion or accelerated respiration, rather than temperature and cough.\nThe study - led by Cardiff University in the UK - shows for the very first time the calcium-sensing receptor (CaSR) plays a key part in causing the airway disorder. Daniela Riccardi, principal investigator and a professor in Cardiff's School of Biosciences, describes their findings as \"unbelievably exciting,\" because for the first time they've linked airway inflammation - that may be activated for example by cigarette smoke and car fumes - with airway twitchiness. She adds: \"Our paper shows how these triggers release substances that activate CaSR in airway tissue and drive asthma symptoms like airway twitchiness, inflammation, and narrowing.\nWhat Does Chronic Bronchitis Sound Like RECORDING (Wheezing symptoms emphysema Need Help Acute Cough\nAudio Recording of how chronic Bronchitis cough sounds like while laying down. The difference between bronchitis & pneumonia is that bronchitis causes an ...\nProf. Riccardi reasons: The researchers believe their findings about the purpose of CaSR in airway tissue could have significant consequences for other respiratory conditions such as chronic obstructive pulmonary disease (COPD), chronic bronchitis. The researchers, from Washington University School of Medicine in St. Louis, consider their findings will lead to treatments for a variety of diseases including asthma, COPD, cystic fibrosis and even certain cancers."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:6f7cec99-63b8-45ea-b4bc-ad7c80cafaf3>","<urn:uuid:70ac49db-3c3e-4094-a20d-edd719169897>"],"error":null}