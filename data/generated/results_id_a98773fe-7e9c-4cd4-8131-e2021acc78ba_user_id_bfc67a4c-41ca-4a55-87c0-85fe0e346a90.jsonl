{"question":"As a utility company planner, I need to understand how to select optimal locations for energy storage projects. Could you explain the key factors to consider in site selection for battery energy storage systems?","answer":"Strategic site selection for battery energy storage systems (BESS) primarily depends on the target customer and intended services. Developers typically seek locations near existing substations. However, the ideal siting can vary significantly based on timing and location. For instance, a project at a substation in Eastern Texas with a 2023 operation date would differ from one planned for 2026. The changing regulations in the power sector also impact site selection. Additionally, there's a trend toward longer duration storage for projects planned for 2025 or 2026 timeframes as part of the interconnection process.","context":["The POWER Interview: Challenges for the Energy Storage Market\nThe global market for energy storage is experiencing rapid growth, in part because the technology is being included in decarbonization and sustainability programs. It also is being used to support the grid integration of more renewable energy resources, and has been touted as an important piece of electricity transmission and distribution system upgrades.\nEnertis Applus+, headquartered in Madrid, Spain, and with offices in a dozen countries including the U.S., is a global consulting, engineering, and quality control firm in the renewable energy and energy storage solutions sector. The company over the past two decades has put together a portfolio of 250 GW of installed power generation capacity, with 15 kWh of energy storage worldwide.\nMatthew Towery, senior manager of Energy Storage for Enertis Applus+, recently provided POWER with insight into the global market for storage, providing his take on the issues that will impact that market in the coming years.\nPOWER: The current decade has been called “The Decade of Energy Storage.” We’re now three years in; do you think growth in energy storage will continue, and if so, at what pace? If you think growth will slow, why?\nTowery: Yes, the demand for energy storage will continue to grow. According to the BloombergNEF (BNEF) 2H 2022 Energy Storage Market Outlook forecast, energy storage installations are set to reach a cumulative 411 GW (or 1,194 GWh) of capacity at global level by the end of 2030.This is 15 times greater than the storage capacity that was online at the end of 2021 (27 GW or 56 GWh). The U.S. and China are set to remain the largest markets, representing over half of global storage installations worldwide by the end of the decade.\nThe growth targets for energy storage in 2023 and 2024 are exponentially higher than in previous years. These targets are largely driven by the Inflation Reduction Act (IRA) bill. However, in order to meet the targets, supply chain constraints will be tested over the next several years. In addition, independent system operator (ISO) markets will need to continue to find ways to expedite or prioritize interconnection applications to avoid delays in committed projects.\nThe IRA bill unleashed a new demand for storage products and their constituent raw materials. Production of these products is concentrated in only a few countries. However, this should level out in the coming year or two, as more domestic battery manufacturing ramps up, all of which will take time.\nPOWER: What are the current major challenges for the energy storage sector?\nTowery: For projections to become a reality, supply chain constraints and interconnection backlog are the current main challenges. A mix of non-lithium and domestically manufactured lithium batteries will be needed to make the battery supply chain more diversified and resilient.\nPOWER: Are there new storage technologies being developed that will support the industry?\nTowery: Over the past three years, more non-lithium-based technologies (iron flow, zinc, salt, etc.) have emerged and several of them are beginning to scale up, both from a production standpoint and economies-of scale-standpoint. These factors are necessary to compete with lithium-based technologies.\nPOWER: How important is the siting of energy storage projects? Should they be installed at substations, and/or at various points along the power grid?\nTowery: Strategic site selection is very important when planning projects with battery energy storage systems (BESS) and it is very much related to who the target customer will be and the services that the BESS will provide.\nUsually, developers look for sites that are near existing substations. However, as more storage assets are connected to the grid, it ultimately changes how the grid is modeled.\nAnother aspect that needs to be taken into consideration is that regulation in the power sector is undergoing constant changes. Therefore, it is not easy to clearly identify the “perfect siting” for energy storage projects. For example, a project that is sited at a substation in Eastern Texas with a commercial operation date in 2023 is different from a project slated for a commercial operation date in 2026.\nIn addition, we are seeing an increasing number of developers pursue longer duration storage as part of the interconnection process because they know the project will come to fruition in the 2025 or 2026 timeframe.\nPOWER: Will there be rapid growth in energy storage for residential, and for commercial & industrial sites?\nTowery: Due to the amount of backlog in the interconnection process for utility-scale and high voltage transmission projects, we are seeing many developers target the commercial/industrial market for a more expedited project lifecycle.\nPOWER: Will utilities drive growth in energy storage, or will more growth come from private (or governmental) investment in storage projects?\nTowery: Utilities, private companies, and government institutions will all play a key role in fostering energy storage deployment.\nOn one hand, the private companies that are leading the energy storage sector are investing a great amount of capital in research and development to lower the costs of this technology.\nOn the other hand, regulated utilities are constantly evaluating their Integrated Resource Plans (IRP), which will greatly drive the demand for storage. Given the increasing competitiveness of this technology, many utilities are moving away from fossil fuels power plants and switching to renewable energy projects coupled with energy storage. These moves can be through contracted Power Purchase Agreements or Build Own Transfer projects, in which the developer takes the majority of the risk.\nFinally, even though the role of the federal government has been limited to set policy and support framework—such as with the IRA—local government groups, including those at the council level, might identify further opportunities for storage requirements, which could add to the investment coming from the utilities.\n—Darrell Proctor is a senior associate editor for POWER (@POWERmagazine).\nThe post The POWER Interview: Challenges for the Energy Storage Market appeared first on POWER Magazine."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:f76fc334-c554-4906-b3fb-2abecce83bb1>"],"error":null}
{"question":"Looking at implementing best practices for our global expansion: Can you compare how human expertise requirements differ between localization management and disaster recovery planning? I need to understand the staffing implications.","answer":"The human expertise requirements differ significantly between these areas. In localization, the emphasis is on linguistic expertise - the documents stress not trusting developers' linguistic abilities and instead requiring professional linguists and editors with solid language knowledge for content creation and review. In disaster recovery planning, the focus is on having adequate technical staff positioned in geographically dispersed locations. The disaster recovery approach specifically warns against plans that rely on transporting staff during emergencies (like putting people on planes or buses), instead recommending having trained personnel already stationed at alternate locations to maintain critical functions. Both areas require systematic management and dedicated expertise, but localization needs centralized language specialists while disaster recovery requires distributed technical teams.","context":["Localization in any organization doesn’t limit only to translating and adapting source texts and other materials to a target market. A good and also cost-effective localization process starts earlier than in the moment when the developers send texts to translators. It naturally contains all necessary technical steps, but we’ll skip them this time and focus on the translatable texts themselves and what you can do to make them efficient from a localization point-of-view.\nMany of these tips are quite general so they will help you improve your original texts too. They have been written especially with UI, help and support materials in mind – writing for your landing page might be a completely different ballgame but many of the rules apply there too.\nFluent and Correct\nWould you trust a linguist to do code for you? I wouldn’t either (and I’m a linguist). Don’t put too much trust in your developer’s linguistic ability either.\nSoftware and other web content are nowadays mostly written in English, which often is a good choice for localization and translation purposes, as the most translators around have English as one of their source languages. But other languages are used too.\nWhichever your writing language of choice is, make sure the texts are well written. If your own or your developer’s English is not that good, have the texts edited by someone whose knowledge is more solid. “Engineering English” might create misunderstandings for your users, and those misunderstandings will get multiplied in localization.\nStick to the terminology you choose and make everybody who is writing content also aware of it. This also means not using one term for several concepts. This might in many cases leave the translator wondering which of the concepts you’re referring to – and even get yourself in trouble when you want to mention those two concepts in the same sentence. Put a system in place to manage and spread terminology in your organization.\nAt school, you learn that you shouldn’t be overly repetitive when writing, so you don’t make the reader bored. This is mostly true, I don’t want to argue with your elementary school teacher, but when writing software texts, repetition is your best friend. And this goes for basically any help and support texts too – or anything you write with localization in mind. Identify parts of text that appear in many places and don’t rewrite them. The commonly used localization tools identify what has been translated earlier and you can usually have those parts translated automatically. The more similarities the new texts have with old ones, the less their translation costs. Repeated terms and structures are easy for the translators.\nUsing the same terms and structures also makes things easy for your user: they will be more familiar with different parts of the UI and have a more fluent experience.\nDon’t write overly long texts unless you have a true motivation for that. English is often shorter than many other languages, e.g. German translations tend to be around 3o% longer than English texts, so don’t aim at filling all available space already in the English version. Other than that, concise texts are mostly user-friendly, too. You don’t want your users to leave your page because of texts that take ages to read and are difficult to understand, right?\nLocalization cost is almost always directly tied to text length, so this point is also directly linked to your bottom line.\nText in Graphics\nIf you want to make localization easy, embed as little text as possible into graphics. Each time you localize texts that you have as graphics means manual labor, which means more costs. Each time you add a language you need to update all graphics manually.\nFunny slang words can nicely spice up a text, as will idiomatic expressions. And make you sound like someone who really masters the language. But at the same time, they make localization more difficult and might even be nearly impossible to translate. For translators, there are ways to get around that, but you don’t want to make translating more difficult and error-prone. The same goes rid of jargon and metaphors, you need to get rid of them.\nA Final Pointer\nBeing customer centric pays off in localization too: if a text is optimal for a user in your own language area, it probably is easy to localize.\nPhoto by Thomas Lefevbre.","Conducting Business in the Cloud, Part 2\nBest Practices to Ensure that Your Data Is Safe\nAn article taken from the January/February issue of HBMA Billing, by Chris Seib\nRead the first segment of this article in the previous edition of HBMA Billing (www.hbma.org) or in HBMA 'Public' News.\nAs more management companies and medical practices transition their electronic records to private clouds, they risk long-term data outages and other crippling failures that may pose significant threats to their bottom lines. The first article of this series highlighted some of these concerns, detailed best practices for transitioning to a private cloud, and offered tips to use in discussions with vendor partners. This article offers additional tips and best practices, with a focus on disaster recovery, business continuity, and security.\nDisaster RecoveryEven with high degrees of local redundancy in a private cloud data center, you need to prepare for significant disasters with a comprehensive disaster recovery plan. Disaster recovery sites should be in geographically disparate areas. Having a data recovery site in close proximity to the primary site is essentially pointless – but still surprisingly common!\nMany vendors take a very low-cost approach to disaster recovery. They may back up their data offsite, but it would take days or weeks to bring the backups online. The best practice is to have a site exactly like the primary site \"ready to go\" at any time. Many vendors back up their data offsite and contract an IT company for equipment rental in the event of an emergency, which would take days or weeks to receive with no guarantee that it will work. This can greatly affect the recovery time objectives (RTO) and recovery point objectives (RPO).\nRTO: how long it will take to restore services from when a disaster is declared\nRPO: how far back the point of data restore is from when a disaster is declared\nAs a best practice, you should look for an RTO and RPO of a few hours or less. Because this requires a significant investment, many vendors skimp. They have a plan, but it may only be tested once per year – or not at all. When tested, there are often multiple flaws found, and commonly there is little or no action taken (but the vendor can still claim that the plan was tested).\nIt is important to consider the human factor as well. Many vendors have a disaster recovery plan that involves putting people on a plane or bus to go to an offsite location. In the event of a disaster, what are the chances that planes and buses will be operational in the immediate area? As a best practice, it is important to have adequate staff in the alternate locations to operate critical functions.\nTip: Ask your vendors about their disaster recovery plans. How often are they tested? What were the test results? What are the RTO and RPO? Were those objectives met in the most recent test?\nBusiness ContinuityBusiness continuity extends the concept of disaster recovery by ensuring that all business functions, not just IT systems, can remain operational with minimal disruption in the event of disaster.\nWhat are the critical business functions that you rely on from your vendors? Often, it is more than just a website or file server: it involves customer service and other human interaction. As a best practice, vendors should have multiple business locations with adequately trained staff that are capable of handling non-IT related business functions such as customer service. Do not rely on busing or flying staff to an alternate location.\nTip: Ask your vendors about their business continuity plans, specifically if they account for customer service and other critical functions.\nSecuritySecurity breaches can cause significant disruption to your business, either through data leakage (which may have significant HIPAA and HITECH Act implications) or by causing downtime and disruption of services. It is important that your vendors take a robust and comprehensive approach to security threat management. Require multiple layers of security, a robust security policy, proactive monitoring, alerting, and independent auditor verification.\nMultiple Layers of Security\nBest practices include both host-based and network-based anti-virus, anti-malware, intrusion detection and prevention, integrity-monitoring network firewalls, and application firewalls configured in an active, online state. This means that security components will \"take action\" to block or prevent attacks before they happen, and not just send an alert to indicate a problem.\nYour vendors (and you) should have a written security policy outlining all aspects of the security program. Trained security personnel need to review and update this at least once a year. This should also include a regular security risk assessment.\nBilling services also should have a designated security officer. This is often not the case, and security is more of an afterthought of the IT department.\nIndependent Auditor Verification\nDo not take your vendor's word for it – ask how they prove their security with independent audits. The Electronic Network Healthcare Accreditation Committee (EHNAC) is a good start, but it does not cover security in a detailed manner. In addition to EHNAC, look for a Payment Card Industry (PCI) \"Data Security Standards Level One\" audit performed by a PCI-approved Qualified Security Assessor, an SSAE16 Type II audit, and regular external and internal vulnerability detection by third parties. As a rule, make sure your vendors are certified by a third party and not just \"compliant\" through self-attestation. There is a difference!\nGet It in Writing\nIf your vendors are down for days or weeks, the costs to your billing service will be serious. It is crucial to ensure that your vendors are able to offer \"true availability\" for the services they provide. Many vendors claim to have availability and disaster recovery, but they take shortcuts to save money, resulting in single points of failure and poor disaster recovery. Vendors should commit to these things in their contracts and publish these commitments on their websites.\nAs businesses in all industries transition to the cloud, it is crucial to ensure that your data will be safe when disaster strikes. I encourage all types of businesses to use these best practices and tips as a checklist when discussing disaster recovery and security with current or potential vendor partners. Leveraging the cloud can significantly enhance the way you conduct business, but you must first take these precautions to protect your business and yourself.\nChris Seib is the co-founder and CTO of InstaMed, the leading Healthcare Payments Network. Prior to InstaMed, Chris was an executive in Accenture's Health and Life Sciences practice, focused on architecting and delivering portal and connectivity solutions. Additionally, Chris has managed multi-project initiatives such as eCommerce development, software application development, and operations. Chris has certifications and expertise in programming, architecture, Microsoft technologies, database technologies, networks, network architecture, security, and project management."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:375fffdd-7fb7-4d56-9155-a7425da7eba6>","<urn:uuid:34717d65-75a4-4322-8ec4-ccf038cfb02d>"],"error":null}
{"question":"How do the soil sampling methods compare between the Veluwe experimental site and the Weber Farm study site?","answer":"At the Veluwe site, soil sampling involved collecting approximately 4kg of soil from the top 10cm by pooling 6-10 individual soil cores, which were then sieved over a 4mm sieve and kept at 4°C for experimental use. At the Weber Farm site, soil sampling was conducted through excavated soil pits in representative slope positions, where profiles were described, photographed and sampled using standard NRCS field methods, focusing on analyzing particle-size distribution and organic carbon.","context":["Steering the soil microbiome by repeated litter addition\nCite this dataset\nVeen, Ciska; ten Hooven, Freddy; Weser, Carolin; Hannula, Emilia (2021). Steering the soil microbiome by repeated litter addition [Dataset]. Dryad. https://doi.org/10.5061/dryad.jwstqjq8d\nTo test our hypotheses, we set up two controlled experiments. Soil and litter samples used in all experiments were collected from a long-term field site on the Veluwe, the Netherlands (Hannula et al., 2017; Kardol, Bezemer, & van der Putten, 2006; Veen, Keiser, van der Putten, & Wardle, 2018) situated between Ede (52°04′20″N, 5°44′12″) and Wolfheze (52°00′77″ N, 5°48′58″). We sampled soils from six independent locations within the Veluwe field site. All locations were situated on sandy soils. Mean annual temperature was around 10.7°C and mean annual precipitation approximately 840 mm (Veen et al., 2018) (Royal Netherlands Meteorological Institute (KNMI)). Each location consisted of a semi-natural grassland and a surrounding broad-leaved forest (Veen et al., 2018).\nIn the fall of 2016, we collected soils (8th of December) and litter (3-27 October depending on timing of litter fall) from three grass species Agrotis capillaris, Festuca rubra, and Holcus lanatus and three tree species Betula pendula, Fagus sylvatica, and Quercus robur at each site. For the grass species we sampled soils within monoculture patches, for the tree species we sampled soils immediately underneath adult trees. For each plant species at each location we collected ~4 kg of soil from the top 10 cm by pooling ~6-10 individual soil cores. Soils were sieved over a 4-mm sieve. Soils were kept at 4°C until set up of the experiment. Litter, i.e., recently senesced biomass, was collected as a bulk sample from locations where the plant species were highly abundant. Litter was cut into 1-cm fragments and sterilized by gamma-irradiation (25 KGray). Using the soil and litter samples, we set up (i) a reciprocal transplant experiment where soils were incubated with all litter types during three subsequent periods of three month and (ii) we used the incubated soils in a subsequent litter decomposition experiment where soils were confronted with original litter (i.e., litter type as in the field) or with the new litter (i.e., litter type used during the incubation in experiment (i)).\n- Reciprocal litter incubation experiment\nOn 20 December 2016, we set up a full-factorial reciprocal litter transplant experiment with soils from the six replicated field locations. We filled microcosms with 240 g equivalent of dry weight soil. For each plant species at each location we set up seven microcosms which were incubated with 2 g of air-dried-sterilized plant litter from each plant species included in the experiment, according to a full-factorial design; and one mesocosm did not receive any litter (no-litter control). This resulted in a total of 6 replicates × 6 plant species × 7 litter treatments (i.e., 6 litter types, one no-litter control) = 252 microcosms (Fig. 1). Litter addition to the same pots was repeated after three months and after six months (Fig. 1). At each litter addition, litter and soil were gently homogenised; the amount of litter added was similar to average rates of litter fall in temperate ecosystems (Penuelas et al., 2007). At each 3-month litter incubation period we also added 1 g of litter in a nylon mesh bag (mesh size 0.9 × 1.0 mm), which was inserted in the soil, in order to calculate litter mass loss. Microcosms were incubated in the dark at 60% water holding capacity (WHC), 20°C and 80% air humidity. Microcosms were organized to a randomized block design, with each replicated site considered as a block. After each three-month litter incubation period, litter bags were harvested, cleaned and dried at 60°C to measure litter mass loss. Microcosms were weighed and watered to maintain WHC every two weeks. In addition, a soil subsample was collected at the start of the experiment and after each three-month incubation period to measure soil abiotic and biotic conditions (details under “Soil and litter measurements”). At the start of the experiment subsamples of the litter were oven-dried, to be able to correct mass loss calculations for the amount of moisture still present in air-dried litter.\n- Litter decomposition feedback experiment\nAt the end of the reciprocal litter transplant experiment, i.e., after nine months of incubation, we collected a soil subsample from each microcosm from experiment (i) to set up a litter decomposition experiment. Each soil sample was split into two subsamples, used to set up two new microcosms using 50 ml falcon tubes: one microcosm was incubated with the litter type from the plant species where the soil originated from in the field, the other microcosm was incubated with the litter type that the soil had been incubated with during the reciprocal transplant experiment (Fig. 1). Each microcosm received the equivalent of 0.50 g of dry soil and 0.50 g of dry litter (Keiser et al., 2011). This resulted in 252 soil subsamples × 2 litter types (i.e., the historical field litter type and the current incubation litter type; for the no-litter control samples we only incubated with the historical field litter) = 468 microcosms. We used small amounts of soil in this experiment in order to inoculate the soil microbiome, while minimizing effects of soil physical and chemical conditions on litter breakdown (Keiser et al., 2011). Microcosms were incubated in the dark at 20°C, 60% water holding capacity and 80% air humidity for three months and then freeze-dried to determine litter mass loss.\nSoil and litter measurements\nAt the start of the experiment we measured initial soil and litter chemical properties from all soil and litter types. A soil subsample was dried at 105°C for 24 hours to determine soil moisture content. Soil organic matter content was determined by loss-on-ignition in a muffle furnace (550°C, 4 hr). We determined pH in fresh soil samples with a Mettler Toledo pH meter after shaking the equivalent of 10 g dry weight soil in 25 ml of demi-water for 2 hr at 250 RPM. Inorganic nitrogen content (N-NOx and N-NH4+) were determined with an autoanalyzer (Quaatro, Seal Analytical, Norderstedt, Germany) after shaking the dry weight equivalent of 10 g soil in 50 ml 1M KCl (2 hr, 250 RPM). Soil inorganic nitrogen content was determined again after 9 months of litter incubation, i.e., at the end of experiment (i). A soil subsample was dried at 40°C and ground and used to determine total soil C and N content with an element analyser (Flash 2000, Thermo Fisher Scientific, Bremen, Germany). Soil P availability was measured as P-Olsen and measured with an AutoAnalyzer (Quaatro, Seal Analytical, Norderstedt, Germany) (Olsen, 1954). Litter C and N content was determined with an element analyser (Flash 2000, Thermo Fisher Scientific, Bremen, Germany). Litter P content was determined by digestion with a 2.5% potassium persulfate solution. The obtained extract was measured colorimetrically with an AutoAnalyzer (Quaatro, Seal Analytical, Norderstedt, Germany) (Murphy & Riley, 1962). We determined lignin content using methanol–chloroform extractions and hydrolysis (Rowland & Roberts, 1994).\nBefore the analysis of the reciprocal litter incubation experiment (i) we standardized litter mass loss values to 90-day periods, in order to correct for differences in litter incubation length (range between 90-94 days). We then used a general linear mixed model to determine how soil source, litter type and experimental period affected litter mass loss. Site (1|site), experimental period and mesocosm (period|mesocosm) were used a random factors to control for the experimental set up (sites as replicated blocks in field and greenhouse) and for repeated measures, respectively. We tested the effect of soil source, litter type and experimental period on home-field advantage effects (expressed as the percentage of additional decomposition at home; ADH) using a general linear mixed model with site and experimental period as random factors (period|site). For the first experimental period, we tested how home-field effects differed between plant functional groups using a general linear model with transplant type (i.e., transplants between two grass species, two tree species or a grass and a tree species) as a fixed factor and site (1|site) as a random factor. Data from the litter feedback experiment (ii) were analysed from two different perspectives. First, we tested how litter incubation history (from experiment i) affected the mass loss and HFA of the original litter type, allowing us to analyse whether microbial lost affinity and thus HFA for the original litter type. We used general linear mixed models with mass loss and HFA as respective response variables, litter incubation history and litter type as fixed factors and site (1|site) as a random factor. Second, we tested how field history affected the mass loss of the incubation litter type, allowing us to analyse whether microbial communities developed affinity and thus HFA for the incubation litter type. We used general linear mixed models with mass loss and HFA as respective response variables, field history and litter type as fixed factors and site (1|site) as a random factor. We were not able to perform one full model for this experiment, because as a result of logistic constraints we did not include all full-factorial reciprocal transplants in this experiment (see Fig. 1 for set up). For all analysis we used post hoc Tukey HSD tests to test which treatments differed from each other. We explored for a normal distribution of residuals using QQ-plots and a Shapiro-Wilk test and homogeneity of variances using a Levene’s test. All data were analysed in R version 3.6 (Team, 2013) using the lme4 (Bates & Maechler, 2009) and lmerTest package (Kuznetsova et al., 2013) package.\nFor this publication we have uploaded:\n1. a file containing the data\n2. a READ ME file supporting the data file\nDutch Research Council , Award: 863.14.013\nDutch Research Council, Award: 863.14.013","Weber Farm Site is located in southern Dunn County, Wisconsin near the\nconfluence of the Chippewa and Red Cedar rivers. The purpose of this website is to present the results of\nresearch designed to: determine\nthe genesis of soils within the study area and to provide the landowner\nwith recommendations for suitable land uses based on soil knowledge.\nstudy site is located in a landscape position that includes a bedrock\ncontrolled upland, adjacent concave lowland reentrants to the north and\nsouth, and the intervening steep, bedrock-controlled escarpment. The\nlandscape position of the study site with respect to these landforms is\nwidely observed in Dunn County. Variability\nof soil characteristics and the processes and factors that produced these\ncharacteristics have received little scientific attention, according to\nNRCS personnel responsible for updating the Dunn County soils map,\nnatural, and interpretive data.\nsoil pits were excavated within the study site in representative slope and\nslope aspect positions in an attempt to capture the variability inherent\nin the soil continua across this complex landscape position.\nSoil profiles exposed in these pits were described, photographed\nand sampled using standard NRCS field methods. Samples were analyzed\nin the laboratory for particle-size distribution and organic carbon\nLike much of rural Dunn County, the study area has been used for cultivated crop production more-or-less continuously since the late 1800s. Soils in the study area have been forming since the end of the Ice Age (about 13,000 years ago) in loess-derived silty parent material (silty glacioeolian deposits) and sandy parent materials weathered from the underlying sandstone bedrock. Other soil forming factors such as climate (humid, continental) and natural vegetation (a mixture of oak savanna and mixed deciduous and coniferous forests) and organisms are uniform across the study area. The soil forming factor that varies most across the site is landscape position and slope. In this regards, the study area is divisible into three roughly equivalent areas: upland, steeply sloping, and lowland portions.\nexhibit considerable variation across the study area. This\nvariability is a reflection of the complex topography. Soils on the\nupland and steeply sloping portions of the study site, mapped as UeD2 (Urne-Norden\nElkmound loam, 12-20% slopes, eroded and HfC2 (Hixton loam 6-12% slopes,\neroded) exhibit: thin solums with weakly developed horizons and structure\nover sandstone bedrock, silty to sandy textures derived from loess\n(glacially derived eolian sediments of unknown, but presumably late\nWisconsinan age) and the underlying weathered sandstone bedrock (Ironton\nMember, Wonowoc Formation, a late Cambrian sandstone).\nSolum thinning and simplified horizonation is especially apparent\non steep slope shoulder positions adjacent to the upland portion of the\nstudy site. These\nsoils are consistent with the Inceptisol order.\nSoils formed in the adjacent concave lowlands are typically thicker but otherwise exhibit weak horizonation and structural development similar to the soils on the upland portion of the study site. Evidence of recent deposition in one profile excavated in this portion of the study area includes a buried A-horizon overlain by bedded very fine to fine sands (and charcoal). These soils are also consistent with the Inceptisol order.\nThe most significant soil forming factor and consideration with regard to land use in the study area is slope. The sandy and silty texture soils in the study area are extremely susceptible to wind and water erosion when the stabilizing protection of vegetation cover is removed. This is especially true in steeply sloping portions of the study area. Soils in these portions of the study area already exhibit characteristics that are the direct result of soil erosion. They are thin and sandy (due to the incorporation of sandy material derived from sandstone bedrock below them). Much of the rich, fertile loess-derived parent material has been removed from this portion of the study area. Soils in lower positions in the study area are thickened suggesting material eroded off adjacent uplands is, at least in part, being stored lower on the landscape. In at least one case, redeposition of silty and sandy material eroded from upslope was rapid enough to bury a preexisting soil.\nSeveral lines of evidence suggest the severe soil erosion characteristic of much of the study area occurred recently, perhaps since the introduction of Euro American agricultural practices. The presence of strongly developed Bt-horizons at depth in some upland settings indicate a substantial period of landscape stability, and soil formation, occurred during post-glacial time. The weakly expressed horizonation above these horizons, and across the entire study area for that matter, suggests this extended period of landscape stability and soil formation has only recently been interrupted.\nWe recommend that future land use of the study area mitigate for soil erosion. Soils in the study area, though already affected by soil erosion, remain moderately fertile and suitable for cultivation. Soils are thin so there is little likelihood of developing the study area as a \"borrow pit\", although sandstone bedrock could be easily reached with heavy machinery, if quarry activity was deemed economically viable. Though thin, they can support some construction and can be used for a variety of earthen fill. However, great care during any land use activity that removes or inhibits the establishment of vegetation should be taken. Soil erosion control practices such as zero-tillage and contour plowing should be implemented if cultivation is to continue (at least sustainably). Silt fences and soil berms should be in place during any construction. Room for vegetated buffer strips should be left if the study area is to be used as a building site. Soils at the site are best suited to \"low impact\" activities such as pasture or recreation areas. Even if used for these purposes, care must be taken to control foot, animal, and vehicle traffic, especially on the steeper portions of the study area. Any such activity that removes stabilizing vegetation will result in soil erosion. Both soil erosion by wind (blowing and deflation) and soil erosion by water (sheetwash and gullying) is to be expected if the protection of stabilizing vegetation is removed and these soils are exposed.\nSoil Forming Factors"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:23aac39f-4231-4a6b-b718-4b5edfa802e5>","<urn:uuid:0798de58-78d8-4319-9a74-c1a1198e1cc8>"],"error":null}
{"question":"Which saves more on energy bills annually: upgrading to energy-efficient windows or improving loft insulation in a typical home? I'm curious about the cost benefits! 🤔","answer":"According to the data, upgrading to energy-efficient windows can save up to £450 on household bills annually, while increasing loft insulation from 100mm to 270mm provides savings of between £150 and £300 per year, depending on fuel type and property size. Therefore, energy-efficient windows typically offer greater annual cost savings.","context":["Considering the carbon footprint and wider sustainability of the farmhouse and farm office can pay surprising financial dividends.\nTo get an idea of the most pressing areas for action, you can commission a domestic (farmhouse) or commercial (office) Energy Performance Certificate, which will identify which measures to prioritise. In many farm businesses energy used in the farmhouse can account for a significant amount of the total energy used on the farm. Especially in older properties, energy use in the farmhouse can be very high and there can be lots of areas where heat (and energy) escapes.\nThe first priority should be insulation, as stopping heat escaping is the best step to reducing energy use and therefore will save money on heating bills. Just adding sufficient insulation to a property can save at least 15% on annual energy bills. The Farm Energy Centre estimate that increasing the level of loft insulation from 100mm to 270mm will give savings of between £150 and £300 per annum depending on fuel type and the size of the property.\nCavity walls can be insulated with injected foam, beads or fibre. Savings can be between £100 and £200 per year. Solid walls can be insulated with dry lining.\nExcluding draughts can be one of the cheapest and most efficient ways to save energy (and money) in any type of building. 20% of heat loss can be avoided by draught-proofing windows and doors.\nOther options to consider in terms of improving efficiency in the farmhouse are listed below:\n- Water heating – including the right tariffs, lagging pipes, and looking at types of heating\n- Domestic appliances and IT equipment – don’t leave things on standby\nFor more information on the items above, the Farm Energy Centre has produced a great info sheet which looks at these options in more detail. Download it here.\nIf you are on a water meter, also consider installing rainwater recycling (greywater) methods which capture and re-use otherwise wasted rainwater. Buildings under construction present the ideal opportunity to install a RWH system – or it can be added to existing structures. A system may involve diverting drainage to a tank or reservoir to collect water and if necessary, could include treatment to improve water quality for specific uses. Distribution pumps and pipes may also be required to supply the water to where it is needed on-farm. There is a RWH system to suit all needs and budgets. It could simply be created by diverting roof gutters into a storage tank or water butt, or it could be more complex involving pumped storage, filters and UV treatment for use on ready-to-eat crops.\nFor more information on installing a rainwater harvesting system on-farm, why not read the Environment Agency’s guide, which has all the information needed about permits, water quality and how to install. Access it here.\nMaterials are an important consideration, for example what is the embodied carbon associated with a particular material and are they recycled or reclaimed alternatives? Architectural salvage yards are good sources of cheap reclaimed materials for example.\nConsider using chemical and Volatile Organic Compound (VOC) free paints – these are less damaging to the environment and to human health, but cost roughly the same amount. Every year chemical paints emit millions of tonnes of CO2.\nEquipment such as furniture and technology have ‘carbon costs’ as well – anything new will have taken resources to create, so bear this in mind when considering furnishings and equipment and again consider reusing items. Locally produced items will have a lower carbon footprint, so buy local.\nFurther sources of information\nThe Farm Energy Centre – a wide range of practical advice for farmers and growers focussing on energy efficiency.\nRainwater harvesting – the SWARM Hub website features a section on installing a Rainwater Harvesting System on-farm, access it here.\nFarming Futures factsheet on energy efficiency\nThe Farming Futures website also has a library of case studies that feature farmers who have implemented rainwater harvesting systems on-farm.","Energy Efficient Windows\nYou’ve paid for your heating – it’s the job of your double glazing to keep the heat in your home. Improvements in energy saving technology is arguably one of the most significant developments over the past few years, not just for the environment but for home comfort and money savings, too.\nUp to a third of a home’s heat is lost through single glazing, and on average upgrading to energy efficient windows can save around 20% in energy use. Thermally efficient windows are at least double glazed, with low emissivity glass, and an inert and non-toxic insulating gas like Argon between the panes.\nCombined with energy efficient frames, new double glazed windows can achieve up to an A+12 Window Energy Rating (WER), and triple glazing can achieve up to A+32. Upgrading your windows could save you up to £450 on household bills every year.\nThe British Fenestration Rating Council (BFRC) created the WER rating (Window Energy Rating) in order to simplify how effective a window is. The rating system offers grades from A-G (with A being the best). If the window is rated higher than an A it will be followed by a number, and anything above an A10 becomes A+. In October 2015, the BFRC introduced the A++ rating for windows that achieve higher than A+20. The highest rating currently available within the UK is A+32.\nThe WER rating is made up of three key elements: U value (low heat loss), solar gain and air loss through ventilation.\n1. Thermal efficiency (U value)\nA window’s U value is the technical way to measure heat loss through a type of building material, such as a brick wall or tiled roof. U values are calculated by the equation W/m2k, which measures heat loss in watts (W) per square metre of material, when the temperature (k) outside is at least one degree lower.\nThe lower the U value, the better the insulation provided by the material. Single glazed windows have a typical U value of 4.8-5.6W/m²K, which means that around 5 watts of heat are lost per hour, for every square metre of window. Modern double glazing can achieve a U value as low as 1.4W/m²K, and triple glazing is even more energy efficient, reaching 0.6W/m²K. This is lower than the U value of an external wall!\n2. Solar gain (G value)\nSolar gain is another factor that can boost a window’s energy rating. Not only are windows now designed to stop heat escaping, but they can also let heat in by capturing the sun’s rays. The idea is to capitalise on solar radiation as a natural or ‘passive’ form of heating, reducing the dependence on carbon energy to heat your home in the winter months.\nThe factors that influence the solar factor are the number of panes, the type of gas between the panes, and also the type of coatings added to the windows, as they dictate whether the glass absorbs or reflects the heat. It is measured by the G value, on a scale between 0 and 1, with a high number indicating high solar gain.\nContrary to what you might think, solar gain windows do not always lead to overheating – the UK has a relatively cool climate and relatively little sunshine, so overheating is rarely a problem! For rooms where sun streams in during the afternoon, it could be worth considering upgrading to triple glazing – the level of solar gain is actually less than double glazing, due to the extra pane of glass and cavity.\n3. Air leakage (L value)\nAir leakage occurs when there is a weak point around the window frame, such as the seals. Most modern windows are fully airtight, and should have an air leakage factor or L-value of zero (0.00W/m²K).\nAir leakage is not to be mistaken with ventilation. Ventilation is a controlled system, letting in small amounts of fresh air to reduce stuffiness and improve air quality, whereas air leakage will compromise the energy efficiency of your window.\nSound insulating windows\nUpgrading to double or even triple glazed windows also has the benefits of improving sound insulation in your home – this means a quieter and more relaxing living space, free from the noises of traffic from roads and flight paths.\nThe same factors that increase a window’s energy efficiency also work for noise cancellation: more panes of glass, Argon gas to fill the cavities and airtight seals work together to give you more peace and quiet! You can even get ‘silent sealed units’ with double-thickness glass that can cut noise down by up to 36 decibels.\nCan’t install double glazing in your property? If you’re restricted by conservation planning rules, secondary glazing can also help to reduce the noise from outside filtering into your home."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:4e9a7c0d-7e4c-4c49-bada-518d2e6229b2>","<urn:uuid:0ef65dc6-ff5d-45e2-8b7b-123ee630e6ce>"],"error":null}
{"question":"Need advice on building adjustable shelving and organic pest control. What's the deal with pin-type shelves, and how do beneficial insects help?","answer":"For pin-type adjustable shelves, you'll need to precisely lay out hole locations by clamping the sides together before marking and use a sharp bit to avoid chipping. For units taller than 4-5 feet, include fixed shelves to prevent side pieces from bowing out. As for pest control, beneficial insects are excellent natural controllers - lacewings larvae can consume over 100 pests daily, feeding on aphids, mites, and thrips. Syrphid flies, praying mantids, spiders, and braconid wasps also help control pests. You can attract these beneficial insects by planting wild bergamot, sweet alyssum, and English lavender, along with plants from the carrot and mint families that have clusters of tiny flowers for easy nectar access.","context":["Add beauty and usability to your wall space with homemade shelving units. Here’s a step-by-step guide to lead you through a project.\nCourtesy of Better Homes and Gardens®\nAdd functional wall shelving with this project. And you’ll not only add a pretty focal point, you’ll also have a wonderful conversational piece when you explain to visitors that you didn’t buy it, you made it.\nAny shelving system project emphasizes the importance of careful measurement. Many of the components are of varying sizes. Middle shelves usually are slightly shorter than the bottom and top pieces. Be sure to measure and then check your measurements … and then measure one more time.\nHere's how to build a shelf unit.\n1. Cut the outside pieces\nFor each outside piece, cut a miter on one end. Be sure the saw is set accurately to a 45-degree bevel; test with scrap pieces to be sure. Use a tablesaw or a radial-arm saw, or hold a speed square firmly against a factory edge as you cut with a circular saw. Measure from outside to outside -- from the tip of one cut to the tip of the next.\n2. Cut the dadoes\nSet the two vertical outside pieces side by side and mark them for the dadoes. Set the depth of your saw blade so it cuts one-third of the way through the board. Make a series of cuts (see Making a Half-Lap or Dado Joint, Related Projects). Clean the dadoes out by prying remaining wood out with a chisel, then smoothing the bottom with the chisel held bevel side down.\n3. Measure for the shelves\nTemporarily fasten the box together by drilling pilot holes and partially driving (tacking) finishing nails at each corner. Or, use a strap clamp (see Gluing and Clamping, Related Projects). Check the box for square. Measure from the inside of each dado to inside of the corresponding dado to determine the length of each shelf. Cut the shelves to length.\n4. Assemble the pieces\nDisassemble the box. Apply glue and drive in the nails that hold one side piece to the top and bottom pieces. Carefully position these fastened pieces so the side piece is lying on a flat surface. Dry-fit the shelves into the dadoes and set the remaining side piece in place. Disassemble and make any needed adjustments. Apply glue, check for square, and nail.\n5. Add the back\nCut a piece of 1/4-inch plywood or 1/8-inch hardboard for the back. It should be 1/4 inch smaller than outside dimensions of the unit, so the backing edge is set back 1/8 inch. Use the back as a check to see that the unit is square. Leaving a 1/8-inch gap on all edges, fasten with 4-penny box nails every 4 inches. Fasten back to inner shelves as well as the perimeter.\nChoose between adjustable and fixed shelving\nWhen building a shelf system using adjustable standards and clips and a central vertical support, the adjustable shelves must be a smaller width than the outside pieces. For a cleaner look, set the metal standards in grooves.\nTo make pin-type adjustable shelves, precisely lay out the locations of the holes on the side pieces by clamping the sides together before marking. Use a sharp bit (hollow-point bits work well) that will not chip the surface of the wood as you bore the holes. If a pin-type adjustable shelf unit is taller than 4 to 5 feet, it should have one or more fixed shelves to keep the side pieces from bowing out. You can add an angle bracket or wooden cleat to which you can attach the fixed shelf.\nThe simplest unit has shelves screwed in place without dadoes. Such a unit is quite strong, as long as you use three or more screws at each joint and the screws are fastened firmly. Also, use pilot holes so the wood doesn't split. Countersink the screw heads, and fill the holes with putty or plugs (see Drilling, Related Projects). If you use trim head screws, the holes will not be much larger than those for finishing nails. To make such shelves even stronger, add cleats.\nDadoed shelves are stronger and present a clean, finished look because there is no hardware to hide. (See Steps 1-5 for how to build this unit.)\nA cleat-supported shelf is simple to build and ideal for utility areas. Use 1x2s for the cleats. Cut the front edge of the cleat at a 45-degree angle so it's not as noticeable. Secure the cleat with countersunk screws.","Concern over toxic pesticides’ affect on soil and human health is a top reason city gardeners consider organic methods of pest management. Organic pest management not only eschews synthetic pesticides, but controls pests naturally. By using integrated pest management (IPM) techniques, city gardeners can manage pests in a way that minimizes the impact on human health and the environment and uses chemicals only as a last resort.\nThe basic principles of IPM apply to organic city gardening but add restrictions to pesticides city gardeners can use. Organic pesticides come from natural sources like plants, fungi and bacteria. One of the primary advantages of organic pesticides is their lower residual toxicity. Most of them break down more quickly than synthetics, which can last for years.\nA list of products approved for certified-organic production is maintained by the Organic Materials Review Institute (OMRI). Ideally, organic pesticides will also have low toxicity for mammals and other vertebrates, but that’s not always the case. Pesticides, organic or not, must always be used in strict accordance with label requirements. The IPM principles of careful timing and targeted application are critical with all pesticides.\nAlso keep in mind that a natural source does not always mean that a pesticide is benign. Some organic pesticides disrupt beneficial insects, and should be avoided. Attracting beneficial insects is actually one way to keep pests at bay.\nLacewings, known for their delicate, transparent wings, bear larvae that feed on common garden pests like aphids, mites and thrips. These larvae can consume more than 100 pests each day. City gardeners can attract them with wild bergamot, sweet alyssum and English lavender. Syrphid flies produce larvae that feed on aphids and other pests as well. Praying mantids, spiders and braconid wasps are also beneficial insects in organic gardens.\nAllot 5 or 10 percent of your city garden to plants that attract beneficial insects that prey on pests. These rules vary from state to state, but in general, keep your garden as diversified as possible to attract varying types of beneficial insects.\nMany beneficial insects relish plants such as those in the carrot and mint families, with clusters of tiny flowers that allow easy access to the nectar inside. They also dine on pollen, so they seek flat flowers, like those in the daisy family, which provide quick access to the pollen-filled centers. Flowers native to your area will also attract hardy beneficial insects. Check with your cooperative extension office to discover native flowers that thrive where you live.\nSome plants, such as chamomile, nasturtiums and marigolds, naturally repel certain pests. Nematodes shy away from marigolds because of their roots’ slowly released natural chemicals. It’s also a good idea to intersperse your vegetable crops with companion plants like parsley, flowering dill, fern-leaf yarrow and coriander to encourage even more beneficial insects to visit.\nHealthy soil produces vigorous plants that are less likely to have problems with pests. However, if your crops are attacked by pests, try organic methods like companion planting or crop rotation. At the end of the growing season, rotate crops among beds. The pests that thrive on those plants and over-winter in the soil will find their food gone when they re-emerge.\nThis article originally ran in Hobby Farms Presents: Orcharding."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:193ece57-92f2-432a-8a5f-49ac5fbd0cdc>","<urn:uuid:6abcb788-a5e6-4716-9039-611c60255851>"],"error":null}
{"question":"What are the chemical hazards in coffee processing facilities, and what key economic factors determine vertical farming viability?","answer":"In coffee processing facilities, workers are exposed to diacetyl and 2,3-pentanedione, two chemicals that can cause deadly lung disease bronchiolitis obliterans. The CDC found levels exceeding safety recommendations of 5 parts per billion over an eight-hour workday, with concentrations reaching as high as 7,000 parts per billion in storage bins. For vertical farming viability, key economic factors include energy costs, labor efficiency, and location-based operating expenses. Pre-existing facilities must support high energy loads, and layout must optimize worker efficiency to reduce labor costs. Location choices significantly impact profitability - while urban locations are possible, positioning near food distribution centers reduces costs and improves logistics. Success is higher in regions where conventional agriculture faces challenges, such as water-scarce areas or locations with extreme climates, and where fresh produce is typically expensive or low quality.","context":["Federal investigators are conducting tests at about a dozen facilities across the country in the wake of a probe that exposed how a naturally occurring chemical endangers coffee workers.\nMILWAUKEE — A warning for coffee workers in roasting factories and corner cafes across the country: Keep your face away from the bins of roasted beans.\nThat’s the latest word from the U.S. Centers for Disease Control and Prevention (CDC), which is examining the health hazards faced by some 600,000 people nationwide who spend their days roasting, grinding, packaging and serving coffee.\nThe agency is conducting tests at about a dozen facilities across the country in the wake of a Milwaukee Journal Sentinel investigation that exposed how a naturally occurring chemical endangers coffee workers. The agency’s first test results, from a midsize roasting facility in Wisconsin, found extremely high levels of two lung-destroying chemicals in the roasting bins.\nInvestigators with the National Institute of Occupational Safety and Health (NIOSH), a research arm of the CDC, spent several days at Madison-based Just Coffee in July. Investigators tested personal air space and took air samples to measure the concentration of the chemicals diacetyl and 2,3-pentanedione. They also evaluated the company’s ventilation and other systems.\nMost Read Nation & World Stories\n- Serial killer who took 10 women's lives executed in Florida\n- Officials fighting U.S. measles outbreaks threaten to use rare air-travel ban\n- Parents forget newborn baby in Hamburg taxi\n- Harriet Tubman is already appearing on $20 bills whether Trump officials like it or not\n- 3 dead, state capital battered as storms rake Missouri VIEW\nDiacetyl has been tied to the deadly lung disease bronchiolitis obliterans, more commonly known as “popcorn lung” for its association with the many illnesses suffered by microwave-popcorn workers in the early 2000s. The chemical’s molecular cousin, 2,3-pentanedione, has shown equal toxicity in animal studies.\nBoth chemicals have been made synthetically to give a buttery flavor to all kinds of foods and beverages and have been deemed safe to ingest in trace amounts. Inhaling the compounds, however — whether natural or synthetic — can prove deadly.\nThe two chemicals form when coffee beans are roasted, then are released into the air in greater concentrations when the beans are ground. Levels also build up as the beans “off gas” in the storage bins.\nNIOSH Director John Howard said the issue is a priority for the agency. “There’s a large number of workers, and the harm is really severe,” he said.\nLast year, the Journal Sentinel hired an industrial hygienist to sample the air in the Just Coffee plant and another Wisconsin roastery. Both agreed to allow the news organization to test for the chemicals.\nResults showed levels at both facilities exceeded the government’s safety recommendations, in some cases by nearly four times. Executives at Just Coffee then asked NIOSH do a full health-hazard evaluation to get a better idea of the scope of the problem and understand how to protect workers.\nNIOSH researchers found levels in three breathing-zone samples that exceeded the safety levels recommended by the CDC.\nThe CDC has proposed that workers not be exposed to more than 5 parts per billion of diacetyl over an eight-hour workday, during a 40-hour workweek. Results showed one Just Coffee roaster was exposed to more than 7 parts per billion.\nResearchers noted typical exposure levels could be worse, depending on the time of the year. That’s because the testing was done when two large bay doors on each end of the production space were open and accessory fans were being used to increase air flow. Had the bay doors at Just Coffee been closed, the level of contaminants could have been higher.\nScientists familiar with diacetyl and 2,3-pentanedione say workers’ exposures to short blasts of high levels are also of significant concern. NIOSH found diacetyl concentrations inside the bins where beans are stored reached as high as 7,000 parts per billion. Thus the warning for workers to avoid sticking their heads in or hovering over the containers.\nRachel Bailey, a medical officer in NIOSH’s respiratory-health division, noted the results were the first among a dozen studies of coffee-processing facilities being done nationwide. Future results will focus more on specific tasks workers complete and look for links between medical data and exposure sources.","Vertical Farming: Location a Key Factor to Success, Says IDTechEx\nVertical farming, the practice of growing crops indoors on vertically stacked layers, has received no small amount of interest over the last few years. Vertical farms commonly tout impressive numbers, such as using 95% less water and providing crop yields 20-30 times that of conventional agriculture. These claims, among many others, have seen many vertical farming start-ups being founded alongside large amounts of industry funding; funding for the industry reached a record high in 2021, with over US$1 billion being raised across the entire industry. The recent IDTechEx report, \"Vertical Farming 2022-2032\", details the economic and technological factors shaping this rapidly growing industry.\nWith crops being grown indoors under controlled environments, a selling point used by multiple vertical farms is that they can grow crops anywhere – even in the heart of a city. This has led to proponents of the industry envisioning \"smart cities\", where vertical farms in city skyscrapers help feed the urban population. While this is achievable in principle, the truth is that the choice of location for vertical farming is much more involved and intricate than it may appear from these claims alone. Choosing an ideal location can be one of the most important factors in determining the success of a vertical farm.\nSome vertical farms may choose to set up their facilities in pre-existing facilities, such as abandoned warehouses. In these cases, identifying the suitability of the venue is the first point of consideration: vertical farms are very energy intensive, and it is important to ensure the facilities chosen can support these energy loads. In addition, the ergonomics of the facility is also important; should the layout not be given proper consideration, this can impede workers and decrease worker efficiency. As labor costs are typically among the largest sources of expenditure for a vertical farm, improving labor efficiency to reduce these costs is of paramount importance.\nWhile growing crops in the center of a city may seem ideal, the reality is that this may be counterproductive. Obtaining and maintaining such a location is expensive and can contribute significantly to the operating expenditure of a vertical farm while presenting logistical challenges in distributing produce; the \"last mile\" of food distribution is often the hardest. Having a farm right next to the consumers themselves may also be less ideal than instead choosing a location near food distribution centers, as this allows for more efficient delivery of produce. As distribution centers are typically located on the outskirts of cities, the cost of land is also much cheaper. This is the approach chosen by UK-based Jones Food Company, which chose Scunthorpe as a location for its vertical farm – this is a relatively low-cost location located near food distribution centers and a network of motorways that could still reach many consumers in a day, even if it isn't right in the middle of the capital city. Vertical farms should carefully consider their place in the supply chain before establishing a base.\nOn a larger scale, vertical farms may prove more profitable in different geographical regions. Vertical farms can reduce water usage significantly over conventional agriculture, and the high degree of control over the growing environment allows them to grow crops in extreme climates – where such crops may not otherwise be able to grow. In return, vertical farms demand more energy to carry out growing operations. To maximize their potential, vertical farms would ideally be located in regions of water scarcity, such as Sub-Saharan Africa and the Middle East, or in areas with extreme climates, such as in Scandinavian countries, where the low amounts of sunlight and high costs of regulating greenhouse environments single out vertical farms as an optimal solution. The amount of agricultural land available is also an important factor – regions looking to increase food security and reduce reliance on imports while facing challenges in acquiring sufficient agricultural land would find vertical farms to be ideal. A particularly prominent example of such a country is Singapore, which has demonstrated much interest in vertical farming over the last few years.\nBeyond the considerations of water scarcity and temperature, the general availability of fresh produce and the distribution networks of given countries should also be considered. Vertical farms use the added freshness and higher quality of their crops as a primary selling point, but these are typically offset by higher prices. Should there already be a large supply of high-quality produce made available at lower costs, vertical farms will find it hard to distinguish their own produce and may struggle to establish a significant market share. The converse would also be true; should a country lack easy access to fresh produce, vertical farms are expected to see much demand for their produce. An example of such a region would be the Middle East: leafy greens typically travel several thousand miles to reach stores, resulting in consumers facing high prices and low-quality products. The high price of conventionally farmed leafy greens, alongside government subsidies, makes it easier for vertically farmed produce to approach price parity while providing much fresher, higher-quality products.\nWhile the choice of location is an important consideration, it is only one of many others that must be given proper thought. Only through proper optimization of growing operations to improve efficiency and reduce costs can vertical farms reach their true potential. In the IDTechEx report, \"Vertical Farming 2022-2032\", many further important factors for consideration are discussed in detail, and the future of vertical farming is evaluated through 10-year market forecasts.\nIDTechEx guides your strategic business decisions through its Research, Subscription and Consultancy products, helping you profit from emerging technologies. For more information, contact research@IDTechEx.com or visit www.IDTechEx.com.\nThis post does not have any comments. Be the first to leave a comment below.\nPost A Comment\nYou must be logged in before you can post a comment. Login now."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:e8607804-af5f-4ed9-bf1b-de1095954573>","<urn:uuid:98c0eed7-8d91-42cd-b0d1-cc7c105f9a4d>"],"error":null}
{"question":"What are the key differences between a star's behavior in the debris disk phase versus its red giant phase? Both are important stages that show how stars change as they age.","answer":"In the debris disk phase, which occurs in a star's 'teenage' years (tens of megayears), the star is surrounded by clouds of gas, dust, pebbles and asteroids that are continuously regenerated by planetesimals. During this phase, the star maintains relatively stable characteristics. In contrast, during the red giant phase, which occurs much later in a star's life (after core hydrogen exhaustion), the star undergoes dramatic changes - it expands significantly (up to 166 solar radii), loses up to 28% of its mass through strong stellar winds, and becomes much cooler (dropping to about 3107 K) and much brighter (reaching 2350 times solar luminosity). The red giant phase is also more destructive, potentially engulfing nearby planets, while the debris disk phase can actually indicate potential planet formation.","context":["Authors: Nicholas P. Ballering, Kate Y. L. Su, George H. Rieke, András Gáspár\nFirst Author’s Institution: Steward Observatory, University of Arizona, USA\nPaper Status: Accepted to ApJ.\nLet’s talk about dust.\nBefore you roll your eyes – I don’t mean any old dust, I mean space dust. In particular, we’re interested today in the giant debris disks that reside around nearby stars – huge clouds of gas, dust, pebbles and even asteroids, a little like our own asteroid & kuiper belts. As you may know, when stars first form they are surrounded by protoplanetary disks. As they age, the dust normally disappears – it is blown out of the system by stellar winds, or falls onto the star surface via the Poynting-Robertson effect. But occasionally, a debris disk is maintained well into the ‘teenage’ period of the star’s life (by which I mean a few tens of megayears).\nThese debris disks are interesting. The dust is generally assumed to be continuously regenerated by planetesimals, suggesting that the building blocks of planet formation are present in the system – a tantalising hint that there may be planets hidden within these disks. We can learn about these planets by looking at the dust – be it observing the complex warps, gaps and other morphologies in an attempt to dynamically find the mass of a planet, or in comparing the composition of the dust and the planet to understand planet formation.\nBut how much do we actually know about these debris disks? Since the disk around nearby star β-Pictoris was first imaged back in 1984, you might assume that astrophysicists would have it pretty much figured out by now . . . sadly, you’d be wrong. In fact, models of various nearby debris disks have consistently struggled to fit both the thermal emission (the dust acting as black body emitters of far-infrared and radio wavelength radiation) and the scattered light emission (the dust acting a bit like a mirror and reflecting visible and near-infrared radiation from the star) at the same time.\nAnd it’s a pretty tricky game to come up with the right model, since it involves understanding dust that covers not-very-many pixels of your telescope CCD, with a model that can be incredibly complex. A perfect model would consider the dust distribution, dust scale height, composition, grain size, grain shape, grain porosity, dust/gas ratio, etc. – there’s a lot going on! It is, of course, difficult to understand all of this diverse physics with the limited data we have.\nToday’s paper focuses on understanding the dust composition of the β-Pictoris debris disk. β-Pictoris is an interesting star, as it hosts a simply huge debris disk, one of the first ones to be discovered. It’s also really nearby – just over 60 light years away – and the system even boasts one of the first ever directly imaged planets, namely β-Pictoris b. So there’s a fair bit of β-Pictoris data floating around. In fact, the authors today use five separate images spanning the range from optical to radio wavelengths, and including two images taken by the Hubble Space Telescope and one by ALMA. Some of this data is shown in figure 1.\nIn an attempt to reduce the complexity of the problem, the authors fit the spatial distribution of the debris disk first, and then the composition entirely separately. This works, because at lower wavelengths the resolution is fairly good. After lots of careful modelling, the authors come up with the first self-consistent model that fits both the thermal and the scattered light emissions, which is a promising sign for their model. Turning their focus to composition, they conclude that the dust is probably a mixture of silicates and organic materials, with not much in the way of water ice – this modelling is shown in figure 2. This seems to fit well with previous studies showing that the β-Pictoris system hosts crystalline material in its debris disk. The composition of the debris disk appears agree with work by other people on a similar debris disk hosting star, namely HR4796A. This is another hint that the authors are probably on the right track.\nThe authors point out that their method can be used to characterise of other nearby debris disk systems. Over the next few years, with the advent of SPHERE and ALMA, more discoveries of and insight into debris disks are to be expected. The more we understand about debris disks and the space dust they contain, the more we can learn about the planetary systems and how they form, and this paper certainly helps towards achieving that goal!","Introduction to Stars, Galaxies, & the Universe\nProf. Richard Pogge, MTWThF 9:30\nLecture 40: The Once and Future Sun\n- Solar Evolution\n- The Sun is a middle-aged, low-mass, main-sequence star\n- Its future evolution can be computed using stellar models\n- The Main Stages of the Sun's Life\n- Main-Sequence Star\n- Red Giant Star\n- Horizontal Branch Star\n- Asymptotic Giant Star & Unstable Pulsations\n- Planetary Nebula Phase\n- White Dwarf\nThe Fate of the Sun\nQuestion: How will the Sun evolve?\nTo answer this question, astronomers have made detailed\n- State-of-the-Art Stellar Evolution codes\n- Detailed Solar Structure Model\n- Inclusion of realistic mass-loss processes\nThe goal is to produce a self-consistent model of the Sun that will\ntrace its evolution from the time it reached the Main Sequence until its\nemergence as a hot white dwarf.\nIn preparing this lecture, I have drawn on two relatively recent\ncalculations of detailed Solar Evolution models:\n- Sackmann, Boothroyd, & Kraemer (1993), Astrophysical\nJournal (Vol. 418, 457)\n- Bahcall, Pinsonneault & Basu (2001), Astrophysical Journal,\n(Vol. 555, 990)\nThe Sun Today\nThe Sun is currently a middle-aged, low-mass star on the Main Sequence.\nThe Sun at this time is in a state of\n- Age = 4.55 Gyr\n- Mass = 1 Msun = 1.99x1033 g\n- Radius = 1 Rsun = 700,000 km\n- Luminosity = 1 Lsun = 3.83x1026 Watts\n- Temperature = 5779 K\n- ~50% of its core Hydrogen has been fused into Helium\n- Note: This and subsequent pictures show the inner solar system with\nthe Sun drawn to scale with respect to the orbits of the planets. The\nscale is also the same between each solar system graph, so you can see how\nmuch the planets move outward as mass loss proceeds.\nThe Sun took about 50 Myr to form from a molecular cloud core.\nIt reached the Main Sequence about 4.50 Gyr ago. When the Sun first\nalighted onto the Zero-Age Main\nSequence it was\nthan we see today.\n- A little fainter: 0.70 Lsun\n- A little smaller: 0.897 Rsun\n- A little cooler: 5586 K\nThe Sun can maintain itself in Hydrostatic and Thermal Equilibrium as\nlong as it can stably burn Hydrogen into Helium in its core.\nAs the Sun consumes its core Hydrogen, it slowly grows brighter with\nage. We see a brighter, hotter, and slighly bigger Sun today than\nwhen it formed. This trend will continue so long as the Sun is on the\nMid-Life Crisis for the Earth\nThe steady brightening trend will spell trouble for the Earth in the\n5.6 Gyr (1.1 Gyr from today):\nA \"Moist Greenhouse Effect\" is one in which most of the water in the\natmosphere is driven off into space. This will likely mean the end of\nlarge surface life on Earth, though some types of marine life and\nsimpler lifeforms might survive in the deep oceans or underground.\n- Sun 10% brighter: ~1.1 Lsun\n- The extra sunlight triggers a Moist Greenhouse Effect.\nVenus on Earth\nThe warming trend will continue through the Sun's Main Sequence phase.\n8 Gyr (3.5 Gyr from today):\nThe oceans will totally evaporate into space, releasing all of the\nCarbon Dioxide (CO2) currently locked up in marine sediments\ninto the atmosphere. The result will be to turn the Earth's moist,\nlight, warm atmosphere today into a hot, heavy, bone-dry CO2\natmosphere like that on Venus today.\n- Sun 40% brighter: ~1.4 Lsun\n- Extra solar energy triggers a Runaway Greenhouse Effect\nHydrogen Core Exhaustion\n10.9 Gyr (6.35 Gyr from today):\nThe Sun's core runs out of Hydrogen, most of the volume of the core\nhaving been replaced by inert Helium \"ash\".\n- The Inert He core starts to contract and heat up\n- H burning moves out into a shell\n- T = 5517 K\n- R = 1.575 Rsun\n- L = 2.21 Lsun\nThe Sun leaves the Main-Sequence and becomes a Sub-Giant star.\n\"Lively Old Age\"\nThe Next 0.7 Gyr:\n- Sun expands a near-constant Luminosity of about 2.2 Lsun towards\nthe base of the Red Giant Branch.\n- Sun swells in size from 1.58 Rsun to 2.3 Rsun\n- Surface layers cool from 5517 K to 4902 K\nThis growth is accompanyied by the start of slow mass loss in a stellar wind:\n- The wind steadily picks up as the Sun approaches the base of the Red Giant\nClimbing the Red Giant Branch\nIt will take the Sun about 0.6 Gyr to climb up the Red Giant Branch.\nDuring this time, the Sun will lose up to 28% of its total mass in a strong\nAll the mass-loss occurs from the outer envelope.\n- This mass-loss causes the planets to move outwards a little:\n- Venus moves out to about 1 AU (where Earth is now)\n- Earth moves out to abou 1.4 AU (about where Mars is now)\n- The other planets also move out similarly.\nAt the Tip of the Red Giant Branch, the outwards appearance of the Sun\nis as follows:\nThe Sun's swelling red-giant envelope will swallow Mercury!\n- T = 3107 K (M0 III Star)\n- L = 2350 Lsun\n- R = 166 Rsun (0.775 AU)\nThe Helium Flash\nWhen the Sun reaches the Tip of the Red Giant Branch:\n- Helium Burning to C & O ignites rapidly in a\n- Extra energy stablizes the core against collapse.\n- Commences Hydrogen Burning in a thin shell outside the He burning core.\nThe Sun descends quickly onto the Horizontal Branch in about 1 Myr\nas it rearranges its internal structure to accomodate this new source of\nThe Horizontal Branch\nWith a new source of energy (He fusion), the Sun settles onto the\nHorizontal Branch for\na brief \"retirement\" period of Hydrostatic and Thermal Equilibrium:\n- Radius: R=9.5 Rsun\n- Temperature: T = 4724 K\n- Luminosity: L = 41 Lsun\nHowever, because He fusion is about 100-times less efficient at\nproducing energy per reaction than H fusion, the Sun can only keep this\nup for about 100 Myr...\nAn All-Too-Brief Retirement\nAfter 100Myr, the Sun runs out of core Helium, and slips out of\n- C-O \"ash\" core begins to contract and heat up\n- Remaining He is displaced into an He-burning shell, surrounded\nby a thin H-burning shell.\nThe Sun starts to swell up & get brighter:\n- R = 18 Rsun\n- T = 4450 K\n- L = 110 Lsun\nHelium Core Exhaustion\nDriven out of Hydrostatic and Thermal Equilibrium by exhaustion of He in\nthe core, the Sun now begins to climb the Asymptotic Giant Branch.\nThis process is rapid, taking only about 20 Myr:\n- The C-O core steadily contracts and heats up.\n- Fusion is moved out into a He burning shell\n- A thin H burning shell sits outside the He-burning shell.\nDuring this time, the Sun swells up rapidly, getting cooler & brighter:\nNote that this is brighter than when it climbed the Red Giant Branch\nbefore, and it happens much faster than before (20 Myr compared to 0.6\n- R = 180 Rsun (0.84 AU)\n- T = 3160 K\n- L = 3000 Lsun\nAGB Phase Mass Loss\nThe Sun's ascent of the Asymptotic Giant Branch is also accompanied by\nmass loss in the form of a strong stellar wind:\n- The Sun's mass is reduced to about 0.6 Msun\nThe surviving planets (Mercury was engulfed during the Red Giant phase)\nmove further outward:\n- Venus moves out to 1.22 AU\n- Earth moves out to 1.69 AU\n- The others move out proportionally...\nNear the tip of the AGB, thermal pulsations in the envelope begin.\nThe Tremors of Old Age\nAs the Sun nears the tip of the Asymptotic Giant Branch, unstable thermal pulsations\nbegin in the He-burning shell:\n- Models predict 4 pulses at roughly 100,000 year intervals.\n- Each pulse puffs the Sun up to 213 Rsun (about 1 AU!)\n- The largest is pulse #4, with L = 5200 Lsun!\nThese pulsations will progressively eject most of the remaining envelope\nof the Sun.\nEach episode of pulsational mass-loss causes the surviving planets to\nmove further away from the Sun.\nA Final Flowering\nThe last of the thermal pulses blows off what is left of the envelope\nover the course of a few thousand years.\nAs the last of the envelope comes off, the hot C-O core of the Sun\nis unveiled. As seen from the outside:\n- T goes from 4000K (envelope) to 120,000K (bare core)\n- L stays constant at ~3500 Lsun\n- The bare core rapidly traverses the H-R diagram\nUV photons from the core ionize the ejected envelope gas, forming a\nActually, there is some controversy about this. The minimum stellar\nmass needed to get a planetary nebula stage is not known, mostly because\nof uncertainty about how much mass is lost during the stellar wind\nphases, and how that effects the evolution of the bare core plus\nexpanding envelope. There are two possiblities: One is the scenario\nabove where the Sun briefly flowers as a Planetary Nebula, the other\nwhere the envelope dissipates before the core gets hot enough to ionize\nit and light up the nebula. There is currently no concensus.\nThe Final Configuration\nThe bare core, now with a Mass of about 0.54 Msun, evolves\ninto a slowly cooling\nWhite Dwarf with a radius\na little smaller than the Radius of the Earth.\nWith mass loss now ended, and the final mass of the white dwarf fixed,\nthe surviving planets settle into their essentially final orbits:\nThe planets will continue orbiting the remnant white dwarf in this\nconfiguration unless a passing star happens to come close enough to\ndisrupt their orbits.\n- Venus is at 1.34 AU\n- Earth is at 1.85 AU\n- Mars is at 2.8 AU\n- and so forth...\nThe white dwarf that was the Sun now begins a long, slow cooling phase\nthat will last for nearly a Trillion years as it fades away into a long\nThe Seven Ages of the Sun\n- 1 Msun Main-Sequence Star: 11 Gyr\n- Red Giant Star: 1.3 Gyr\n- Horizontal Branch Star: 100 Myr\n- Asymptotic Giant Branch Star: 20 Myr\n- Thermal Pulsation Phase: 400,000 yrs\n- Planetary Nebula Phase: ~10,000 yrs\n- 0.54 Msun White Dwarf as the final state.\nReturn to [\nUnit 6 Index\nAstronomy 162 Main Page\nUpdated: 2006 March 5\nCopyright © Richard W. Pogge, All Rights Reserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d3450c57-f1bc-4e66-bbf1-4b45b27bbcea>","<urn:uuid:77e63fba-2201-4c41-8a29-78bb368b1811>"],"error":null}
{"question":"What are the key forms required for Adjustment of Status, and how long does USCIS typically take to process these applications?","answer":"Several forms are required for Adjustment of Status, including the G-28 (for attorney representation), G-325A (biographical information), I-130 (Petition for Alien Relative), I-485 (Application to Register Permanent Residence), I-131 (Application for Travel Document), I-765 (Application for Employment Authorization), and I-864 (Affidavit of Support). According to USCIS's current processing methodology, the I-485 application takes between 7 to 33 months to process, with the exact timeline varying based on office location, basis for filing, and other factors. The typical process includes receiving receipt notices within 2-3 weeks, biometrics appointment within 3 weeks, EAD card within 90 days, and interview scheduling within 3-4 months of filing.","context":["What is Adjustment of Status?\nAdjustment of Status is the process by which a foreign national can change their immigration status from a temporary nonimmigrant to an immigrant (permanent resident), while in the United States. There must be a basis under which a foreign national can apply for adjustment of status. In most cases the foreign national must have an immediate relative who is a U.S. Citizen or have an employer willing to file an immigrant petition on their behalf.\nGenerally, a foreign national can apply for adjustment of status, if they were inspected by a customs official at a United States port of entry and admitted or paroled into the United States, and meets all requirements to apply for a green card (permanent residence). The Immigration and Nationality Act (INA) allows an eligible foreign national already living in the United States with their U.S. Citizen spouse, to obtain permanent resident status without having to return to their home country to apply for an immigrant visa at a United States consulate abroad. Spouses of U.S. Citizens are eligible for adjustment of status to permanent residence once the US Citizen spouse files a petition on their behalf called the I-130 Petition for Alien Relative. The I-130 Petition for Alien Relative is typically filed at the same time (concurrently) as the I-485 Application to Register Permanent Residence or Adjust Status. For immigration purposes, the intending immigrant (or foreign national) is referred to as the ‘beneficiary’ of the application, while the U.S. Citizen spouse is referred to as the ‘petitioner’ of the I-130 application. The petitioner allows the beneficiary to apply for adjustment of status on the basis of their marital relationship (established with the filing of the I-130 Petition).\nIn general, most immigrants become eligible for permanent residence once an immigrant petition is filed on their behalf by either a qualifying family relative (I-130 Petition) or through an employer (I-140 Petition) although there are special categories of green card applicants that exist. Unlike distant relatives of U.S. Citizens and alien workers, spouses and immediate relatives of U.S. Citizens are not subject to any visa limitations. This means that they do not need to wait in line to receive permanent residence; an immigrant visa is immediately available to them and there are no quotas. The process of immigrating a foreign spouse through adjustment of status takes approximately 4-6 months depending upon the volume of adjustment of status application being processed by USCIS at the time of filing, and the amount of applications waiting in line for an interview at your local field office.\nSpouses of U.S. Citizens residing abroad are not eligible for adjustment of status\nFor spouses of U.S. Citizens residing abroad, adjustment of status is not an option because the intending immigrant and U.S. Citizen spouse must be living together in the United States in order to apply. Instead, spouses of U.S. Citizens who are living abroad must resort to consular processing, in order to obtain an immigrant visa and permanent residency. Consular processing is also utilized to immigrate a foreign spouse who is ineligible to adjust status, for example in the case where the foreign spouse entered the United States illegally.\nIn order to apply for permanent residency, the Beneficiary must:\n- Have been inspected and admitted or paroled into the United States (entered legally);\n***Note: Exceptions include foreign nationals who qualify for 245i or foreign nationals who have received an I-601Waiver of Grounds of Admissibility, or special categories of green card applicants such as U Visa and asylum recipients.\n- Be legally married to the U.S. Citizen spouse;\n- Have a bona fide marriage;\n- Be living with the U.S. Citizen spouse in the United States;\n- Not be inadmissible to the United States;\n- Not committed any crimes of ‘moral turpitude’;\n- Not committed any immigration violations (such as fraud and willful misrepresentation);\n- Not have entered the marriage for the purpose of receiving immigration benefits;\n- Be legally free to marry (all prior marriages must have been legally terminated);\n- Abide by health regulations and vaccination requirements as required by I-693 (medical examination);\n- Not have entered the United States with the intention of marrying a United States citizen.\nThe Petitioner must:\n- Be a U.S. Citizen;\n- Be legally free to marry (all prior marriages must have been legally terminated);\n- Be living with the foreign spouse in the United States;\n- Have a bona fide marriage with the intending immigrant;\n- Prove that they meet the income requirement to sponsor the foreign spouse according to their household size (see federal poverty guidelines).\nUnderstanding the Forms Filed with the Adjustment of Status Application\nThere are several forms that are filed along with the adjustment of status application. In order to fully understand the application process, we will explain the purpose of each form below.\nNOTE: The immigration filing fees for the I-130/485 are $1,490.00. These fees are SEPARATE from any legal fees that may be charged by your attorney. There are no filing fees associated with the other forms.\nThe G-28: Notice of Entry of Appearance as Attorney or Accredited Representative\nThe G-28 is a form that both the petitioner and beneficiary must sign in order to authorize an attorney or accredited representative to act as your legal representative in your immigration case. The G-28 accompanies each application. If you are not filing your application with an attorney, you do not need to sign this form.\nThe G-325A: Biographical Information\nThe G-325A form must be signed and completed by the petitioner and beneficiary. This form contains basic biographical information including dates of birth, city and country of birth, social security number, father and mother’s names, dates of birth, city and country of birth, city and country of residence, name of current husband or wife, date of birth, city and country of birth, date and place of marriage, information regarding former husbands or wives (names, date of birth, date and place of marriage, date and place of termination of marriage), applicant’s residence history for the last 5 years, employment history for the last five years, etc. It is important to be as accurate as possible.\nThe I-130: Petition for Alien Relative\nThe I-130 petition for alien relative is signed and completed by the petitioner only. This form establishes the family relationship that exists between the U.S. Citizen and the intending immigrant, thereby creating the basis under which the intending immigrant can file for adjustment of status. The I-130 and I-485 forms are the most incorrectly completed forms.\nThe I-130 form requests basic information about the petitioner and the beneficiary (name, address, place of birth, DOB, date and place of marriage, SSN, marital status, prior spouses, I-94 number and expiration, date of arrival, name & address of beneficiary’s employer, date employment began, etc.)\nOther information includes:\n- Address in the US where the beneficiary intends to live;\n- Information regarding the beneficiary’s spouse and children;\n- Beneficiary’s address abroad;\n- Beneficiary’s name & foreign address in native alphabet;\n- Last address the petitioner and beneficiary lived together;\n- Information regarding whether the beneficiary has been under immigration proceedings;\n- City and state where the beneficiary will apply for adjustment of status;\n- Whether the petitioner has ever filed a petition for the same alien or any other alien.\nThe I-485: Application to Register Permanent Residence or Adjust Status\nThe I-485 application is signed and completed by the beneficiary only. The I-485 is the green card aspect of the application. The I-485 application consists of the beneficiary’s basic information such as address, date of birth, country of birth, country of citizenship, SSN, date of arrival to the US, I-94 number and expiration, USCIS immigrant status, current occupation, mother and father first names, place of last entry into the U.SA. and status the beneficiary entered, nonimmigrant visa number, consulate where the visa was issued, date visa was issued, spouse’s name and date of birth, as well as a series of very important ‘have you ever’ questions relating to immigration violations, criminal activity, and other military involvement. The beneficiary must read these ‘have you ever’ questions very carefully before answering and signing the application. Failure to disclose accurate information may result in the denial of your application.\nThe I-131: Application for Travel Document\nThe I-131 application is filed as part of the I-130/485 application and is signed by the beneficiary only. Form I-131 is filed as a request for a re-entry permit after temporary foreign travel also known as request for advance parole. The I-131 form is very difficult to complete since this form is also used for humanitarian requests for advance parole. It is important to read the form instructions carefully or consult with an attorney before submitting the application. Much of the information on this form is left blank. There is no additional fee for the I-131 form when submitting with the I-130/485 applications. The I-131 and I-765 applications culminate in what is known as an ‘employment authorization card’ which also serves as a re-entry travel permit. It takes approximately 90 days for the EAD/travel combo card to be issued from the date of filing. Applicants MAY NOT travel internationally once the I-130/485 application is filed, unless they have obtained the advance parole re-entry permit. The I-131 form requests the following information:\n- Beneficiary name\n- Country of birth\n- Country of citizenship\n- Class of admission\n- Phone number\n- Application type\n- Processing information (date of departure, length of trip, purpose of trip, address document will be sent, etc.)\nThe I-765: Application for Employment Authorization\nThe I-765 form is a request for employment authorization. This form is signed only by the beneficiary and culminates in an ‘employment authorization card’ (EAD) that is mailed to the beneficiary within 90 days of filing of the application. There is no additional fee for this form when filing the I-765 with the I-130/485 applications. You may not seek employment or receive employment income until you receive the EAD card from USCIS. The EAD card is issued for a one-year period while your green card application is in process with USCIS. If you have also filed form I-131 request for travel authorization, your EAD card will contain an annotation on the bottom center of the card indicating that you may use the card for ‘advance parole’ purposes. The I-765 is the easiest form to complete. The form requests the following information:\n- Beneficiary’s address, DOB, country of citizenship, place of birth city, state, country, SSN, I-94 no. or Alien registration no.;\n- Date of last entry into the US and place of entry;\n- Last entry status;\n- Current immigration status;\n- Eligibility category;\n- Telephone number;\n- Asks whether you have applied for employment authorization in the past;\nThe I-864: Affidavit of Support\nAn affidavit of support, (USCIS Form I-864), must be signed and completed by the petitioner of the I-130 for all intending immigrants seeking permanent residence. When the petitioner (U.S. Citizen spouse) signs the affidavit of support, they become the sponsor of the intending immigrant(s). The U.S. Citizen spouse is both the intending immigrant’s petitioner and sponsor for the I-864 Affidavit of Support. The Affidavit of Support is one of the most important aspects of the adjustment of status application. Without it, the adjustment of status application CANNOT be approved. It is also a very difficult form to complete. It is recommended that the petitioner read the I-864 instructions very carefully before completing this form or seek counsel from an attorney. Failure to complete this form accurately and correctly may result in a request for evidence or a denial.\nThe affidavit of support requires the petitioner/U.S. Citizen spouse to demonstrate to USCIS that they meet the income requirements based on their household size to sponsor the intending immigrant. If the petitioner does not meet the income requirements, a joint sponsor must sign and complete the I-864 and provide financial documents proving that they have the sufficient income to sponsor the intending immigrant.\nThe affidavit of support is a legally enforceable contract with the United States government. The affidavit of support ensures to the United States government that your relative/intending immigrant(s) will not become a financial burden on American taxpayers should they fall into financial hardship. A legal permanent resident cannot seek local, state, and government benefits with the exception of Medical, Medicaid, and a few exempt programs outlined below. If a legal permanent resident you have sponsored seeks government assistance, you as the sponsor may be both liable and responsible for reimbursing that agency for monies owed.\nResponsibilities of the Joint Sponsor\n- As a sponsor bringing your relative to live permanently in the United States, you are accepting legal responsibility for financially supporting the intending immigrant(s) until they become a US Citizen or until the intending immigrant(s) can be credited with 40 quarters of work which normally equates to 10 years. You are a sponsor if you have filed or are filing a USCIS Form I-130, Petition for Alien Relative. The obligation of a sponsor ends when the legal permanent resident becomes a US Citizen, dies, or ceases to become a lawful permanent resident and departs the United States.\n- Divorce does not end the sponsorship obligation. If you filed for your spouse’s permanent residence and divorce, you still remain the sponsor unless the above conditions apply.\nThe I-864 requires you to provide basic information such as your name, the beneficiary’s name and mailing address, country of citizenship, date of birth, your mailing address, place of residence, date of birth, country of domicile, social security number, household size (based on your income taxes), number of dependents, sponsor’s employment and income information.\nIn addition to signing the affidavit of support, the sponsor is required to present certain financial documentation including:\n- Most recent income tax return;\n- Most recent wage statements and/or 1099’s;\n- 2-3 Recent Pay Stubs or last 6 months of bank account statements if self-employed or retired;\n- Letter of Employment, Self-Employment, or Income Explanation Letter (whichever applies);\n- Proof of assets (if supplementing income with assets) such as deed of property ownership, home appraisement report, deed of vehicle ownership, proof of 401K, IRA, mutual investment funds etc.\nOther Aspects of the Application\nThe Beneficiary must visit a USCIS approved civil surgeon who will conduct a medical evaluation. The physician will prepare Form I-693 and provide this form to the Beneficiary in a sealed envelope. The Beneficiary MUST NOT open the sealed envelope. The envelope must be opened by USCIS only. This envelope is mailed to USCIS along with the signed forms.\nWhat to Expect Once You File Your Adjustment of Status Application\n- Within 2-3 weeks of filing the application, the beneficiary will receive 4 receipt notices (for the I-130, I-485, I-131, and I-765 applications) in the mail indicating that the applications have been received by USCIS and are processing normally.\n- Within 3 weeks of filing the application, the beneficiary will receive a biometrics appointment in the mail scheduling them for fingerprints at an ASC location near them. The beneficiary must attend this appointment for collection of biometrics.\n- Within approximately 90 days of filing the application, the beneficiary will receive their EAD card in the mail (employment and travel authorization)\n- Within 3 months of filing the application, the couple will receive their initial interview appointment notice scheduling them for the green card interview. At this time the couple must begin to prepare for their interview and collect any documents proving cohabitation and bona fide marriage (joint lease agreements, joint bank account statements, joint utility bills, photographs of the couple together, joint trips taken, joint insurances, etc).\n- Within 4 months of filing the application, the interview will take place. If all goes well and the couple is approved, the green card is received within 2-3 weeks of approval.\nIf you are ready to file your adjustment of status application, please contact us. It is our pleasure to be a part of your immigration journey.","How long does it take to process an application for permanent residence in the United States, or a “green card?” You might be surprised by how difficult it is to find a reliable answer to this common question. U.S. Citizenship and Immigration Services (USCIS)—the agency that adjudicates applications for permanent residence and other immigration benefits—provides a range as a way to estimate the time needed to process an immigration application.\nThe agency introduced a pilot program in March that changed how USCIS estimates these time ranges for four of its most popular types of application. The change follows longstanding criticism about inaccurate processing times from federal oversight offices, elected officials, and stakeholders. Applicants and immigration practitioners have reported that USCIS’ posted processing times do not reflect the actual time it takes a case to reach completion.\nPreviously, USCIS published processing times for all types of applications and petitions as a single figure in months, a specific date, and even in relation to a goal processing time.\nThe agency now uses an automated methodology in an attempt to more accurately estimate how long it will take to process certain common immigration benefit filings. According to USCIS, an application for permanent residence (Form I-485) will take anywhere from 7 months to 33 months to process. The time range fluctuates depending on the office location, basis for the filing, and other factors.\nThe pilot program only applies to the following four immigration forms:\n- N-400, Application for Naturalization\n- I-90, Application to Replace Permanent Resident Card\n- I-485, Application to Register Permanent Residence or Adjust Status\n- I-751, Petition to Remove Conditions on Residence\nAlthough the calculation method being piloted may improve accuracy in some ways, concerns remain. The pilot program includes only four of the many fee-based immigration forms USCIS adjudicates. Additionally, the ranges are still estimates, have broad variation, and do not reflect the complexity of many cases.\nFor example, “in the case of a foreign national applying for an employment-based green card, an employer must file an I-140 Immigration Petition for Alien Worker and the worker must also submit a Form I-485 to adjust status to permanent residence, when a visa number is immediately available.” USCIS will process these forms sequentially—meaning the time it takes to adjudicate each form and related steps must be added together to get an approximate estimate of processing time. USCIS processing time methodology does not account for such complexity.\nUSCIS provides time ranges as processing time estimates for other immigration filings as well, however there is significantly less transparency about the agency’s calculation method.\nAccessing this information is particularly important given the longstanding backlog of filings at USCIS. An application or petition that allows a person to work or travel internationally may be pending for several months to many years, leaving applicants—or employers petitioning for potential or current employees—in limbo for indeterminant times.\nAccurate processing time estimates can significantly affect the lives of applicants, employers, and the local communities they support. USCIS should prioritize transparency in its methods while improving the accuracy of processing times. Doing so not only supports the agency’s mission but would support economic and social stability in the United States—a benefit for everyone.\nFILED UNDER: featured, green card, processing times, USCIS"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:561ebe5d-38ab-4990-96f4-77b04dfa8570>","<urn:uuid:792c3725-77ae-478f-8e01-158d83689dcb>"],"error":null}
{"question":"How did Mt. Washington's heat-sensitive glass innovations relate to legal disputes, and how did British manufacturers approach sugar basin design compared to American counterparts?","answer":"Mt. Washington's heat-sensitive glass led to legal conflict when they received a patent for Amberina on May 25, 1886, which was identical to glass previously patented by Joseph Lock of New England Glass Company in 1883. New England Glass Co. sued to stop production, leading Mt. Washington to rename their product to Rose Amber, though the glasses remained virtually identical. Regarding sugar basin design, British manufacturers took a different approach from Americans. While American pattern glass sets typically included a covered butter dish, covered sugar bowl, creamer and spooner, British sets generally omitted the spooner and paired the creamer and sugar basin together, with the butter dish standing alone. British manufacturers produced both 'sugars' and 'sugars & covers,' with the latter corresponding to the American covered sugar bowl style.","context":["|Mt. Washington Glass Toothpick\nby Sarah Jenkins\nReturn to N.T.H.C.S. Club Page\nI started collecting toothpick holders more than 20 years ago and like so many other collectors, my first one was inherited from family. It was the common cut glass Strawberry Diamond and Fan. For several years cut glass and clear pattern glass were all I bought. Then I started specializing in companies such as Heisey, Fostoria, Duncan & Miller and the series known to toothpick collectors as the State Patterns. Needless to say as you view the items from my collection which illustrate this article, the ñaddicitonî has progressed far from that point with me now.\nOne of my primary interests now is Mt. Washington Glass, which I will briefly outline in the article. The information for this will be taken from early articles from Mt. Washington Art Glass Society Publications and notes from a seminar given by David. C. Fuchshuber at < a href= ñî>The National Toothpick Holder CollectorÍs Society annual convention in 1994.\nIn 1870, William L. And Captain Henry Libby moved the Mt. Washington glass works to this factory. The company was reorganized in 1876 and the name changed to the Mt. Washington Glass Company. During the late 1870s, Frederick S. Shirley, an Englishman, became the chief designer for the Mt. Washington Glass Company. The greatest achievements of the company occurred during ShirleyÍs time. Frederick Shirley and Albert Steffin were responsible for the invention and production of various types of art glass which made the company famous.\nA patent for heat-sensitive shaded glass called Amberina was issued on May 25, 1886 to Frederick Shirley. This was identical to the glass patented by Joseph Lock of the New England Glass Company on July 24, 1883. The New England Glass Co. filed suit to stop Mt. WashingtonÍs production of the glass. The suit was settled when Mt. Washington changed the name of its glass to Rose Amber. However the two remain virtually identical.\nGlass that has a small amount of soluble gold in it appears amber in color when taken from the pot. The object is then shaped and a portion of it reheated at the glory hole until it becomes a ruby red to fuschia color depending upon the length of time it is reheated.\nAmberina was and remains extremely popular and continues to be produced by many glass companies. Mt. Washington produced many shapes in Rose Amber. The easiest way to identify a Mt. Washington form is to compare the shape to other known Mt. Washington forms.\nPhotograph #1: will show three toothpicks in Rose Amber that were also made in Burmese, Glossy Diamond Quilted. The shapes are Bulbous Base Square Top, Tri-Corner, and Square. The quilting on these is larger than the New England Venetian Diamond and the color appears more fuschia.\nFrederick S. Shirley was issued another patent on December 15, 1885 for a shaded translucent glass called Burmese. To the basic formual for amberina three additional ingredients were added including uranium oxide. Burmese glass is a translucent glass that is shaded from a bright yellow at the base to a coral or salmon pink at the top. Two types of Burmese glass were produced; the original shiny finish or a soft matte finish achieved by washing the glass in acid. Burmese glass was blown, blown molded and press molded. Press molded Burmese was produced in a quilted, hobnail, or melon shaped pattern.\nBurmese was extremely popular and produced for over ten years. More than 250 shapes and forms were made including table lamps with Burmese chimneys, beverage sets, condiments sets (including toothpick holders).\nNot only was the Mt. Washington Glass company inventive in the color of glass it produced. It excellend in the decoration of that glass. Albert Steffin was the superintendent of the Mt. Washington decoration department. Mt. Washington decoration was considered the finest in the country. The decoration applied to Burmese was elegant and often unique. Many of the decorations are worthy artistic renditions.\nMt. Washington was the only glass company in the United States to manufacture Burmese. In 1886 Frederick Shirley presented Queen Victoria a gift of Burmese glass. Then, in June of 1886 a patent was issued in England for production of Burmese glass. Then a license was awarded to Thomas Webb and Sons for its production in England.\nMt. Washington continued to produce Burmese after 1895 until 1900, only in smaller quantities. Pairpoint, the Mt. Washington successor, produced a limited amount of Burmese original formula. The Gunderson-Pairpoint Company produced some Burmese in 1956 and the Pairpoint Glass company produced some in the 1970s.\nPhotograph #2 shows shiny Diamond Quilted Burmese in the same shapes as Rose Amber.\nBy substituting a small amount of cobalt or copper oxide in the place of the uranium oxide used in the production of Burmese, Frederick Shirley created Peachblow glass. On July 20, 1886 he was issued a patent for this heat sensitive glass, which was very different and unique. Upon reheating, the glass turned a rose pink at the top and shaded to a bluish grey-white at the base. Peachblow and Burmese share the same forms and shapes. Some Peachblow contains enameled and painted decorations, including the QueenÍs and Verse designs. The rarest forms of Peachblow have the applied Peachblow decorations of flowers, leaves and cherries.\nMt. Washington Peachblow is the rarest of all Heat Sensitive Glass and was only produced from 1886 until 1888. Unfortunately, it never achieved the popularity or success of its predecessor, Burmese. It remains elusive today on the market.\nPhotograph #3: Square decorated Peachblow.\nNote: this is the same shape as the Square shown in the first two photographs. Unfortunately, the other two shapes are not in my collection. Other shapes do exist.\nCrown Milano was usually made of opal glass that was blown, blown molded, or pressed. Most of the glass was given an acid or matte finish.\nAlbert Steffin and Frederick Shirley were awarded a joint patent for decorating opal lamp shades and bases which became known as Mt. Washington Crown Milano. A trademark paper for this glass was issued to the Mt. Washington Glass Compnay on January 31, 1893. The assigned mark was a monogram of ñ C and Mî (note photograph #11 Flat Fingers has this mark). Sometimes a crown was used with the ñC and Mî; somtimes just a variation of the monogram appeared. The decorations were many and varied. Occasionally, a color other than opal was used for the glass blank such as Burmese or a soft pale yellow to green opaque glass.\nAlbertine is decorated opal glass, usually with a matte finish, which is the same as Crown Milano. The only documented examples have been found with paper labels. It is thought that Albertine was the name first given to Mt. WashingtonÍs line of decorated opal ware.\nPhotographs 4-12 will show some examples with my description. I am not sure of the distinction between Crown Milano and Albertine.\nThe trademark for this glass is a reversed ïRÍ combined with a forward ïFÍ usually enclosed in a diamond shape. It is a clear transparent crystal that was given a matte finish and then heavily decorated. Produced during the late 1880s, the trademark was not granted until February 1894. Royal Flemish glass is the most exotic glass produced by Mt. Washington. Its decoration is usually elaborate. There are rare examples of this glass in blue, green, pink, and yellow. Royal Flemish is very rare, scarce and highly collectible today.\n||Photo #4-a: Figmold, yellow satin with fingers, Royal Flemish|\n||Photo #4-b: Figmold, lusterless white, satin inside and out|\n||Photo #4-c: Figmold, Burmese, glossy inside, satin outside|\n||Photo #5: Simple Scroll, (L-R) lusterless white with fingers, Burmese, (soft pink) scalloped top; Unfired Burmese, beaded top|\n||Photo #7: Footed Lobe, lusterless white and Burmese|\n|Photo #8: Urn, left & center, lightly fired Burmese, note decoration previously shown on other shpae, on the right is simulated Burmese (lusterless white painted to look like Burmese)|\n||Photo #9: Bulbous Base Square Top, left Burmese, center Burmese, right Diamond Quilted Burmese|\n||Photo #10: Tri-Corner, both are Diamond Quilted Burmese, left is lightly fired almost yellow; right is rosy pink to yellow, same size as center item in Photo 2 & 3.|\n||Photo #11: Fine-Rib, lusterless white: left is simulated Burmese, signed with a crown and CM; center tri-corner top, inside painted orange to simulate Burmese; right beaded top, pale blue paint.|\n||Photo #12: Square Burmese, left deep pink; right lightly fired (almost white with just a hint of pink). This shape also comes with diamond quilting. ItÍs the same shape and size as Peachblow and the Rose Amber and Glossy Diamond Quilted Burmese.|\nThe following statements are personal observations and may not always apply. I have some shapes in Figmold, Urn, Simple Scroll and Footed Lobe in both white and Burmese whereas Fine-Rib is only in lusterless white. Bulbous Base Square Top and Tri-Corner are in Burmese. Some shapes I havenÍt discussed because I did not have an example in the collection, such as Parallel Greek Key, which I understand comes in both white and Burmese.\nOriginal article appeared in the Nov. 18, 1998 Antique Trader and has been reproduced with the permission of the author. REFERENCES ON TOOTHPICK HOLDERS:\nOut of Print\nReturn to N.T.H.C.S. Club Page\nPrint out an application and apply now!","The National Glass Collectors Fair\nPass The Sugar Basin!\nby James S. Measell\n|A commemorative sugar and cream in 'opal' by Henry\nGreener & Co. Celebrating the landing in Halifax (Nova Scotia) of the Marquis of\nLorne and Princess Louise on 25 November 1878. Image Courtesy of Philip Housden\nSome years ago, my wife and I purchased an item described as “a milk glass footed compote dish with much writing and the likeness of Disraeli, the Earl of Beaconsfield.” We were intrigued by the intricate detail of the portrait of Benjamin Disraeli, and we learned that Greener & Co., who operated the Wear Flint Glass Works at Sunderland, made this piece in the late 1870s. The design was registered on 31 August, 1878, not long after Prime Minister Disraeli’s success in negotiating a peace treaty at the Congress of Berlin in July, 1878.\nWe have since learned that “milk glass” was originally called opal [pronounced “o-pal”], a term employed by glassmakers in England and in America but seldom used by glass collectors now. More importantly, we also learned that our piece is, in fact, a “sugar basin.” The phrase seems to be distinctively British in origin and generally applies to nineteenth- and twentieth-century pressed glassware.\nGlass history researchers Charles Hajdamach and Raymond Slack indicate that the term “sugar basin” was in use as early as 1848 in England, when Thomas Webb II of Molineaux Webb & Co. was describing pressed glass procedures and products. Jennie Thompson notes that the phrase “sugar basin” was often used in the descriptions of glassware designs that were officially registered and recorded at the Patent Office.\nCollectors of American pattern glass are familiar with the standard four-piece table set, which consists of covered butter dish, covered sugar bowl, creamer and spooner (incidentally, both the manufacturers and glass workers often shortened these, respectively, to “butter, sugar, cream and spoon”). The British sets generally do not include a spooner, and the creamer and sugar basin are generally paired together while the butter dish stands alone.\n|A covered sugar bowl in 'turquoise' by Sowerby. Pattern No.\npattern 1430 (Crirca 1879).\nImage Courtesy of Philip Housden\nBritish glassware manufacturers produced both “sugars” and “sugars & covers.” The latter, as shown in Cottle’s book on Sowerby glass, correspond to the American expectation of a standard covered sugar bowl. On the other hand, “sugars” appears to be a diminutive form of the phrase “sugar basins,” for either term may occur in manufacturers’ catalogues or in advertisements in glass trade publications such as The Pottery Gazette and Glass Trades Review.\nSome insight into the sugar basin can also be gained from a brief consideration of the history of table sugar. Today, one has a choice among packets of various artificial sweeteners and, perhaps, real sugar (sucrose) that is a fine, white powder that dissolves quite readily in coffee or tea. A century or so ago, real sugar was coarse in texture and rather lumpy. A roomy sugar basin provided space enough to tap the lumps with the underside of the spoon or, if necessary, to employ the spoon’s edge to cleave a stubborn lump.\nRaymond Notley notes another factor that influenced the size of sugar basins, namely, the elimination of a tariff on sugar imported into Britain. Notley quotes this 1901 reminiscence: “In the early days of glass pressing, sugar basins were smaller, from the fact that sugar was dearer; but duty-free sugar caused the glass manufacturers in the far north to compete in making the biggest sugar and cream they could to retail for sixpence ....” Manufacturer Thomas Kidd and Co. of Manchester was certainly eager to compete, for this firm’s advertisement in the Pottery Gazette and Glass Trades Review for June 1, 1897, offered sugar basins and cream jugs (and many other items) for just 1p each!\nA typical sugar basin may seem rather too large when accompanied by the cream jug in the same motif (Manley’s book, p. 109, shows nine different sets, and this disparity is readily seen). Some original catalogues depict the cream jug positioned above the sugar basin, and some show the cream jug sitting inside the sugar basin! This is impractical for actual use, of course, but the economy of space does make it easier to display one’s collection!\nAbout the Author\nDr. James S. Measell is Historian at the Fenton Art Glass Company in Williamstown, West Virginia, USA. He and his wife Brenda have collected British pressed glass since the late 1960s.\nThey have enjoyed several recent visits to England and are looking forward to the National Glass Fair in May 2010.\nThey can be reached via email: email@example.com\nChiarenza, Frank and Slater, James. The Milk Glass Book (Schiffer Publishing, 1998).\nCottle, Simon. Sowerby Gateshead Glass (Tyne and Wear Museums Service, 1986).\nHajdamach, Charles. British Glass, 1800-1914 (Antique Collectors’ Club, 1991).\nLattimore, Colin R. English 19th-Century Press-Moulded Glass (Barrie & Jenkins, 1979).\nManley, Cyril. Decorative Victorian Glass (Ward Lock, 1981).\nMorris, Barbara. Victorian Table Glass and Ornaments (Barrie & Jenkins, 1978).\nMurray, Sheilagh. The Peacock and the Lions (Oriel Press, 1982).\nNotley, Raymond. Pressed Flint Glass (Shire Publications, 1986).\nSlack, Raymond. English Pressed Glass, 1830-1900 (Barrie & Jenkins, 1987).\nThompson, Jennie. The Identification of English Pressed Glass, 1842-1908 (Dixon Printing Co., 1989).\nPlease note that the content of this article is the sole intellectual property of the author. No reproduction or reference to the text of this article may be made without the express permission of the author."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:a38b6dca-3f7f-4170-9483-a12512cbf884>","<urn:uuid:b4cd6254-48a2-4db7-b1eb-7fce3cd5dc7d>"],"error":null}
{"question":"How does validation review contribute to security program excellence?","answer":"Validation review contributes to operational excellence, regulatory compliance, and civil liability reduction by validating the controls, notification, and response for programs, systems, and mitigation strategies, and is considered a non-optional component of a professionally run security program.","context":["Strategic Planning: Program Life Cycle\nCreated by the Security Executive Council\nThe following is an abbreviated portion of the Security Executive Council's strategic planning process. The process represents a compilation of methods successfully used at several companies we have collaborated with. It should be noted that your situation will be unique and, therefore, you should make the needed modifications to make it fit with your organization's risk profile, corporate culture and policies.\nIf you would like to learn more about this process, contact us.\nSECURITY PROGRAM LIFE CYCLE\nThe Security Program Life Cycle is a process whereby security improvements are reviewed on a continuous basis. The following provides a summary of each of the segments in the image:\n- Senior Management Input. The cycle begins with a meeting between the security leader, senior executives and business unit leaders. The purpose is to gain insight into the management philosophy, the culture of the business, the long- and short-term objectives, and management’s expectation of security needs.\n- Crime Risk Assessment. In order to understand the physical environment in which business will be conducted, it is imperative that a review of crime statistics in the surrounding area be conducted. This is best accomplished by a direct interface with local, state and federal public safety officials. Country assessments may also be considered.\n- Peer Company Benchmarking. Because no one company has all the answers, it is a good idea to benchmark with peer companies to determine the successes and failures they have encountered when identifying and applying security solutions. However, recognize that your organization will be unique to your peers so benchmarking gives only a portion of the picture.\n- Organizational Security Risk Assessment. This is designed to assess the security posture at the organization; identify risks that impact both the short-and long-term survivability of the organization; and provide cost-effective solutions to reduce or eliminate identified risks.\n- Baseline Security. In order to ensure that all operations maintain an acceptable level of protection, minimum security guidelines should be developed. These guidelines ensure that the organization meets an acceptable baseline level of security.\n- Enhanced Security. Solutions to security risks beyond the baseline risks should be measured on a scale. The scoring system is used to measure progress toward the implementation of security solutions identified in the security risk assessment.\n- Security Systems Design. The results of Baseline Security and Enhanced Security processes provide the foundation for a security plan that is customized for the organization, based on its needs and risks. This security system is designed to reduce risks without impeding business operations.\n- Security Program Plan Design. The resulting security plan is a living process that will recycle itself through continued risk assessments and benchmarking efforts. Costs and restrictions that impede the business operation will determine the degree of risk that management is willing to accept.\n- Validation Review. To achieve operational excellence, regulatory compliance and civil liability reduction validation or audits are no longer optional in a professionally run security program. Validation of the controls, notification and the response for programs, systems and mitigations strategies is imperative.\nFor more information on this topic see Program Strategy & Operations: Strategic Planning/Management\nWatch our 3-minute video\nto learn about how the SEC works with security leaders. Contact us at: contact @secleader.com\nCopyright Security Executive Council. Last Updated: October 26, 2018\nYou can download a PDF of this resource below."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:744b0fad-3b1b-4ba1-88be-90b43daf246b>"],"error":null}
{"question":"What are the technical challenges of using hydrogen as a carrier gas in mass spectrometry, and how does this connect to recent EV adoption trends in terms of overcoming infrastructure barriers?","answer":"Using hydrogen as a carrier gas in mass spectrometry faces several technical challenges. These include the need for efficient vacuum pumps to maintain required vacuum levels, reduced sensitivity (2-5 fold reduction), potential ion-molecule reactions in the source, and the requirement for proper conditioning routines. Additionally, MS vacuum pumps must work harder with hydrogen due to its lower viscosity, and special source components may be needed. Similarly, EV adoption also faces infrastructure barriers, particularly in terms of charging stations. However, progress is being made with initiatives like the National Highway Charging Collaborative planning to install charging stations at 4,000 highway locations, and Democratic lawmakers proposing a bill for a nationwide charging network within five years.","context":["05 Aug 2019\nUsing Hydrogen Carrier Gas with Mass Spectrometric Detection\nOur recent discussion on the use of hydrogen as a carrier for gas chromatography applications elicited many questions and comments. A particularly common question was ‘what are the considerations for using hydrogen carrier with MS detectors?’.\nThe answer – it’s possible but not straightforward!\nThere is a spectrum of issues when switching to hydrogen carrier but rarely have any of the issues been insurmountable with a bit of determination. Although the drivers for change may not be convincing at present, as the availability of helium decreases costs go up. This means that it’s time to get serious about making the change.\nWhat follows is a brief discussion on the known issues and various approaches to adopting hydrogen as the carrier for GC-MS applications, to help inform decisions and practice.\nFlow rates and column dimensions\nHydrogen is around half as viscous as helium and there is a vacuum at the column outlet. This means that the head pressures required to generate the desired column flow rate can be too low for instrument pneumatic controls to deliver in a reproducible fashion. In most cases, the answer is to use a shorter, narrower internal diameter column, to achieve good linear velocity (>35 cm sec-1) at lower volumetric flow rates and head pressures that the instrument pneumatic systems can reproducibly deliver. Flow rates of between 0.3 and 0.4 mL/min are prevalent in literature, with linear velocity in the 40 cm sec-1 range or higher. This is appropriate as hydrogen is around twice as efficient as helium in this range.\nFigure 1: Plots of inlet pressure vs. column length, with hydrogen carrier gas and vacuum compensation on.\nColumn inner diameters (mm): (a) 0.20, (b) 0.25, (c) 0.32, (d) 0.53; column temperature: 50 °C; average linear velocity: 40 cm/s.\nThe blue shaded area designates negative inlet pressures.\nDelivering lower volumes of hydrogen into the ion source offers other advantages as we will see later.\nTwo further considerations result from the use of narrow internal diameter columns;\n1) The stationary phase capacity of the column will be more limited and therefore injection volumes or sample concentration may need to be reduced\n2) Translation software may be required to maintain retention times between the original and new methods. As linear velocity changes the rate of change in temperature per unit time may need to be adjusted to maintain the elution order of the analytes. At the very least, verify the identity and elution order of analytes under the new column conditions.\nA further impact of the use of lower head pressures is the rate at which the sample is transferred to the GC column, which can be further exacerbated using a lower viscosity carrier gas. To avoid issues of backflash (overfilling of the inlet liner due to solvent expansion, leading to carry-over and/or irreproducible peak areas), a pressure pulsed injection may be considered, which applies an increased head pressure to the system during the injection phase of analysis.\nMS Vacuum System\nDue to the lower viscosity of hydrogen, MS vacuum pumps must work harder to maintain the required vacuum levels. Some vacuum pumps are not capable of achieving the required vacuum.\nInsufficient vacuum within the detector will result in lower inherent instrument sensitivity and may also risk a build-up of hydrogen within the detector. This is an obvious safety concern but will also increase the possibility of reactions between analyte ions and hydrogen carrier (more on this later). Monitor vacuum level and ensure the minimum vacuum level is being achieved (one manufacturer recommends a vacuum no lower than 5 x 10-5 torr for example).\nMore recently, instruments have been designed with pumping systems capable of achieving the required vacuum levels with hydrogen carrier. Check with the manufacturer before making the change to hydrogen. It is also important that the roughing pump outlet is directed into a vented fume-hood or outside.\nMS Source Components\nMost manufacturers will either offer a hydrogen specific ion source design or will provide change-out components to ensure that the source is optimally configured. In most cases, this will be componentry, ensuring the optimum ionisation efficiency within the ion source combined with ‘slotted’ components to allow the most effective pumping-away of the carrier gas from within the ion source.\nThere are also ad hoc reports of hydrogen carrier altering the metallurgic properties of certain metals used within ion sources, as well as changes to how magnets within the ion source, operate. Once again some of these components may need to be changed to work with hydrogen carrier. Contact the manufacturer for more advice on this subject.\nHydrogen is more likely than helium to displace contaminants which build up on roughened or unswept surfaces within the GC system and the mass spectrometer. For this reason, spectra acquired soon after changing the carrier gas will appear very noisy.\nFigure 2: ‘Noisy’ spectrum obtained from PFTBA tune compound ‘scan’ on switching to hydrogen carrier gas.\nManufacturers suggest overnight ‘conditioning’ routines. These might include running at the highest source temperatures and reducing detector (electron multiplier) voltage with the filament turned on. Despite being in the early days of experimenting with hydrogen carrier for MS detection, conditioning periods of up to one week are common. Most manufacturers now suggest overnight conditioning routines.\nFurther, if the system is not properly conditioned, very pronounced chromatographic peak tailing may occur. In some cases, this tail may be chromatographic in origin (i.e. secondary interaction of the analyte with active sites within the system) but occasionally this may be due to the formation of analyte degradation products due to ion-molecule reactions with hydrogen in the ion source. This phenomenon will reduce significantly after proper conditioning.\nFigure 3: Spectra from a tailing GCMS peak which reveals that the tail contains a separate species – postulated to be a degradation product (Figure courtesy of Agilent Technologies, Santa Clara, USA)\nThe use of hydrogen does require more conditioning pre-analysis, even on standing overnight, but this does not significantly impact on the ‘practical usability’ of hydrogen as a carrier.\nTypically, a factor of between 2 - 5 fold reduction in sensitivity is to be expected when using hydrogen as the carrier gas. Baselines may be noisier; however, this can be offset by improvements in peak efficiency due to the higher inherent efficiency of hydrogen carrier and the (typically) shorter retention times.\nThe reduction in sensitivity is typically attributed to the higher source pressures (lower vacuum) associated with the decreased pumping efficiency that comes from using hydrogen. Therefore, using the optimum volumetric flow into the ion source, hydrogen approved ion source components, and highly efficient pumping systems is a pre-requisite. Especially, when carrying out trace analysis.\nHydrogen is inherently more reactive than helium. It is possible, especially at elevated source pressures, to induce chemical ionisation (CI) ‘like’ reactions between analytes and hydrogen ions within the source to form an M+1 ion. Some reports cite hydrogen-induced degradation, in-source reactions, such as the reduction of nitrobenzene to aniline, or hydrogenation of unsaturated species. However, the literature on ion-molecule reactions in electron ionisation (EI) GC-MS, using hydrogen carrier, does not reveal a significant amount of supporting information.\nOnly one or two real examples of changes in analyte composition which can be attributed to hydrogen ion-molecule reactions are known. The general advice would be to assess the possibility of ion-molecule reactions when translating an established method or developing new ones.\nWhen using chlorinated solvents such a dichloromethane or hydrogen sulphide (H2S), encountering the formation of hydrochloric acid at the elevated temperatures within the GC inlet is possible. Reactions in the MS ion source may also be encountered if the temperature is high enough. This acid has the potential to strip deactivation away from the liner and GC column, leading to tailing peaks and/or problems with quantitative reproducibility or linearity. For this reason, working with these solvents when using hydrogen as the carrier is to be avoided. Consider using a highly deactivated liner with bottom restriction, to avoid contact of the analyte and solvent with the hot metal surfaces at the bottom of the inlet. Changing GC consumables regularly is also advisable.\nMS Spectra and Tuning\nMost MS spectra remain very similar. However, there is a tendency for the quality of spectral ‘match’ to drop when using hydrogen carrier gas. Typically, this doesn’t affect positive identification. It's rare to experience a situation where qualifier ion ratios need to be adjusted when quantifying in selected ion mode; however, these limitations should be acknowledged and considered when converting.\nAlthough not all spectra are affected in the same way, there are examples where appreciable changes occur in certain ion ratios between spectra obtained in either helium or hydrogen carrier gas. There are also examples where extra peaks are seen (usually at low abundance) in the spectra acquired with hydrogen as the carrier.\nFigure 4: Spectral differences for EI-GCMS analysis of Lindane (Figure courtesy of Agilent Technologies, Santa Clara, USA)\nIn terms of tuning, in our experience, electronic ionisation tuning characteristics obtained using perfluorotributylamine (PFTBA, HEPTACOSA) are usually consistent with those obtained using helium carrier, and in most instances, a satisfactory tune can be obtained (i.e. the tune ‘passes’ and is within the tolerance of the instrument manufacturers tune targets).\nNevertheless, there are regulatory methods which use alternative tune compounds to PFTBA and also require certain relative ion abundances to be within defined tolerances. Decafluorotriphenylphosphine (DFTPP) for EPA method 8270 appears to give reasonable tune agreement between the two carrier gases. However, it is reported that bromofluorobenzene (BFB), used for EPA methods 524, 624 and 8260B, for example, can suffer from skewing of some key ion ratios when using hydrogen carrier. Especially, in qualifying the relationship between masses 95 and 96 m/z.\nPossible. But not straightforward\nIn summary, it is possible to change carrier gas to hydrogen. But not straightforward.\nMajor considerations for converting to hydrogen carrier gas when using MS detectors are;\n- Use of reduced dimension columns at higher linear velocity but lower volumetric flow\n- Use of efficient vacuum pumps, rated for use with hydrogen and optimise vacuum to the best levels possible\n- Confirm if a source change or source component change is required\n- Ensure a proper conditioning routine is undertaken\n- Use method translation software to ensure peak retention order is maintained and check peak identities to confirm\n- Be alert to the possibility of ion molecule reactions and investigate / mitigate where necessary\n- Confirm spectral integrity and consider impact on quantitative ion ratios in SIM mode\n- Verify tuning compatibility","Electric vehicles (EVs) hold a lot of promise for the private sector — especially as consumers, who are increasingly aware of the relationship between emissions and climate change, are starting to demand eco-friendly delivery options. EV adoption, however, has been slowed down by a few different challenges — the US’s poor EV charging infrastructure in particular.\nNow, however, we’re beginning to see signs that major businesses are willing to buy into EVs, despite potential road bumps.\nHere are the businesses that are leading the way when it comes to EV adoption.\nAmazon and UPS Lead Way on EV Adoption\nTwo delivery giants — Amazon and UPS — have begun to aggressively add EVs to their delivery fleets.\nEarlier this year in January, Amazon ordered 100,000 electric delivery trucks from EV manufacturer Rivian, as well as 10,000 electric delivery rickshaws for their operations in India. Then, around the end of the month, UPS announced that it had ordered 10,000 electric trucks from the UK-based manufacturer Arrival Ltd., and would soon be teaming up with self-driving car manufacturer Waymo for a pilot test of self-driving delivery vehicles.\nThe moves are part of broader pushes towards carbon neutrality and self-driving delivery by the two companies. Last year, Amazon announced the company’s plan to be 100 percent carbon-neutral by the year 2040. UPS already offers carbon-neutral and carbon-offset delivery options.\nThe moves also come as more cities around the U.S., including New York and Philadelphia., have begun to adopt anti-idling laws that allow the city to fine companies over idling delivery vehicles.\nSome cities have even developed apps that allow citizens to report idling vehicles based on that vehicle’s DOT number — making these policies even more costly for delivery companies. Because electric vehicles produce no emissions, they’re typically free from being fined — meaning savings for businesses that adopt EVs for city deliveries.\nThe announcements are both historic. While other companies have announced EV purchases — like Lyft, which plans to deploy 200 EVs in Denver as part of its rental vehicle program there — there’s been nothing near scale of these announced by Amazon and UPS.\nWhile neither UPS nor Amazon has plans to go fully electric any time soon, the purchases are a welcome sign for the EV industry. Coupled with similar positive signals from the individual consumer side of the industry, they likely demonstrate that despite early growing pains, EVs may be on track for widespread adoption in the near future.\nChallenges Facing Further EV Adoption\nHowever, there still remain significant barriers that may slow or prevent full EV adoption, primarily the weak EV charging infrastructure in the US and limited number of charging stations — although this, too, seems like it’s starting to change.\nChargePoint, in coalition with the National Association of Truck Stop Operators (NATSO) has formed the National Highway Charging Collaborative, which plans to install new charging stations at more than 4,000 highway-side locations in the U.S., in order to increase the availability of EV charging stations in rural areas.\nAt the same time, legislative support for stronger EV infrastructure is beginning to build. In February, Democratic lawmakers in the House of Representatives announced a new bill that would create a nationwide EV charging network within the next five years.\nUpgrades to existing infrastructure would likely encourage further adoption. They may also be especially beneficial for businesses like Amazon and UPS, as both companies regularly make deliveries to rural parts of the country — areas that don’t always have the charging infrastructure needed to support EVs.\nThe Future for EVs in Business\nEV adoption in the private sector, which has lagged in the past, seems to be accelerating. Two major delivery companies have now announced that they will be adding significant numbers of EVs to their delivery fleets, with more likely to come in the near future as both pursue low-carbon delivery options.\nWhile challenges remain that may slow down EV adoption — primarily the nation’s weak EV charging infrastructure — the purchases are likely a good sign for the industry and the future of EVs in the private sector."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:1d62af91-72f6-44bb-a4e3-4d1c569d1485>","<urn:uuid:a3b2914f-bfcd-4107-a76d-db9ca5d880cd>"],"error":null}
{"question":"hello what battls happened in Pennsylvania during revolution and civil war??","answer":"During the Revolutionary War, key battles occurred in Pennsylvania when the British attacked and captured Philadelphia in 1777 at the Battle of Brandywine. The Continental Army then spent the difficult winter at Valley Forge before the British retreated in 1778. During the Civil War, the most significant battle in Pennsylvania was the Battle of Gettysburg in 1863, which is considered the turning point of the Civil War. This battle was where the Union forces beat back the Confederate Army of Northern Virginia's final supreme effort at Cemetery Ridge. Pennsylvania remained loyal to the Union throughout the Civil War, providing over 360,000 troops and experiencing Confederate raids in its southern regions.","context":["« PreviousContinue »\nwe have a due regard for the honor and the interest of our mighty nation; and that we keep unsullied the renown of the flag which beyond all others of the present time or of the ages of the past stands for confident faith in the future welfare and greatness of mankind.\" ,\nII It is character that counts in a nation as in a man. It is a good thing to have a keen, fine intellectual development in a nation, to produce orators, artists, successful business men; but it is an infinitely greater thing to have those solid qualities which we group together under the name of character - sobriety, steadfastness, the\nsense of obligation toward one's neighbor and one's God, hard common sense, and, combined with it, the lift of generous enthusiasm toward whatever is right. These are the qualities which go to make up true national great\nIn the long run, in the great battle of life, no brilliancy of intellect, no perfection of bodily development, will count when weighed in the balance against that assemblage of virtues, active and passive, of moral qualities, which we group together under the name of character; and if between any two contestants, even in college sport or in college work, the difference in character on the right side is as great as the difference of\n1 Address at the Quarter-Centennial Celebration of Statehood in Colorado, at Colorado Springs, August 2, 1901. From The Strenuous Life. Second augmented edition. Copyright, 1901. The Century Company, publishers.\n* From The Strenuous Life. Copyright, 1900.\nintellect or strength the other way, it is the character side that will win.\nI RECOLLECT saying to a young friend who was about to enter college, \"My friend, I know that you feel that you ought to be a good man; now, be willing to fight for your principles whenever it is necessary; if you 're willing enough to fight, nobody will complain about your being too virtuous.”\nIf you accept only the weak man, who cannot hold his own, as the type of virtuous man, you will inevitably create an atmosphere among ordinary, vigorous young men in which they will translate their contempt of weakness into contempt of virtue. My plea is that the virtuous man, the decent man, shall be a strong man, able to hold his own in any way, just because I wish him to be an agent in eradicating the misconception that being decent somehow means being weak; I want this to apply to every form of decency, public as well as private.\nThe worst development that we could see in civic life in this country would be a division of citizens into two camps, one camp containing nice, well-behaved, wellmeaning little men, with receding chins and small feet, men who mean well and who if they are insulted feel shocked and want to go home; and the other camp containing robust and efficient creatures who do not mean well at all. I wish to see our side - the side of decency - include men who have not the slightest fear of the\n1 From The Strenuous Life. Copyright, 1900. The Century Company, publishers.\npeople on the other side. I wish to see the decent man in any relation of life, including politics, when hustled by the man who is not decent, able so to hold his own that the other gentleman shall feel no desire to hustle him again. My plea is for the virtue that shall be strong and that shall also have a good time. You recollect that Wesley said he was n't going to leave all the good times to the Devil. In the same way we must not leave strength and efficiency to the Devil's agents. The decent man must realize that it is his duty to be strong just as much as to be decent. There are a good many types of men for whom I do not care; and among those types I would put in prominent place the timid good man — the good man who means well but is afraid. I wish to see it inculcated from the pulpit by every ethical teacher, and in the home, that just to be decent is not enough; that in addition to being a decent man it is the duty of the man to be a strong man. And also this; to let the fact that he is a decent man dawn on his neighbors by itself, and without his announcing it or emphasizing it.\"\nIt is a good thing that of these great landmarks of our history – Gettysburg and Valley Forge - one should commemorate a single tremendous effort and the other what we need, on the whole, much more commonly, and what I think is, on the whole, rather more difficult to do - long-sustained effort. Only men with a touch of the heroic in them could have lasted out that three days' struggle at Gettysburg. Only men fit to rank with the\n1 Permission to use this excerpt granted by The Hart Wagner Pub lishing Company, publishers of Theodore Roosevelt's Realizable Ideals.\ngreat men of all time could have beaten back the mighty onslaught of that gallant and wonderful army of Northern Virginia, whose final supreme effort faded at the stone wall on Cemetery Ridge on that July day fortyone years ago.\nBut after all, hard though it is to rise to the supreme height of self-sacrifice and of effort at a time of crisis that is short, to rise to it for a single great effort - it is harder yet to rise to the level of a crisis when that crisis takes the form of needing constant, patient, steady work, month after month, year after year, when, too, it does not end after a terrible struggle in a glorious daywhen it means months of gloom and effort steadfastly endured, and triumph wrested only at the very end.\nHere at Valley Forge Washington and his Continentals warred not against the foreign soldiery, but against themselves, against all the appeals of our nature that are most difficult to resist — against discouragement, discontent, the mean envies and jealousies and heartburnings sure to arise at any time in large bodies of men, but especially sure to arise when defeat and disaster have come to large bodies of men. Here the soldiers who carried our national flag had to suffer from cold, from privation, from hardship, knowing that their foes were well housed, knowing that time went easier for the others than it did for them. And they conquered, because they had in them the spirit that made them steadfast, not merely on an occasional great day, but day after day in the life of daily endeavor to do duty well.\nWhen two lessons are both indispensable, it seems hardly worth while to dwell more on one than on the other. Yet I think that as a people we need more to learn the lesson of Valley Forge even than that of Gettysburg. I have not the slightest anxiety but that this people, if the need should come in the future, will be able to show the heroism, the supreme effort that was shown at Gettysburg, though it may well be that it would mean a similar two years of effort, checkered by disaster, to lead up to it. But the vital thing for this Nation to do is steadily to cultivate the quality which Washington and those under him so preëminently showed during the winter at Valley Forge - the quality of steady adherence to duty in the teeth of difficulty, in the teeth of discouragement, and even disaster, the quality that makes a man do what is straight, and decent, not one day when a great crisis comes, but every day, day in and day out, until success comes at the end.\nVI REMEMBER always that the securing of a substantial education, whether by the individual or by a people, is attained only by a process, not by an act. You can no more make a man really educated by giving him a certain curriculum of studies than you can make a people fit for self-government by giving it a paper constitution. The training of an individual so as to fit him to do good work in the world is a matter of years; just as the training of a nation to fit it successfully to fulfill the duties of self-government is a matter, not of a decade or two, but of generations. There are foolish empiricists who believe that the granting of a paper constitution, prefaced by some high-sounding declaration, of itself confers the\n1 Remarks at the Washington Memorial Chapel, Valley Forge, Pennsylvania, June 19, 1904.","History >> US Geography >> US State History\nThe land of Pennsylvania was inhabited by Native American tribes long before the first Europeans arrived. These tribes included the Shawnee in the southwest, the Susquehannock in the south, the Delaware in the southeast, and the Iroquois (Oneida and Seneca tribes) in the north.\nEuropeans began to explore the region around Pennsylvania in the early 1600s. English explorer Captain John Smith sailed up the Susquehanna River and met with some of the Native Americans in the area in 1608. Henry Hudson also explored the area on behalf of the Dutch in 1609. Although both England and the Netherlands laid claim to the land it was several years before people began to settle Pennsylvania.\nWilliam Penn founded the colony of Pennsylvania\nAn English Colony\nThe first settlers in the region were the Dutch and the Swedish. However, the British defeated the Dutch in 1664 and took control over the area. In 1681, William Penn was given a large area of land by King Charles II of England. He named the land Pennsylvania after his family name \"Penn\" and after the forests in the land (\"sylvania is \"forest land\" in Latin).\nPenn wanted his colony to be a place of religious freedom. Some of the first settlers were Welsh Quakers looking for a place where they could practice their religion without persecution. Throughout the early 1700s more people from Europe immigrated to Pennsylvania. Many of them came from Germany and Ireland.\nDuring the 1700s, Pennsylvania had many border disputes with other colonies. Portions of northern Pennsylvania were claimed by New York and Connecticut, the exact southern border was in dispute with Maryland, and parts of the southwest were claimed by both Pennsylvania and Virginia. Most of these disputes were ironed out by 1800. The border with Maryland, which was called the Mason-Dixon Line after surveyors Charles Mason and Jeremiah Dixon, was established in 1767. It would later be considered the border between the North and the South.\nWhen the American Colonies decided to fight for their independence during the American Revolution, Pennsylvania was at the center of the action. Philadelphia served as the capital throughout much of the revolution and was the meeting place for the First and Second Continental Congress. It was at Independence Hall in Philadelphia where the Declaration of Independence was signed in 1776.\nThe clocktower at Independence Hall\nby Captain Albert E. Theberge (NOAA)\nSeveral battles were fought in Pennsylvania as the British wanted to capture Philadelphia. In 1777, the British defeated the Americans at the Battle of Brandywine and then took control of Philadelphia. That winter General George Washington and the Continental Army stayed at Valley Forge in Pennsylvania, not too far outside Philadelphia. The British left the city a year later in 1778, retreating back to New York City.\nAfter the war ended, the Constitutional Convention met at Philadelphia to create a new Constitution and government for the country in 1787. On December 12, 1787, Pennsylvania ratified the Constitution and became the 2nd state to join the Union.\nWhen the Civil War broke out in 1861, Pennsylvania remained loyal to the Union and played a vital role in the war. The state provided over 360,000 troops as well as supplies for the Union army. Since Pennsylvania was near the border between the North and the South, southern Pennsylvania was raided by the Confederate Army. The largest battle to take place in the state was the Battle of Gettysburg in 1863, which many consider to be the turning point in the war. Gettysburg was also the site of Abraham Lincoln's famous Gettysburg Address.\nPennsylvania Memorial, Gettysburg Battlefield\nMore US State History:\n- 1608 - English explorer Captain John Smith sails up the Susquehanna River.\n- 1609 - Henry Hudson claims much of the region for the Dutch.\n- 1643 - Swedish settlers found the first permanent settlement.\n- 1664 - The land comes under the control of the British.\n- 1681 - William Penn is given a large tract of land by King Charles II. He names it Pennsylvania.\n- 1701 - The Charter of Privileges is signed by William Penn establishing a government.\n- 1731 - The first U.S. library is opened by Benjamin Franklin.\n- 1767 - The Mason-Dixon Line is agreed upon as the southern border with Maryland.\n- 1774 - The First Continental Congress meets in Philadelphia.\n- 1775 - The Second Continental Congress meets, creating the Continental Army with George Washington as the leader.\n- 1777 - The city of Philadelphia is occupied by the British.\n- 1780 - Slavery is abolished.\n- 1787 - Pennsylvania ratifies the Constitution and becomes the 2nd state.\n- 1812 - The state capital moves to Harrisburg.\n- 1835 - The Liberty Bell cracks.\n- 1863 - The Battle of Gettysburg occurs. It is the turning point of the Civil War.\n- 1953 - Dr. Jonas Salk discovers the vaccine for polio while working at the University of Pittsburgh.\nHistory >> US Geography >> US State History"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:acaf0d3b-f395-4c81-b3ea-b88fff929cc5>","<urn:uuid:0c040db0-f89b-4484-a619-b57696c6e6b0>"],"error":null}
{"question":"Could you explain both the optimal timing for food digestion and the potential risks of late-night eating? I'm particularly interested in understanding how different meal times affect the body's digestive processes and overall health.","answer":"Food digestion typically takes 24-72 hours, with 6-8 hours for food to pass through the stomach and small intestine. Different foods have varying digestion times - for instance, fruits digest in 20-40 minutes, vegetables in 30-50 minutes, and proteins like beef can take 3-4 hours. As for late-night eating, it can have several negative health effects. It alters the body's natural rhythm, affects hormones and digestion, and increases the risk of acid reflux when eating less than 4 hours before bedtime. Late eating can also lead to metabolic diseases, disturbed sleep patterns, and weight gain due to decreased carbohydrate metabolism and altered cortisol concentrations.","context":["Digestion Time Of Various Foods\nDr. Michael Klaper believes inside every person, there's an innate force that wants to be healthy.Digestion time is basically a process of breaking down big food particles into individual molecules, tiny enough to squeeze through the intestinal lining into the bloodstream.\nYour body uses mechanical and chemical means to digest food and it varies depending on the individual according to Michael Picco, M.D With Mayo Clinic gastroenterologist.\nThe digestion process takes between 24 and 72 hours, six to eight hours to pass through your stomach and small intestine. Then the food enters your large intestine (colon) for further digestion and absorption of water. Elimination of undigested food residue usually begins after 24 hours. Complete elimination from the body may take several days.\"\nWater: when stomach is empty, leaves immediately and goes into intestines.\nJuices: Fruit vegetables, vegetable broth - 15 to 20 minutes.\nSemi-liquid: Blended salad, vegetables or fruits) - 20 to 30 min.\nFruits: Watermelon - 20 min.digestion time. Other melons - Cantaloupe, Cranshaw, Honeydew, etc. - 30 min. Oranges, grapefruit, grapes - 30 min. Apples, pears, peaches, cherries, etc. - digest in 40 min.\nVegetables: Raw tossed salad vegetables - tomato, lettuces, cucumber, celery, red or green pepper, other succulent vegetables - 30 to 40 min. digestion. - Steamed or cooked vegetables Leafy vegetables - escarole, spinach, kale, collards etc. - 40 min. - Zucchini, broccoli, cauliflower, string beans, yellow squash, corn on cob - all 45 min. digestion time Root vegetables - carrots, beets, parsnips, turnips etc. - 50 min.\nSemi-Concentrated Carbohydrates - Starches. Jerusalem artichokes & leafy, acorn & butternut squashes, corn, potatoes, sweet potatoes, yam, chestnuts - all 60 min. digestion.\nConcentrated Carbohydrates - Grains: Brown rice, millet, buckwheat, cornmeal, oats (first 3 vegetables best) - 90 min. Legumes & Beans - (Concentrated Carbohydrate & Protein) Lentils, limes, chick peas, peas, pigeon peas, kidney beans, etc. - 90 min. digestion time soy beans -120 min. digestion time\nSeeds & Nuts. Seeds - Sunflower, pumpkin, pepita, sesame - Digestive time approx. 2 hours. Nuts - Almonds, filberts, peanuts (raw), cashews, Brazil, walnuts, pecans etc. - 2 1/2 to 3 hours to digest.\nDairy: Skim milk, cottage or low fat pot cheese or ricotta - approx. 90 min. digestion time whole milk cottage cheese - 120 min. digestion whole milk hard cheese - 4 to 5 hours digestion time\nAnimal proteins: Egg yolk - 30 min. digestion time Whole egg - 45 min. Fish - cod, scrod, flounder, sole seafood - 30 min. digestion time Fish - salmon, salmon trout, herring, (more fatty fish) - 45 min. to 60 digestion time Chicken - 11/2 to 2 hours digestion time (without skin) Turkey - 2 to 2 1/4 hours digestion time (without skin) Beef, lamb - 3 to 4 hours digestion time Pork - 41/2 to 5 hours.\nTop Tips for Good Digestion\nHere are the time sequences for different food groups:\nIce cold drinks can slow down the digestive process, think of it as putting ice on a muscle. The muscle stiffens and does not function as well. Warm or room temperature water, juice, or decaf tea will encourage proper digestion. (Just remember the traffic jam – drink liquids prior to meals)\nIt is important to be regular with what you eat and the times of day you eat. Eating similar food groups and at similar times each day has a regulating effect on your digestive system. Regular in means regular out.\nOver consumption is the number one cause of indigestion. Our brain signals the feeling of fullness about ten minutes after we're actually full. So stop eating before you are full. Odds are you'll feel full ten minutes later!\nIncomplete chewing and talking while eating can cause premature swallowing. Our digestive systems are not designed to digest large pieces of food, when we put large pieces in our stomachs it can lead to incomplete digestion (aka: digestive discomfort).\nEating when you are rushed increases your stress and slows down the digestive process. Create a nice calming atmosphere when eating and make sure you can devote time to eating.\nWhen you slouch or hunch over extra pressure is put on the digestive organs in your abdomen. This extra pressure can cause poor digestion. You should practice sitting with your shoulders back and your chin tucked in. This will allow more room for the digestive organs and will help improve digestion.\nOur bodies, including our digestive system, slow down in the evening hours as it gets ready to rest and rejuvenate. When we put food into our stomachs at these late hours there are not enough digestive enzymes to properly digest it. This undigested food sits in your stomach and will often disturb your sleep.\nForget about not be active for 30 minutes after each meal. Increased physical activity actually helps jump start your digestive system and increases the production of digestive enzymes. This will lead to more complete digestion of your food and less digestive discomfort!\nSpinal twists allow excess toxins in the digestive system to be released, which has a calming effect. While in a cross legged sitting position, slowly turn to the right and hold while taking 5 deep breathes then repeat this process on the left side. Source: Puristat.com, Mayo Clinic.com","Whether it is eating a full meal close to bed time or a light snack at night, eating late at night can have adverse effect on the body. Eating late at night alters the body’s natural rhythm and can affect the hormones, digestion, sleeping pattern and even memory. Eating late at night also increases the risk of developing lifestyle diseases such as diabetes, heart issues and unintentional weight gain.\n7 Reasons Why Eating Late at Night is Bad For You\nThe most common bad effects of eating late at night include:\nEating late at night makes it difficult to lose weight. Weight loss therapy not only depends on the calorie intake and distribution of macronutrient, but it also depends on the timing of food intake. Snacking at night causes intake of calories which is stored as fat in the body and thus more difficult to get rid of.\nIncreased Risk of Metabolic Diseases:\nEating late at night causes decrease in rate of carbohydrate metabolism, decreased resting energy expenditure, decrease in glucose tolerance, and alteration in cortisol concentration. This in turn affects the normal metabolic health and increases the risk of developing metabolic diseases such as diabetes. It also increases the risk of having heart attack and other heart issues.\nAltered Memory and Concentration Levels:\nLate night eating affects the brain health too. Eating at odd hours affects the normal circadian system of the body which in turn lowers the brain’s ability to learn, concentrate and memorize. It affects the brain and learning behaviour.\nEating late at night can affect the quantity and quality of sleep. It alters the normal sleeping pattern and can cause lead to bizarre dreaming while sleeping. Spicy food and dairy items has been linked to disturbed dreams.\nIncreased Risk of Acid Reflux:\nEating a heavy meal before sleeping or leaving less than 4 hours between the last meal and bedtime can lead to acid reflux or heart burn. This is mainly because of opening of the lower sphincter of the oesophagus, which causes backward flow of acid from the stomach into the mouth. Leaving adequate gap between dinner and bedtime provides ample time for the stomach to clear and remain acid free while sleeping.\nSnacking late at night repeatedly without actually being hungry has been associated with deeper health issues such as night eating syndrome and binge eating disorder. This could also be related to psychological issues such as depression, stress, loneliness etc.\nExcessive Hunger the Next Day:\nEating late at night can lead to a feeling of excessive hunger the next day. This is likely due to altered production of hunger hormone called ‘ghrelin’. This can actually lead to weight gain as well.\nWays to Control Eating Late at Night\nIt is advised to eat a balanced meal between 7:00 to 7:30 pm. A balanced meal will keep you satiated and provide better control over mid night craving until bedtime.\nAvoid stocking junk food at home. If there are no junk food at home, the chances of mid night snacking will be drastically reduced. Replacing junk food with healthy alternative such as fruits, dry fruits, oatmeal crackers etc. is better than eating unhealthy food at night before sleeping.\nSometimes the body may mistake feeling thirsty with feeling hungry. It is therefore important to understand if the body need water to quench the thirst instead of eating snacks first.\nStress management can also help us in reducing anxiety and other psychological issues. Relaxing and calming bed time routine can help is minimizing stress and the urges to binge eat.\nDespite all the measures, if the body stills craves for a snack at night, one can consider eating the following instead of an unhealthy snack.\n- A piece of grilled chicken\n- Half an apple\n- Have a banana with a teaspoon of peanut butter\n- A few crackers with tablespoon of hummus\n- Hard-boiled egg\n- Handful of nuts and dry fruits\n- Mashed avocado on toast\n- Small bowl of fruits with cottage cheese.\n- ill Effects of Eating Late at Night\n- Why are We Hungry in the Morning When We Eat Late at Night\n- 12 Ways to Stop Binge Eating At Night\n- Does Late Night Dinner cause Acid Reflux?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:c07515ee-bc1b-4252-b5a1-ec1618a3239d>","<urn:uuid:f44bb2b5-467e-4ac2-9b02-8f50a7c4c9b9>"],"error":null}
{"question":"How are Conus marmoreus shells used commercially and what size do adults typically reach?","answer":"Conus marmoreus shells are traded in the collector market and gathered for the local tourist market. While there are no quantitative data on shell removal numbers, this species is commonly available and typically trades at low prices. The shells are also collected for biomedical research, specifically for treatment of neuropathic pain. Adult specimens of the species grow to approximately 150 mm in size.","context":["|Scientific Name:||Conus marmoreus|\n|Species Authority:||Linnaeus, 1758|\nConus crosseanus Bernardi, 1861\nConus maculatus Perry, 1811\nConus pseudomarmoreus Crosse, 1875\nConus suffusus G. B. Sowerby II, 1870\n|Taxonomic Notes:||There is a subspecies described of Conus marmoreus marmoreus: Conus marmoreus granulatus, Sowerby ii, 1839 (Röckel et al. 1995).|\n|Red List Category & Criteria:||Least Concern ver 3.1|\n|Assessor(s):||Kohn, A., Raybaudi-Massilia, G., Poppe, G. & Tagaro, S.|\n|Reviewer(s):||Peters, H. & Bouchet, P.|\n|Facilitator/Compiler(s):||Hines, A., Peters, H. & Checkley, J.|\nThis species occurs from India east through the Coral Triangle, northern Australia and the Western Pacific to the Marshall Islands and Fiji, is locally common, not known to have any major threats, and likely to occur in marine protected areas. It listed as Least Concern.\n|Range Description:||This species occurs from India east through the Coral Triangle, northern Australia and the Western Pacific to the Marshall Islands and Fiji (Röckel et al. 1995). It has a depth distribution of 1-30 m and may occur deeper.|\nNative:Australia (Coral Sea Is. Territory, Lord Howe Is., Northern Territory, Queensland, Western Australia); Bangladesh; Brunei Darussalam; Cambodia; China; Christmas Island; Cocos (Keeling) Islands; Cook Islands; Disputed Territory; Fiji; French Polynesia (Marquesas); Guam; India (Andaman Is., Andhra Pradesh, Nicobar Is., Orissa, Tamil Nadu, West Bengal); Indonesia; Malaysia; Marshall Islands; Micronesia, Federated States of ; Myanmar; New Caledonia; Northern Mariana Islands; Papua New Guinea; Philippines; Samoa; Singapore; Solomon Islands; Sri Lanka; Taiwan, Province of China; Thailand; Timor-Leste; Vanuatu; Viet Nam; Wallis and Futuna\n|FAO Marine Fishing Areas:|\n|Range Map:||Click here to open the map viewer and explore range.|\n|Population:||There are currently no data in the literature concerning populations of this species. It is locally common.|\n|Current Population Trend:||Unknown|\n|Habitat and Ecology:||This species occurs at depths of 1-30 m on coral reef platforms and lagoon pinnacles, on coral debris and in sand often under rocks or among weed. Some forms are active during the whole day, others only active at rising tide where they feed on various gastropods including Conidae. Adults of the species will grow to approx 150 mm (Röckel et al. 1995).|\n|Use and Trade:||In common with all Conus spp, shells of this species are traded for the collector market. The shells are also gathered for the local tourist market and by tourists visiting the country. There are no quantitative data available on the number of shells removed, however, this iconic species is traded for prices typically in the low range; availability; common (Rice 2007). This species is gathered for biomedical research for treatment of neuropathic pain (Livett 2004).|\n|Major Threat(s):||There are no known threats to this species at this time.|\n|Conservation Actions:||The wide distribution of this species is likely to overlap numerous marine protected areas.|\nBruce G. Livett, Ken R. Gayler and Zeinab Khalil. 2004. Drugs from the Sea: Conopeptides as Potential Therapeutics.\nIUCN. 2013. IUCN Red List of Threatened Species (ver. 2013.1). Available at: http://www.iucnredlist.org. (Accessed: 12 June 2013).\nRice, T. 2007. A Catalog of Dealers' Prices for Shells: Marine, Land and Freshwater. Sea and Shore Publications.\nRöckel, D., Korn, W. & Kohn, A.J. 1995. Manual of the Living Conidae, Vol 1. Verlag Christa Hemmen.\n|Citation:||Kohn, A., Raybaudi-Massilia, G., Poppe, G. & Tagaro, S. 2013. Conus marmoreus. The IUCN Red List of Threatened Species 2013: e.T192701A2144744.Downloaded on 24 July 2016.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:d166e213-7659-4b1c-8094-3b2ded2a529a>"],"error":null}
{"question":"What role does the DSM-5 play in autism diagnosis and what are the identified risk factors for developing ASD?","answer":"The DSM-5 serves as the authoritative diagnostic guide for medical professionals diagnosing autism spectrum disorders, particularly those without extensive autism-related experience. While the manual helps ensure consistent and reliable diagnoses, research has identified several risk factors for developing ASD, including having a sibling with ASD, having older parents, having certain genetic conditions, and having a very low birth weight. The relationship between these risk factors and the formal diagnostic process helps provide a comprehensive understanding of ASD.","context":["Causes And Risk Factors\nResearchers dont know the primary causes of ASD, but studies suggest that a persons genes can act together with aspects of their environment to affect development in ways that lead to ASD. Some factors that are associated with an increased likelihood of developing ASD include:\n- Having a sibling with ASD\n- Having older parents\n- Having certain genetic conditions\n- Having a very low birth weight\nWhat Is The Dsm\nMedical professionals and researchers often consult the DSM-5, a manual sometimes referred to as the bible of mental conditions. In this article the criteria for an autism diagnosis according to the DSM-5 will be examined.\nThe Diagnostic and Statistical Manual of Mental Disorders is a manual often cited in scientific journals medical professionals like psychiatrists and pediatricians refer to it when diagnosingbut for some of us it appears to be a bit of a daunting read reserved for those with multiple abbreviations accompanying their name.\nThe name of the handbook, The Diagnostic and Statistical Manual of Mental Disorders contributes to the intimidation factor. While it was never intended as a beach read for the public, the DSM-5 contains a lot of diagnostic information that may be useful for educators and parents, in addition to its intended medical and research audience.\nMost doctors in the US use the manual as the authoritative guide when diagnosing autism spectrum disorders . For medical professionals without a lot of autism related experience, the DSM-5 provides guidelines and criteria to facilitate consistent and reliable diagnoses.\nIt may be a valuable diagnostic tool, but its also been criticized by many cliniciansspecifically criticism regarding its validity, reliability and utility . Issues relating to overdiagnosis and the risk of pathologizing normal behavior or conditions are further areas of concern according to Young .\nCriteria C And D For Social Anxiety Disorder\nCriteria C and D for Social Anxiety Disorder\nC. Social situations almost always provoke fear or anxiety.\nCriteria C captures the pervasiveness of the experience. We arent talking about situational anxiety that comes and goes were talking about anxiety that is persistent and present in many contexts.\nD. The social situations are avoided or endured with intense fear or anxiety.\nAvoidance and anxiety go hand and hand. When we experience anxiety, we often avoid the thing that triggers the anxiety . Criteria C and D capture this element of social anxiety. The person nearly always pushes through the avoidance but experiences intense fear and anxiety , or they avoid these situations whenever possible .\nRecommended Reading: How To Relate To An Autistic Person\nUnderstanding The Three Levels Of Autism\nSteven Gans, MD, is board-certified in psychiatry and is an active supervisor, teacher, and mentor at Massachusetts General Hospital.\nThere are three levels of autism spectrum disorder , which are described in the Diagnostic and Statistical Manual of Mental Disorders, 5th Edition .\nEach person with ASD is further diagnosed with either ASD level 1, level 2, or level 3, depending on how severe their disorder is and how much support they need in their daily life.\nThe levels range from least to most severe, with ASD level 3 describing an individual who has the most severe level of ASD symptoms, and ASD level 1 describing someone with symptoms on the milder end of the spectrum.\nThis article discusses the symptoms that are typical of each of the three ASD levels. It also includes realistic examples of the strengths and limitations that are unique to each level.\nVerywell / Cindy Chung\nAbout The Dsm And Autism Spectrum Disorder Diagnosis\nWhen diagnosing autism, professionals like paediatricians, psychiatrists, psychologists and speech pathologists use the Diagnostic and statistical manual of mental disorders , or DSM-5, produced by the American Psychiatric Association.\nThe DSM-5 lists the signs and symptoms of autism and states how many of these must be present to confirm a diagnosis of autism spectrum disorder. The DSM-5 refers to signs and symptoms, but this article talks about signs and characteristics.\nTo find out whether a child has autism signs and characteristics and meets DSM-5 criteria, professionals also need to do extra tests. These tests are called adiagnostic assessment.\nYou May Like: Can 2 Autistic Parents Have A Normal Child\nQuestion: Whats New In The Dsm\nANSWER: As you can see, there have many changes to the diagnostic criteria! Communication and social deficits have been grouped together because they often overlap. Under the DSM-IV, language delay was a very important diagnostic piece however, under the DSM-5, a language delay will no longer be considered a part of the diagnosis.\nAnother important difference includes the changes to the criteria regarding restricted and repetitive behaviors. New is the inclusion of hyper- or hyporeactivity to sensory input. Sensory differences have long been recognized as a challenge for many individuals with ASD and are now recognized as part of the official diagnostic criteria.\nCriteria B For Social Anxiety Disorder\nCriteria B for Social Anxiety Disorder\nCriteria B for social anxiety includes: “The individual fears that he or she will act in a way or show anxiety symptoms that will be negatively evaluated.” Examples include things such as:\n1) Fear of being humiliated or embarrassed\n2) Fear of situations where one may be judged negatively\n3) Fear of physical manifestations of anxiety\n4) Fear of offending others\n5) Fear of being Rejected\nHere the person is highly anxious that others will perceive them as “anxious, weak, crazy, stupid, boring, intimidating, dirty, or unlikable” . The person is often particularly nervous about showing their anxiety symptoms through blushing, trembling, sweating, stumbling over one’s words, or staring, which will lead to being rejected or negatively evaluated.\nThose from more collectivistic cultures are more prone to worry that they will accidentally offend others .\nAlso Check: Is There Different Types Of Autism\nIs Autism A Mental Illness Dsm\nAccording to the Diagnostic and Statistical Manual of Mental Disorders , autism spectrum disorder is also classified as a mental illness. As a result, autism is classified as a neurodevelopmental disorder in the DSM-5.\nThe DSM-5 describes autism spectrum disorder as a mental disorder. The DSM-5 defines autism as a psychiatric disorder . Many characteristics of autism overlap with those of other mental illnesses, which is why it is frequently misdiagnosed. The obsession of children with autism with a particular field of study may be accompanied by an indifference to the interests and concerns of others. Self-obsession is a common feature of narcissistic personality disorder. Autism is more likely than other illnesses to result in mental illness. In addition to mental illness, the majority of children and adults with autism have developmental disabilities. It is possible for a secondary diagnosis to direct therapy, academic support, and mental health services. People with autism and those suffering from other mental illnesses may exhibit behaviors similar to one another.\nWhat Is Autism Spectrum Disorder\nAutism spectrum disorder is a complex developmental condition involving persistent challenges with social communication, restricted interests, and repetitive behavior. While autism is considered a lifelong disorder, the degree of impairment in functioning because of these challenges varies between individuals with autism.\nDon’t Miss: How Does Autism Affect The Brain\nQuestion: What Are The Dsm\nANSWER: Under the newest changes, the DSM-5 diagnostic criteria for Autism Spectrum Disorder require the person to demonstrate characteristics in two categories.\n- Impairment in social communication and interaction. In the DSM-IV, communication and social were seen as two separate areas. In the DSM-5, because communication and social skills operate together, they are now combined. Characteristics may include the following:\n- Deficits in reciprocity\nFor a person to meet criteria, characteristics must be present during a childs early development. However, the characteristics may not become evident until the child is older and is placed in social situations that exceed his or her social abilities.\nRead Also: Autism Schedule Board\nQuestion: What About Those Who Have A Diagnosis Now\nANSWER: Anyone diagnosed with any of the four disorders from the DSM-IV should still meet the criteria for ASD in the DSM-5, or be found to have another, more accurate, DSM-5 diagnosis. According to the DSM-5: Individuals with a well-established DSM-IV diagnosis of autistic disorder, Aspergers disorder, or pervasive development disorder not otherwise specified should be given the diagnosis of autism spectrum disorder. Individuals who have marked deficits in social communication but whose symptoms do not otherwise meet criteria for autism spectrum disorder should be evaluated for social communication disorder . .\nIndividuals with a long-standing diagnosis, therefore, may continue to carry the new ASD diagnosis. As noted above, the DSM-5 also includes a new communication disorder called Social Communication Disorder, or SCD. Even though the definitions of ASD and SCD share similar social communication deficits, the difference is that the criteria for ASD includes restricted and repetitive behaviors, interests, and activities.\nAlso Check: I Think I Might Be Autistic\nSocial Communication / Interaction Behaviors May Include:\n- Making little or inconsistent eye contact\n- Appearing not to look at or listen to people who are talking\n- Infrequently sharing interest, emotion, or enjoyment of objects or activities\n- Not responding or being slow to respond to ones name or to other verbal bids for attention\n- Having difficulties with the back and forth of conversation\n- Often talking at length about a favorite subject without noticing that others are not interested or without giving others a chance to respond\n- Displaying facial expressions, movements, and gestures that do not match what is being said\n- Having an unusual tone of voice that may sound sing-song or flat and robot-like\n- Having trouble understanding another persons point of view or being unable to predict or understand other peoples actions\n- Difficulties adjusting behaviors to social situations\n- Difficulties sharing in imaginative play or in making friends\nWhy This Terminology Is No Longer Used By Doctors\nThe spectrum illustrates a broad range of developmental delays and symptom severity.\nASD includes people who have a few mild autistic traits to those who need help with day-to-day functioning. It represents every intelligence level, as well as varying degrees of communication and social abilities.\nThe differences between one type and another type can be subtle and difficult to determine.\nDonât Miss: Aspergers And Stuttering\nRecommended Reading: Autism Statistics By Year\nAsd Criteria And Manifestations In Adults\nThe DSM-5 specifies diagnostic critera for ASD.24 The following table summarizes the DSM-5 criteria, with examples of how these criteria may manifest in adults.22\nThough the DSM-5 conceptualizes ASD primarily as a social-communication disorder, there is a growing literature supporting the hypothesis that ASD is primarily characterized by differences in information processing.23 See, for example, the intense world theory of ASD.\nAdults on the autism spectrum may display autistic traits differently from children. Most people, regardless of whether or not they are on the autism spectrum, mature and behave differently as they get older. As such, adults on the spectrum may not fit societyâs images of autistic children. In addition, adults often find coping strategies that help them function in the world, but that may make autistic traits harder to recognize.\nThere is great heterogeneity in the clinical presentation of ASD. Although anyone on the spectrum would be expected to have challenges with social communication, these challenges can show up in many different ways. For example, a person may not be able to speak, may misunderstand facial expressions and body language, or may take language too literally. A person may have difficulty starting a conversation, may need more time alone than most people, or may feel uncomfortable socializing with others without a planned activity.\nYou May Like: What Is High Functioning Aspergers\nRisk And Prognostic Factors\nThe best established prognostic factors for individual outcome within autism spectrum disorder are presence or absence of associated intellectual disability and language impairment and additional mental health problems. Epilepsy, as a comorbid diagnosis, is associated with greater intellectual disability and lower verbal ability.\nEnvironmental. A variety of nonspecific risk factors, such as advanced parental age, birth weight, or fetal exposure to valproate, may contribute to risk of autism spectrum disorder.\nGenetic And Physiological. Heritability estimates for autism spectrum disorder have ranged from 37% to higher than 90%, based on twin concordance rates. Currently, as many as 15% of cases of autism spectrum disorder appear to be associated with a known genetic mutation, with different de novo copy number variants or de novo mutations in specific genes associated with the disorder in different families. However, even when an autism spectrum disorder is associated with a known genetic mutation, it does not appear to be fully penetrant. Risk for the remainder of cases appears to be polygenic, with perhaps hundreds of genetic loci making relatively small contributions.\nAlso Check: What Is An Autism Spectrum Disorder Specialist\nRestricted Repetitive Patterns Of Behavior\nThis kind of behavior should be present and for a diagnosis at least two of these should be apparent:\nEven if these symptoms are present, further requirements are still needed for an autism diagnosis. For example, the symptoms should be present from early onit is however possible that full manifestation only occurs later due to circumstances. These symptoms should cause significant problems in important areas of the childs life and should not be better explained by intellectual disability or global development delay.\nRisks And Benefits Of Adult Diagnosis\nMany adults who meet diagnostic criteria for ASD do not carry formal medical diagnoses of ASD, either because they have never come to medical attention or because they have been misdiagnosed with a differential condition . When deciding whether to refer an adult patient for a diagnostic evaluation for ASD, one should consider potential risks and benefits of a diagnosis, and should discuss these possibilities with the patient and, if applicable, their supporters.\nPotential benefits of a formal diagnosis are as follows.\n- Would confer legal rights to accommodations in school, at work, in healthcare, or in other settings.\n- May assist the individual in developing a better understanding of self.\n- May provide peace of mind through the professional confirmation of life experiences.\n- May provide means to experience better coping or quality of life by more directly helping in recognizing strengths and accommodating challenges.\n- May provide others means to understand and support the individual.\n- May qualify the individual for benefits and services for people who have an ASD diagnosis.\n- May qualify the individual for programs for people with disabilities, such as scholarships or incentives that are meant to increase workplace diversity.\nPotential risks associated with seeking an ASD diagnosis are as follows.\nRecommended Reading: Is Autism A Disability Under The Equality Act\nAutism Spectrum Disorder: A New Umbrella Term\nOne of the biggest changes in the DSM-5 is the revised diagnosis of individuals with autism-related disorders.\nPrior to the revision, patients could be characterized as having one of four disorders: autistic disorder, Aspergers, childhood disintegrative disorder or an unidentified developmental disorder not otherwise specified. After medical and scientific review, researchers found that these labels were not consistently applied across clinics and treatment centers. The DSM-5 has therefore done away with the prior mentioned labels and has redefined these symptoms under one umbrella term. In doing so, they hope it will improve diagnoses without limiting criteria or changing the number of individuals being diagnosed.\nPeople with autism spectrum disorder tend to display the following traits:\n- Communication deficits\n- High sensitivity to changes in their environment\nFree Brochures And Shareable Resources\n- Autism Spectrum Disorder: This brochure provides information about the symptoms, diagnosis, and treatment of ASD. Also available en español.\n- Digital Shareables on Autism Spectrum Disorder: Help support ASD awareness and education in your community. Use these digital resources, including graphics and messages, to spread the word about ASD.\nYou May Like: How To Calm Down An Autistic Person\nDoes Aspergers Syndrome Still Exist In Dsm 5\nWith the new model of Autism, it means that Aspergers Syndrome which was added to the DSM back in 1994 and has existed for a period of time, was removed in May 2013. Which begs the question does Aspergers still exist? In official terms no, because the diagnosis would be Autistic Spectrum Disorder Level I.\nThe reason for diagnosing what was Aspergers as Autistic Spectrum Disorder Level I is that in general terms the need is of a low level of support. On the face of it, that is correct, but the type of low-level support that is needed has to be of a different genre than for someone who is ASD Level III.\nHowever, to just give a label of ASD Level I, is not sufficient. Autism, at different level, affects each child or adult differently. On a personal experience, I have encountered clients with similar autistic traits but how the traits impact on their lives will depend on their personality levels of sensitivity resilience and the effectiveness of their support. To counteract the impact of ASD labelling, there should be specific descriptors to highlight individual signs and symptoms.\nAspergers may not exist as a definitive within the DSM 5 Autism Spectrum Disorder, but clinicians will still carry on using the international coding system especially when they are dealing with medical insurance companies , as Aspergers is still included in that system. Groups and organisations that support their members that have Aspergers will continue to use the descriptor.","Causes And Risk Factors\nResearchers dont know the primary causes of ASD, but studies suggest that a persons genes can act together with aspects of their environment to affect development in ways that lead to ASD. Some factors that are associated with an increased likelihood of developing ASD include:\n- Having a sibling with ASD\n- Having older parents\n- Having certain genetic conditions\n- Having a very low birth weight\nWhat Is The Dsm\nMedical professionals and researchers often consult the DSM-5, a manual sometimes referred to as the bible of mental conditions. In this article the criteria for an autism diagnosis according to the DSM-5 will be examined.\nThe Diagnostic and Statistical Manual of Mental Disorders is a manual often cited in scientific journals medical professionals like psychiatrists and pediatricians refer to it when diagnosingbut for some of us it appears to be a bit of a daunting read reserved for those with multiple abbreviations accompanying their name.\nThe name of the handbook, The Diagnostic and Statistical Manual of Mental Disorders contributes to the intimidation factor. While it was never intended as a beach read for the public, the DSM-5 contains a lot of diagnostic information that may be useful for educators and parents, in addition to its intended medical and research audience.\nMost doctors in the US use the manual as the authoritative guide when diagnosing autism spectrum disorders . For medical professionals without a lot of autism related experience, the DSM-5 provides guidelines and criteria to facilitate consistent and reliable diagnoses.\nIt may be a valuable diagnostic tool, but its also been criticized by many cliniciansspecifically criticism regarding its validity, reliability and utility . Issues relating to overdiagnosis and the risk of pathologizing normal behavior or conditions are further areas of concern according to Young .\nCriteria C And D For Social Anxiety Disorder\nCriteria C and D for Social Anxiety Disorder\nC. Social situations almost always provoke fear or anxiety.\nCriteria C captures the pervasiveness of the experience. We arent talking about situational anxiety that comes and goes were talking about anxiety that is persistent and present in many contexts.\nD. The social situations are avoided or endured with intense fear or anxiety.\nAvoidance and anxiety go hand and hand. When we experience anxiety, we often avoid the thing that triggers the anxiety . Criteria C and D capture this element of social anxiety. The person nearly always pushes through the avoidance but experiences intense fear and anxiety , or they avoid these situations whenever possible .\nRecommended Reading: How To Relate To An Autistic Person\nUnderstanding The Three Levels Of Autism\nSteven Gans, MD, is board-certified in psychiatry and is an active supervisor, teacher, and mentor at Massachusetts General Hospital.\nThere are three levels of autism spectrum disorder , which are described in the Diagnostic and Statistical Manual of Mental Disorders, 5th Edition .\nEach person with ASD is further diagnosed with either ASD level 1, level 2, or level 3, depending on how severe their disorder is and how much support they need in their daily life.\nThe levels range from least to most severe, with ASD level 3 describing an individual who has the most severe level of ASD symptoms, and ASD level 1 describing someone with symptoms on the milder end of the spectrum.\nThis article discusses the symptoms that are typical of each of the three ASD levels. It also includes realistic examples of the strengths and limitations that are unique to each level.\nVerywell / Cindy Chung\nAbout The Dsm And Autism Spectrum Disorder Diagnosis\nWhen diagnosing autism, professionals like paediatricians, psychiatrists, psychologists and speech pathologists use the Diagnostic and statistical manual of mental disorders , or DSM-5, produced by the American Psychiatric Association.\nThe DSM-5 lists the signs and symptoms of autism and states how many of these must be present to confirm a diagnosis of autism spectrum disorder. The DSM-5 refers to signs and symptoms, but this article talks about signs and characteristics.\nTo find out whether a child has autism signs and characteristics and meets DSM-5 criteria, professionals also need to do extra tests. These tests are called adiagnostic assessment.\nYou May Like: Can 2 Autistic Parents Have A Normal Child\nQuestion: Whats New In The Dsm\nANSWER: As you can see, there have many changes to the diagnostic criteria! Communication and social deficits have been grouped together because they often overlap. Under the DSM-IV, language delay was a very important diagnostic piece however, under the DSM-5, a language delay will no longer be considered a part of the diagnosis.\nAnother important difference includes the changes to the criteria regarding restricted and repetitive behaviors. New is the inclusion of hyper- or hyporeactivity to sensory input. Sensory differences have long been recognized as a challenge for many individuals with ASD and are now recognized as part of the official diagnostic criteria.\nCriteria B For Social Anxiety Disorder\nCriteria B for Social Anxiety Disorder\nCriteria B for social anxiety includes: “The individual fears that he or she will act in a way or show anxiety symptoms that will be negatively evaluated.” Examples include things such as:\n1) Fear of being humiliated or embarrassed\n2) Fear of situations where one may be judged negatively\n3) Fear of physical manifestations of anxiety\n4) Fear of offending others\n5) Fear of being Rejected\nHere the person is highly anxious that others will perceive them as “anxious, weak, crazy, stupid, boring, intimidating, dirty, or unlikable” . The person is often particularly nervous about showing their anxiety symptoms through blushing, trembling, sweating, stumbling over one’s words, or staring, which will lead to being rejected or negatively evaluated.\nThose from more collectivistic cultures are more prone to worry that they will accidentally offend others .\nAlso Check: Is There Different Types Of Autism\nIs Autism A Mental Illness Dsm\nAccording to the Diagnostic and Statistical Manual of Mental Disorders , autism spectrum disorder is also classified as a mental illness. As a result, autism is classified as a neurodevelopmental disorder in the DSM-5.\nThe DSM-5 describes autism spectrum disorder as a mental disorder. The DSM-5 defines autism as a psychiatric disorder . Many characteristics of autism overlap with those of other mental illnesses, which is why it is frequently misdiagnosed. The obsession of children with autism with a particular field of study may be accompanied by an indifference to the interests and concerns of others. Self-obsession is a common feature of narcissistic personality disorder. Autism is more likely than other illnesses to result in mental illness. In addition to mental illness, the majority of children and adults with autism have developmental disabilities. It is possible for a secondary diagnosis to direct therapy, academic support, and mental health services. People with autism and those suffering from other mental illnesses may exhibit behaviors similar to one another.\nWhat Is Autism Spectrum Disorder\nAutism spectrum disorder is a complex developmental condition involving persistent challenges with social communication, restricted interests, and repetitive behavior. While autism is considered a lifelong disorder, the degree of impairment in functioning because of these challenges varies between individuals with autism.\nDon’t Miss: How Does Autism Affect The Brain\nQuestion: What Are The Dsm\nANSWER: Under the newest changes, the DSM-5 diagnostic criteria for Autism Spectrum Disorder require the person to demonstrate characteristics in two categories.\n- Impairment in social communication and interaction. In the DSM-IV, communication and social were seen as two separate areas. In the DSM-5, because communication and social skills operate together, they are now combined. Characteristics may include the following:\n- Deficits in reciprocity\nFor a person to meet criteria, characteristics must be present during a childs early development. However, the characteristics may not become evident until the child is older and is placed in social situations that exceed his or her social abilities.\nRead Also: Autism Schedule Board\nQuestion: What About Those Who Have A Diagnosis Now\nANSWER: Anyone diagnosed with any of the four disorders from the DSM-IV should still meet the criteria for ASD in the DSM-5, or be found to have another, more accurate, DSM-5 diagnosis. According to the DSM-5: Individuals with a well-established DSM-IV diagnosis of autistic disorder, Aspergers disorder, or pervasive development disorder not otherwise specified should be given the diagnosis of autism spectrum disorder. Individuals who have marked deficits in social communication but whose symptoms do not otherwise meet criteria for autism spectrum disorder should be evaluated for social communication disorder . .\nIndividuals with a long-standing diagnosis, therefore, may continue to carry the new ASD diagnosis. As noted above, the DSM-5 also includes a new communication disorder called Social Communication Disorder, or SCD. Even though the definitions of ASD and SCD share similar social communication deficits, the difference is that the criteria for ASD includes restricted and repetitive behaviors, interests, and activities.\nAlso Check: I Think I Might Be Autistic\nSocial Communication / Interaction Behaviors May Include:\n- Making little or inconsistent eye contact\n- Appearing not to look at or listen to people who are talking\n- Infrequently sharing interest, emotion, or enjoyment of objects or activities\n- Not responding or being slow to respond to ones name or to other verbal bids for attention\n- Having difficulties with the back and forth of conversation\n- Often talking at length about a favorite subject without noticing that others are not interested or without giving others a chance to respond\n- Displaying facial expressions, movements, and gestures that do not match what is being said\n- Having an unusual tone of voice that may sound sing-song or flat and robot-like\n- Having trouble understanding another persons point of view or being unable to predict or understand other peoples actions\n- Difficulties adjusting behaviors to social situations\n- Difficulties sharing in imaginative play or in making friends\nWhy This Terminology Is No Longer Used By Doctors\nThe spectrum illustrates a broad range of developmental delays and symptom severity.\nASD includes people who have a few mild autistic traits to those who need help with day-to-day functioning. It represents every intelligence level, as well as varying degrees of communication and social abilities.\nThe differences between one type and another type can be subtle and difficult to determine.\nDonât Miss: Aspergers And Stuttering\nRecommended Reading: Autism Statistics By Year\nAsd Criteria And Manifestations In Adults\nThe DSM-5 specifies diagnostic critera for ASD.24 The following table summarizes the DSM-5 criteria, with examples of how these criteria may manifest in adults.22\nThough the DSM-5 conceptualizes ASD primarily as a social-communication disorder, there is a growing literature supporting the hypothesis that ASD is primarily characterized by differences in information processing.23 See, for example, the intense world theory of ASD.\nAdults on the autism spectrum may display autistic traits differently from children. Most people, regardless of whether or not they are on the autism spectrum, mature and behave differently as they get older. As such, adults on the spectrum may not fit societyâs images of autistic children. In addition, adults often find coping strategies that help them function in the world, but that may make autistic traits harder to recognize.\nThere is great heterogeneity in the clinical presentation of ASD. Although anyone on the spectrum would be expected to have challenges with social communication, these challenges can show up in many different ways. For example, a person may not be able to speak, may misunderstand facial expressions and body language, or may take language too literally. A person may have difficulty starting a conversation, may need more time alone than most people, or may feel uncomfortable socializing with others without a planned activity.\nYou May Like: What Is High Functioning Aspergers\nRisk And Prognostic Factors\nThe best established prognostic factors for individual outcome within autism spectrum disorder are presence or absence of associated intellectual disability and language impairment and additional mental health problems. Epilepsy, as a comorbid diagnosis, is associated with greater intellectual disability and lower verbal ability.\nEnvironmental. A variety of nonspecific risk factors, such as advanced parental age, birth weight, or fetal exposure to valproate, may contribute to risk of autism spectrum disorder.\nGenetic And Physiological. Heritability estimates for autism spectrum disorder have ranged from 37% to higher than 90%, based on twin concordance rates. Currently, as many as 15% of cases of autism spectrum disorder appear to be associated with a known genetic mutation, with different de novo copy number variants or de novo mutations in specific genes associated with the disorder in different families. However, even when an autism spectrum disorder is associated with a known genetic mutation, it does not appear to be fully penetrant. Risk for the remainder of cases appears to be polygenic, with perhaps hundreds of genetic loci making relatively small contributions.\nAlso Check: What Is An Autism Spectrum Disorder Specialist\nRestricted Repetitive Patterns Of Behavior\nThis kind of behavior should be present and for a diagnosis at least two of these should be apparent:\nEven if these symptoms are present, further requirements are still needed for an autism diagnosis. For example, the symptoms should be present from early onit is however possible that full manifestation only occurs later due to circumstances. These symptoms should cause significant problems in important areas of the childs life and should not be better explained by intellectual disability or global development delay.\nRisks And Benefits Of Adult Diagnosis\nMany adults who meet diagnostic criteria for ASD do not carry formal medical diagnoses of ASD, either because they have never come to medical attention or because they have been misdiagnosed with a differential condition . When deciding whether to refer an adult patient for a diagnostic evaluation for ASD, one should consider potential risks and benefits of a diagnosis, and should discuss these possibilities with the patient and, if applicable, their supporters.\nPotential benefits of a formal diagnosis are as follows.\n- Would confer legal rights to accommodations in school, at work, in healthcare, or in other settings.\n- May assist the individual in developing a better understanding of self.\n- May provide peace of mind through the professional confirmation of life experiences.\n- May provide means to experience better coping or quality of life by more directly helping in recognizing strengths and accommodating challenges.\n- May provide others means to understand and support the individual.\n- May qualify the individual for benefits and services for people who have an ASD diagnosis.\n- May qualify the individual for programs for people with disabilities, such as scholarships or incentives that are meant to increase workplace diversity.\nPotential risks associated with seeking an ASD diagnosis are as follows.\nRecommended Reading: Is Autism A Disability Under The Equality Act\nAutism Spectrum Disorder: A New Umbrella Term\nOne of the biggest changes in the DSM-5 is the revised diagnosis of individuals with autism-related disorders.\nPrior to the revision, patients could be characterized as having one of four disorders: autistic disorder, Aspergers, childhood disintegrative disorder or an unidentified developmental disorder not otherwise specified. After medical and scientific review, researchers found that these labels were not consistently applied across clinics and treatment centers. The DSM-5 has therefore done away with the prior mentioned labels and has redefined these symptoms under one umbrella term. In doing so, they hope it will improve diagnoses without limiting criteria or changing the number of individuals being diagnosed.\nPeople with autism spectrum disorder tend to display the following traits:\n- Communication deficits\n- High sensitivity to changes in their environment\nFree Brochures And Shareable Resources\n- Autism Spectrum Disorder: This brochure provides information about the symptoms, diagnosis, and treatment of ASD. Also available en español.\n- Digital Shareables on Autism Spectrum Disorder: Help support ASD awareness and education in your community. Use these digital resources, including graphics and messages, to spread the word about ASD.\nYou May Like: How To Calm Down An Autistic Person\nDoes Aspergers Syndrome Still Exist In Dsm 5\nWith the new model of Autism, it means that Aspergers Syndrome which was added to the DSM back in 1994 and has existed for a period of time, was removed in May 2013. Which begs the question does Aspergers still exist? In official terms no, because the diagnosis would be Autistic Spectrum Disorder Level I.\nThe reason for diagnosing what was Aspergers as Autistic Spectrum Disorder Level I is that in general terms the need is of a low level of support. On the face of it, that is correct, but the type of low-level support that is needed has to be of a different genre than for someone who is ASD Level III.\nHowever, to just give a label of ASD Level I, is not sufficient. Autism, at different level, affects each child or adult differently. On a personal experience, I have encountered clients with similar autistic traits but how the traits impact on their lives will depend on their personality levels of sensitivity resilience and the effectiveness of their support. To counteract the impact of ASD labelling, there should be specific descriptors to highlight individual signs and symptoms.\nAspergers may not exist as a definitive within the DSM 5 Autism Spectrum Disorder, but clinicians will still carry on using the international coding system especially when they are dealing with medical insurance companies , as Aspergers is still included in that system. Groups and organisations that support their members that have Aspergers will continue to use the descriptor."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:cd64230e-44c4-443e-9a48-16031f9c92f0>","<urn:uuid:cd64230e-44c4-443e-9a48-16031f9c92f0>"],"error":null}
{"question":"How to maximize thermal relief in PCB copper design?","answer":"To maximize thermal relief in PCB design, define an area around your most critical parts and give them priority. You can easily flood both sides with a matrix of stitching vias. This method is particularly effective when it's important to maximize the copper surface area. While a simple alternative is adding a big copper square to the decal to conduct heat away, this may not be optimal when space is limited.","context":["How to Maximize Copper in Your PCB Design: The Pros and Cons of Copper Pouring Versus Placing\nThere is a saying in copper pour PCB design, “Copper is free.” It means a PCB editor designer must think in reverse. A board starts off as solid copper, and the copper you don’t want is removed. It is faster to build, less consumptive, and less expensive to make a board that is mostly copper as compared to the same size board that is mostly bare. Picking the correct technique will make the difference between an effortless or frustrating experience.\nThe act of maximizing copper is most commonly achieved two ways:\nManually – This method is usually faster, but sloppy. By defining and placing specific shapes, copper can be quickly placed as objects. These routing objects can be assigned to a net and will be checked for shorts or errors during the continuity check. This technique is preferred for quick turns or prototype builds.\nAutomatically – This method is more time consuming; however, a copper pour can better facilitate maximizing copper usage. Instead of laying out the board and then going back and placing copper shapes to fill it in, maximum copper can be left behind by drawing a border around the board area and pouring the copper in.\nHow Copper Pours Can Save Time\nWhen pouring copper, a boundary is defined and everything inside it is connected automatically when the pour operation is performed. This technique is usually easier and quicker if the area is large, an unusual shape, or filled with several irregularly shaped objects. A pour operation will automatically fill several irregular areas, additionally, it will automatically isolate other parts or traces in the area.\nObjects on the same net, such as ground plane, will all be connected depending on the setup page or design rules. Objects that a copper pour can automatically handle are vias, traces, nets, decals, voided areas, and pads. Used correctly, copper pouring will automatically make the connections it should and avoid the connections it shouldn’t. These connections can be double checked using the continuity checker tool.\nWhen to Take a Manual or Automatic Approach to Your PCB Designs\nA lot of the time we’re inclined to do things manually since it seems like a faster solution in the short term and we don’t need to relinquish control of our designs. However, when you look at the big picture, an automatic approach can be a better long-term option even though it is more work upfront. Here are three situations where using a copper pour might be the better option for your design.\nManual - If you are making a prototype, don’t waste your time making the design look pretty, especially if the design is guaranteed to change. Creating a conventional closed shape of copper is quick and easy, but when you’re dealing with unusual polygons it can be time-consuming. A quick and dirty technique is to drop overlapping squares, triangles, and trapezoids next to, and on top of, one another to build your shape. While this is useful for short term solutions, eventually you will need to do some vertex editing to clean up your design, which can be tedious.\nAutomatic - Using a copper pour for this process is automatic in the sense that it stops you from having to manage polygons at all. By defining a border and pouring copper in, you avoid creating objects that will need to be adjusted later if and when changes are made.\nManual – In printed circuit board PCB design a plane is a large area of copper where all connections are one net. If you are building a board with separate planes and adding and changing parts often, it is an easier short term solution to route traces point to point.\nAutomatic – If your design uses a plane for ground, which is a great way to avoid several traces that all go to the same point, a copper pour is especially effective. Defining and pouring a copper plane will automatically connect all the connections on one net.\nManual - A quick way to cool down a hot part is to add a big copper square to the decal to help conduct the heat away. This may work best if you have a lot of space to work with, which many times is not the case.\nAutomatic - You can easily maximize the potential thermal relief by defining an area around your most critical parts, and giving them the priority. Even a matrix of stitching vias can be easily flooded on both sides. Use this when it is important to maximize the copper surface area.\nWhen using copper pours, the time spent defining the rules for connections will allow future designs to be poured perfectly, allowing you to focus on functionality, rather than connectivity. This investment will leverage the time savings into future designs.\nSmart Copper Editing can organize these differences in a simple and meaningful way to allow general, marginal, and specific requirements to be automatically applied while pouring copper. Smart Copper Editing can also help manage other areas too, like board edges and unique regions. In design rules, define your vias, traces, nets, decals, voided areas, and pads with specific rules unique to the copper pour.\nHaving a quality tool like Smart Copper Editing removes the frustration of having irregular copper shapes consume way more time than they should. Save time and avoid the frustration by quickly adjusting your design to meet the requirements you know it must. Contact Altium for more info on their PCB Designer software and how this tool can help you."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:15783816-75e6-47a3-9268-e847e3d06bad>"],"error":null}
{"question":"How do CDM regulations affect contractor responsibilities, and what training options are available for construction workers to demonstrate compliance?","answer":"CDM 2015 regulations state that contractors must not employ anyone on a construction site unless they have or are obtaining the necessary skills, knowledge, training, and experience to work safely. To demonstrate compliance, individuals can gain relevant NVQs for their occupational role and obtain CSCS cards that represent their skill level. Workers can also enhance their qualifications through additional training courses such as First Aid at Work, Working at Heights, Sharps Awareness, Manual Handling, Abrasive Wheels, and UKATA Asbestos Awareness. A Level 2 NVQ in Construction Operations can help show competency, and the Level 1 Certificate in Construction Skills is recommended as a starting point.","context":["What are CDM Regulations?\nThe Construction (Design and Management) Regulations (CDM) are the main set of regulations for managing the health, safety and welfare of construction projects.\nCDM Regulations or ‘CDM 2015’ as they are often referred to came into force on 6 April 2015 and replaced Construction (Design and Management) Regulations 2007.\nCDM applies to all building and construction work and includes new build, demolition, refurbishment, extensions, conversions, repair and maintenance.\nHow does CDM 2015 affect the construction industry?\nCDM 2015 introduces strict liability for the client; particularly for the performance of the ‘duty-holders’ appointed by the client to carry out the work. The implications for the contractors themselves are also significant. Responsibility for health and safety management in construction lies with business owners and can result in legal action.\nCDM regulations 2015 state that “A contractor must not employ or appoint a person to work on a construction site unless that person has, or is in the process of obtaining, the necessary skills, knowledge, training and experience to carry out the tasks allocated to that person in a manner that secures the health and safety of any person working on the construction site.”\nHow can individuals demonstrate CDM compliance?\nIndividuals can demonstrate compliance to CDM Regulations and also increase their own work opportunities by gaining the relevant NVQ appropriate to their occupational role and gain the CSCS card that accurately represents the true level of their skills, knowledge and experience. Individuals with access to higher level NVQs also have access to chartered status with institutions such as CIOB, RICS, ICCWI and CABE which are all considered to be benchmarks of competence.\nHow do Principal Designers play a key role in CDM 2015?\nThe Principal Designer can be an organisation or an individual that is appointed by the client to take the lead in planning, managing, monitoring and coordinating health and safety during the pre-construction phase. They work alongside the client and the principal contractor and are responsible for planning, managing and co-ordinating health and safety at design stage to identify, eliminate and control any potential risks.\nThe Principal Designer is key to CDM because the role is very much focused on prevention and seeks to reduce, control and eliminate any possible health and safety risks before the construction project has even started.\nFor small-scale domestic development projects a Principal Designer’s role may be carried out by either the Principal Contractor or the designer responsible for the pre-construction phase. On larger projects, a PD can appoint a Health and Safety specialist to assist them in performing their duties.\nHow can employers demonstrate CDM compliance?\nEmployers can demonstrate compliance to CDM 2015 by ensuring that everyone working on the project has the correct level of skills, knowledge, experience and training to carry out their role effectively and safely. NVQs are the perfect vehicle to do just that because they provide proof that an individual has both the knowledge and technical ability to carry out a job role in the work place to a recognised industry standard. Achievement of an NVQ in construction provides access to CSCS and CPCS Cards that demonstrate this.\nThe role of Principal Designer and Principal Contractor\nThe importance of the role of the Principal Designer and the Principal Contractor cannot be underestimated and should never be omitted from a development project. Like all aspects of health and safety legislation, it is intended to ensure that everyone involved in construction stays safe during the course of their work.\nPrincipal Designer’s responsibilities extend beyond the design phase. They need to consider the safety of those people who maintain, clean, repair and eventually demolish their structures.\nHow do Principal Contractors play a key role in CDM 2015?\nThe Principal Contractor is the contractor in overall control of the construction phase on projects with more than one contractor. They are appointed by the client and there should only be one principal contractor for a project at any one time. For example a roofing contractor would be the principal contractor on a project if they employed other contractors, such as a contractor to provide scaffolding. A contractor can be an individual as well as a business (for example, a self-employed electrician).\nThe Principal Contractor is key to CDM because they are responsible for the delivery and implementation of health and safety on site once the construction project has began. The Principal Contractor is responsible for things like access and egress, ensuring that there is sufficient induction on site, ensuring that there are good safety systems and supervision in place, ensuring adequate waste management systems are in place.\nWhich NVQs are appropriate to the Principal Designer or Principal Contractor?\nCADUK have identified the following NVQs as suitable to develop the desired characteristics for the role of Principal Designer and Principal Contractor, as specified by HSE:\nWhere can I find more information on CDM?\nThe HSE is a good source of information and guidance. It has produced the key publication Managing health and safety in construction – Construction (Design and Management) Regulations 2015 – (L153) on the legal requirements for CDM 2015.\nOn the HSE website there are also some very useful Frequently Asked Questions pages. These are not formal HSE guidance and are not intended to cover every aspect of the topic or be a ‘one size fits all’ answer, but provide consistent and helpful answers to some of the most common questions which have arisen about CDM 2015.","How to become a Construction Labourer\nTo become a construction labourer, you need to make sure that you have the correct qualifications to on-site. The job requires very little in terms of requirements, but there are legal steps you need to take to be allowed on site.\nWhat is a Construction Labourer?\nA construction labourer is employed to carry out various manual labour jobs on a construction site.\nA construction labourer would generally be able to work in several roles in construction. Functions may include site preparation, loading materials, tidying the site, building and digging trenches. However, the job can be very demanding and unsafe if you do not attend the proper training before working.\nWhat Qualifications Do I Need To Become a Construction Labourer?\nThe first thing you will need to get on a construction site is the CSCS (Green) Labourer card. This card is imperative for you to get on-site to show that you have knowledge of health and safety on a construction site.\nTo obtain the card, you need to complete the GQA CSCS Course, Test and Card. This package is a one-day course that will allow you to get your Green card in one day.\nAlternatively, you can follow these three simple steps:\n- First, attend and pass a recognised UK health and safety in construction qualification. The route you can take is the CITB Health & Safety Awareness.\n- Pass the CITB Health, Safety and Environment test at your local Pearson Vue test centre.\n- Contact CITB or CSCS and pay for your card.\nDepending on the kind of work you are doing, you may need to improve your knowledge of other skills such as; bricklaying, plastering and window fitting. You may also be required to use various tools and machines, so you would need to make sure you are familiar and competent to use those.\nA Level 2 NVQ will help you show competency in the field, such as Construction Operations. The NVQ covers all general aspects of building and construction. The Level 1 Certificate in Construction Skills would be an excellent place to start in college.\nTo improve your resume, even more, you may want to attend other site safety related courses that will make you more attractive to current and potential employers. Take a look below at other courses that you could attend:\n- First Aid at Work\n- Working at Heights\n- Sharps Awareness\n- Manual Handling\n- Abrasive Wheels\n- UKATA Asbestos Awareness\nSkills and Knowledge\nAs a Labourer, you’ll need to work well as a team under pressure when working to deadlines. You’ll need to have some knowledge of construction and good attention to detail.\nOperating Plant Machinery\nTo operate machinery on-site, you must have a driving license and be 18 or above. You will also need the relevant NPORS card for the machine you are using.\nCSCS Cards: How to apply for a Green Card\nFor more information, please look at our blog, CSCS Cards: How to apply for a Green Card.\nBack to News View Our Courses"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:984e4bfa-ff3f-44a5-a997-0384e96ae4d4>","<urn:uuid:b1e60ce7-4605-4236-b656-5fb954f0fd74>"],"error":null}
{"question":"How do different fats affect pie crust texture and stability, and what's the ideal combination to use?","answer":"The ideal combination is to use butter for flavor along with a small amount of shortening. European-style butter with higher fat content, like Kerry Gold Irish Butter, is recommended for the best results. Shortening is beneficial because it's 100% fat and has a higher melting point than butter, which helps prevent the crust from shrinking during baking. Since shortening contains no water, unlike butter, it helps maintain the crust's shape while baking. However, butter remains the primary fat as it provides the best flavor.","context":["“I’ve been baking pies my entire life, but making them with Kate was a liberating experience. With pioneer spirit she throws the textbook out the window and comes up with absolutely perfect crust filled with fruit that actually sings to you telling you when it’s ready to be removed from the oven. Great fun and great food.”\n—Ruth Reichl, Editor in Chief, Gourmet Magazine\nBaking has been my passion since I was a little girl. I’ve always preferred it to cooking. Except when it came to making pie crust. That was something to avoid until I met Kate McDermott, the “pie whisperer”. Her legendary pie making classes in Seattle gave me the confidence I needed. I am forever indebted to you Kate!\nPie wisdom, as spoken by Kate-\nI like to keep things simple in my life. Pie dough is no exception.\n“Flour, salt, fat and water.”\nIt’s a simple mantra that I repeat when making dough.\nThere are a lot of recipes out there for pie dough. I’ve had great success with this simple one. I teach it in all my classes. It’s simple and basic. So many have asked for it that I’d like to share it with you.\nEquipment You Will Need\nA big bowl, one that is big enough to get your hands into comfortably. A 6-quart size is great.\nA fork (optional, use your hands if you like…really!)\nPut the bowl and flour into the freezer.\n“What?”, you say.\n“In the freezer?”\nPie Making Rule #1: Keep everything as cold as possible; bowl, flour, fats, hands.\nI keep a mixing bowl in the freezer all the time along with a bag of flour and my pastry cloth.\nI love King Arthur All Purpose Flour (Red Bag). After experimenting with different flours over the years, I can honestly say that there is none that I like better. King Arthur, in Norwich, VT has been around for 220 years so they know a thing or two about flour.\nThis is not to say that I’m not open to different flours. I would love a flour that is within my 100 miles and I’ve tried some that are. IMHO, none approach the quality of King Arthur. It’s silky, smooth and just makes a great crust.\nI would like to give a pie class to wheat farmers in the Pacific Northwest and show them just what a pie maker needs. Can you imagine regional wheat grown specifically for pie with a big picture of an apple pie on the front of the bag?\nOK, I digress. Let’s talk about measuring.\nThis may blow your mind but I’m not an exact measurer. I don’t sift, fluff or mix the flour. I just dip in with a metal cup that I bought for five cents at a yard sale. It’s my cup. I’ve also found that a standard coffee cup (not mug) measures pretty close to one cup!\nExact measuring is needed for cakes, but pie dough…at least mine…is pretty forgiving. A little extra here or there…and it all comes out ok.\nPut about 2-1/2 cups of flour in the chilled bowl. Add some salt, a half a teaspoon is about enough, and that finishes up the dry ingredients.\nThere’s much talk about what is the best fat to put in pie. My grandmother, Geeg, swore by “the- stuff-in-the-blue-can-which-shall-remain-nameless”. But as I came of age in the 1960’s, my “Mother Earth” leanings moved me swiftly away from that product. (Be sure to check this link out. It’s pretty funny!)\nI use Kerry Gold Irish Butter, either the gold (salted) or silver (unsalted) package. If that is difficult to find in your region, use a foil wrapped European style butter which has a higher fat content and less moisture. Now, if none of these are available where you are, for goodness sake don’t let this stop you from making pie! But, do use butter real butter.\nCut the cube at about the 8 tablespoons mark and chunk it up into 6-8 big pieces.\nNow add, an equal amount of leaf lard. I dip into the leaf lard container with a tablespoon or a soup spoon from my kitchen drawer and add approximately 8 tablespoons to the bowl. If I make one tablespoon too big, I make the next smaller.\nMaking the dough\nThink good thoughts before you put your hands in the bowl and make sure they are cold! If it’s hot out, put a bowl of ice water out on the kitchen counter and dip your hands in for a minute to chill them out. Dry them off .\nCold hands+warm heart=good pastry maker!\nPut your hands in and smoosh the fat into the flour. Highly technical language here don’t you think? Just rub the cold fat into the cold flour with your cold hands.\nWork quickly. When the flour, salt and fats appear to have the sizes of cracker crumbs, peas, almonds and a few small walnut meats, you are finished.\nAdd water. Not too much and nor too little. Generally recipes call for somewhere between 6-8 tablespoons of water. In truth I found it to be anywhere from 3-15!\nHave you ever made a dough and felt like YOU are the one who must be wrong because even though you followed everything that the recipe said to do, it just doesn’t turn out right?\nYou are not wrong. The recipe just did not give you enough information. This is the art of making a pie.\nIt seems that every day and every dough require a different amount of water even if it is the same recipe!\nAdd ice water. Start with 3 Tablespoons. Be sure that the ice doesn’t get into the bowl. If it does, when your finished dough is chilling, the ice will have melted and you will find a “mookie-mess” inside the dough. (Another highly technical term!)\nSprinkle the water over the dough in the bowl and then move it around in the bowl with a fork or with your hands. Don’t spend a lot of time in there. You aren’t making cookie dough; you are just moving the water around so that it is in all parts of the bowl.\nSqueeze a handful of dough with your hand to see if it holds together. If it doesn’t hold together in one place, it’s probably not going to hold together in another place and all you will be doing is warming up the fat so just one squeeze. Remember Rule #1.\nSprinkle 2 Tablespoons in the bowl and move the water around. Squeeze a handful. Does it hold together easily?\nSprinkle in 1 Tablespoon and move it around with your fork. Keep adding a spoonful at a time until your fork starts to feel a bit sluggish in the bowl. The dough should keep together in your hand and feel moist.\nPull it all together and compress into a big ball. It should feel like cool clay or play-dough.\nNow, cut it in half and shape it into two chubby discs. When Saveur Magazine printed my recipe, they changed chubby to thick. Chubby makes me think of my son’s sweet apple cheeks when he was a little boy and is a very happy word for me, so chubby it is!\nWrap the disks tightly in plastic wrap. Let rest and chill in the fridge for about an hour.\nDuring that time you can make the filling for your pie and have a cup of tea.\nRolling Out the Dough\nBring the dough out of the fridge. If it feels hard and solid, let it rest at room temperature until it feels a bit pliable.\nNow some of you may be trembling at the thought of rolling out pie dough. Dough wants to please you. If you say to it, “My pie dough always falls apart”, indeed, you have given it it’s marching orders.\nInstead, think about other things: A beautiful rose, a stunning sunset, the first time your baby smiled at you…let the dough know that it is going to be just perfect no matter what.\nNot to worry. There is always a way.\nDough is kind of like life. The path isn’t always smooth. Sometimes what feels like an insurmountable boulder blocks our path but we always find a way over, under or around. It may not turn out exactly as planned, but it will be perfect none the less but perhaps in a different way than you expected. Just keep going and it will be fine. Try and keep this in mind as you approach the dough.\nI roll out on a pastry cloth but really most anything is fine. Marble, wood, plastic, freezer wrap or wax paper.\nAs for a rolling pin, I use a French tapered pin but again there are many options. Wine bottles, canning jars work just fine, too!\nPlace a generous hand-full of flour onto the pastry cloth. Unwrap one chubby disc and set it down on the flour. Turn it over so that both sides are covered with flour.\nRolling out the dough.\nNow take your pin and thump the dough a few times on each side. I call this “waking up the dough.” I like to let it know that the main event is just ready to start.\nRoll from the center away from you and leave a bit of an edge, say 1/2 inch, unrolled. Lift the pin and re-place it in the center of the dough. Now roll towards you leaving that 1/2 inch edge again unrolled.\nTurn the dough a quarter turn and do it all again. Repeat this until the dough is large enough to roll out from the center like spokes of a bicycle wheel. Move it around a bit to make sure it isn’t sticking on the surface. Remember to think happy thoughts!\nRoll the dough only as large as it needs to be, about 1-1/2 to 2 inches bigger than your pie pan. It should be about as thick as glass.\nIf it has torn, don’t worry; we can patch it back together with a little ice water. Take a few drops and put it on the back of a patch, pat it in place and move on. Don’t obsess. We want those fats to stay chilly. Remember Rule #1.\nOnce it is big enough, brush off the extra flour. I use a 69-cent brush from my local hardware store. If I don’t have one, I wad up a piece of dry paper towel and brush the flour off with that. Works just fine.\nPlace the rolling pin in the center of the dough and drape it over the pin. Brush off the back side of the dough and gently lift the pin and brush off the back side.\nSlide the pie plate on to a clean spot on your counter, place the rolling pin with the dough on it in the middle of the pan, put your hands on the wood end of the pin, and quickly and deftly, roll the pin to the edge of the plate. I can’t tell you just how much fun this is when you do it.\nAdjust the dough in the pan as if you are covering a sleeping baby. Dough has a memory so we don’t want to stretch it. In the baking it will stretch right back. Just let the weight of the dough ease itself down into the pan.\nFilling and Finishing\nOK, get your filling and pour it into the pan. You can pop everything into the fridge at this point while you roll out the top crust in the exact same way.\nPlace it over the filling, get your scissors or knife and give your pie a haircut. Trim so you have about an inch overhang.\nQuickly turn both edges up all the way around the pan. It’s probably time to remember Rule #1. Too much fussing will just make the fats melt. Don’t worry if it isn’t pretty yet. That’s still to come.\nYou can pop it back in the fridge to chill up for a few minutes if it is feeling a bit soft.\nA chubby disc ready to roll.\nNow, make whatever kind of edge that you want; fork crimp or scalloped, the main purpose of the edge and crimp is to make a sealed reservoir to keep the juices of your pie inside and not on your oven floor. Yes?\nCut a few vents. Paint with an egg white wash and sprinkle sugar over the top.\nMake pie, Be happy!\nMy sour cherry pie with lattice crust","15 Tips for Perfect Pie Crust\nMaking the perfect homemade pie crust isn’t as difficult as you might think! It just takes some practice and a bit of attention to detail. These are my top tips for making the best flaky pie crust.\nTable of contents\n- 15 Tips for Making Perfect Pie Crust\n- 1: Jump in and Practice\n- 2: Keep Everything as Cold as Possible\n- 3: Use a Bit of Shortening\n- 4: Use the Tips of Your Fingers to Press the Fat into Sheets\n- 5: Use Vodka in Place of Water to Control Gluten Development\n- 6: Take Your Time When Adding the Liquid\n- 7: Go by Sight and Feel When Adding the Liquid\n- 8: Flatten into a Round Disk to Chill\n- 9. Plan Ahead & Make the Dough the Day Before\n- 10. Focus Your Pressure on Rolling Across the Dough Rather Than Down Into the Table\n- 11. Rotate the Dough as Your Roll\n- 12. Roll Your Dough Wide Enough\n- 13. Do Not Stretch the Bottom Crust to Fit\n- 14. Lighty Stretch the Top Crust to Prevent the Pie Gap\n- 15. Bake on a Preheated Surface to Prevent a Soggy Bottom\nI think pie crust is something that a lot of bakers tend to be a little intimidated by. It can feel fussy and achieving that perfectly flaky texture can feel daunting. But I’m here to tell you that it isn’t all as difficult as you might think and that making your own pie dough is absolutely worth it!\nOver the years I have made countless pie crusts and learned quite a few tricks along the way. Today I want to share with you my top tips for making the flakiest pie crust. I’m also going to troubleshoot some things along the way.\n15 Tips for Making Perfect Pie Crust\nThese tips all work with any pie dough recipe. I find my recipe pretty fail-proof and it includes an in-depth tutorial for how to make pie crust, including multiple crimping techniques.\n1: Jump in and Practice\nMy first tip is that you really just need to get in there and try it. Pie crust, like bread dough, is something that is really about feel and you just need to get some practice to learn how the dough should feel. I can try my best to explain the texture and feel of the dough you are aiming for, but reading a recipe or watching a video isn’t quite enough.\nIf you’ve never made a pie crust before I highly encourage you to jump in and try! It’s really not as hard as you might think!\n2: Keep Everything as Cold as Possible\nIt is extremely important to use cold butter and ice water when making your pie crust. Keep the butter in the refrigerator until right before you are going to use it.\nThe reason is that you want the butter to stay solid as you work it through. You don’t want it to melt or even get soft as you are working the dough because that is going to make a completely cohesive dough. What you want is solid pieces of fat dotted throughout the crust. These are going to melt in the oven and create little pockets of steam which is where the flakiness comes from.\nIf you live somewhere very warm you may even want to put your bowl and flour in the refrigerator or freezer before you start so that your butter doesn’t warm up too quickly.\n3: Use a Bit of Shortening\nI always highly suggest using a bit of shortening as part of the fat in your crust. Butter is what is going to give us all of the amazing flavor, so I keep that ratio high. But I do like to add a little bit of shortening because that helps the crust not shrink as much in the oven.\nShortening is 100% fat which means when it melts there is no water evaporating off like there is with butter. It also has a higher melting point than butter. These two things help keep the crust’s shape and prevent it from shrinking as much. It also makes it a bit easier to handle so if you are brand new to making pie crust this is going to help you out.\n4: Use the Tips of Your Fingers to Press the Fat into Sheets\nUse the tips of your fingers to press the butter into sheets rather than working it through with a food processor, pastry blender, or grating it on a cheese grater.\nStart with your butter cubed up into about half-inch cubes and then start pressing each piece of butter between your thumb and fingers to create flat sheets. As you work it through, some of the butter will break up into smaller pieces and some will stay rather large. These uneven pieces of fat create the most flakiness.\nI prefer this method to the others stated above because it also really helps to coat the flour in the fat which also helps prevent gluten formation. And I can control how much the butter is worked through. The more you work the butter through, the more sturdy the crust will be.\n5: Use Vodka in Place of Water to Control Gluten Development\nIf you are brand new to making pie dough or if you’ve had issues with your crust being really tough in the past I suggest trying vodka in place of water for the liquid in the dough.\nWhen water hydrates flour that is when gluten starts forming. If the gluten is formed too strongly, then that is when your crust will become tough. However, gluten cannot form with alcohol. Using vodka gives you an extra insurance policy to control gluten development.\nIf you feel confident in your mixing technique, you will not need the vodka. But as you’re learning it can definitely help! Use the cheap stuff. It will cook off and your crust will not taste like vodka.\n6: Take Your Time When Adding the Liquid\nYou will notice that most pie crust recipes give a range of flour to use in your dough. This is because depending on the brand of flour, where you live, and the time of year, it will hydrate differently.\nYou want to worry less about how much liquid is going into your dough and more about how your dough looks and feels. Go slowly, adding bit by bit and turning the mixture over itself to allow the flour to get hydrated. Be gentle here and take your time so you do not overwork the gluten structure.\n7: Go by Sight and Feel When Adding the Liquid\nKnowing when there is enough liquid in your dough will take a bit of getting used to. You are done when there are no big dry patches of flour and when you gently press it together it does not crumble.\nI find most pie crust recipes encourage you to keep it very dry and barely hydrated. I find this to create more problems than it solves. You want the dough to be slightly moist but not overly so. Watch the video tutorial to get a good idea of what it should look like.\n8: Flatten into a Round Disk to Chill\nIt is much easier to roll the dough into a circle when it starts in a circle. So press your dough into a nice round disk when you go to wrap it for chilling. It will make the rolling process much easier!\n9. Plan Ahead & Make the Dough the Day Before\nPie dough needs a minimum of 1 hour of chilling time before you roll it out. This allows the flour to fully hydrate and the gluten structure to relax. I always recommend making it the day before and then it will be ready to go for you the next day!\nYou can also prep much farther in advance and freeze the pie crust up to 3 months before you need it. Let it thaw in the refrigerator overnight before rolling.\n10. Focus Your Pressure on Rolling Across the Dough Rather Than Down Into the Table\nI find that people often struggle with rolling out dough and the biggest issue tends to be where their pressure is focused. Think about focusing your pressure on going across the dough rather than down into the table.\nIf your dough is really fighting you, put a cloth over it and let it sit for about 10 minutes. This will allow the gluten to relax and it will be easier to roll out. The more you mess with the dough, the more stubborn it will become.\n11. Rotate the Dough as Your Roll\nWhile you can roll your dough out between two pieces of parchment paper, I find it easier to do it right on a floured countertop. As you roll it, rotate it 90 degrees between every few rolls. This allows you to check for sticking and redistribute flour underneath if needed. It also helps keep the dough in a circular shape.\n12. Roll Your Dough Wide Enough\nIt’s really important that you roll your dough wide enough so that it does not need to stretch at all to fit in your pie plate. Aim for at least 1-1 1/2-inches wider than your pie plate all the way around.\nI find that most pie dough recipes are barely enough to make this work. For this reason, my pie crust recipe is a bit larger than most to make sure you have plenty to make it wide enough.\n13. Do Not Stretch the Bottom Crust to Fit\nWhen you place your bottom crust into your pie plate make sure to let it naturally fall into the plate and do not stretch it to fit. Any stretching you do will cause the crust to shrink up on you.\n14. Lighty Stretch the Top Crust to Prevent the Pie Gap\nIn contrast to the bottom crust, I do like to purpose stretch the top crust just slightly. Since the filling will naturally shrink as it bakes, giving the top crust a little stretch will cause it to shrink with the filling. This is my tip for avoiding the dreaded pie gap!\n15. Bake on a Preheated Surface to Prevent a Soggy Bottom\nAnd of course, we can’t walk about pie crust tips without addressing how to avoid a soggy bottom! My biggest recommendation for this is to bake the pie on a preheated surface. I like to put a sheet pan in my oven while it is preheating. Then when I’m ready to bake, the pie goes right onto it. Not only does this catch any spills if the filling boils over a bit, but it helps the crust to set quickly and you will prevent a soggy bottom!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:5a1f2623-51fb-41f4-b278-9a6a6499dd51>","<urn:uuid:ed6e5708-9136-445b-87a4-b7ba4d3f6921>"],"error":null}
{"question":"How do clear instructions contribute to the user experience in responsive web design?","answer":"Clear instructions are important because not all devices are the same. Instructions should be provided for both mobile and desktop users to ensure they have the best experience. Ideally, these instructions should be available in both the user's native language and in English to make sure users can effectively use the site or app regardless of the device they're using.","context":["Responsive web design is a new trend in the world of web design. It requires you to think about your website’s user experience from the perspective of the end user. This can be a difficult task, but it is essential for creating a great user experience.\nThink About What Users Want\nResponsive design is all about taking into consideration what users want, and designing the site or application to be as user-friendly and comfortable as possible. Websites that are responsive design-friendly generally look good on any device, whether it be a desktop computer, laptop, tablet device or even a mobile phone.\nSome of the benefits of responsive design include:\n1. Increased Visibility – Sites that are responsive design-friendly generally look good on any device, no matter how large or small. This means that potential customers can see your site regardless of their screen size or what kind of device they’re using.\n2. Improved User Experience – Websites that are responsive design-friendly generally load faster than those that are not, making the site more user-friendly and convenient. Users who have to wait long periods of time for a website to load can quickly become frustrated and switch to another website instead. Responsive design-friendly sites minimize these delays by adapting content and layout automatically so there’s never a need to reload the page completely.\nUse Media Queries For Responsive Design\nResponsive design is all about making websites that look great on all devices, from desktop to phones and tablets. But how do you make sure your website looks good on a range of devices without resorting to using different versions of the site? One way is to use media queries.\nProvide Clear Instructions For Users\nResponsive design is all about making a site or app that looks great on any device, regardless of its size or resolution. In order to make sure users have the best experience, it’s important to provide clear instructions for how to use the site or app. Not all devices are the same, so it’s important to provide instructions for both mobile and desktop users. Ideally, these instructions should be available in both the user’s native language and in English.\nAvoid Flashy Effects In Your Designs\nResponsive design is all about making a site adaptive to different devices and screen sizes. This means that you need to use simplified graphics, avoid flashy effects, and make sure your site looks good on a phone as well as a desktop. Grungy or outdated designs can be jarring on smaller screens and can actually take away from the overall user experience.\nKeep Images Small And Lightweight\nResponsive design is all about making websites that work well on a wide variety of devices. One of the things you need to keep in mind when designing a responsive site is how to reduce the size of images. Images can make a website feel heavy and slow, so it’s important to make sure you use light images that won’t bog down your website.\nMake Navigation Easy To Use\nWhen users visit a website, they need to be able to easily navigate through the site. Navigation is important because it allows users to find what they are looking for quickly and easily. A website that is easy to navigate will be more user friendly, which will lead to more visits and more sales.\nThere are a few things that can help make navigation easy. First, the layout of the website should be designed so that all of the major sections are easy to find. Second, all of the buttons and links on the page should be big enough for users to see and use easily. Third, all of the content on the page should be organized in an intuitive way so that users can find what they are looking for quickly and easily. Finally, any animations or other effects used on the page should not interfere with navigation. By following these tips, businesses can make their websites easy to use for both visitors and customers alike.\nEnsure That Fonts Are Clear And Readable\nFonts can make or break an online presence. Clear and readable fonts ensure that your website is easy to navigate, read, and use. Not all fonts are created equal and some may be harder to see or read on a screen due to their size or style. When selecting fonts, keep the following factors in mind:\n-The font should be legible at a large size\n-The font should be appropriate for the content\n-The font should be easy to find on a website\nOptimize Images For Mobile Viewers\nResponsive design is important for web and mobile devices. Not only does it ensure that a site looks good on a range of devices, but it can also optimize images for download and storage on smaller devices. Images that are optimized for mobile viewing can take up less space on a device, which can save you money in the long run.\nTest Your Designs On Multiple Devices\nWhen it comes to web design, one size definitely doesn’t fit all. That’s why it’s essential to test your designs on multiple devices, including mobiles and tablets. Not only will this help you ensure that your website looks great on all devices, but you’ll also learn which features work best on different devices. In short, responsive design is key when designing for the web!\nBe Flexible With Your Designs\nResponsive web design is all about making sure that the layout and design of your website adjusts automatically to fit any device or screen size. This means that you’re not limited by the dimensions of a desktop computer screen, or the width of a phone’s display. You can make your site look good on smartphones, tablets, laptops, desktops and even large-screen TVs!\nBut being flexible with your designs isn’t just about making things look good on different devices. It’s also about making sure that your website is easy to use no matter what device someone is using it on. If someone is using a smartphone, they might not want to have to zoom in and scroll around just to find what they’re looking for. Responsive web design makes sure that all content is easily accessible from anywhere on the page.\nBeing responsive isn’t always easy – but it’s definitely worth it if you want your website to be as successful as possible!\nThese are some tips on how to create responsive web designs that will satisfy users and increase conversions."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:4b8e6a0e-4328-4ccd-9084-71f3cd1dbb67>"],"error":null}
{"question":"How do backup data and archived data differ in terms of their treatment under organizational data policies? What role does email retention play in both contexts?","answer":"Backup data and archived data serve different purposes and are treated differently under organizational policies. Backup data is used for recovery in case of data loss and needs to be immediately accessible, while archived data is no longer actively used but must be retained for compliance or reference on cheaper storage media. Email retention spans both contexts - from an electronic communications perspective, emails must be monitored for appropriate content and security, while from a data retention view, emails must be archived according to regulatory timeframes and business needs. Organizations need comprehensive policies covering both backup and archival requirements for emails and other communications.","context":["When it comes to essential security requirements for businesses, the electronic communications policy is decidedly unsexy. A painstakingly detailed document is rarely read in full outside of the employee onboarding process, and often languishes, unchecked, in the abyss of corporate paperwork.\nThat said, an electronic communications policy serves as the foundation for basic internet safety guidelines, business instant messaging practices, email standards, and general corporate policy for today’s digital workplace. Without a solid policy in place, businesses open themselves up to a bevy of security issues, potential employee mishaps, and sometimes serious legal challenges.\nWhat does a good policy look like?\nIn general, an electronic communications policy needs to be comprehensive — meaning it covers all forms of electronic communication — and well-defined.\n“It’s important to identify scope and purpose to help employees understand what you mean by electronic communications, and why this policy exists,” said Heidi Shey, a senior analyst with research firm Forrester. “Does this only apply to email? What about VoIP calls, or texting, chat and messaging apps? Without a well-thought policy, everyone makes their own assumptions about what is acceptable use, and people may not know what they don’t know about risks to the enterprise with using different forms of electronic communications.”\nShey said it’s also important to avoid making assumptions about the reader and to use clear, concise language that employees understand. A policy document should also provide a date for when it was last updated and a contact person for employees to go to if they have questions or concerns.\nThe most comprehensive, well-defined communications policies are usually written by a team of experts within an organization, spanning the departments of human resources, legal, audit and compliance, and information technology.\n“That’s because the document isn’t about any one of these things individually,” said Sean Pike, program VP for IDC’s security products group. “It’s about reducing risk throughout the business.”\nAs far as terminology goes, the common bullet points in an electronic communications policy include:\n- Guidelines on the appropriate use of email and other communication platforms\n- Retention policies\n- Proper internet usage\nThe policy should also contain clear language about prohibited uses of email, messaging platforms, internet and other electronic communications, as well as consequences and disciplinary actions for policy violations.\nThe security rationale\nWhen it comes to email usage, the communications policy should set standards for appropriate content to send under the company banner, as well as rules for acceptable use and behavior, like avoiding personal messages and maintaining professionalism.\nPrecise guidelines are also needed to ensure that certain types of information remain within the confines of the business and only reach the eyes of intended recipients.\n“The drivers are are often risk or regulation,” said Pike. “Accidentally leaking corporate crown-jewel intellectual property via email could be devastating, and accidentally emailing unencrypted personally identifiable information of customers could also create challenges.”\nProper email usage is also key to preventing phishing scenarios. Corporate employees should be well-trained to avoid email that looks suspicious, and up-to-date anti-phishing training should be part of the email regimen in an effort to reduce security risks.\nPolicies surrounding email retention are needed to help companies ensure that they meet various data protection or retention requirements for relevant regulations, explained Shey. In healthcare, for instance, the Health Insurance Portability and Accountability Act (HIPAA) requires health care businesses to encrypt health data in transit and storage.\nFor financial services, the Financial Industry Regulatory Authority (FINRA) has issued guidance for social media and digital communications that requires archiving text messages for records retention purposes.\n“This is so employees who are communicating with each other or clients using text messaging or a chat app for business purposes don’t put the company at risk of non-compliance and possible data leakage,” Shey said.\nBoth usage and leakage are important for internet guidelines as well. For the most part, companies want to make sure that users only go to approved web resources to reduce the risk of viruses or downloading unapproved software. Some companies even have policies that dictate behavior on an employee’s personal social media accounts to reduce brand risk.\nThe exact details of a communications policy will vary depending on an organization’s precise needs, but Pike noted that modern policies have trended toward being longer and more specific to ensure that every calculable risk is managed.\n“There are plenty of ways to be destructive with communication, whether that’s leaking information — accidentally or purposefully — or creating hostility toward a coworker,” said Pike. “At the end of the day, these policies are in place to establish the way companies believe employees should act, or must act, given corporate culture or legal and regulatory obligations.”\nIf you need a place to start in creating or updating your company’s policies, these templates from our sister site Tech Pro Research (a paid resource) can help:\n- Electronic communication policy\n- Internet and email usage policy\n- Electronic Retention Policy\n- Social media policy\n- Information security policy\n- Brazilian government to create national information security policy\n- Leaked: Facebook security boss says its corporate network is run “like a college campus”\n- Australians will trade privacy for security if you frame it right\n- How the White House’s new policy for reporting security flaws will affect businesses (TechRepublic)\n- Protect your data assets when disposing of old storage media (TechRepublic)\n- Geek Squad’s FBI informant case illustrates need for good IT policies (TechRepublic)\n- All apps, including WhatsApp, must maintain 'sanctity' of personal communication: Prasad\n- Govt to discuss proposed India e-commerce policy on Thursday\n- What's Section 230? The social media law that's clogging up the stimulus talks\n- 6 I-Day promises from 2014 that Modi (partially) fulfilled\n- New economic priorities for a new year\n- America's future in space ensures fairness for all in an unjust world\n- VIETNAM BUSINESS NEWS JANUARY 11\n- Why your privacy could be threatened by a bill to protect children\n- VIETNAM BUSINESS NEWS JANUARY 12\n- Why all of Trump's tweets and other social media posts must be archived for future historians\n- Big ISPs pause donations to 147 Republicans who tried to reverse Biden’s win\n- VIETNAM BUSINESS NEWS JANUARY 13\n- “Make in Việt Nam” key to Việt Nam’s target of high income by 2045: researcher\n- After two decades and almost $100 million, Colorado launches a new online unemployment benefits system\n- Emirates spreads wings in hardest times\n- VIETNAM BUSINESS NEWS JANUARY 14\n- VIETNAM NEWS JANUARY 14\n- Digital transformation offers better transportation services\n- 5 alternatives to using WhatsApp in 2021\nElectronic communication: What needs to be in a good policy have 1080 words, post on www.zdnet.com at April 3, 2018. This is cached page on Talk Vietnam. If you want remove this page, please contact us.","data retention policy\nWhat is a data retention policy?\nA data retention policy, or records retention policy, is an organization's established protocol for retaining information for operational or regulatory compliance needs.\nWhen writing a data retention policy, you must determine how to: organize information so it can be searched and accessed later, and dispose of information that's no longer needed.\nSome organizations find it helpful to use a data retention policy template that provides a framework to follow when crafting the policy.\nA comprehensive data retention policy outlines the business reasons for retaining specific data and what to do with it when targeted for disposal.\nWhy is a data retention policy important?\nA data retention policy is part of an organization's overall data management strategy. A policy is important because data can pile up dramatically, so it's crucial to define how long an organization must hold on to specific data. An organization should only retain data for as long as it's needed, whether that's six months or six years. Retaining data longer than necessary takes up unnecessary storage space and costs more than needed.\nWhat are the benefits of a data retention policy?\nThere are numerous benefits to establishing a solid data retention policy. Some of the more compelling benefits include:\n- Automated compliance. With an established policy, organizations can ensure they comply with regulatory requirements mandating the retention of various types of data.\n- Reduced likelihood of compliance related fines. Even if an organization retains all the data that's legally required, the organization must be able to produce that data if it's requested by auditors. Retaining only the minimally required volume of data makes it easier and less time-consuming to locate this data, thereby reducing the chances that an organization is fined for its inability to produce data that's required to be retained.\n- Reduced storage costs. There's a direct cost associated with data storage and reducing the volume of data that is being stored also reduces storage costs.\n- Increased relevancy of existing data. Data becomes less relevant as it ages, and a data retention policy removes irrelevant data that's no longer needed.\n- Reduced legal exposure. Once data is no longer needed, it's removed, eliminating the possibility that the data can be surfaced during legal discovery and used against the organization.\nWhat are data retention policy best practices?\nWhen it comes to creating a data retention policy, every organization's needs are different. Even so, there are several best practices that organizations should adhere to when establishing a data retention policy. Some of these best practices include:\nIdentifying legal requirements. Organizations must determine the laws and regulations that govern their data retention requirements so those requirements can be incorporated into the data retention policy.\nIdentifying business requirements. Creating an effective data retention policy involves more than just complying with applicable regulations. The retention policy must also take the organization's business requirements into account. It could be that there are operational requirements that mandate retaining data for longer than what's legally required.\nConsidering data types when crafting a data retention policy. In any organization some data is more valuable than other data. An organization should avoid creating a blanket data retention policy that applies to all types of data. Instead, the policy should specifically define the type of data that must be retained and establish retention requirements for each type.\nAdopting a good data archiving system. If regulatory requirements require certain types of data to be retained for longer than the data is needed by the business, then consider adopting a data archiving system. A data archival system can help reduce the cost of storing archival data, while automating data lifecycle management and giving you the tools to locate data in the archives.\nHaving a plan for legal hold. If the organization is involved in litigation then it will likely need to pause the data lifecycle management process so the data that was subpoenaed won't be automatically deleted once it reaches the end of its retention period.\nCreating two versions of your data retention policy. If an organization is subject to regulatory compliance, it will likely have to document its data retention requirements to satisfy regulatory mandates. This is a formally written document that can be filled with legal jargon. As a best practice, consider drafting a simpler version of the document that can be used internally as a way of helping stakeholders in the organization better understand retention requirements.\nHow do you create a data retention policy?\nCreating a data retention policy is rarely a simple process and some organizations might find it better to outsource the policy creation and implementation process rather than doing it internally. For organizations creating their own data retention policies, there are 10 basic steps:\n- Decide who'll be responsible for creating the policy. This task won't usually be handled by a single person in the organization because it requires expertise in various areas. Typically, the data retention policy creation process is a team effort with members of the IT staff, the organization's legal department and other key stakeholders.\n- Determine the organization's legal requirements. The policy must meet or exceed the requirements outlined in any regulations that apply to the organization. Identify the legal requirements upfront, as they'll be the foundation of the policy.\n- Define the organization's business requirements. This means identifying various types of data and figuring out how long each data type should be retained. Typically, data is active for a period, then moved to archival storage and eventually purged from the archive as a part of the organization's data lifecycle management process.\n- Determine who'll be responsible for ensuring that data retention is being performed according to the policy.\n- Determine how to perform internal audits to ensure policy compliance.\n- Decide the frequency with which the data retention policy should be reviewed and revised.\n- Work with the organization's HR or legal departments to establish a means of enforcing the policy.\n- Determine how the data retention requirements are implemented and enforced at a software level.\n- Write the official data retention policy.\n- Once the policy has been drafted, present the policy to key stakeholders for approval.\nThe operational reason for implementing a data retention policy involves proper data backup. An organization's backup data helps it recover in the event of data loss. A policy is important to make sure the organization has the right data and the right amount of data backed up. Too little data backed up means the recovery won't be as comprehensive as needed, while too much causes confusion.\nA data retention policy should treat archived data differently from backup data. Archived data is no longer actively used by the organization, but still needed for long-term retention. An organization might need data shifted to archives for future reference or for compliance. Archives are stored on cheaper storage media, so they reduce costs and the volume of primary data storage. A user should be able to search archives easily.\nFor proper creation and implementation of a data retention policy, especially regarding compliance, the IT team should work with the legal team. The legal team will have a better idea of how long data must be retained by law, while IT is responsible for the actual implementation of the policy.\nBe careful with the data retention policy. Just because a file was created decades ago doesn't mean it should be automatically deleted after a certain time. That old file could be an important contract the organization must retain, or it could contain other valuable information.\nA storage system can retain or remove data based on rules set up by IT. The use of metadata is one way to figure out when a data object is scheduled for deletion or designated to a given storage location. Automated software moves old data to archives, which is especially helpful for organizations with large data volumes. Some software can automatically delete data based on age, outlined in a retention schedule. But administrators must be certain that deleted data serves no further purpose.\nA data retention policy must consider the value of data over time and the data retention laws an organization might be subject to. In 2006, the U.S. Supreme Court recognized that it isn't financially possible to retain all information indefinitely. However, organizations must demonstrate that they only delete data that isn't subject to specific regulatory requirements and use a repeatable and predictable process to do so. This means various types of information are held for different lengths of time. For example, a hospital's retention period for employee email would be different than that of its patient records.\nAlthough it's common for an organization to establish its own data retention requirements, certain data retention laws must be adhered to. This is especially true for organizations operating in regulated industries. For example, publicly traded companies in the U.S. must establish a Sarbanes-Oxley Act (SOX) data retention policy. Similarly, healthcare organizations are subject to Health Insurance Portability and Accountability Act data retention requirements and organizations that accept credit cards must adhere to a Payment Card Industry Data Security Standard data retention and disposal policy.\nSimply retaining data isn't enough. Federal laws commonly require organizations in regulated industries to create a documented data retention policy.\nAn organization must also consider the General Data Protection Regulation (GDPR), which went into effect in May 2018 and updated data privacy laws across the European Union (EU). Mandates apply to personal data produced by EU citizens, whether the company collecting the data is in the EU, as well as any people and organizations whose data is stored in the EU. It's critical to have a data retention policy that explains which data is being held, why and where it's being held and for how long, as it relates to GDPR directives. Especially with a sweeping compliance regulation such as GDPR, only keep the personal data that's needed.\nData retention policy examples\nLength of time in a data retention policy ranges from minutes to years. Use a policy engine that involves many different fields, such as user, department, folder and file type.\nA data retention policy should include email messages. Emails pile up quickly, and some take up a lot of space, so set a reasonable timetable for retention. As with the data retention policy, the IT team should work with legal on email retention schedule details.\nRegarding targets, object storage is a popular choice in a data retention policy, as it provides solid data protection at a moderate cost.\nPublic cloud storage is another common location for data that requires long-term retention. It's typically cheaper than on-premises storage, especially in infrequent access tiers. Cloud service providers offer off-site data protection, which is important in the event of a disruption to the organization's main data center. Speed of restore depends on the tier and size of the data set.\nIn addition, tape continues to play a key role in long-term data retention. Infrequently accessed historical data finds a good home on tape, where it takes longer to restore than other formats. Storing data on tape for years is typically cheaper than storing it in the cloud and uses less energy than disk storage. Like the public cloud, tape also provides off-site storage.\nWhat are some common data retention policy issues?\nData continues to increase dramatically, not only in primary storage but in backup data and archives as well. Backup takes a particularly burdensome toll when the same data gets backed up. A data retention policy is one way to reduce volume and eventually automate the process of retaining data sets.\nHowever, creating a data retention policy is complex. Setting a data retention schedule isn't cut and dried. Certain data sets require retention of different lengths of time for legal and operational reasons. An organization must be careful, especially if it's instituting an automated form of data retention.\nStorage can be a burden as well. That's why a good data retention policy is clear about the type of storage where retained data goes to optimize budget and space.\nProper data disposal\nWhen a protected record's age exceeds that of the applicable data retention policy, the record must be disposed of properly. Organizations aren't required by law to dispose of old data, but it's often in their best interest to do so.\nMany organizations use an automated system, typically a dedicated archive software product, to securely delete data that no longer falls within the required data retention period. Automation ensures data is disposed of in the proper time frame without manual intervention. Some organizations might use their backup software's archiving functionality to automate data disposal."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:de827cde-0d71-4453-9cfa-c8112f06416e>","<urn:uuid:88bf9260-6721-42c2-bbd0-fad59b616cb0>"],"error":null}
{"question":"¿Cuál es la relación entre los patrones secuenciales de actividad cerebral y la organización de los microcircuitos prefrontales? In English: And how does modern fMRI technology help us study these patterns?","answer":"Small-world functional organization of prefrontal microcircuits is optimal for generating stereotyped sequential patterns of activity, outperforming both random and maximally clustered arrangements. This organization effectively balances between random and clustered regimes to produce sequential patterns. Regarding fMRI technology, modern multiecho fMRI techniques, particularly when combined with independent component analysis (ME-ICA), allow researchers to study these brain activity patterns more accurately by effectively separating true neural signals from artifacts. This technology is especially valuable for studying spontaneous brain activity and inferring functional relationships between brain areas through BOLD signal analysis.","context":["Sequential patterns of prefrontal activity are thought to mediate important behaviors, e. small-world practical corporation of prefrontal microcircuits were able to reproduce the levels of sequences observed in actual data. AMG 548 As expected, small-world data units contained many more sequences than surrogate data units with randomly arranged correlations. Remarkably, small-world data units outperformed data units where correlations had been maximally clustered also. The small-world useful company of cortical microcircuits Hence, which amounts the arbitrary and maximally clustered regimes successfully, is optimum for making stereotyped sequential patterns of activity. ? ? ? ? ? was produced by processing the mean relationship of data pieces where each event was arbitrarily reassigned (within each cell) by moving a variety of structures which range from 1 body (100 ms) to 80 structures (8 s). AMG 548 Every individual epoch, i.e., each constant amount of activity within one neuron’s activity raster, was shifted by a distinctive arbitrary offset, instead of shuffled data pieces where large sections of a task raster AMG 548 (including many intervals of activity and inactivity) had been shifted jointly. Fig. 2. Spontaneous prefrontal network activity is normally enriched in positive events and correlations where multiple neurons are coactive. = 29 tests) … The typical deviation projection in Fig. 1 was attained as follows. For every pixel, we computed the typical deviation of (? nodes linked to confirmed node and calculating may be the final number of sides between your nodes linked to the main node divided by the full total possible variety of sides between all nodes, which is normally neighbours exist, whereas a clustering coefficient of 0 would suggest that nothing from the neighbours talk about an advantage. To compare actual, experimentally observed, networks to random ones, we 1st generated random networks with an Erdos-Renyi model in which all possible edges are equally likely. Specifically, if the real network has an edge probability of and then identified all the additional cells that became active inside a 1-s (10 frames) window following a reference event. This was stored like a template vector of cell IDs and activation instances relative to the research event (i.e., offset instances). This template was then shifted to each subsequent event of was adopted sequentially by events in 37) as illustrated in Fig. 3. A pattern vector comprising the cell IDs and offset instances of each matched event was stored for each recognized sequence. If this pattern vector matched an existing pattern vectoragain permitting one framework of jitterthen it was counted as an additional incidence of that pattern; otherwise, it was stored as a new pattern. For the purpose of defining unique patterns, patterns had to repeat at least three times in data to be counted. This process was repeated iteratively, and every active state in every cell was used as a research event. The algorithm was not parallelized and required 4 h per data arranged operating on a 2.0-GHz dual-core processor. Types of simulated data units. We compared the numbers of Cd248 sequential patterns of activity (quantified as explained above) in our actual, experimentally observed data units to those in various forms of simulated data units. First, we generated shuffled data units simply by shifting large chunks of each neuron’s event train in time. Specifically, we randomly subdivided each even train into six segments of random length, circularly shuffled each of these by a random amount, and then recomposed them together to construct the full event train. For example, a cell’s event train might be separated into segments of 4,000, 6,000, 5,000, 8,000, 10,000, and 3,000 frames, which were individually shuffled and then recombined to form the shuffled data set. Second, we generated scrambled versions of experimentally observed data sets in which we preserved the number of neurons active at each point in time but randomly reassigned the identities of the specific neurons that are AMG 548 active or inactive. Specifically, we AMG 548 identified all of the epochs of activity within each data set, i.e., all the periods of time during which a neuron was continuously active. Each epoch is defined by a start time, duration, and associated neuron, and a data set is fully specified by the corresponding list of.","(Medical Xpress)—Functional neuroimaging technologies – such as functional Magnetic Resonance Imaging (fMRI), Positron emission tomography (PET), multichannel electroencephalography (EEG), magnetoencephalography (MEG), Near Infrared Spectroscopic Imaging (NIRSI), and Single Photon Emission Computed Tomography (SPECT) – are used to measure brain activity and its relationship to specific brain areas. Of these, resting state fMRI (known as rs-fMRI) is employed to evaluate regional interactions that occur when a subject is not performing an explicit task. Since these interactions represent network communication between brain regions, their study is integral to further understanding human behavior, cognitive development and neuropsychiatric disease. More specifically, this resting brain activity is observed noninvasively using fMRI, which detects changes in blood flow in the brain as Blood Oxygen Level Dependent (BOLD) MRI signals. While powerful, so-called BOLD fMRI has two drawbacks: even 1 mm or less of head movement while scanning can impact connectivity estimates, and BOLD sensitivity can be reduced by high subject motion. (This limitation means that when comparing the rs-fMRI data of adult controls versus high-movement subjects such as neuropsychiatric patients or children, false positive functional connectivity findings can become very prevalent.) Recently, however, scientists at the National Institute of Mental Health, University of Cambridge and other locations devised an integrated strategy for data acquisition, denoising, and connectivity estimation by combining data acquisition with multiecho (ME) echo planar imaging and analysis with spatial independent component analysis (ICA), called ME-ICA, which distinguishes BOLD (neuronally related) and non-BOLD (artifactual) effects. The researchers found that when compared with traditional techniques, their proposed strategy results in a fourfold signal-to-noise ratio improvement, more specific functional connectivity analysis, and valid statistical inference and error control between groups with different levels of subject motion.\nLead researcher Prantik Kundu – who is supported by the NIH-Cambridge Scholars program, under the joint supervision of Dr. Peter Bandettini at NIH and Dr. Ed Bullmore at Cambridge University – discussed with Medical Xpress the research he and his colleagues conducted. \"We're studying spontaneous brain activity, captured as BOLD MRI signals, for the purpose of inferring functional relationships between brain areas based on the correlation of their BOLD fMRI signal time courses,\" Kundu tells Medical Xpress. \"Unfortunately, fMRI methods are susceptible to artifact signal from subject motion, magnetic field changes and other sources. That being said, the biggest problem is that functional BOLD signals and artifact signals occur in approximately equal proportions and may manifest with similar time courses and spatial localization patterns.\"\nIn essence, Kundu explains, an analysis of spontaneous BOLD fluctuations can't take advantage of a prior model for expected signals – and artifacts can manifest in virtually any form. As a result, separating functional BOLD signals from artifact signals is a fundamental challenge for all studies of functional brain connectivity. \"This separation has been so challenging, in fact, that some recent studies have implied that this may be too difficult to do in a physically and statistically principled way, suggesting that potentially corrupt fMRI images be deleted completely.\"\nIn recognizing that BOLD signals and artifacts from fMRI were not totally distinct in their spatial or temporal properties, Kundu points out that the team's insight was to use a third domain of information that would completely distinguish BOLD and artifact signals. \"This third domain was the NMR decay of BOLD signals from functional brain activity,\" Kundu says. \"BOLD signals have a specific and robust signature called echo-time, or TE, dependence that artifacts do not.\" The key is that TE-dependence is observable in data acquired with an emerging MRI pulse sequence called multiecho fMRI – but not with standard fMRI sequences that do not acquire multiple NMR signal echoes. Kundu adds that using this third signal domain to disentangle BOLD and artifact has been attempted in a few prior studies using model-fitting approaches – but these approaches, while informative, have had limited applications.\n\"We realized that joining a powerful multivariate statistical decomposition called independent component analysis (ICA) with the analysis of TE-dependence could be an effective way to comprehensively separate all BOLD signals from all artifact signals,\" Kundu continues. \"In essence, the dataset is broken up into statistically coherent components, with each then being analyzed to determine its BOLD and non-BOLD weighting.\" The final step is to denoise the data by removing non-BOLD ICA components.\nKundu points out that the new strategy is based on their previously-published technique combining data acquisition with multiecho planar imaging and analysis with spatial independent component analysis. \"Our first paper demonstrated that 'statistically independent' components arising from ICA decomposition specifically captured signals representing actual physical processes of BOLD and artifact,\" he explains, adding that the level of physical specificity of ICA components was a new inference not previously established in the ICA literature.\nThe standard ICA implementations for fMRI, however, did not use TE-dependence analysis throughout the many processing steps required. \"This meant lower sensitivity overall, so low amplitude signals – such as from the subcortex, which are important in studies of emotion – could be incompletely assessed. \"We developed an implementation of ICA for ME fMRI that does use TE-dependence analysis throughout, resulting in a comprehensive analysis that fully decomposes fMRI datasets.\" Kundu emphasizes that this was a substantial improvement over the approach used in their earlier paper.\nAnother key factor was developing their independent components regression (ICR) estimator. \"Besides separating functionally-related BOLD signals from non-BOLD artifacts, another fundamental problem for functional connectivity analysis based on inter-regional BOLD time series correlation is determining the statistical significance of correlation,\" Kundu says. \"That means determining whether a particular correlation is arising by chance, an anatomical – that is, white matter – connection, or functional process involving multiple regions.\" In identifying a chance correlation, he adds, it has to be taken into consideration that spontaneous BOLD fluctuations are oscillatory and manifest smoothly over space – so identifying a significant correlation is analogous to finding synchronized waves in a turbulent ocean.\n\"We realized that ICA decomposition actually transformed spontaneous BOLD signals into a new basis of statistically independent coefficients on which statistically well-conditioned interregional correlation can be computed,\" Kundu recounts. By incorporating TE dependence analysis in prior analysis steps, processing sensitivity increased. As a result, statistically well-conditioned re-expression by ICA represented all BOLD signals in the data as 'independent' components.\n\"We then realized that the number of BOLD independent components was also the statistical degrees of freedom of the spontaneous BOLD signals – that is, their complexity or variability. This allowed statistical inference using well-understood classical statistical tests.\" By combining interregional correlation on the basis of independent coefficients, and then normalizing correlation values with BOLD degrees of freedom (DOF), they created the ICR functional connectivity estimator. \"Lastly, we realized that greater artifact in fMRI data had the unforeseen effect of reducing BOLD DOF in fMRI datasets, since artifact processes from subject motion interfered with NMR relaxation and thus reduced BOLD sensitivity.\"\nPractically, Kundu says, this meant that, after filtering conventional fMRI data, greater subject motion led to apparently greater BOLD correlation due to less BOLD variability from less sensitive acquisitions. \"By having the BOLD DOF measurement from multiecho fMRI data, however, we were able to control for this variability when computing functional connectivity using ICR.\"\nKundu relates that the scientists addressed these challenges through two main insights. First, TE-dependence analysis is indeed a very versatile and robust basis of differentiating BOLD and non-BOLD signals. Second, ICA – particularly when used to decompose full datasets (which is not a trivial task) – is very powerful and can be leveraged in more ways than is currently appreciated in fMRI methodology.\n\"By combining TE-dependence analysis and ICA for denoising, we realized that virtually all artifacts can be removed from fMRI data without prior knowledge of what the artifact is supposed to look like in terms of spatial localization or time courses. Prior approaches of fMRI analysis and denoising, such as modeling the physical translations and rotations of head motion and then removing those models based on linear fitting, were far less effective than employing a statistical nonlinear decomposition like ICA, and using the physical signal property of TE-dependence to determine what is BOLD and what is not.\"\nAfter combining these approaches for connectivity estimation with ICR, the researchers found that tests on statistical conditioning indicated that functional connectivity analysis became much more consistent in assessments across subjects. This suggested, Kundu says, a more sensitive and specific approach, based on spontaneous BOLD fluctuations, to determining functional brain relationships. \"To determine specific gains after using ME-ICA, we tested denoised BOLD signals with various metrics of signal quality, and compared them to the same metrics computed from fMRI data from conventional acquisition and processing. After removing artifact signals using ME-ICA, signal-to-noise ratio calculations indicated a fourfold increase in signal stability due to the removal of artifactual signal variation.\" In addition, a related analysis, employing metrics previously used to argue that separating BOLD and non-BOLD signal is not feasible, showed that ME-ICA eliminated both linear and more complex non-linear manifestations of artifact processes. \"The increase in signal stability is consistent with the prior knowledge that easily fifty percent of fMRI signal variance comes from artifact, and this value can be even greater in data that is acquired during high subject motion.\"\nAfter the isolated spontaneous BOLD signals were re-expressed for functional connectivity analysis using ICR, Kundu continues, ICR functional connectivity maps for many brain regions were computed across multiple subjects. \"When these maps were compared to corresponding connectivity maps computed with conventional functional connectivity analysis, an intra-class correlation statistical analysis showed that ICR substantially improved connectivity map consistency and specificity.\"\nKundu points out that the test for spurious connectivity findings (also called type I statistical error) was based on the premise that across a set of healthy volunteer data, there should be no average difference in connectivity between random sets of equally sized groups – including groups that are biased by subject motion. \"Any increased likelihood of greater average correlation in high motion groups represents a bias in connectivity,\" Kundu explains. \"Using permutation testing, we showed that ICR maintains a false positive rate that is expected and can be controlled for, whereas conventional connectivity estimation produces two to-five more false positive findings than expected – but the latter cannot be controlled for. This bias is likely due to unaccounted changes in BOLD fMRI sensitivity in the presence of high subject motion and other artifactual factors.\"\nThe researchers also found that their results were well-suited to studying patient populations with poorly characterized functional networks, as well as computing connectivity differences between healthy volunteers and patients. \"Because ME-ICA and ICR connectivity estimation does not require spontaneous BOLD fluctuations to manifest in specific spatial or temporal patterns, any brain with functional BOLD fluctuations can be studied,\" Kundu tells Medical Xpress. \"This includes normal controls, patients, and non-human subjects such as primates and rodents.\" Moreover, ME-ICA and ICR are more versatile than current methodologies that require BOLD signals to have specific spatial and temporal signals determined beforehand.\nIn terms of next steps, Kundu says that the team is currently applying their methodology across many fMRI studies. \"We're especially interested in studies of development and aging, which may come with a wide variety of unexpected findings that can be definitely verified to be BOLD phenomena. Additionally,\" he concludes, \"we plan to build atlases of well-conditioned functional connectivity – and in fact we've already open-sourced our software for public evaluation and use.\"\nExplore further: A new tool for brain research\nIntegrated strategy for improving functional connectivity mapping using multiecho fMRI, PNAS Published online before print September 13, 2013, doi:10.1073/pnas.1301725110"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:ad6aa715-c2b2-4473-94ea-3174a458a4cb>","<urn:uuid:a59803dc-521f-4f22-8903-7a2a5ba50fe7>"],"error":null}
{"question":"How do formal salutations in letters compare to closing lines in business correspondence?","answer":"In formal letters, salutations determine the relationship between sender and recipient. For unknown recipients, one should use 'Dear Sir', 'Dear Madam', or 'Dear Sir/Madam'. When addressing someone by name, use 'Dear Mr. Horse' (not full name). For closing formal letters, acceptable options include 'Sincerely', 'Sincerely Yours', 'Yours Truly', 'Yours faithfully', or 'Best regards'. In British business etiquette specifically, a letter starting with 'Dear Sir or Madam' must end with 'Yours faithfully', while a letter starting with 'Dear [Name]' must end with 'Yours sincerely'.","context":["Mistakes to Avoid When Writing Letters\nKnowing how to write a letter is an essential art one needs to know, even in these days of the Internet and high-tech phones. People still use letters to have information confirmed. Also, letters are a sort of ambassador for a writer. There are lot of kinds of letters that correspond to varying occasions. Commonly, they are divided into formal and informal. Mistakes are made due to writers not knowing the correct format and manner that is associated with a certain type of letter. Thats why our attention will be concentrated on this kind of letters.\nWriting is the record of thoughts and has a advantage over speech and non-verbal communications – it reduces the chance of misunderstanding and misinterpretation. So, you should know how to write a letter, avoiding the main mistakes.\n1. Mistake in the address. An address means the actual address of the person or organization you are writing to. No matter how you send your mail, the most common mistake in writing letters is the address. It’s important to write the correct address, because the letter can be delivered to another person or physical address. Take care to check the address you’ve written.\n2. A comma follows all lines in the inside address. Punctuations ends before postal codes, the company name, identification line.\n3. Does not state the person or organization that it is being directed to. Business letters have two addresses: the recipient’s and sender’s. Always direct your letters to a particular person – this will ensure that your letter is not thrown in the garbage.\n4. The date is written as an abbreviation. For example 12 Nov. instead of 12 November. The second variant is correct.\n5. Mistake in choosing a salutation. A salutation is the part of the letter that determines the relationship between the sender and recipient. Usually the type of salutation is selected from the type of the letter: formal or informal. Most of the writers use informal salutations in formal letters. People usually expect senders to keep to the right type of salutation (especially in formal letters), and it’s not a problem to choose the correct one.\nHere are some examples:\n- To address someone whose name you don’t know, you can write: Dear Sir; Dear Madam; Dear Sir/Madam.\n- To address someone whose name you know, you can write: Dear Mr. Horse (not Dear James Horse or Dear Mr. James Horse)\n- To end a formal letter, you can use: Sincerely; Sincerely Yours; Yours Truly; Yours faithfully; Best regards.\n6. Not stating the purpose of the letter. At the beginning of the letter, you need to state why you are writing. No matter what type of letter you are writing – formal or informal, state the purpose of the letter. Some senders don’t write the intention of the letter, so the reader is wondering why such letters came in the first place.\n7. Writing the text without paragraphs. The letter should contain paragraphs. This will link the ideas in the letter. This will make your sentences more organized and the purpose of the letter will be clearly understood. In formal writing, your ideas should be connected with formal words and phrases.\n8. Use of stale expressions and clichés. Try to avoid phrases like “Kindly be advised”; “As safe as houses” etc. These phrases waste your and readers time.\n9. Using too many abbreviations and indulging in technical language. Avoid such words if you can use more common and understandable words. Remember that the reader may not know all the specific words you use.\n10. Using ambiguous words. Ambiguous words are words that have more than one meaning. Such words can confuse the reader and your letter will be misunderstood. Make your letter simple for understanding, be clear and straightforward – write what you mean.\n11. Misused words and bad spelling. If you write and use words with mistakes, the reader can think that you are a care-free person and making business with you will be a big farce.\n12. Sending a letter without proofreading. Most mistakes appear because writers do not consider looking over their text with a discerning eye in order to catch their mistakes. Here is some advice on how to proofread your letter:\n- Proofread the text on a hard copy. It’s harder to spot mistakes on a screen.\n- Read over your letter later. If you’ll read it just after finishing writing, the chance that you’ll miss mistakes is much higher.\n- Proofread three times: first time for looking through the content, the second time – for grammar and use of punctuation, and the last – to be sure that you’ve used the right words and the letter sounds clear.\nThere are endless examples of mistakes that can be present in letters. These mistakes are commonly committed when writers compose formal and informal letters. Now you know them, so try to remember these instances of errors. Good luck!","A business letter is a letter written in formal language, usually used when writing from one business organization to another, or for correspondence between such organizations and their customers, clients and other external parties. The overall style of letter will depend on the relationship between the parties concerned. There are many reasons to write a business letter. It could be to request direct information or action from another party, to order supplies from a supplier, to identify a mistake that was committed, to reply directly to a request, to apologize for a wrong or simply to convey goodwill. Even today, the business letter is still very useful because it produces a permanent record, is confidential, formal and delivers persuasive, well-considered messages.\nThe most important element you need to ensure in any business letter is accuracy. One of the aspects of writing a business letter that requires the most accuracy is knowing which type of business letter you are writing. A number of options are available for those looking to trade in business correspondence, and you will significantly increase your odds for getting a reply if you know the form you need to send.\n1. Letter of Complaint\nA letter of complaint will almost certainly result in an official response if you approach it from a businesslike perspective. Make the complaint brief, to the point and polite. Politeness pays off regardless of the extent of anger you are actually feeling while composing this type of business letter.\n2. Resume Cover Letter\nA cover letter that accompanies a resume should revel in its brevity. You should take as little time and as few words as possible to accomplish one task: persuading the reader to anticipate reading your resume. Mention the title of the job for which you are applying, as well or one or two of your strongest selling points.\n3. Letter of Recommendation\nA recommendation letter allows you to use a few well-chosen words to the effect of letting someone else know how highly you value a third party. Resist the temptation to go overboard; approach your recommendation in a straightforward manner that still allows you to get the point across.\n4. Letter of Resignation\nAn official letter of resignation is a business letter that should be fair and tactful. Be wary of burning any bridges that you may need to cross again in the future. Offer a valid reason for your resignation and avoid self-praise.\n5. Job Applicant Not Hired\nIn some cases you may be required to write a business letter that informs a job applicant that he was not chosen for an open position. Offer an opening note of thanks for his time, compliment him on his experience or education and explain that he was just not what the company is looking for at the present time.\n6. Declining Dinner Invitation\nDeclining a dinner invitation is a topic for a business letter that, if not done tactfully, may result in a social disadvantage. Extend your appreciation for the invitation and mention that you already have an engagement for that date. Do not go into detail about what the engagement is.\n7. Reception of Gift\nIt is very polite to return a formal business response letting someone know that you have received her gift. Extend a personalized thanks to let her know that you are exactly aware of the contents of the gift. If possible, it is a good idea to include a sentiment suggesting that you have put the gift to use.\n8. Notification of Error\nWhen sending a business letter that lets the receiving party know that an error has been corrected, it is good business sense to include a copy of the error in question if there is paperwork evidence of it. Make the offer of additional copies of material involved in the error if necessary.\n9. Thanks for Job Recommendation\nA letter of thanks for a party that helped you get a job should be professional and courteous. Above all else, avoid the temptation to go overboard in offering your thanks. Be aware that your skills also helped you land the job and it was likely not handed to you as a result of the third party.\n10. Information Request\nA business letter that requests information should make the request specific and perfectly understandable. It is also a good idea to state the reason for the information request. Extend advance appreciation for the expected cooperation of the recipient.\nCompanies usually use printed paper where heading or letterhead is specially designed at the top of the sheet. It bears all the necessary information about the organisation’s identity.\n2. The date of the letter\nDate of writing. The month should be fully spelled out and the year written with all four digits October 12, 2005 (12 October 2005 – UK style). The date is aligned with the return address. The number of the date is pronounced as an ordinal figure, though the endings st, nd, rd, th, are often omitted in writing. The article before the number of the day is pronounced but not written. In the body of the letter, however, the article is written when the name of the month is not mentioned with the day.\n3. The Inside Address\nIn a business or formal letter you should give the address of the recipient after your own address. Include the recipient’s name, company, address and postal code. Add job title if appropriate. Separate the recipient’s name and title with a comma. Double check that you have the correct spelling of the recipient ‘s name. The Inside Address is always on the left margin. If an 8 1/2″ x 11″ paper is folded in thirds to fit in a standard 9″ business envelope, the inside address can appear through the window in the envelope.\n4. The Greeting / Salutation\nAlso called the salutation. The type of salutation depends on your relationship with the recipient. It normally begins with the word “Dear” and always includes the person’s last name. Use every resource possible to address your letter to an actual person. If you do not know the name or the sex of of your reciever address it to Dear Madam/Sir (or Dear Sales Manager or Dear Human Resources Director). As a general rule the greeting in a business letter ends in a colon (US style). It is also acceptable to use a comma (UK style).\n5. The Subject Line (optional)\nIts inclusion can help the recipient in dealing successfully with the aims of your letter. Normally the subject sentence is preceded with the word Subject: orRe: Subject line may be emphasized by underlining, using bold font, or all captial letters. It is usually placed one line below the greeting but alternatively can be located directly after the “inside address,” before the “greeting.”\n6. The Body Paragraphs\nThe body is where you explain why you’re writing. It’s the main part of the business letter. Make sure the receiver knows who you are and why you are writing but try to avoid starting with “I”. Use a new paragraph when you wish to introduce a new idea or element into your letter. Depending on the letter style you choose, paragraphs may be indented. Regardless of format, skip a line between paragraphs.\n7. The Complimentary Close\nThis short, polite closing ends always with a comma. It is either at the left margin or its left edge is in the center, depending on the Business Letter Style that you use. It begins at the same column the heading does. The traditional rule of etiquette in Britain is that a formal letter starting “Dear Sir or Madam” must end “Yours faithfully”, while a letter starting “Dear ” must end “Yours sincerely”. (Note: the second word of the closing is NOT capitalized).\n8. Signature and Writer’s identification\nThe signature is the last part of the letter. You should sign your first and last names. The signature line may include a second line for a title, if appropriate. The signature should start directly above the first letter of the signature line in the space between the close and the signature line. Use blue or black ink.\n9. Initials, Enclosures, Copies\nInitials are to be included if someone other than the writer types the letter. If you include other material in the letter, put ‘Enclosure’, ‘Enc.’, or ‘ Encs. ‘, as appropriate, two lines below the last entry. cc means a copy or copies are sent to someone else.\n- Block Style\n- Semi Block Style\n- Modified Block Style\n- Indented Style ( a69014.wordpress.com)\n- Simplified Style ( dsmlmdblog.blogspot.com)\n- Hanging indentation Style"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:6b040676-fe4e-4acb-901d-c74f7f26474c>","<urn:uuid:fafcb5a3-0f7d-40ed-9ba2-aa094d1f6c97>"],"error":null}
{"question":"I'm looking into historical musical equipment. What's the connection between British and American music culture when comparing O'Connor's musician dolls and the British Range Master's influence on guitar players?","answer":"O'Connor's dolls represent American musical culture through figures like Mahalia Jackson and a blues singer doll inspired by Bonnie Raitt, while the British Range Master unit represents British rock culture, being used by prominent British musicians between 1965 and 1969, including Eric Clapton, Tony Iommi, Brian May, and Mark Bolan. This equipment was particularly influential in British rock, with almost all British rock guitarists using germanium Treble Boosters from the late 60's to mid 70's, until they were eventually replaced by Japanese alternatives like the Tube Screamer.","context":["These dolls are characters\n‘They relate to somebody, they tell you who they are’\nWriting teachers like to say that character development is an important tool in any writer’s arsenal. But these days, most of Peggy O’Connor’s characters are not in books.\nThese “characters” at O’Connor’s home are for sale at Mormor’s gift shop in Shelburne Falls. They are cloth dolls with expressive faces and jointed arms and legs. Some have pretty faces and some don’t. They’re of all ages and races.\n“A lot of dollmakers are into fairies,” says O’Connor. “That doesn’t grab me.”\nO’Connor, a writer and former high school English teacher, now has a growing family of 18- to 20-inch tall “people.” Some are replicas of real people she has known at different stages of their lives. There are also dolls of whimsy: a bearded guru on a meditation pillow and a slender, adoring follower that O’Connor has named “The Sprite.”\n“She’s about 40 years old,” O’Connor says of The Sprite. “She’s like the fan you always see at a concert, the woman who dances by herself in front of the band.”\nA fancy bejeweled “Princess Pat” doll was inspired by a character in a child’s song that was sung at Marks Meadow School in Amherst, where O’Connor had worked as an aide. The lyrics say that the princess has a “rig bamboo” of “red, gold and blue,” so O’Connor made her a royal staff out of bamboo, decorated in the appropriate colors. A pearl-studded hair barrette that O’Connor found at a tag sale, became Princess Pat’s “wig” and was the first inspiration for the doll.\nOne flaxen-haired musician — in a jazzy fedora and suspenders, jamming on his harmonica — represents her husband, Richard O’Connor, from an earlier time in his life. He is sitting in a chair, with a cardboard amplifier beside him. Next to “Richard” is a doll that looks like Peggy — but she is re-imagined as a guitar-playing blues singer with a white and aqua guitar.\nO’Connor calls her “The Singer Doll,” and she represents O’Connor’s fantasy, “to be Bonnie Raitt.”\n“This is in my dreams,” O’Connor says with a laugh.\nBut the guitar in the doll’s arms is a facsimile of Eric Clapton’s, based on one of his guitars that O’Connor found on the Internet and made herself, out of painted Styrofoam. The tuning pegs are pearl beads.\nO’Connor has created a doll that resembles her cousin, a Boston University professor, in his 1970s “dashiki era,” sporting a goatee and large spectacles. He is standing next to a doll that, O’Connor says “turned out to be a librarian.” Her prim, red-and-white striped suit is made out of a jacket that O’Connor found in the Wendell “free box.”\nAnother doll resembles Mahalia Jackson.\nThere are also two sisters that were inspired by characters in the 1991 movie, “Daughters of the Dust,” about Gullah women preparing to migrate north from St. Helene’s Island in 1902. “My mother was from the island,” O’Connor remarked.\nOne doll, wearing elegant sandals sits in a movable wheelchair made of wooden dowels and toy wheels. Beside her is her “service dog” — a doll/dog who looks a lot like O’Connor’s real dog, Paki.\n“The whole point of these dolls is they relate to somebody — they tell you who they are,” she says.\n“If you bought a doll and if it worked for you, in my mind, the doll would be personal to you. I imagined the dolls would be something that would inspire your own creativity — your own story.”\nShe calls them her “character dolls.”\n“Some are real people and some I’ve made up,” she explained. “You can live with them and they become friends.”\nO’Connor first learned to sew by making her own doll clothes when she was 7 or 8 years old, she said. Later on, she took up sewing in Home Economics class and started making her own clothes.\nWhen her daughter, Kate, was a child, O’Connor sewed both for Kate and for her dolls. She also made cloth animals for her daughter, as toys.\nAfter moving to Greenfield about seven years ago, O’Connor was inspired by a doll-making book by Patti Medaris Culea and O’Connor started making her dolls. The doll bodies are mostly made of cotton muslin and stuffed with Fiberfill. The noses, lips and eyes are needle-sculpted, to be three dimensional. The faces are colored with pencils and gel-markers. “Paint tends to stiffen the cloth,” she says.\nBesides making the doll body, O’Connor often makes the items that help to tell the doll’s story. “They like their things,” she jokes about the dolls and their possessions.\n“I have to make up a story that leads me into making the doll. I start out with the body and by the time I do the face, I sort of have a good idea of who the doll is: How do I show that this is a mall rat? Or that that’s a professor?”\n“Once I get a name for the face, I just look for the items in a store or a free box that would complete the image. And, if I can’t find it, I have to make it.”\n“I’ve never gone to a doll-making workshop. I’ve just looked at dolls,” she said. “You can’t do what someone else does and be happy with it. I don’t want rules.”\nO’Connor sews the dolls on a portable Singer sewing machine. “Each doll I think of, I have to think up another concept,” said O’Connor.\nO’Connor has a master’s degree in creative writing so, hopefully, she’ll think up many more dolls.\nMormor’s is the only store where her dolls are for sale. Most are in the $100 to $200 range. Mormor’s on Deerfield Avenue is open Tuesday through Sunday.\nFor more information, call O’Connor at 413-773-8297 or email her at:\nStaff reporter Diane Broncaccio has worked at The Recorder since 1988. Her beat includes west county. She can be reached at: firstname.lastname@example.org or: 413-772-0261, ext. 277.\nStaff photographer Paul Franz has worked for The Recorder since 1988. He can be reached at email@example.com or 413-772-0261 Ext. 266. His website is www.franzphoto.com.","The BSM RM is based on the british Range Master unit produced between 1965 and 1969, as used by Eric Clapton in John Mayall´s Bluesbreakers. Eric also used the booster when playing in the legendary group Cream. Its biting, powerful sound is clearly in evidence on the Bluesbreakers \"Beano\" album. Blues rocker Rory Gallagher, Glam rocker Mark Bolan, Heavy Rock pioneer Tony Iommi and Brian May from Queen were also heavy users of the original Range Master unit.\nThe RM Early Days is the result of many customers´ demands, looking for the early RM sound. The RM Early Days is loaded with the same highly selected OC44 transistor, used for the famous stock RM model. Only the passive circuit has small differences to the stock RM unit. Therefore the RM Early Days sounds a little clearer and brighter in treble and tighter in the bass-response, compared to the stock RM. It's a fine machine for a brilliant classic rock tone.\nAlmost all British rock guitarists using single coil pickup guitars used a germanium Treble Booster from the late 60's to the mid 70's. By the end of the seventies, the Treble Booster was replaced by a new circuit from Japan, the so called Tube Screamer and other similar overdrive circuits. These were based on the old Treble Boosters and therefore had a very similar frequency response. The germanium boosters on the other hand, sadly fell into oblivion despite their unique and inimitable sound. The RM Treble Booster has been specifically designed for single coil and humbucking pickups with a relatively low/medium inductance, such as pickups in Fender, Gretsch, Burns or Gibson guitars. On these pickups, the RM produces (with the volume control of the guitar turned up to the max) a fat, biting and penetrating lead sound, minus any shrill characteristics. By lowering the guitar's volume control, many shades of crunch can be easily dialed in. Another thing to consider is that when a guitar amp's volume is turned up (reaching saturation), the power amp normally begins to mute the highs, which results in a duller sound. At an amp's full tilt, the RM Treble Booster allows a more transparent tone (covering the entire frequency range) to shine through. The presence control on the amp need only be minimally used (if at all). When vintage amps are used (such as the Fender Deluxe, Fender Bassman, Vox AC30, Marshall JTM 45 or Marshall Plexi) it is amazing what effect the use of a good treble booster has. It is also of interest to note that Eric Clapton used a new 2 x12\" Marshall combo (Model #1962 with KT66 output tubes) with the Bluesbreakers, and 100 Watt Plexi heads with Cream. Brian May used a couple of VOX AC30 combos with the RM in front of the \"Normal\" channel.\nThe Treble Booster is inserted between guitar and amplifier, not into the FX loop. The magical tone is achieved by the interaction between guitar pickup, treble booster and amplifier. The unit is powered by a 9V battery with a current consumption of aprox. 250 A. The average output level is 10dBm, the maximum output voltage when the strings are struck really hard is 7V max.\nNote: The positive pole of the battery is ground. By turning the volumepot, the specially circuit design makes noise - just like the orginal!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:47a37be8-1f10-458b-8b40-27d169ab088d>","<urn:uuid:4ed128f5-e90b-41a7-91f9-4daebde15ffe>"],"error":null}
{"question":"How do the official judging procedures in bodybuilding competitions evaluate physical appearance, and what are the key differences compared to combat sport scoring like wrestling in terms of match termination conditions?","answer":"In bodybuilding, judges evaluate competitors through multiple rounds focusing on overall physique, with the IFBB system using separate rounds for standing relaxed poses, mandatory poses, and free posing routines. While bodybuilding judging is largely subjective, focusing on physical attributes like symmetry, muscularity, and presentation, wrestling has more objective scoring criteria based on specific moves and positions. Both sports have match termination conditions, but they differ significantly: bodybuilding competitions continue through all rounds unless there's a disqualification, while wrestling matches can end early through various means including falls (pins), technical falls (15-point lead), injury default, or disqualification. Additionally, wrestling matches have strict time periods with specific rest intervals, whereas bodybuilding judging proceeds through predetermined rounds without time-based constraints.","context":["Bodybuilding contests over the years have been judged using a number of different procedures. In the early days of \"Physical Culture\" contests, competitors were required to demonstrate their athletic ability on stage---a precursor of today's fitness contests for women. Many early bodybuilding contests also involved evaluating the personalities and public speaking abilities of the contestants, who would talk briefly on stage in addition to showing off their physiques.\nBodybuilders have been divided into categories based on age and experience (novice, teenage and masters, for example) as well as by height or by weight. Scoring has been done using a rigid points system in an effort to emulate various established Olympic sports and, more informally, with the judges going off in private, discussing the contest among themselves, and coming to a conclusion much like a jury in a court trial.\nThe point of any and all of these judging systems and procedures is (or at least should be) to identify the best bodybuilders and rank them according to excellence. That is, the goal of judging is not to define what good bodybuilding is supposed to be but to recognize it.\nPicture of the old system of judging!\nUltimately, it is the bodybuilders themselves who define what is possible in physique competition. Judges would not expect to see, or to demand to see, for example, cross-striations in the glutes unless or until they had already seen such a level of development on stage. As in other sports such as gymnastics and diving, a competitor comes along and establishes a new standard---maybe one previously thought to be impossible---and then suddenly everyone else starts working to duplicate that feat. As an example, looks how long it took for runners to break the four-minute mile barrier. But once one runner did it, suddenly hundreds were accomplishing the same thing.\nFundamentally, judging in bodybuilding is based on the ability of the judges to know what a good bodybuilder looks like when they see one. If a judge doesn't have a good \"eye\" for bodybuilding, it doesn't really matter what kind of system of rules is used. So the effectiveness of judging rules and procedures should be evaluated basically using one criteria---does the system help the judges in identifying the best bodybuilders, or does it make it difficult or even impossible for them to do so?\nThe NPC Judging System\nThe National Physique Committee, which governs amateur bodybuilding in the United States, uses a comparatively simple system of judging. All the judging for individual classes takes place during the prejudging (recently, the federation has begun doing some of the judging during the finals, but most still takes place during afternoon prejudging). Selecting an overall champion from among the class winners takes place during the finals. When an NPC competitor poses in the finals to music, this is strictly a show for the audience. The scoring has already been completed.\nIn an NPC contest, each bodybuilder in a class comes out individually and does a one-minute free-posing routine, with no music. The entire line-up is then brought out on stage together and the judges bring out groups of competitors---starting with what seems to be the top five or so---for comparison posing. There are no \"mandatory\" poses as such, but the bodybuilders are generally asked to do most of the common, conventional physique poses---front double-biceps, lat spread, side-chest and so forth. However, the judges can call for any pose they want---a most muscular, for example, or simply for \"your best leg shot.\"\nNPC judges don't score the rounds separately, but simply rank each bodybuilder in order once prejudging for the class is over. So when an NPC judge finally places a competitor, that score doesn't reflect any single aspect of the bodybuilder's physique or presentation but a complete, overall evaluation of his or her excellence.\nThe IFBB Judging System\nThe IFBB, which sanctions international amateur and professional bodybuilding competition, has a more complex and structured set of judging procedures. The prejudging consists of two rounds. In the first, the bodybuilders assume a \"standing relaxed\" posture (something like standing at attention, arms at the sides) and are looked at from the front, back and both sides. In the second, the competitors go through a very specific set of mandatory poses. In both rounds, the bodybuilders are looked at individually and are compared as well, although unlike the NPC system - where the judges can call out any number of competitors in a group - usually only three competitors are brought out for comparison at a time.\nAnother difference between the NPC and the IFBB procedures is that IFBB judges score each round separately, placing the bodybuilders in order of excellence for that individual round, and a competitor's score represents his or her cumulative scores from all of the rounds.\nIFBB scoring is not completed once prejudging is over. The competitors don't do their free posing during prejudging, but are evaluated by the judges as they perform their routines to music in the finals. Once the third round is completed and the scores are added up, the top finalists are then brought out for an additional \"pose-down\" round and have an opportunity to make a final impression on the judges.\nAll Rounds Are Physique Rounds\nThe IFBB system of judging has been largely effective, with comparatively few controversial decisions resulting (except for the intrusion of the politically-inspired \"guidelines\" recently applied to female bodybuilding) when you consider how difficult it is to achieve an \"objective\" consensus of something with so subjective a nature as bodybuilding judging.\nBut when problems have arisen, it is often because there is a tendency on the part of some judges, when working within such a highly structured system, to forget that bodybuilding is a physique contest and that every round in a bodybuilding contest is supposed to be a physique round. That is, the judges are supposed to look at every aspect of the competitors in every round and to judge each round as if that were the entire contest. Judges should say to themselves, \"If this were all I was going to get to see of these bodybuilders, if this were the whole contest, whom would I place first, second, third and so forth?\"\nBut this often doesn't happen. For example, the \"standing relaxed\" round is often called the \"symmetry round.\" This is because, when the bodybuilders are not flexing and posing, and they can't cover up physique faults by how they hit their poses, the aspects of their physiques that stand out tend to be their overall shape, symmetry and proportion. But the first round is not symmetry round - that is, it is not a round in which the judges are looking at symmetry as if it were a separate and distinct element of the bodybuilding physique. It is a physique round in which every feature of the body that can be observed should be considered.\nFor example, a judge should not give a high placing to a competitor with an aesthetic shape and good proportions, but who lacks mass or density, is not hard and defined or who is lacking in any other aspect of physical development or conditioning. As with the other rounds, the judges should score the competitors as if this were the entire contest, decided on the basis of what they can see while the competitors are \"standing relaxed\" who the best bodybuilders on stage are in order of excellence.\nThe same is true of the second round. The bodybuilders do mandatory poses, flex their muscles, and this tends to show off their muscularity, but it is not \"muscularity\" round. Of course, when competitors start to pose a physique that seemed relatively unimpressive in round one can seem to come alive, muscles popping out all over. But while this should certainly count in her or his favor, it doesn't mean a bodybuilder's other obvious faults or weaknesses should be ignored. Again, the judges are supposed to look at the competitors in this round and ask themselves, \"Who would I place first, second and so forth if this round were the entire contest and this was all I was going to see?\"\nThe point of the different rounds is giving the judges as much information as possible on the physiques they are evaluating. You see different things when a competitor is just standing there or when he or she flexes and poses. Someone who doesn't impress you in one round might well change your mind in another. But the bodybuilding physique must always be looked at as a whole and be judged and evaluated in its entirety in each and every round.\nWhen it comes to round three, presentation is a vitally important aspect of a physique contest, but it is not a \"posing\" round. The idea of posing is a simple one: the competitor has had to show the judges his or her physique the way they wanted to see it; in round three, the bodybuilders have a chance to show-off their development in a way that maximizes their strong points and minimizes their weaknesses.\nBut the physique being presented in round three is the same one the judges have seen in the previous two rounds---and they pretty much know what it looks like by now. So bodybuilders shouldn't expect to jump a whole lot of places in the scoring because they are able to do an exciting routine. They might, however, in calling attention to certain of their strengths, cause a judge to think, \"You know, that bodybuilder is really better than I gave him (or her) credit for.\"\nIts an unfortunate fact of genetics that many physiques don't look all that good in certain poses---although they might be very impressive in others. And that's what round three is all about---showing the judges your physique in a way that is the most flattering to you and impressive as possible.\nThe IFBB judging system obviously works, and the judges who officiate in IFBB contests have proven track records. But there has been a tendency on the part of judges, bodybuilders and spectators alike to think of round one as a \"symmetry\" round, round two as a \"muscularity\" round and the third round as a \"posing round,\" and therefore to look at isolated aspects of the physique (or non-physique aspects such as posing) rather than the physique as a whole. This approach to judging can skew the results in such a way that, in spite of the best intentions of the judges, some of the competitors might not receive the placings they really deserve.\nThe remedy for this situation is for the judges to keep in mind at all times that they should base their evaluations on everything they see in every round, to judge each round as if it were the entire contest, and to keep in mind that bodybuilding is a physique contest and therefore every round in a bodybuilding competition is a physique round.","Rules of wrestling competitions explained\nAmateur wrestling is the most widespread form of sport wrestling. There are two international . Dual meet scoring is very similar on the high school level. TEAM POINTS – DUAL MEET SCORING. PIN FALL – 6 team points – Awarded to the team whose wrestler holds both of the opponent's scapulas to the mat for. Overview of Wrestling Rules and Scoring. The object The wrestlers meet at center mat and shake hands and step back so that one Dual Meet Team Scoring.\nA wrestler gaining control over their opponent from a defensive position. Exposure or the Danger Position: A wrestler exposing their opponent's back to the mat, also awarded if one's back is to the mat but the wrestler is not pinned. Under the — changes to the international styles, a wrestler whose opponent takes an injury time-out receives one point unless the injured wrestler is bleeding. Penalty points are awarded in collegiate wrestling according to the current rules, which penalize moves that would impair the life or limb of the opponent.\nHowever, the manner in which infractions are penalized and points awarded to the offended wrestler differ in some aspects from the international styles. Collegiate wrestling also awards points for: This is similar to the exposure or danger position points given in Greco-Roman and freestyle. A wrestler scores points for holding their opponent's shoulders or scapulae to the mat for several seconds while their opponent is still not pinned.\nTime Advantage or Riding Time: On the college level, the wrestler who controlled their opponent on the mat for the most time is awarded a point; provided that the difference of the two wrestlers' time advantage is at least one minute. A wrestler getting from a defensive position to a neutral position.\nThis is no longer a way to score in freestyle or Greco-Roman. Period format[ edit ] Women's wrestling In the international styles, the format is now two three-minute periods. A wrestler wins the match when they were able to get more points than their opponent or 10 points lead in two rounds. For example, if one competitor get lead in first the period, they will win by superiority of points.\nOnly a fall, injury default, or disqualification terminates the match; all other modes of victory result only in period termination.\nOne side effect of the old format was that it was possible for the losing wrestler to outscore the winner. For example, periods may be scored 3—2, 0—4, 1—0, leading to a total score of 4—6 but a win for the wrestler scoring fewer points. In collegiate wrestling, the period structure is different.\nA college match consists of one three-minute period, followed by two two-minute periods, with an overtime round if necessary. Victory conditions in the international styles[ edit ] Two U. Air Force members wrestling in a Greco-Roman match.\nA match can be won in the following ways: A fall, also known as a pinoccurs when one wrestler holds both of their opponents' shoulders on the mat simultaneously. Medical personnel should check each athlete thoroughly for skin infections before he or she is allowed to weigh in. When cleared, the athlete reports for the weigh-in.\nOverview of Wrestling Rules\nUSA Wrestling requires, as do some other organizations, that athletes weigh in wearing uniforms or shorts because officials may be women, and girls and women may enter some competitions. When the wrestler comes to the scale, the official confirms that the medical clearance is complete and then directs the wrestler to step on the scale.\nDigital scales are used most often now. When using a digital scale, the readout should not be visible to the athlete. This is done to keep the athlete from moving around on the scale in an attempt to affect the result. If a balance scale is used, the official should direct the athlete to stand in the middle of the platform and stand still. Most associations and leagues give an athlete just one chance on the scale. If athletes are over the allowed weight, they are not allowed to compete.\nIt is the responsibility of the athlete and the coach to be sure that the stated weight is correct before presenting for weigh-in. Match Structure A match is made up of three timed periods. The time can vary depending on local rules. For example, USA Wrestling specifies three 2-minute periods with 30 seconds of rest between each period for all age categories see table 3. Before the start of a competition, check the rules to make sure you know what the time periods are for each age group. The first period starts with both wrestlers standing.\nAt the end of the first period, the official determines which wrestler gets to choose how to start the second period.2017-18 Wrestling Rule Change Demonstration\nIf it is a dual meet, this protocol is decided before the first match, and the teams alternate who gets the choice. In a tournament, the athletes wear colored ankle bands to help the official and scorers identify athletes. The official flips a colored disc, and the wrestler with the winning color gets the choice.\nThe wrestler with the choice at the start of the second period has four options. Athletes can choose to defer the choice to their opponent so they can make the choice they want in the third period. They can choose to start in the neutral position, both wrestlers standing.\nThey can choose to start down so that they can escape or get a reversal while they are still fresh.\nRules of wrestling competitions explained\nFinally, they can choose to start in the top position so they can work to get the fall, again, while fresh. A match is over if one wrestler achieves a pin, or fall see chapter 9 for more information on pins. Matches are also stopped if one wrestler gets ahead of the other by 15 or more points, a technical fall.\nDisqualification for misconduct, stalling, or other severe violations of the rules also stop a match. Although wrestling is one of the martial arts, or combat sports, any hold or maneuver applied with the intent to injure the opponent is prohibited. Starting a Match All youth wrestling matches begin with the two wrestlers on their feet, facing each other in a neutral position, with no advantage to either one. The duration of a match is specified according to the wrestling style and the age group involved.\nWrestling does not have a time-out in the way we understand them in basketball or football. There are no stops to be used strategically, to compose the team, or simply to catch a rest. It is possible for an athlete to ask for a time-out for injury.\nInjury time-outs have time limits, and once the time is reached, the athlete must compete or withdraw. The time limit is most often one and a half minutes. Different organizations allow a different number of injury time-outs. Coaches and wrestlers must know the rule for the specific competition before it begins.\nIn college wrestling, for instance, only one time-out is allowed, and if the athlete asks for a second, the match is terminated. The official will stop a match when blood is present so that the bleeding can be stopped and the wrestlers and the mat cleaned and disinfected.\nThere is no time limit for blood issues, and the match will continue until it becomes clear that the bleeding is interfering with the match too much."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:9c77760c-d44c-4b57-87ff-031ef7a86715>","<urn:uuid:f3ec5f2c-ffb9-4b2d-b20a-2d5d84e5fc61>"],"error":null}
{"question":"I'm doing a report about climate change effects... how are baby harp seals being affected by declining sea ice? 🥺","answer":"Baby harp seals are being stranded in greater numbers due to declining sea ice. Harp seals need stable winter sea ice as a safe platform to give birth and nurse their young. In years with light ice cover, younger seals are being forced to fend for themselves before they're ready, leading to increased stranding rates among seals less than one year old.","context":["Harp Seals Abandoning Their Pups Thanks To Climate Change\nLee Rannals for redOrbit.com – Your Universe Online\nDuke University scientists have determined that young harp seals off the eastern coast of Canada are at a greater risk of getting stranded than adult seals due to climate change.\nResearchers wrote in the open-access journal PLOS ONE that declining sea ice is leaving baby harp seals stranded in greater numbers.\n“Stranding rates for the region’s adult seals have generally not gone up as sea ice cover has declined; it’s the young-of-the-year animals who are stranding (those less than one year old),” said David Johnston, a research scientist at Duke’s Nicholas School of the Environment. “And it’s not just the weakest pups – those with low genetic diversity and presumably lower ability to adapt to environmental changes – that are stranding. It appears genetic fitness has little effect on this.”\nHe said that harp seals rely on stable winter sea ice as a safe platform to give birth and nurse their young until the pups can go off on their own. However, in years with light ice cover, younger seals are being left to fend for themselves before they’re ready.\nJohnston and his colleagues expanded on a study published last year that found seasonal ice cover in all four harp seal breeding regions has declined by up to six percent since 1979. The team compared winter ice from 1992 to 2010 in a region off Canada’s east coast with yearly reports of dead harp seal strandings along the US northeast coast that were grouped by gender and estimated age of the seal.\nAccording to the latest analysis, in years when ice cover was reduced, stranding rates for younger seals rose sharply, even though stranding rates of adult seals remained relatively stable. The team also compared DNA samples from 106 harp seals that had been stranded ashore with those from seals that had been caught by fishing boats in the region during the same period.\n“We used measures of genetic diversity to determine if the dead seals that came ashore were less fit than the presumably healthy ones that had been caught by fishermen, but found no difference,” said Thomas Schultz, director of Duke’s Marine Conservation Molecular Facility. “The stranded animals appear to have come from a genetically diverse population, and we have no evidence to suggest that genetic fitness played a role in their deaths.”\nThe study found that male seals were stranded more frequently than females during the study period, and that this relationship was strongest during light ice years.\n“Our findings demonstrate that sea ice cover and demographic factors have a greater influence on harp seal stranding rates than genetic diversity,” said Brianne Soulen, who co-led the study while she was a master’s degree student in marine ecology at Duke.\nKristina Cammen, a Duke PhD student who also co-led the study, said this study provided more context for how climate change is affecting younger animals during a crucial part of their life."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:4e019bff-6f29-45b7-aa5f-0db677a4cf9d>"],"error":null}
{"question":"Which military cooperation projekt has more focus on Arctic security - NORDEFCO or Visegrad Group?","answer":"NORDEFCO has a significantly stronger focus on Arctic security compared to the Visegrad Group. NORDEFCO specifically addresses Arctic challenges through dedicated discussions on security and defense aspects in the region, information sharing initiatives, and military winter training coordination. They have established a working group on increased information sharing in the Arctic and created a course catalogue for Arctic-related activities. In contrast, the Visegrad Group's defense cooperation focuses mainly on EU Battlegroup formation, joint military exercises, and regional defense in Central Europe, with no specific mention of Arctic security matters.","context":["The beginnings of a European Army? Only time will tell.\nWe, the Prime Ministers of the Czech Republic, Hungary, Poland and Slovakia, recognize that current security trends in Europe call for even closer regional defence cooperation and multinational programs deeply rooted in NATO and EU policies. The Visegrád Cooperation continues to serve as a platform for coordination of our countries in all European and transatlantic security policy fora.\nIn our statement in October 2013 we tasked our Defence Ministers to deepen defence cooperation among our countries by drafting a Long Term Vision specifying our shared strategic objectives, strengthening cooperation in the field of training and exercises of the armed forces in the V4 format, and creating a framework for an enhanced defence planning cooperation. We thereby endorse the results achieved and, in particular, welcome the adoption of the Long Term Vision this March. The areas of cooperation stipulated in the Long Term Vision, and subsequently identified opportunities for common V4 defence projects and cooperative ventures provide the sufficient foundation for the New Opening in our defence relations. We encourage further efforts in implementing these forward-looking ideas.\nIn accordance with the Long Term Vision and other documents approved during the Hungarian V4 Presidency we are committed to make further progress in our defence cooperation. We will use the recently designed structures to identify areas of practical cooperation and pursue specific projects in the field of defence capability development including joint development projects and military equipment acquisitions. In this process we support the involvement of V4 national defence industries as much as possible. At the same time we are open to cooperation with other partners outside the V4 where and when useful.\nWe will continue preparation for the V4 EU Battlegroup (V4 EU BG) to be on stand-by in the first half of 2016. The Visegrád Battlegroup serves as a linchpin for our defence cooperation in the area of training, exercises and capability development. The current security situation in Europe underlines our intention to link the V4 EU Battlegroup certification exercise with NATO’s Trident Juncture 2015 exercise. Based on the experience gained during the creation of our EU Battlegroup, we see merits in establishing a permanent V4 multinational force.\nWe endorse the commitment of our Defence Ministers to organize annual V4 military exercises starting from 2015 focused also on collective defence according to Article 5 of the North Atlantic Treaty. These exercises will be essential for our armed forces to remain interoperable. In this context, involvement of other regional partners, as well as the U.S. forces is desirable. This is our contribution to NATO’s Connected Forces Initiative and also supports the implementation of reassurance measures to enhance our collective defence and deterrence.\nThe aggression of Russia against Ukraine and the subsequent annexation of Crimea have changed the security environment in Europe and made us re-think our defence posture. We are determined to actively and substantially contribute to reassurance measures within the Alliance. These events could also provide stimulus for further cooperation. We recognize the increasing importance of Multinational Corps Northeast (MNC NE) in the region. We also welcome the MNC NE framework nations’ work to enhance its operational capabilities and usability as a platform for regional defence cooperation.\nThe V4 brand is already recognized by our Allies and Partners. We are committed to continue our common efforts to be a visible element of the global security environment in line with the EU strategic tasks set up by the European Council in December 2013, as well as part of the “European share” in the transatlantic relations by generating our own creative defence solutions.\nTherefore, we, the Prime Ministers of the Visegrád countries, task our Defence Ministers to further enhance our defence cooperation by:\n– preparing the Action Plan of the V4 defence cooperation during the Slovak Presidency;\n– elaborating the V4 Training and Exercise Strategy during the Slovak Presidency;\n– exploring the possibility of strengthening cooperation among V4 defence industries in the field of research and development and production;\n– exploring options of the common development and procurement project of universal modular tracked platform and wheeled armoured personal carrier;\n– exploring the possibilities of forming a permanent V4 modular force which could be used as a Visegrád contribution to NATO and EU rapid reaction forces as well as in crisis management operations, building on the experience and knowledge gained through the preparation of the V4 EU BG.\nWe, the Prime Ministers of the Visegrád countries, task our Defence Ministers to report back to us at the end of the Slovak V4 Presidency about the progress achieved in these areas.\nFull article: Visegrad Countries May Turn EU Battlegroup into Permanent V4 Rapid Reaction Force (Atlantic Council)","NORDEFCO Annual Report 2020\nThe Nordic Defence Cooperation, or NORDEFCO, is the intergovernmental collaboration of the Nordic countries on matters of military and defense. Its five member states are Denmark, Finland, Iceland, Norway, and Sweden. This report details the accomplishments of the recent Danish chairmanship, the organization's structure, and the goals for the cooperative in the coming years.\nFind the full version here: https://www.nordefco.org/files/NORDEFCO-annual-report-2020.pdf\n\"Foreword: Danish Chairmanship 2020\nThe Nordic Defence Cooperation is important to Denmark. We need a strong Nordic defence cooperation to contribute to stability and security in our region. Therefore, the Danish NORDEFCO chairmanship in 2020 has been a key priority for Denmark - and for me personally.\nThis annual report is a testament to the important progress made in 2020 to develop and strengthen NORDEFCO. It accounts for the priorities and focus areas that have guided the Danish chairmanship and achievements obtained. We were only at the beginning of the Danish chairmanship, when our new NORDEFCO crisis consultation mechanism proved its relevance in very real terms; A missile attack in Iraq affecting Nordic troops on training mission followed by the outbreak of the COVID-19, underlined the importance of being able to rapidly convene and consult each other during crises.\nThese examples clearly illustrate how far we have come in deepening our Nordic defence cooperation. How close together we have moved. They underline the importance of continuing to improve the ability to act together in crisis and conflict - a core ambition in NORDEFCO Vision 2025 and a main goal for the Danish chairmanship. Together, we have taken significant steps this year to help us to achieve this goal. Furthermore, the COVID-19 pandemic, as the defining global challenge in 2020, has shown the robustness and agility of NORDEFCO. Our work has continued in a largely undisrupted manner – yet virtually.\nOne important focus area this year has been to strengthen Nordic-Transatlantic relations and move forward with the aim to ensure joint participation in important exercise activities. Moreover, we have shared perceptions of the Arctic to enhance situational awareness and be better equipped to address security challenges in the region. Within the area of cyber security we have improved our understanding of challenges in regard to recruitment, education and retainment of competences and mapped areas where increased cooperation could be of particular benefit. We have broadened the discussion on total defence with perspectives from the Haga-cooperation on emergency management, and pushed the important agenda of green defence forward to contribute to overall CO2 reductions.\nFurthermore, at the virtual fall ministerial meeting on 5th November, Denmark joined Finland, Norway and Sweden in the commitment to improve and sustain security of supply between Nordic countries. As COVID-19 has shown us, security of supply is key in a crisis situation, and the agreement allows for better cooperation both in peacetime and beyond. In addition, all four countries signed an agreement on Nordic export control in order to simplify procedures and facilitate Nordic cooperation in the defence material area.\nIn closing, I would like to thank my Nordic colleagues and everyone who has contributed to promoting and developing the Nordic defence cooperation this year. The Nordic countries have different security affiliations, but we all share the commitment and responsibility to keeping our region safe and secure. Nordic defence cooperation is key to this endeavour.\nI look forward to the coming year under the Finnish chairmanship of NORDEFCO.\nTrine Bramsen Danish Minister of Defence\nNORDEFCO Vision 2025\nOn the 13th of November 2018, the Nordic Ministers of Defence adopted the following vision for enhance Nordic defence cooperation towards 2025: We will improve our defence capability and cooperation in peace, crisis and conflict. We ensure a Close Nordic political and military dialogue on security and defence. Acknowledging our different security affiliations, we pursue an agenda based on joint security perspectives, efficient and cost-effective cooperation to strengthen our national defences and the ability to act together. In order to operationalize the vision, we will strive towards the following targets: By 2025, we have:\nMinimal restrictions on movement and storage of military units and equipment, between and through the nations in support of national and multinational activities, operations and deployments.\nIncreased cooperation in total defence, military security of supply and civil-military cooperation.\nImproved regional and common situational awareness in peace, crisis and conflict, in all relevant domains, through real-time information- and data sharing.\nEnhanced NORDEFCO as a platform for crisis consultation and established mechanisms for that purpose.\nImproved our readiness and sustainability in order to improve our ability to act together.\nCoordinated relevant training and exercises between the Nordic countries, and we have improved interoperability.\nEnhanced our transatlantic relations by seeking closer cooperation in areas such as training, exercises and other activities, and improved cooperation with our European partners.\nContinued to strengthen our dialogue and cooperation with Estonia, Latvia and Lithuania.\nImproved our resilience in light of the dangers posed by hybrid threats and growing cyber threats. Coordinated our international operations with a focus on national contributions, command and control and joint logistics, when possible.\nEnhanced logistical cooperation where possible and desirable, with mutual measures to support national needs in crisis and conflict. Established a strategic dialogue to enhance capability development in order to meet the requirements needed to address the security environment\nAn active and flexible partner in the Nordic defence industry in developing capabilities and finding new solutions to armaments and total defence requirements, including through utilization of the possibilities inherent in the proposed European Defence Fund as well as other relevant for a and instruments.\nWe will continue to explore and adopt new beneficial possibilities for cooperation which may emerge\nEstablished options for common education and training to maximize effectiveness and availability in all Nordic development and procurement programs\nEnhanced our armaments coordination and cooperation\nWe will continue to explore and adopt new beneficial possibilities for cooperation which may emerge.\nDanish Chairmanship Priorities and Results\nNordic-Transatlantic relations A key priority of 2020 has been to strenghten the Nordic-Transatlantic relations as one of the targets in Vision 2025. To achieve this, the Nordic Ministers of Defence agreed on a joint approach to continued U.S. engage- ment in the Nordic region. A key deliverable in this respect has been to ensure that the Nordic-led Arctic Challenge Exercise (ACE) maintains so-called flag-level according to the agreed-upon criteria, and U.S. participation in the exercise is essential in this regard. The U.S. Air Force Europe is already deeply engaged in ACE 2021. This was consolidated in the first two ever meetings between the US European Command (USEUCOM) and the NORDEFCO Military Coordination Committee (MCC).\nTo support the strong Nordic-Transatlantic Military cooperation on ACE, the political level in NORDEFCO has initiated the development of a political Letter of Intent to ensure future Nordic-Transatlantic engagement in ACE in order to maintain it at flag level, with a signing ceremony scheduled for February 2021.\nWhile the COVID-19 pandemic did lead to cancellations, postponements and downscaling of both national and multinational exercises, a positive development included the establishment of new procedures for further synchronization and coordination of military exercises and training activities with both Transatlantic and Baltic partners.\nThe Arctic An important theme during the Danish chairmanship was to strengthen the Nordic se-curity policy dialogue, in particular in regard to the Arctic. In light of emerging security challenges, the Arctic has become of increasing interest. During 2020, the Danish chairmanship facilitated discussions on security and defence related aspects of the situation in the Arctic and information exchanges in regard to national Arctic strategies in the Nordic countries. The work of a military NORDEFCO Working Group on increased information sharing in the Arctic was delayed due to COVID-19, but is scheduled to be completed by spring 2021. Supplementing this, information sharing on joint Nordic military winter training and exercises will also be strengthened and a course catalogue on activities has been made available at the NORDEFCO webpage.\nCyber security and resillience The cyber security threat knows no borders, and to build resilience, nations need to work together. This is why strengthened Nordic cooperation on cyber security and resilience is one of the goals in Vision 2025. To achieve this, a Danish-led working group explored possible areas for Nordic cooperation within cyber security, exchanged information and shared best practices during 2020. Specifically, the working group explored best practices in regard to cyber security education in the area of the respective MoDs, including a general map- ping of challenges and lessons learned of respective national cyber security education initiatives. A similar working group on the military level focused on establishing methods and procedures of information sharing related to cyber operations, which led to enhanced coordination regarding participation in exercises and courses. The work showed that existing frameworks for collaboration could be utilized further, for example to exercise scenarios for experts already working together in the Nordic military CERTs (Computer Emergency Response Teams). During the Chairmanship, a Maritime Hybrid Warefare Webinar was held, at which the European Hybrid Centre of Excellence presented current scenarios. The purpose was to bring awareness to the risks and dangers of this type of warfare, and it showed that preparedness within a broad range of fields such as legal, technical, tactical, political as well as interagency and international cooperation would be necessary to counter this type of threat.\nInternational operations A new NORDEFCO concept for ‘information sharing about ongoing and coming operational planning’ was developed and implemented during 2020. The concept aims to enable identification of cooperation possibilities in the first phases of operational planning. A concrete example has been to explore possible cooperation in Iraq within the framework of Operation Inherent Resolve (OIR). It led to the conclusion that support from a small number of instructors on established bases could be facilitated. The question of possible NORDEFCO or Nordic-Baltic joint ventures within the framework of the NATO Mission in Iraq (NMI), as well as possible cooperation in Sahel, will be further explored.\nImplementing NORDEFCO’s Vision 2025 As regards the most central goal of NORDEFCO’s Vision 2025 – to improve the ability to act together in peace, crisis and conflict – Finland will continue the work in accordance with the decisions made during the Danish chairmanship. Last spring, military mobility was selected as the test project for examining possibilities for future cooperation during crises and conflicts. The first NORDEFCO Table Top Discussion was organised during Finland’s previous chairmanship in 2017. In 2020, a plan for similar exercises in the coming years was made and adopted. According to the plan, the Finnish chairmanship will organise a Table Top Discussion (TTD) at senior official level. This will be NOR- DEFCO’s second Table Top Discussion.\"\nLet us know what you think of this report here: https://www.biedsociety.com/forum/european-union/sub-regional-european-entities"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:6ab4c67b-3d16-4aec-996d-fbbd9f4def0c>","<urn:uuid:e718d7f4-05b0-4147-a4e1-4b114e59b9b6>"],"error":null}
{"question":"What are the key differences between product costing and process costing in terms of their application and effectiveness in modern manufacturing? In particular, how do they compare when dealing with standardized vs unique products in today's industrial setting?","answer":"Product costing and process costing differ significantly in their modern applications. Product costing faces challenges with modern manufacturing techniques, as 21st century facilities can assemble products so quickly that component inventories become less relevant, making traditional calculation methods obsolete. It also struggles with variations in production techniques that create complicated accounting situations. Process costing, on the other hand, is specifically designed for standardized, homogeneous products produced in large quantities, making it more suitable for industries like chemicals, textiles, and petroleum. Process costing is particularly effective for continuous or repetitive operations where products are identical and cannot be distinguished. However, process costing has limitations - it cannot be applied to different products or partial products, and the cost obtained is historical, making it less useful for managerial decisions.","context":["Cost accounting and product costing are two accounting methods for determining the cash needed to create goods and services. A company's decision to use either accounting technique can have lasting implications on how the business interprets financial data and makes business decisions. Product costing may work better for a business lacking modern manufacturing facilities, while cost accounting better suits a company using large-scale production methods.\nProduct Costing Definition\nProduct costing is the accounting process of determining all business expenses pertaining the creation of company products. These costs can include raw material purchases, worker wages, production transportation costs and retail stocking fees. A company uses these overall costs to plan a variety of business strategies, including setting product prices and developing promotional campaigns. A company also uses product costing to find ways to streamline production costs to maximize profits. For example, choosing raw materials that are more cost-effective can allow a company to increase profit from retail sales by lowering its product creation costs.\nProblems with Product Costing\nThe modernization of manufacturing techniques and improvements in product shipping have greatly changed the ways businesses calculate product cost. According to eNotes, an education website, manufacturing facilities in the 21st century can assemble products so quickly that there's little need for component inventories. This renders many old methods of calculating product cost irrelevant. Additionally, shifts in manufacturing focus to meet customer needs through production have led to manufacturing lines with small variances in production techniques. These seemingly small differences in production techniques create complicated accounting situations where companies have difficulty determining actual production costs in the short term. Compensating for this lack of clarity requires companies to make long-term projections regarding costs over the life of product lines instead of costs leading up to the sale of products.\nCost Accounting Definition\nCost accounting is the process of collecting, classifying and recording all the costs associated with accomplishing a business objective or particular company project. According to BusinessDictionary.com, a business uses cost accounting to analyze data collected from completing a business task to determine the fair value or selling price of the product created from that task. For example, a company creating a line of snow skis performs cost accounting to determine a selling price for the skis that both covers the company's costs and allows the business to return a profit on each sale. Cost accounting can also help a company streamline its production process to reduce costs and return a greater profit on individual product sales.\nCost Accounting Advantages\nUnlike product costing, cost accounting doesn't have the problems associated with adjusting projections to suit modern manufacturing techniques or counting individual inventory components. This allows cost accounting to deliver detailed reports regarding the cost of each phase of production. A business can use these reports to specifically target areas of the company for cost reduction or efficiency improvement. Additionally, cost accounting focuses solely on the cash spent to create goods as an economic factor of production. This means a business using cost accounting views money as the single factor affecting the company's ability to produce goods and services.\n- Ablestock.com/AbleStock.com/Getty Images","What is process costing?\n* Process costing\nis the method involving allocation of manufacturing cost to products in order to determine per unit cost.\n* It is used for those products, which are similar and identical.\n* The primary focus of this method is on the processes involved in producing homogeneous products.\n* Process costing is applicable where output results from continuous or repetitive operations and products are identical and cannot be distinguished.\n* It is used to trace and accumulate the direct cost and is also used in the allocation of the indirect costs of a manufacturing process.\n* It is a type of operation costing, which is used to ascertain the cost of a product at each stage or each process of manufacturing.\n* Process costing involves certain expectations about standard costs and can be subjective; therefore, process costing is only appropriate for companies that produce large units of a product.\n* Normal losses and abnormal losses may also arise in production. Units of normal loss will be included in the equivalent units whereas units of abnormal loss will not be included in the equivalent units.\nWhy is process costing important?\nIt is important because it helps keep track of where money is spent in the distribution and production processes. Process costing system requires less bookkeeping than job costing system.\nWhat are the advantages and disadvantages of the process costing system?\nAdvantages of process costing system\n* Process costing helps save time because in process costing, allocation is done on the basis of the number of production processes.\n* It is an easier system to use when costing is related to homogenous products.\n* It is the most flexible method of costing and helps ensure that production can take place at the most competitive price in the economic marketplace.\n* It helps in determining the weak link in a production chain.\n* It is considered flexible because of one more reason that process costing allows companies to remove or add a process as necessary.\n* It helps in controlling the production process effectively.\n* Discrepancy or inefficiency in a specific department or process can be easily tracked without even checking each process or department.\n* In process costing, the overall cost from each process or department is collected and the cost of specific job is ignored, which makes process costing easier and simple.\nDisadvantages of process costing system\n* The major disadvantage of process costing is that it creates cost errors in the production system.\n* Process costing is not applicable in a situation where different products are produced.\n* In case of partial products, it becomes impossible to apply process costing.\n* It is not useful for decision making and managerial control.\n* The cost obtained by this method is historical cost, which is of less use for managerial decisions.\nIn spite of many advantages and disadvantages, still process costing is considered a useful costing approach for a few companies, especially for those that are into the production of homogenous products and that too in bulk.\nWhen is process costing applied?\nWhen companies are to produce a product in bulk and a series of operations is required, process costing is applied. It is used when the production process is not affected by one product. It is applied in industries like coal mining, petroleum, textiles, chemicals, food, glass, plastic and paper.\nWhat is the difference between process costing and job costing?\n1. Process costing is used by companies that produce identical or similar products whereas job costing is used by companies that produce unique products.\n2. Companies that use process costing normally deal in paint, petroleum, chewing gum etc. Companies that use job costing are associated with advanced technology system, airplanes, accounting etc.\n3. Record keeping: Job costing requires a lot more record keeping than process costing.\n4. Uniqueness of product: Job costing is used for unique products whereas process costing is used for standardized products.\n5. Size of job: Process costing is used for large production runs whereas job costing is used for small production runs.\n6. Job costing is applied to ascertain the specific order cost whereas process costing is applied to ascertain standardized products cost.\n7. Job costing helps accumulate the manufacturing cost for each job, whereas process costing helps accumulate the manufacturing costs for certain processes or departments.\n8. In process costing, costs are accumulated for certain months or years whereas in job costing, cost is accumulated for the whole job irrespective of the time period.\n9. In process costing, the total process or department cost is divided by the process output in order to calculate the unit cost, whereas in job costing, the total cost of job is divided by job order units to calculate the unit cost.\n10. Job costing can be carried out while a particular job is going on, whereas process costing can be carried only when a process has been completed.\n11. In job costing, the final value of a cost can be calculated beforehand whereas in process costing, the final value of cost is calculated only at the end of a process.\n12. Job costing is used by industries that produce heterogeneous products, whereas process costing is used by industries that produce homogenous products.\nWhat are the steps involved in process costing?\nThere are five steps in process costing, which are mentioned below:\nStep 1. Summarizing the flow of physical units of output.\nStep 2. Computing output in terms of equivalent units.\nStep 3. Computing equivalent unit costs.\nStep 4. Summarizing total costs to account for.\nStep 5. Assigning total costs to units completed and to units in final phase in process inventory.\nDiscuss the various methods involved in process costing.\nThe various process costing methods are as follows:\nMethod 1. Process costing FIFO\nThe First-in, first-out (FIFO) process-costing method involves assigning cost to the earliest equivalent units available. In this method, it is assumed that the earliest equivalent units in work in process are completed first.\nMethod 2. Process costing LIFO\nThe last-in, first out (LIFO) process costing method involves assigning cost to the latest equivalent units available. In this method, it is assumed that the latest equivalent units in work in process are used first.\nMethod 3. Process costing weighted average\nThe weighted-average process-costing method involves assigning average equivalent unit cost to all the work done until a certain date to equivalent units completed and transferred out, and to equivalent units in ending inventory. It is simply the average of various equivalent unit costs entered in the work in process account.\nLet’s take one process costing example:\nStep1. Physical units\nWork in process, beginning\n60% conversion costs 1000\nUnits started in process 35000 36000\nUnits transferred out 31000\nUnits in ending inventory\n20% conversion costs 5000 36000\nStep 3. Computing equivalent unit costs\nBeginning inventory $2350 $5200\nCurrent costs $84050 $62000\nTotal $86400 $67200\nEquivalent units 36000 32000\nCost per unit $2.40 $2.10\nStep4. Summarizing and Assigning total costs\nWork in process beginning inventory:\nTotal beginning inventory $7550\nCurrent costs in Assembly department:\nCosts to account for $153600\nDepartment’s cost to units transferred 31000 units *$4.50 = $139500\nUnits in ending work in process inventory\n$12000+ $2100 = $14100\nCost transferred out:\n31000 * ($2.40+42.10) $139500\nCosts in ending inventory:\nMaterials 5000* $2.40 $12000\nConversion 1000* $2.10 $2100\nTotal costs accounted for: $153600\nWhat are the reasons behind the use of process costing?\nProcess costing is used for the following reasons:\n- With process costing, a company can manufacture thousands or millions of units of a product in a given period of time.\n- It is not necessary to sell the same quantity that the products are produced in. A company can produce large quantities and can sell the same in small quantities.\n- Process costing helps in offering feedback to managers, which they can use to draw comparisons with similar products.\nSome of the questions on 'Process Costing' our experts have answered are -\nQ In spite of the differences, what are the similarities between process costing and job costing?\nQ Out of process costing and job costing, which method is preferred by companies in general?\nQ Among last in first out, first in first out and weighted average, which is the best and the most practicable process costing method?\nTranstutors is the best place to get answers to all your doubts concerning process costing. You can submit any of your Accounting homework and our tutors will make sure that you get timely answers. So what are you waiting for? Submit your process costing questions now."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:41eae9ec-df36-47a7-bc4c-7ec596fd3740>","<urn:uuid:1d173732-8863-4bf0-a091-0fefb5fae9c6>"],"error":null}
{"question":"I live in Alaska and want to know: what is hydrokinetic energy and how are schools using renewable energy there?","answer":"Hydrokinetic energy uses the force of moving water to generate electricity by turning turbines placed directly in rivers, estuaries, coastal waters or tidal flows. Alaska has 90% of the total U.S. tidal energy resource. As for schools, some Alaskan institutions like the Tok High School have implemented renewable energy solutions, specifically biomass systems. The Tok school installed a 5.5 million Btu chip-fired biomass boiler that not only provides heating but also produces 60 kW of electricity, covering 75% of the school's electrical needs. This has helped the school save money and recover programs like preschool and rehire staff members.","context":["What is marine and hydrokinetic?\nThe method of capturing energy from waves, tides, ocean currents, the natural flow of water in rivers, and marine thermal gradients, without building new dams or diversions.\nWhat is the difference between hydroelectric and hydrokinetic?\nTraditional hydropower (river dams and conduits) is also produced by moving water, but is described here. In-stream Hydrokinetic Energy: In-stream hydrokinetic projects generate electricity directly from the flow of water in rivers, inland waterways, irrigation canals and other man-made conduits.\nWhat is hydrokinetic system?\nHydrokinetic energy conversion systems are the electromechanical devices that convert kinetic energy of river streams, tidal currents, man-made water channels or waves into electricity without using a special head and impoundment.\nWhat is hydrokinetic energy used for?\nHydrokinetic Energy uses the force of moving water (current) to turn turbines placed directly in domestic rivers, estuaries, coastal waters or tidal flows, which in turn drive electrical generators that convert the kinetic energy of the moving water (hence its name “Hydrokinetic Energy”) into electrical energy.\nHow does marine and hydrokinetic energy work?\nMarine and hydrokinetic power technologies generate electricity from the kinetic energy of moving water, including waves, currents, and tides. Similar technology can exploit the constant ow of water in rivers or in large-scale ocean currents like the Gulf Stream.\nWhat is marine energy technology?\nMarine technologies are devices immersed in the earth’s ocean capturing the energy from waves, tides and the heat stored in the water. They offer huge potential, as the resource is largely untapped so far. This is a relatively young branch of renewables.\nWhat are the best places to use hydrokinetic energy?\nHydrokinetic Energy in Alaska Most inland communities in Alaska are situated along navigable waterways that could host hydrokinetic installations, and Alaska, with 90% of the total U.S. tidal energy resource, is home to some of the best tidal energy resources in the world.\nWhat are common examples of how hydrokinetic energy is used?\nVarious hydrokinetic technologies are available, including:\n- Wave power buoys capture the energy in the up-and-down movement of waves generating power that is transmitted by an underwater cable to the electric grid onshore.\n- Underwater turbines use water currents to spin underwater blades and generate electricity.\nHow is hydrokinetic used?\nHydrokinetic devices convert the energy of waves, tidal currents, ocean currents or river currents into electrical power. Rural Alaskan villages—as well as anyone with individual, commercial, or scientific needs for remote power in areas near river systems—could benefit from the use of hydrokinetic power.\nHow is marine energy used?\nWave energy (or wave power) is the transport and capture of energy by ocean surface waves. The energy captured is then used for all different kinds of useful work, including electricity generation, water desalination, and pumping of water.\nHow much does hydrokinetic energy cost?\nThe estimated capital cost of the array is $123 million, or about $2,500 per installed kW, with an annual operations and maintenance cost of $4.5 million. The cost of electricity for utility generation would be $0.11/kWh.\nWhat countries use hydrokinetic energy?\nChina is the leading hydro power generating country, followed by Canada, USA, and Brazil (World Energy Sources: 2013 Survey, 2016).","Presentation on theme: \"Tanana River School Districts- Converting Wood into Educational Dollars and Economic Development July 29, 2013 Presenters: Daisy Huang, Research Engineer.\"— Presentation transcript:\nTanana River School Districts- Converting Wood into Educational Dollars and Economic Development July 29, 2013 Presenters: Daisy Huang, Research Engineer Alaska Center for Energy and Power University of Alaska Fairbanks Art Nash, Energy Specialist Cooperative Extension Service University of Alaska Fairbanks 1\nWhy woody biomass? Alaskan context Why biomass is a solution Three examples of woody biomass usage in Interior Alaska Tok Delta Junction Tanana Outline 2\nAlaska has a rural population of 200,000. Communities range in population from ~10s to ~1000s. Most of them run isolated diesel-fired generator sets. Most are not on the road system and are accessible by air or boat only. The median household income in rural Alaska is $40,000. Highest heat loads in the U.S., with temperatures regularly reaching -50 (F or C hardly matters at this point!). Both electricity and fuel oil costs are high and volatile. Electricity: $0.22/kWh in Fairbanks $0.40-1.00+/kWh in rural Alaska Fuel oil: $4.00/gallon in Fairbanks $4.50-$12+/gallon in rural Alaska What is the problem? 3\n80% of energy in Alaska is used for heating* Biomass is local, renewable, may be sustainably harvested. Why are we interested in biomass heating? 4 *WHPacifc Report: Alaska Energy Authority End Use Study: 2012, April 30, 2012\nBiomass is often widely available: Land that is being cleared for development Forest that is being cleared for fire remediation River drift logs (unverifiable anecdotes indicate that 200,000 cords come down the Yukon each spring) Unusable waste product from lumber mills Wind and beetle kill Even when biomass is deliberately harvested, it may be done so sustainably. Why are we interested in biomass heating? 5\nFor example… A phenomenal wind event in September 2012 brought down 70-90% of the trees around Tanacross and Dry Creek (~15 road miles from Tok) Photo credits: Jeff Hermanns, AK DNR\nTok, population 1300, on the road system Example I: Tok 7\nTok has an arid climate and is surrounded by thick foresta recipe for fire disaster. In the past 25 years, 2 million acres in the area have burned, costing $60 million for fire suppression and causing 6 evacuations In 1990, a single fire burned over 100,000 acres (400 km 2 ); in 2004, the fire almost destroyed the school and other city buildings. Over 1000 firefighters could not stop the fire in 1990. Example I: Tok 8 Bitter Creek fire currently burning east of Tok: Photo by Gordon Amundson Photo by AK DOF\nWood is cut and deliberately burned for fire remediation (3,000 acres is the target); wood is low quality (small trees) and cant be used for lumber. In 2010, a 5.5 million Btu chip-fired Messersmith biomass boiler system was installed in the 88,000 square foot Tok High School. In 2011, a steam turbine was added that currently produces 60 kW, matching the average load of the school (it still needs grid power during peak times) and covering 75% of electricity needs. Excess heat is still available; a greenhouse is being built to provide vegetables to school lunches, and a district heating system to neighboring city buildings is under assessment. Example I: Tok 9 Photo courtesy of Alaska Department of Natural Resources\nExample I: Tok 10 Annual heating and fire remediation costs before installation of the biomass boiler: 53,000 gallons of oil at $5.50/gallon $300,000 total DOF spent $1000/acre for tree removal. Annual heating costs after installation of the biomass boiler: 300 tons of chips at low variable costs. 75% of electrical power usage also displaced. Funding sources: $3.2 million grant from Alaska Energy Authority $750,000 from the State of Alaska This savings to the school has enabled the recovery of Toks preschool program, as well as the rehire of three staff members for the schoola music teacher, counselor, and boiler operator\nDelta Junction, population 1000, on the road system, 110 road miles from Tok Example II: Delta Junction 11\nDelta Junction leveraged the expertise developed in Tok and installed the same 5.5 million Btu chip-fired Messersmith biomass boiler system in its Delta High School, a 77,000 square-foot building, which serves 200 students. Chips are purchased from Dry Creek Lumber, which rents Toks chipper to make chips out of scrap wood. Example II: Delta Junction 12\nAnnual heating costs before installation of the biomass boiler: 102,000 gallons of fuel oil at $4/gallon Cost: $408,000 Annual heating costs after installation of the biomass boiler: 500 tons of chips at $60/ton 25,500 gallons of fuel oil at $4/gallon Cost: $132,000 Efficiency increases annually as local skill increases; 1 st year, 50% of oil was displaced, 2 nd year, 75% of oil was displaced. Funding sources: $2 million grant from Alaska Energy Authority $800,000 from the State of Alaska Example II: Delta Junction 13\nTanana, population 300, not on the road system, 180 air miles from Fairbanks Example III: Tanana 14 Tanana City Hall (source: Nicholas School of the Environment, Duke University)\nIn November 2007, the City Manager, Bear Ketzler, kicked off Tananas now burgeoning biomass program with the installation of two 425,000 Btu/hour cordwood-fired Garn boilers in its city washeteria. The cost of installation of that first project, which included a new boiler building, was under $100,000. It has displaced annually about 6000 gallons of diesel with 50 cords of wood. The first project has reduced oil consumption in the washeteria by about 50%. Example III: Tanana 15 Photos courtesy of State of AK Division of Community & Regional Affairs\nMore biomass! 2 Garn boilers installed to heat the city office. 2 Garn boilers installed in the senior center. 4 Econoburn boilers to heat the city shop and the teachers housing. 3 more Garns are slated to be installed into various city buildings within the next year. Ketzlers goals were: To reduce reliance on diesel fuel (which costs about $6.50 per gallon in Tanana To create a healthy local economy around biomass. In order to achieve this, he set the price of wood at $250 per cord (which has since been raised to $300 per cord), and informed the locals that the city would purchase wood from anyone who cared to harvest it, either by pulling driftwood from the river, or by harvesting it on legal lands. Example III: Tanana 16\nAnnual heating costs before installation of the biomass boiler: 12,000 gallons of fuel oil at $4.50/gallon Cost: $54,000 Annual heating costs after installation of the biomass boiler: 40 cords of wood at $250/cord: $10,000 6,000 gallons of fuel oil at $4.50/gallon: $27,000 Cost: $37,000 This savings has enabled: Building a new childrens playground. Improvement and weatherization of the school and teacher housing; weatherization of the school has resulted in saving 7000 gallons of oil annually; it is next to be retrofitted with wood heat. Money used to purchase firewood is now being circulated in the community rather than being sent Outside by oil purchase. Full-time maintenance person now employed by the City. Example III: Tanana Washeteria 18\nTanana project area woody biomass cost per ton The regions in the left column are economically feasible at oil cost of greater than $4/gallon.\n20 Thank you! Daisy Huang, email@example.com@alaska.edu Art Nash, firstname.lastname@example.org@alaska.edu\nBackground of AEA biomass program 21 Alaska Wood Energy Development Task Group (AWEDTG) a coalition of federal and state agencies and not-for-profit organizations… to explore opportunities to increase the utilization of wood for energy and biofuels production in Alaska. (AEA website) In 2000- 2008, and 2012, over 60 communities have applied for grants for community heating projects.\nInterview communities/applicants regarding their status and progress of AEAs pre-feasibility biomass program. Document the current status of and collect available fiscal, operational, and mechanical data from those 50+ communities who received a pre-feasibility study. Interview key community members to get an idea of what it takes to make a biomass program succeed. ACEP biomass data collection program objectives 22\nSchools or tribal halls to start; plan to extend to other community buildings, such as clinic, washeteria, etc. Former usage was diesel; diesel heater is left in place as backup. ~$5-7 per gallon ~10,000 – 50,000 gallons per year A typical community saves $80,000-$400,000 per year. Most buildings currently heated with wood-fired boilers are not using full capacity; there is excess heat; district heating loops may be installed. Typical situations 23\nBackground: How do these things work? GARN system: 24 Source: http://garn.com/how-garn-works-full/ Load up and fire away!\nBackground: How do these things work? Messersmith: 25 It loads itself!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:ec0a6d31-78f9-4aaa-9519-f89a352eb0dc>","<urn:uuid:fdd6d733-7ec7-4919-b3eb-6eec11dcb42f>"],"error":null}
{"question":"Why is setting optimal pump suction levels crucial for milk expression, and how does educational attainment influence access to EV charging stations?","answer":"Setting optimal pump suction levels is crucial because the strongest suction doesn't always yield more milk - it should be set to the highest comfortable level but never cause pain, as the body doesn't release milk well when in pain. The speed/cycles should be adjusted based on milk flow, starting fast to stimulate milk release and slowing down when milk flows. Regarding educational attainment's influence on EV charging access, there is a strong correlation - about 80% of Level 2 chargers are located in areas where more than 36% of residents have bachelor's degrees. These areas, representing only 51% of population and 40% of area, are 3.55 times more likely to contain a charger compared to areas with lower educational attainment.","context":["Read below to learn the three easy steps to get ready to pump.\n1. Prepare to Pump\nFirst, read your breast pump instructions. Always wash hands well with soap and water before handling the product. Assemble the kit. Attach the kit to the pump. Center the breast flanges over your nipples. Press them lightly against your breasts to make an air seal. If you are pumping with a double milk collection kit, see the photo of double pumping with one hand. By holding the flanges this way, you will have a hand free to start your pump. (If you want to single pump, just close off one of the adapter ports as instructed in the instructions for use.)\n2. Turn the pump on\nAmeda Purely Yours®/LactalineTM-Turn the suction dial to the right to turn on the pump\nAmeda EliteTM-Turn the vacuum dial to the right to turn on the pump\nAmeda Platinum®-Push the power button on\n3. Set the Dials for Comfort and Milk Flow\nSet suction/vacuum for comfort\nSet SUCTION/VACUUM to the highest setting that feels comfortable and no higher. The strongest pump suction does not always pump more milk. You can increase the suction as your milk starts to flow and you become used to the pump, but remember pumping should never hurt. Your body does not release milk well when you are in pain.\nSet speed/cycles for comfort and milk flow\nSet SPEED/CYCLES to the fastest setting when you start pumping to stimulate your breasts to release oxytocin, which causes a milk ejection reflex (let-down). This release causes milk to be squeezed out of the alveoli, into the ducts and out of your nipple into the flange.\nOnce the milk is flowing, slow down the SPEED/CYCLES to keep the milk flowing. When the milk flow slows to a trickle or drip, return to the fastest setting until you trigger another milk ejection reflex, then slow the SPEED/CYCLES down again. This fast/slow pattern can be repeated several times to help drain your breasts.\nKeep in mind that a baby sucks like this when she is breastfeeding-sucks fast to get the milk flowing, then slows down her suck to draw the milk out and drain the breast.\nStimulate the Milk Ejection Reflex (MER) When Pumping\nThe Milk Ejection Reflex is the process in which the hormone oxytocin causes glandular tissue or alveoli in the breast that stores milk to squeeze, causing the stored milk to low into the ducts that transport milk, and out of your nipples.\n- Some mothers feel tingling during milk ejection reflex, others feel nothing and just see the milk flow start.\n- While breastfeeding, most mothers have three or four milk ejection reflexes (MER) without even knowing it. This is why it is important to try to stimulate more than one MER when you are pumping. It will help you drain your breasts.\n- A milk ejection reflex can happen with a touch at the breast, hearing a baby cry, or even by thinking about your baby. Pain or feelings of stress, anger, and upset can block the milk ejection reflex. So, try to relax and use your mind and senses. One or two senses may work better than the others, so test them all to find out which work best for you.\n- Mind: Close your eyes, relax, and imagine your baby breastfeeding.\n- Sight: Look at your baby or at your baby’s photo.\n- Hearing: Listen to a recording of your baby cooing or crying. If you’re apart, call and check on your baby.\n- Smell: Smell your baby’s blanket or clothing.\n- Touch: Apply a warm cloth or gently massage your breasts.\n- Taste: Sip a favorite warm drink to relax you.\nIf you are pumping with a manual pump like the Ameda One-Hand Breast Pump, you can do the same by using both fast and slow squeezes. Again, watch your milk flow and use it as your guide. Instead of double pumping, you will be single pumping (one side at a time). Change which breast you are pumping every 5-7 minutes for a total of 20 to 30 minutes.\nThis is general information and does not replace the advice of your healthcare provider. If you have a problem you cannot solve quickly, seek help right away. Every baby is different. If in doubt, contact your physician or healthcare provider.","As electric vehicles (EVs) roll onto the roads in large volumes across the U.S, there has been a corresponding demand for more robust charging infrastructure. Despite the appealing environmental and economic benefits of EVs, the convenience of charging stations heavily influences adoption. And there begins the problem. While electric vehicle use is growing rapidly in well-to-do, predominantly White communities, minority neighborhoods have largely been left behind to date.\nThe U.S. government has emphasized the importance of equity when planning infrastructure investments in bills like the Build Back Better plan, and has incentivized a large portion of EV infrastructure funding in programs like NEVI and policies that seek to ensure EV charging infrastructure is deployed equitably.\nBut how well are these policies performing?\nTaking a granular approach to assess the current state of EV infrastructure\nIn an effort to understand the current state of equity in charger deployment, identify gaps that may exist, and add to the body of knowledge surrounding EV infrastructure deployment – we decided to leverage our vast data core, analytical expertise, and powerful software platform to conduct an analysis of Public EV Level 2 chargers in Columbus, Ohio.\nIn the sections below, we explore where access to EV chargers is most prevalent based on variables like population density, various socio-demographic statistics, and with different definitions of what constitutes a charging gap.\nEV Charging Infrastructure Basics\nFor EVs to achieve broad adoption and utilization, drivers need easy access to charging infrastructure. While many EV owners charge at home, people with longer commutes or irregular driving habits are unlikely to see themselves in EV ownership without excellent access to public charging stations.\nThere are three levels of charging equipment, determined by charging speed.\n- Level 1 (L1) – less than 2% of public EV chargers in the U.S. are L1.\n- Level 2 (L2) – the most common type of public EV chargers, accounting for more than 80% of public EV chargers in the U.S.\n- Level 3 (L3) – more than 15% of public EV chargers in the U.S. are L3.\nDue to the overwhelming preference for and majority of public charging stations being Level 2, we focused our analysis on L2 chargers.\nThe Landscape of EV Charging Infrastructure in Columbus\nAt first glance, you see that Columbus has Level 2 EV chargers spread across the city. A high concentration of chargers are located in the downtown area. So during our study, we took population density into consideration when drawing any conclusions regarding charger placement.\nRacial Factors and public EV Charger Locations\nCloser inspection of the distribution of public chargers in Columbus reveals disparities when comparing majority White areas to majority non-White areas. This is in line with other recent studies on public EV charging distribution. For example, Axios did an analysis of 35 U.S. cities and found that majority-White census tracts are 1.4 times as likely to have a charger.\nGiven UrbanFootprint’s unique ability to aggregate data across all census resolutions, we looked a level of granularity deeper, analyzing census block groups in Columbus using our Analyst application. It revealed that the EV charging locations in our study area are even more heavily skewed towards majority-White areas than what Axios had found in other cities around the country.\nIn Columbus, majority-White block groups are 2 times more likely to have a charger, and 2.3 times more likely to have at least three chargers.\nBut we knew there were likely other factors that may be more strongly correlated with the prevalence of charging stations than race and ethnicity.\nRelationship between EV Charger Presence and Educational Level\nOur study found an even stronger correlation between high education levels and the presence of EV chargers. It was immediately apparent from looking at the map that the median US educational attainment level (36% with at least a bachelor’s degree) was a tipping point for whether a block group would likely contain an L2 charger.\nTo be specific, around 80% of L2 chargers in our study were located in block groups with above the US median for bachelor’s degree attainment – and those block groups were 3.55 times as likely to contain a charger than those below the median.\nThe really shocking piece of this statistic is that these same block groups accounted for only 51% of population and only 40% of area.\nMedian Income and its Impact on EV Charger Distribution\nMedian income also appears to play a significant role in charger presence. Within our study area, block groups with chargers had median incomes 1.1 times higher than those without. Moreover, block groups with average incomes above the US median for household income ($68,703) were 2.24 times as likely to have a charger.\nThis finding supports the argument that historically, charger placement has favored higher income areas, and raises concerns for how access to EV infrastructure will lead to income-based disparities in future EV usage.\nEquitable Access to EV Chargers\nThe end goal of public EV charging infrastructure is to serve the public. That is, having enough chargers in a given block group to satisfy demand. We wanted to get an overall picture of who is being “served” versus “unserved” in Columbus. We chose 4 L2 chargers as a threshold for which to consider a block group “served.”\nWhen we looked at the data through this lens, all three ‘metrics’ (education level, median income, and racial composition) showed equity-related differences.\nConclusions and Recommendations\nOverall, our findings indicate that education level and median income are the most closely related metrics to the distribution of EV chargers in Columbus, Ohio. While racial factors are not quite as strong, there is still a trend for chargers to be more present in predominantly White block groups – in line with studies of other major cities across the country.\nAdditionally, the results were progressively more compelling as we peeled back all of the layers of data. For example, when limiting the analysis to the areas of the city with the highest population density, and increasing the threshold of what counts as a charging gap, the trends are magnified significantly.\nPolicymakers, local governments, utilities, and private companies should consider these findings when incentivizing, funding, planning, and placing future EV infrastructure. By focusing on ensuring equitable access to chargers, they can support the wider adoption of EVs. This includes prioritizing charger installation in diverse neighborhoods and areas with lower educational attainment and income levels.\nBy leveraging the right data at the intersections of climate, community, and the built environment, we can surface the actionable insights that will ensure a more equitable distribution of EV chargers – that will ultimately contribute significantly to higher EV adoption rates, pushing us closer towards a more resilient, sustainable, and inclusive future. Resilience Insights, when paired with our comprehensive Analyst application for data visualization, provide answers to many complex questions related to Infrastructure & Mobility for any location in the United States. Contact us if you want to learn more!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:1e995848-7cec-4385-9c2c-c73161f6491c>","<urn:uuid:5fbf5a0f-b48f-437c-a774-25770944f520>"],"error":null}
{"question":"Hello! I'm interested in ancient battle poetry - what did Simonides write about the Battle of Plataea?","answer":"In a lengthy elegiac poem about the Battle of Plataea, Simonides wrote at least 45 consecutive verses that began with an invocation of the Muse and an apostrophe to Achilles, drawing parallels between the Trojan War and the later Greek victories. The poem included a detailed narrative account starting with the departure from Sparta in company with the Dioscuri and Menelaus.","context":["M. L. West, Iambi et Elegi Graeci ante Alexandrum cantati vol. II, editio altera. Oxford: Oxford University Press, 1992. Pp. 277. $69.00. ISBN 0-19-814096-7.\nReviewed by Michael W. Haslam, UCLA.\nIEG was published twenty years ago, in two volumes, vol. I containing Archilochus, Hipponax and Theognis, vol. II the rest, i.e., according to the sub-title, \"Callinus Mimnermus Semonides Solon Tyrtaeus Minora Adespota.\" Here is a second edition of vol. II, \"aucta atque emendata.\" The sub-title remains the same, but in one respect is no longer an accurate representation of the contents, for what we find now occupying more pages than any other poet except Solon, and obviously deserving better than subsumption under Minora, are the elegiac remains of Simonides. A newly published papyrus -- in fact West's edition here actually comes in advance of its official first appearance in The Oxyrhynchus Papyri, thanks to the generosity of its editor Peter Parsons -- has contributed extensive fragments of Simonidean elegy. The importance of this last-minute E(/RMAION is such that it throws everything else that is new about this volume into the shade. I shall return to it below. But I should say something of the less dramatic ways in which this second edition differs from the first.\nAccessions to the corpus of pre-hellenistic iambic and elegiac verse over the last twenty years have been comparatively meager. Apart from the new Simonides, we have gained significantly more of only three poets: Archilochus, Tyrtaeus, and now Homer, or rather \"Homer,\" the composer of the Margites. Everyone knows the Cologne Archilochus epode, published in 1974. The new fragment of Tyrtaeus, published in 1980, was an interesting but hardly riveting piece, and less important than it would have been if we did not already have the Berlin fragments. These two texts appeared in time for inclusion in West's highly serviceable Delectus ex Iambis et Elegis Graecis (Oxford 1980), as Arch. 196a and Tyrt.23a. The burlesque curiosity that was the Margites, for its part, will probably always be one of those things that is richer in testimonia than in actual fragments, but the piece published in 1954 (fr.7 West) is now joined by two more (making no fewer than three manuscripts of the poem at Oxyrhynchus), and they appear here for the first time (frr.8-9 West, P.Oxy.3963-4). The attribution seems safe enough, given the metrical mix of hexameters and trimeters. I note, though, that in both bits the iambics heavily predominate over the hexameters -- not what one would have predicted either from Hephaestion's description of \"iambics scattered among the hexameters\" or from the earlier fragment. In content they are far from perspicuous -- only damaged line-ends survive -- but West discerns interesting sexual possibilities in them. 8.11 is an unsignalled oddity: either the line was quite extraordinarily short, or it was left blank. Fifth-foot anapests occur in the trimeters (9.15, 2, 8.8?), and differentiation of stylistic level between the hexameters and the iambics is less strong than might have been expected: the iambic 8.9 ends in (-)FA/SGANON. A 7th-cent. date of composition seems unlikely to me (\"vii vel vi\" West).\nThe first edition had a very pleasing choliambic preface (counterbalanced by the elegiac one in the Delectus), and the second follows suit, conveniently signalling the main differences between the two editions and incorporating a wickedly contrived hidden allusion to the rival Teubner Poetae Elegiaci (connoisseurs of such things will compare Livrea's translingual pun in the cynic elegy prefaced to his edition of Cercidas). Pride of place goes of course to the papyrological windfalls. Other differences from the first edition are of a minor order, though surprisingly numerous. As West quietly proclaims, \"omnia polivi.\" He has made corrections to the apparatus, added and improved testimonia, and updated references to modern editions, but in the poetic texts themselves I observe no changes of any greater moment than the restitution of E)PH\\N for E)PEI\\ at Mimn.12.4. Unsurprisingly, West was not swayed by conservative critics of the first edition. To those who object to such paleographically minimal changes as MA/Q' for PA/Q' (Tyrt. 11.16) or E)/XWN for E(LW/N (ib.34) or DH\\ for A)\\N (Sol. 36.12) belongs the burden of showing that the transmitted text is actually better, not merely that it can be made sense of. Echembrotus has lost his one remaining hexameter in favor of two lyric cola; West does not eject him, but one hopes that editors of early Greek lyric will be equally hospitable. A certain Philiadas gains a place as one of three appended \"poetae addendi,\" a couplet that had been deemed epigrammatic now being deemed elegiac.\nA few other small accessions have been gleaned from various sources, and incorporated within the body of the book. Readers are not warned of an undesirable side-effect: wherever such new material has been added, some spatially compensating deletion is liable to have been made, either on the same page or the facing one. Victims of this shearing include adesp.el.*2, adesp.ia.44, adesp.ia.60, Herm.7-8, 'Hom.'1, 2, Ion 32. The cuts have been made as judiciously as could be, and in each case the added material is worthier of inclusion than what it displaces, but it is clear that the excisions have been made solely in order to minimize resetting. If the result is a book that is less expensive than would otherwise have been the case (and it is not cheap!), complaint would be ungenerous, but it does mean that this second edition is not in quite every respect an improvement on the first. There are also mechanical accidents: e.g. apparatus to Tyrt.23.6ff. turns up on the wrong page, after 23a, and repagination has disrupted the apparatus to Ion 26.\nThe word-index of the first edition is reprinted unchanged, and a \"supplementum\" to it has been provided (largely reassigning to Simonides what had been adespota). So we shall have to consult them both. The new index is not without its anomalies: while it does not exclude conjectural supplements (beware of using it as a list of attested occurrences, therefore), it does not include everything that West has seen fit to print in the text (e.g. Sim.14.5-6). As crude a tool as it is, in some ways the index is thus a better guide to the degree of textual security than is the text itself. The index of papyri is selective, and should have been stated to be so.\nSo we come back to what far outshines everything else in the volume, the accession of Simonidean elegy. The new papyrus, P.Oxy.3965, consists of numerous fragments, none very sizable, and none giving a single complete line, but collectively of great interest and importance; they have received wonders of physical, textual and contextual reconstruction from Parsons and from West himself. The fragments come, it seems, from a book of the elegiac poems of Simonides, and they not only contribute text of their own but also serve to unite a variety of previously known pieces. Textual overlap with fragments of P.Oxy.2327, previously adespota (adesp.el.frr.28-60 in the first edition, now redistributed under Simonides), suggests that that papyrus was another manuscript of the same book; the arrangement of the corpus will have been Alexandrian. (Textual variants are recorded by collation in both MSS, along with occasional reference to Ap(ion) and to Ni(canor); the MSS are roughly contemporary, both 2nd cent.) Edgar Lobel, in publishing P.Oxy.2327, had mooted Simonidean authorship (\"It may be worth while to recall that Simonides is known to have written elegiac pieces about battles of the Persian wars\"), but had added \"But I doubt whether Simonides would be considered, if it were not for the fact ... that there are Simonides fragments in the same hand,\" and his scepticism prevailed. No-one will resist the ascription now. (Lobel himself announced the confirmation in 1981, at P.Turner 3, but subsequent editors went on printing the fragments as adespota.) If this is not how we would have guessed Simonidean elegy would look, all the more valuable to know this is how it does look. If the style is less distinctively Simonidean than in some of the lyric pieces, we can say it is because generic proprieties assert themselves. And if I romantically confess I would still rather have had a dirge, that is not to depreciate what we have now gained.\nThe new papyrus also overlaps with the well-known piece on Homer's OI(/H PER FU/LLWN GENEH/, quoted by Stobaeus. That has sometimes been ascribed to Semonides of Amorgos, but is now vindicated for Simonides; in the first edition West had printed it as a Simonidean \"dubium\" (fr.8), commenting with enviable sensibility \"ego si non manum, at aetatem Cei sentio\" (cf. his Studies in Greek Elegy and Iambus, 179f.) And it is now shown to be not one extract but two, discontinuous between the quoted lines 5 and 6 (the break coinciding with a shift in transmissional status in Stobaeus).\nWhat we seem now to have are the following: scanty remains of a poem on the battle of Artemisium (frr.2-4; Simonides apparently also wrote a lyric poem on the same battle, PMG 533); perhaps a scrap of a poem on the battle of Salamis (frr.6-7); extensive fragments on the battle of Plataea (frr.10-18); and substantial remains of \"convivalia\" (frr.19-33); plus the inevitable residue of scraps of uncertain context (frr.34-85).\nTwo pieces (or rather amalgams of pieces) of exceptional interest stand out. Fr.11, parts of 45 consecutive verses (according to the proffered reconstruction), evidently comes towards the beginning of a lengthy poem on Plataea. An invocation of the Muse (20ff.) is preceded by an evocation of the Trojan War, framed as an apostrophe of Achilles: analogy is implicit between the victorious Greeks who returned from Ilium (and those who did not), immortalized by Homer, and their Spartan successors. The transition from proemium to Muse-invocation replicates the old connection between hymn and epic song, here transmuted into elegy: A)LLA\\ SU\\ ME\\]N NU=N XAI=RE, QEA=S E)RIKU[DE/OS UI(E/ / KOU/RHS EI)N]ALI/OU *NHRE/OS: AU)TA\\R E)GW/ / KTL. (cf. e.g. the end of the Homeric Hymn to Apollo). The poem then swings into a detailed narrative account, starting with the departure from Sparta in company with the Dioscuri and Menelaus. West adduces points of contact with Herodotus' later account; historians take note. The other fragment that must be singled out is fr.22, which incorporates adesp.el.frr.29-30 of the first edition. As reconstituted, this is a romantically homo-erotic piece in which it seems the aged poet envisions rejuvenation (\"[?losing] white(?) wrinkles\") on an island (the Isle of the Blest?), reclining on flowers in the sensual company of the lovely Echecratides.\nIt looks as if we may distinguish two kinds of Simonidean elegy: fairly lengthy narrative poems on the Persian Wars, composed presumably for performance at public festival, and personal poems evidently belonging in sympotic context. (For studies of the types and performance contexts of elegy cf. West's Studies, 10-18, and E.L. Bowie in JHS 106, 1986, 13-35.) The fragmentary nature of the material imposes caveats: it has to be remembered e.g. that assignment of fragments to different poems depends on prior notions of what belongs in a single poem. And many questions remain. Was the Plataea elegy performed competitively (see Bowie), or was it commissioned? I would imagine the latter. And what was the occasion? The inaugural Eleutheria makes a tempting guess.\nSome of West's actions are bold, and would seem temerarious if they came from anyone else (e.g. the placing of Sim.11.1-4, or the alteration of indicative to optative at 22.12). Some of his readings stretch the evidence to or beyond the breaking point (e.g. PANDAMA/[TWR at 20.15, or PA]IDO\\S at 22.13), and an unkind critic would call them irresponsible. But the laser-like quality of West's instincts is not something I would care to contest. Not quite everything will be right -- the task is one of constructing coherence out of the incoherent, after all -- but everything is illuminating.\nIn Sim.8 the attestation of Homeric KUANOPRW(E)I/OUS will refer to Od.3.299. At Sim.22.8 West prints EU)AE/A NH=SON in the text (the contraction of -EA should have been marked), but reports the papyrus as having EUAG[.]A (the break comes between two fragments newly combined by Parsons [previously adesp.el.30.1 and 31.1]): I do not see why EU)AGE/A should not stand. (Preceded by E)S [TH\\N] ?) At Sol.5.1 the preference for the commonplace E)P- (Plut. codd.) over A)P- ( Ar.AthPol pap.) seems perverse. A daffy Bophocles-type typo mars the main new Simonides fragment (and at 14.7 read POTAMOU=), and there are other signs of things being done in rather a hurry. But E(SPE/RIOS KEI=NO/S GE TELEI= TA/ KEN H)=RI NOH/SH|. By now there will be another book or two."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:598787bb-3680-4e11-8ee0-571ec1d7478a>"],"error":null}
{"question":"How does the impact of cars on GDP in Latin American cities compare to the historical trend of car usage in Australian cities?","answer":"In Latin American cities, cars have significant negative economic impacts, with traffic congestion costing 3.4% of GDP in Buenos Aires and 2.6% in Mexico City. In contrast, in Australian cities, car usage saw a dramatic rise from just over 20% of urban travel after World War II to around 75% by 1970, reaching 85% by 1980, despite their high private and social costs. This shows a contrasting pattern where Latin American cities suffer economic losses from car usage, while Australian cities historically embraced cars as the dominant mode of transport despite their costs.","context":["Cities can drive better economic growth while also reducing climate risk. They are at the forefront of the fight to protect the climate and eradicate poverty, and are often trailblazers in a world in which nation states typically move more slowly to address global challenges. These are the crucial conclusions from a series of studies released this week by the Global Commission on the Economy and Climate, which draws from examples worldwide, including cities as diverse as Lima, Peru and Leeds, UK.\nBusiness-as-usual urbanization is creating huge economic and social costs. Traffic congestion, for example, costs 3.4% of GDP in Buenos Aires and 2.6% in Mexico City. In Beijing, the social costs of motorized transport – including air pollution and congestion – are as high as 15% of GDP. Urban sprawl in the United States adds some US$ 400 billion in extra costs each year as a result of higher infrastructure, public service delivery, and transport costs.\nGraham Floater of LSE Cities highlights the scale of the opportunity and the risks of inaction, “over the next two decades, cities will grow by over a billion people and generate two thirds of global economic growth. If this rapid urban growth is managed badly, we face a world of sprawling, inefficient, polluted cities – and a major climate change risk.”\nGiven the long-lived nature of urban infrastructure, the way in which we build, rebuild, maintain and enhance the world’s growing cities will not only determine their economic performance and their citizens’ quality of life; it will also define the trajectory of global greenhouse gas (GHG) emissions for much of the rest of the century.\nCities lead the way on low-carbon economic growth\nSo, how can cities realize opportunities for better economic growth that creates a safer climate? Connected, compact, and coordinated development is the central model to low-carbon economic growth. Connected infrastructure requires investment in innovative urban infrastructure and technology, with a focus on smarter transport systems. Compact urban growth means managed expansion that encourages higher-density living and walkable, local urban environments. While coordinated governance needs effective and accountable institutions to support better planning and implementation – which is particularly important for transport.\nPete Erikson – an author of one of the studies – suggests reasons to be optimistic about city leadership. “Cities are playing an increasingly important role in climate change mitigation, showing leadership and creativity and demonstrating that there is plenty of overlap between good economic development and emissions reduction.”\nIndeed, cities around the world are leading the way. In Curitiba, Brazil – where per capita emissions are about 25% lower than Brazil’s urban average – bus rapid transit (BRT), in conjunction with improved cycling and pedestrian infrastructure and land-use policies has reduced gasoline consumption and income spent on transport. While in Europe, Stockholm reduced emissions by 35% from 1993 to 2010, but grew its economy by 41%. Car ownership in London decreased 6% from 1995 to 2011, while the city’s economy grew by 40%. In Asia, Beijing is helping reverse China’s trend of sprawling cities, with density in the city’s core increasing by 50% over the past decade.\nAnd there are striking opportunities to take things further and build better, smarter cities that people want to live in. Adopting low-carbon technologies – such as new building technologies and electric buses – across 30 megacities like Mumbai or Beijing could create more than two million jobs, while avoiding three billion tons of cumulative GHG emissions. These technologies also have the potential to reduce local air pollution, meaning healthier, happier citizens.\nLima – which next week hosts global climate negotiations – currently has over seven million inhabitants and is one of the fastest growing cities in Latin America. However, this growth hasn’t always been well-managed and visitors to the city next week will see that its transport systems are far from perfect, though they are improving. Lima’s investments in BRT and the metro network, combined with new emissions standards are expected to reduce emissions from the transport sector by 15% by 2025 compared to business as usual.\nCities face a critical juncture in their growth\nWithout further action in cities worldwide, there is a real risk of substantial increases in energy bills – which will be bad for the poor – and more GHG emissions – which will be bad for the climate. However, connected, compact, and coordinated governments pave the way for low-carbon growth. New work by the Commission estimates that more connected, compact urban development could reduce global urban infrastructure requirements by more than US$ 3 trillion between 2015 and 2030.\nCommenting on the reports, Lima’s Mayor Susana Villarán said: “The challenge of supporting economic growth and tackling climate change will be met in the world’s cities. Investing in public transport is good for citizens, good for business and good for the climate. Clear leadership is now needed.”","How has city travel changed over the last century?\nThe long view of travel in Australian cities shows the rapid decline of public transport and the remarkable ascendancy of cars. The latter still overwhelmingly dominates urban travel\nI haven’t seen much detailed and reasonably reliable data before showing how transport within Australia’s major cities evolved over long periods of time.\nSo the first exhibit, which shows historical changes in the percentage shares of the three main modes of travel in Australian cities between 1900 and 2010, is enormously interesting.\nIt’s from a paper by David Cosgrove of the Bureau of Infrastructure, Transport and Regional Economics (BITRE). It’s titled Long-term patterns of Australian public transport use.\nThe data offers some interesting insights. Most notable is the spectacular fall in the share of travel by public transport and walking in Australian cities from after WW2 (travel is passenger kms).\nThe growth in private transport (cars, vans, motorcycles, horses) over the same period is even more spectacular. It increased from zero in 1910 (excluding horses) to well over 80% at present.\nProbably less well known is that the decline in public transport’s share began in earnest from around 1925. It was slowed, although not turned around, by the Great Depression of the 1930s.\nThe introduction of petrol rationing during WW2 reversed the fall temporarily. Yet the relentless decline resumed immediately after 1945.\nAt present, 10% of passenger travel in Australia’s major cities is by public transport, down from the peak of over 60% in the early 1920s. It’s mode share is considerably higher for work journeys, but since they account for less than a fifth of all travel in Australia’s cities, it’s not enough to boost the overall figure beyond the first decile.\nWhile there’s been an increase in public transport’s share in recent years in some cities, that wasn’t at the tail-end of a continuing decline. As the exhibit shows, public transport has consistently maintained a one tenth share of urban travel for the last 30 years.\nPatronage has in fact grown strongly in absolute terms since 1980, but until recently the underlying driver was population growth, not a shift in mode toward transit. The State of Australian Cities 2012 report notes that on average Australians living in major cities made 108 trips per capita by public transport in 1980 and 105 in 2012 i.e. much the same.\nThe trends in mode share show what an extraordinarily attractive proposition private vehicles evidently were for urban Australians, even well before construction of the first freeways started. Their share of all urban travel jumped from just over 20% at the end of WW2 to around 75% by 1970.\nIt’s even more remarkable considering their high private and social costs. Private vehicles were very expensive to purchase and operate relative to incomes, they required huge public investment in specialised infrastructure (e.g. traffic lights), and they produced multiple negative externalities (e.g. noise, pollution, pedestrian injuries and fatalities).\nPrivate transport’s mode share reached 85% by 1980 but, like public transport, it has been relatively flat for the last 30 years (with a small decline in recent years). Up to then it grew as people consumed more travel (mainly longer trips), but subsequent growth was in line with the increase in population.\nThe second exhibit breaks the data down into more specific modes (note that the light rail curve is actually dashed). There are some interesting historical insights here, too.\nWalking was the majority mode in 1900 when cities had a substantially smaller footprint. Horses were expensive so they were only ever a minority mode and by the end of the 1920s their share was insignificant.\nOther than for an increase during the Depression, walking’s share fell consistently over the period. Surprisingly, WW2 provided only a modest boost for walking compared to its impact on private vehicles and public transport.\nTrams and trains were at their peak in the 1920s, but other than for the war period, continuously lost share until the 1980s.\nTrams in particular were devastated by the closure of entire metropolitan systems from the 1960s onwards (except in Melbourne) and their replacement with buses. It’s ironic the O’Farrell Government has just committed to replacing buses in George St, Sydney, with light rail.\nMost Australian cities have experienced significant increases in public transport patronage over the last five years or so. However when looked at in terms of mode share, it’s clear cars still overwhelmingly dominate travel.\nThe direction of change is of course important, but much of the low-hanging fruit in urban transit has already been picked (exemplified by current crowding levels).\nConverting that up-tick into an accelerating and self-sustaining trend will require most of the following: a massive investment in improving and expanding infrastructure; major improvements in the way transit’s managed; a profound rearrangement of land uses (especially employment); and a significant increase in the relative cost of travel by car.\nNote: Keep in mind that harmonising data from different sources and eras is bound to involve compromises in accuracy, especially in earlier periods."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:59359b9c-c564-426b-bfb2-ad6bc2d6b29c>","<urn:uuid:a454b6b7-7b56-42d7-82cc-b139020b9b00>"],"error":null}
{"question":"As someone studying recent changes in federal environmental policy, how did environmental enforcement practices differ during the Trump administration compared to what Biden is proposing?","answer":"During the Trump administration, the Justice Department and EPA focused on 'compliance assistance' - prioritizing education and outreach to companies instead of violation notices and penalties. While there were some increases in civil penalties and criminal fines collected, there were declines in the number of civil enforcement cases, inspections, and criminal prosecutions. In contrast, Biden's order calls for ramping up enforcement actions, particularly those targeting violations that disproportionately impact underserved communities, with an emphasis on holding polluters accountable.","context":["The Justice Department must consider establishing an environmental justice office under a sweeping executive order President Joe Biden issued Wednesday.\nThe order aims to carry out Biden’s pledge to take a “whole of government” approach to addressing climate change and the disparate effects of pollution on disadvantaged communities—calling on agencies across the executive branch to devote resources to the cause.\nIt directs the Justice Department to consider adding an office “to coordinate environmental justice activities among Department of Justice components and United States Attorneys’ Offices nationwide.” It also directs the department to consider changing the name of its existing Environment and Natural Resources Division to the Environmental Justice and Natural Resources Division.\nBiden’s order further calls on the division to coordinate with the Environmental Protection Agency and others “to develop a comprehensive environmental justice enforcement strategy, which shall seek to provide timely remedies for systemic environmental violations and contaminations, and injury to natural resources.”\nThe language of the order differs from how it was described in a press briefing Wednesday, when White House climate adviser Gina McCarthy said the order calls on the Justice Department to create an office of “climate justice.”\n“We know the communities who are being hurt, and we know we have to start enforcing the standards today in ensuring that they are part of the solution,” she said.\nA DOJ spokesperson said the department “will implement the Executive Order and will share additional specifics on the office as we get further into the implementation process.”\nDOJ’s Environment and Natural Resources Division has long overseen a broad portfolio of enforcement cases and defense of agency actions. A name change to emphasize environmental justice, if adopted, would be a symbolic but powerful move, said Baker Botts LLP attorney Nadira Clarke, an ENRD lawyer from 1992 to 1997.\n“I wouldn’t want to overstate it, but it does feel significant to me,” she said. “It’s been an upward climb to make it a more representative division in terms of the lawyers in it and the cases that have been brought.”\n“There is a way in which having the name change would symbolize not only what has been achieved to date but also what should be perhaps the focus going forward, or the shift that is expected,” Clarke added.\nENRD has gone through names changes before, said Vinson & Elkins LLP attorney Corinne Snow, who served in ENRD during the Trump administration. It was once called the Lands Division, a nod to its early focus on public lands litigation, and has had multiple names since then.\nThe executive order also envisions a new office devoted to environmental justices issues, but it wasn’t immediately clear where such an office would fit within DOJ’s organizational structure.\n“In order for it to be really effective, I think that would be a hard thing to do if it’s just a component section of ENRD,” said Sidley Austin LLP attorney David Buente, a former DOJ lawyer from 1979 to 1990.\nHe explained that an office within the existing division wouldn’t have enough authority to handle issues that arise in other divisions and U.S. attorneys offices scattered around the country.\nInstead, a new environmental justice office could be a stand-alone component that’s part of the office of the attorney general, he said.\nBut an environmental justice office outside of ENRD could create “a bit of tension and power struggle,” said former DOJ environment lawyer Amanda Shafer Berman, now a partner at Crowell & Moring LLP.\n“It would be tough to figure out how an Office of Environmental Justice housed outside ENRD could work efficiently and cooperatively with the ENRD attorneys who are handling most of the environmental cases that the Department handles on a day-to-day basis,” Berman said in an email.\nSeveral former DOJ lawyers pointed to the existing Office of Tribal Justice, which coordinates on tribal issues across the department, as a good model for an environmental justice office.\nThe Office of Tribal Justice reports directly to top DOJ officials and serves as a point of contact for tribes navigating the department—a setup that would translate well for environmental justice issues, said Lois Schiffer, who led ENRD during the Clinton administration.\n“It works well in assuring that the large number of offices in the department that may have a role in environmental justice can be recognized and work in the most effective way,” she said.\nIn addition to ENRD, the Civil Rights Division, Civil Division, Office of Justice Programs, and several other DOJ components work on issues that could have an environmental justice angle, she said.\nBut creating a new office could be a heavy lift, said Baker Botts LLP attorney Jeffrey H. Wood, who was acting head of ENRD during the first half of the Trump administration.\n“It’s easier to open a new office than it is to staff it and fully resource it,” he said.\nSome lawyers in private practice have adopted a wait-and-see approach on the value of a new environmental justice office.\n“At the end of the day, they could probably accomplish more in the short term if they just focused on the working groups and prioritizing these issues as opposed to creating a new office,” Vinson & Elkins’s Snow said.\nHolland & Hart LLP lawyer Tom Sansonetti, who led ENRD during the George W. Bush administration, said environmental and climate justice issues fit better within the portfolios of policymaking agencies, including the EPA and the Interior Department, rather than DOJ.\n“I think it’s a little more problematic to place the responsibility for environmental justice and climate change in the hands of the litigators,” he said.\nThe Justice Department and the EPA faced criticism for their approach to environmental enforcement during the Trump administration. The agencies adopted a “compliance assistance” focus, prioritizing education and outreach to companies instead of violation notices and penalties.\nRecent data show some increases in civil penalties and criminal fines collected during the Trump administration, but they also show declines in the number of civil enforcement cases, inspections, and criminal prosecutions.\nWednesday’s order calls on the EPA to ramp up enforcement actions that target violations with a disproportionate impact on underserved communities.\n“We must hold polluters accountable for their actions,” the order said. “We must deliver environmental justice in communities all across America.”\nThe Department of Health and Human Services, which is also named in Biden’s climate order, has worked before on environmental justice issues. The agency participated in the Obama administration’s Federal Interagency Working Group on Environmental Justice, issued its first environmental justice strategy in 1995, and released progress reports every year from 2012 to 2017.\n—With assistance from Shira Stein."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:9cf5e99d-c35e-4271-a669-5f36699679ab>"],"error":null}
{"question":"What is the significance of aseptic processing in food preservation, and how does it compare to the basic safety requirements for homemade cider production?","answer":"Aseptic processing is a method that kills almost all microorganisms and requires the product to be packed in a sterile package in a sterile environment, resulting in a microbiologically stable product. In comparison, safe homemade cider production relies on different but specific safety measures, including proper sanitation of equipment (washing with warm soapy water, rinsing with clean water, and sanitizing with a bleach solution of one-tablespoon bleach per gallon of water), maintaining cold storage temperatures at 40°F or lower, and using only clean, unspoiled apples. The cider's safety depends on its acidity, refrigeration, and sanitation during processing.","context":["Sugar pack - four parts fruit for one part sugar or equivalent sweetener = 20% sugar\n\"Log reduction\" is a mathematical term used to show the relative number of live microbes eliminated from a surface by a pasteurization process. 5 log reduction means the number of microbes is 100,000 times smaller.\nA low-acid food to which acid(s) or acid food(s) are added and which has a finished equilibrium pH of 4.6 or below and a water activity (aw) greater than 0.85.\nAntioxidants are vitamins, minerals, and other phytonutrients that absorb free radicals and help protect cellular damage caused by free radicals, thus boosting your immune system.\nMethod of processing which kills almost all microorganisms. Sterile product is packed in a sterile package in a sterile environment. Aseptic product is microbiologically stable. Freezing is not recommended for aseptic products, as it may break the aseptic seal. Chilled storage may be used to protect color and flavor sensitive products.\nA product packed in poly bag in a cardboard box, designed with the food service industry in mind.\nBill of Lading is the official document prepared by the carrier duly accepting the goods for shipment containing information like item, quantity, value, vessel details, date, port, consigner , consignee etc. Bill of lading is a contract to carry the goods to the said destination based on which seller can claim consideration and buyer can take delivery of the goods.\nA method for measuring the viscosity or consistency of various suspensions of juices, purees, preserves, pastes, and other highly viscous products.\nThe measurement by refractive index of the soluble sugar content in the fruit or vegetable, puree or juice\nA viscometer instrument used for measuring the viscosity of purees and other liquids of a thick nature.\nPectin, protein, starch and insoluble items are removed during the production process to clarify the juice prior to concentration.\nA Cloudy appearance arises as a result of evenly-distributed small pulp, pectin and starch suspensions in the juice concentrate.\nA document that reports and certifies the lab test results of a product.\nFruit variety developed through horticulture means; not a wild variety.\nItem that has been specifically processed to reduce color so that juice is almost water white and highly transparent.\nA physical process which uses ion exchange resins to remove organic acids, minerals and proteins from aqueous solutions.\nThe thickness of the composition of an item and is measured by the specific gravity at a specified brix level and temperature.\nThe edible cell structure, seeds, and skins of the various fruits and vegetables.\nThe high-boiler esters, which are water soluble and clear liquids that are derived from the fruit components after the juicing and/or pureeing processes. These subtly complete the flavor profile of a specified item.\nA water/ alcohol based liquid obtained from the fruit during concentration process, which contains volatile flavor and aroma of the fruit or vegetable. Perfect for enhancing flavor and aroma of beverages and diary products containing fruit ingredients .\nA concentrated juice or puree where the essence recovered in evaporation process has been added back to the concentrate for enhanced flavor.\nA concentrated juice or puree where the essence removed in evaporation process is \"not returned\" to the concentrate.\nThe process of changing a liquid to a vapor. Evaporation removes water from a juice, usually by thermal vacuum process. Concentration is done for ease-of-use and economical storage and transport.\nFruit or vegetable originating in a foreign country, typically from the tropics, and having unusual flavor and health benefits.\nAn extract is a substance made by extracting ( removing) a part of a raw material, often by using a solvent such as ethanol or water. Extracts may be sold as tinctures or in powder form\nFOB, Free On Board, is a transportation term that indicates that the price for goods includes delivery at the seller's expense to a specified point and no further. The FOB term is used with an identified physical location to determine 1) the responsibility and basis for payment of freight charges, and 2) the point at which title for the shipment passes from seller to buyer.\nA fruit syrup is another name applied to deioinized juice concentrates which have essential fruit nutrients, including acids, removed using ion exchange resins.\nFormulated food products that are developed with pre-determined and specified health benefits when consumed.\nMaterial that is not liquid soluble. It will not mix or go into solution.\nA process in which the fruit or vegetable is frozen separately preventing a block frozen effect. This allows the item to keep its shape, color and flavor.\nThe product resulting from the removal of water from a juice. Concentration is done for ease-of-use and economical storage and transport. Concentrates can also be made by freeze concentration or reverse osmosis.\nThis is fresh juice extracted or pressed from the fruit or vegetable, and it has not been concentrated. The lack of heat in processing helps to maintain the fresher juice flavor and better nutrition. Primarily used for beverages, and some dairy products.\nNutraceutical, a term combining the words \"nutrition\" and \"pharmaceutical\", is a food or food product that provides health and medical benefits.\nORAC is a method of measuring antioxidant capacity in fruits and vegetables. The higher the measurement, the better the ORAC value.\nOrganic food is normally grown without using chemical fertilizers, fungicides and pesticides. Different types of organic certifications exist.\nA treatment process that subjects a product to high pressure or temperature, thus reducing harmful microorganisms by a 5 log reduction.\nA document signed by the consignee or recipient, confirming delivery of a shipment.\nWater-soluble plant pigments that are also known as bioflavonoids, which encompass more than 4,000 chemically unique flavonoids that can be categorized according to their chemical structure, and are known to have high antioxidant value.\nSubstances that have no technical or functional effect in a finished food but may be present in that food by having been used as ingredients of another food in which they had a technical effect.\nPuree concentrate is processed from s/s puree through a thermal vacuum concentrating process, or through other means like freeze concentration or reverse osmosis. Puree concentrates provide more economical storage benefits and ease of use for product applications requiring reduced water content (activity).\nSingle strength juice is either NFC juice, as described above, or juice reconstituted from a concentrate with the addition of water to reach the defined natural single strength brix level for that specific item.\nS/S puree is processed through the grinding and maceration of fruit, followed by pressing through a porous screen to remove fibrous tissue including the peel and rinds. Seeds can be removed or retained by altering the screen size. The fruit or vegetable retains it's original non-concentrated brix of the fresh and/or frozen item.\nProduct or ingredient that can be easily dissolved in aqueous solution.\nStandards of identity define a given food product, its name, and the ingredients that must be used, or may be used, in the manufacture of the food. Standards of quality are minimum standards only.\nA food which is represented or purports to be a food for which a standard of identity has been promulgated must comply with the specifications of the standard in every respect.\nUnsweetened - No sugar or other ingredients added\nA term applied to fruits which combines exceptional nutrient richness and antioxidant qualities with an appealing or unique taste.\nTropical fruits may grow on plants of all habitats, but are usually associated with the tropics. The only characteristic that they share is an intolerance of frost and cold temperatures.\nTurbidity is a measure of the degree to which the water loses its transparency due to the presence of suspended particulates.\nA measure of the resistance of a fluid to flow. Viscosity indicates the thickness of a fluid; the less viscous the fluid, the greater its flow potential.","Bulletin #4191, Food Safety Facts: Safe Home Made Cider\nFood Safety Facts\nSafe Home Made Cider\nPrepared by Jim Schupp, Tree Fruit Specialist, University of Maine Cooperative Extension.\nRevised by Jason Bolton, Assistant Extension Professor for Food Safety, University of Maine Cooperative Extension.\nReviewed by Beth Calder, Extension Food Science Specialist, University of Maine Cooperative Extension, and Alfred Bushway, Department of Food Science and Human Nutrition, University of Maine.\nFor information about UMaine Extension programs and resources, visit extension.umaine.edu.\nFind more of our publications and books at extensionpubs.umext.maine.edu.\nOutbreaks of foodborne illness have been attributed to the consumption of fresh, unpasteurized cider contaminated with a foodborne pathogen like E. coli O157:H7 The risk is low but it is still probable. Certain age groups are at a greater risk of complications from harmful bacteria like E. coli O157:H7 especially children, the elderly and persons with compromised immune systems. You can prevent these risks by boiling unpasteurized apple cider before drinking it, or drinking pasteurized cider or juice.\nIn addition to bacterial concerns, there is a fungal toxin called Patulin that can form during apple cider production and storage. Patulin forms when there is mold growth on or in the apples. This toxin is heat stable and can survive pasteurization.\nThe safety of cider relies on its acidity and refrigeration, as well as sanitation during processing and manufacturing. Producers who make cider for sale must be licensed by the Maine Department of Agriculture and are inspected regularly to make sure that safe and sanitary practices are being followed. According to Maine State Law, cider producer may not sell, advertise, offer or expose for sale any cider that has not been heat-treated to a temperature of 155 degrees F or higher for 10 seconds unless it is labeled as being unpasteurized and is labeled as unpasteurized. In the case of hand-pressed or homemade cider, it is your responsibility to assure the safety of the cider.\nThe following list of guidelines has been prepared by Maine’s apple growers, the Maine Department of Agriculture, Food and Rural Resources and the University of Maine Cooperative Extension to help you make safe, healthful delicious cider. If you have questions about cider production or other food safety issues please contact Jason Bolton at the UMaine Extension: email@example.com or 207-942-7396.\nGrowing and Harvesting the Apples:\n- Pick apples at the proper stage of maturity, before they drop.\n- Pick apples when they are dry.\n- Allow animals to feed in the orchard.\n- Use animal waste as fertilizer.\n- Allow apples to begin to spoil on the ground.\n- Use apple drops to produce cider.\nStoring the Apples:\n- Use only clean, dry containers.\n- Store the apples at a cold temperature, if possible below 40° F.\n- Let the apples be contaminated by rodents, birds or insects.\n- Store the apples out in the open or directly on the ground.\n- Store wet apples.\n- Wash the apples with clean water to remove debris just before grinding them.\n- Keep press cloths and racks off the floor in a clean place between batches.\n- Make sure equipment and cloths are clean and sanitized before using them, including the apples press, which should be scrubbed with a warm soapy water, rinsed with clean potable water and sanitized.\n- Blend in some tart apples to increase the cider’s acidity.\n- Use food-grade plastic or stainless steel containers to catch the cider.\n- Heat the cider to 155° F for 10 seconds.\n- Store cider at 40° F or lower if not bottling immediately.\n- Bottle in clean and sanitized containers as soon as it is pressed.\n- Use spoiled, moldy or defective apples.\n- Expose the juice to air and insects.\n- Allow spoiled or partially spoiled apples to enter the grinder.\n- Leave cider at room temperature for longer than two hours.\nUsing and Storing the Cider:\n- Use the cider promptly.\n- Cool the cider to 40° F or lower as quickly as possible after bottling.\n- Reuse food containers that cannot be thoroughly cleaned.\n- Use containers with porous surfaces, such as crockery, to store cider.\n- Forget to clean and sanitize equipment before putting it away.\n- Sanitize: Removal of microorganisms both spoilage and pathogenic\n- Clean: Removal of dirt and debris\nCleaning and Sanitizing\nAfter each day’s cider making, all equipment, including press cloths, should be:\n- Rinsed with potable water to remove apple and cider residue.\n- Wash with warm soapy water and a clean brush. This is the cleaning step that will remove the dirt and debris.\n- Sanitize with bleach diluted with clean, potable lukewarm water in a ratio of one-tablespoon bleach to one-gallon clean water. All utensils and equipment that comes in contact with your cider should be rinsed with this sanitizing solution.\n- Air-dried in a well-ventilated, clean area away from flies.\nInformation in this publication is provided purely for educational purposes. No responsibility is assumed for any problems associated with the use of products or services mentioned. No endorsement of products or companies is intended, nor is criticism of unnamed products or companies implied.\n© 2008, 2011\nPublished and distributed in furtherance of Acts of Congress of May 8 and June 30, 1914, by the University of Maine Cooperative Extension, the Land Grant University of the state of Maine and the U.S. Department of Agriculture cooperating. Cooperative Extension and other agencies of the U.S.D.A. provide equal opportunities in programs and employment.\nCall 800-287-0274 or TDD 800-287-8957 (in Maine), or 207-581-3188, for information on publications and program offerings from University of Maine Cooperative Extension, or visit extension.umaine.edu."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:10bce981-6366-407d-bee0-e02a47ada394>","<urn:uuid:f577822c-3d5d-420e-bdb5-d238acb9ef05>"],"error":null}
{"question":"How human resource data management work, and what privacy concerns exist?","answer":"Human resource departments collect extensive workforce data including work performance, completion times, and employee information, using HCM software systems to analyze and manage this data efficiently through cloud-based technology. However, this data collection raises privacy concerns as computational systems can infer sensitive personal information about individuals from their digital footprints, including personality traits, political views, and personal characteristics, often without their knowledge. This creates potential issues around data protection and the need for transparency in how this information is used and processed.","context":["Human capital management (HCM) refers to all the kinds of practices engaged in managing, recruiting, and developing human resources for an enterprise. This term encourages one to think of the human labour of a company as assets. This kind of thought process, in turn, allows for more financial and managerial investment in them. But the question which frequently pops up this field is- \\”Why is there even a need to invest in human capital management?\\”\nSo here\\’s a list of the fields where human capital management works its magic:\nTackling changing demographic dynamics: A human workforce is bound to get old. It means a new and young workforce will inevitably have to bring. These new workers will bring in different styles of working, which will be reflective of their age and the times they live. But this might not always align with the needs of the organization. So it will have to be adjusted for optimum company-workforce happiness.\nFreelancing economy: In the list of a company\\’s human resources, other temporary workers like freelancers and contractors also come in. Since these workers are not on the permanent payroll of a company, so they complicate various worker issues related to taxes, contract schedules, employment regulations, and more. It is where human capital management comes in to regulate it all.\nRemote workforce: Changing times and rising costs force existing businesses to adapt to new ways of doing business. One of the means of doing so is by modifying the remote-working model. In this setup, full-time employees carry out organizational work but from a remote place. But before that, human resource management has to come up with creative ways, ranging from traditional workshops to using VR to train them in what the organization wants.\nBig HR data: Every organization collects vast amounts of data on its workforce. It includes the work done, how it is done, the time is taken to do it, and more. Human capital management comes with the tools and methodology which allow a human resource to analyze and manage the data in an efficient manner properly.\nWhat is HCM software?\nWhat many might not realize is that human capital management makes use of software systems for a variety of purposes. It ranges from training employees to gathering comprehensive data related to human resources and more. This technology helps in streamlining various operations like payroll, timekeeping processes, etc. These software systems are brought in to replace the on-site HR programs. All this enables people to get better investment returns. Most of these HCM solutions use cloud-based technology to operate. But that\\’s not the only benefit which HCM software has:\n- HCM technologies help in recruiting, managing, compensating, as well as developing and regulating performance management.\n- It helps in workforce management by coordinating the dispersal of employees with specific skill sets to those particular areas which need it.\n- HCM technologies also help with attendance, labor scheduling, budgeting, time management, and more.\n- HCM software systems also assist in cross-platform communication like in the case of social media collaboration, analytics, and more. Human resource personnel also get mobile access to some HR applications and data.\nPeople frequently fail to realize that the success of a business is closely intertwining with human capital management. In today\\’s continuously changing business world, human resource managers need to act more proactively rather than retroactively. Overall, human capital management is a very underrated area of work. This field, if effectively put to work, has the potential to make the entire organization operate smoothly, only because it specializes ins the most moving and volatile part of it-workers.","When you browse online for a new pair of shoes, pick a movie to stream on Netflix or apply for a car loan, an algorithm likely has its word to say on the outcome.\nThe complex mathematical formulas are playing a growing role in all walks of life: from detecting skin cancers to suggesting new Facebook friends, deciding who gets a job, how police resources are deployed, who gets insurance at what cost, or who is on a “no fly” list.\nAlgorithms are being used—experimentally—to write news articles from raw data, while Donald Trump’s presidential campaign was helped by behavioral marketers who used an algorithm to locate the highest concentrations of “persuadable voters.” But while such automated tools can inject a measure of objectivity into erstwhile subjective decisions, fears are rising over the lack of transparency algorithms can entail, with pressure growing to apply standards of ethics or “accountability.”\nData scientist Cathy O’Neil cautions about “blindly trusting” formulas to determine a fair outcome. “Algorithms are not inherently fair, because the person who builds the model defines success,” she said.\nO’Neil argues that while some algorithms may be helpful, others can be nefarious. In her 2016 book, Weapons of Math Destruction, she cites some troubling examples in the United States:\nPublic schools in Washington, D.C. in 2010 fired more than 200 teachers—including several well-respected instructors—based on scores in an algorithmic formula which evaluated performance.\nA man diagnosed with bipolar disorder was rejected for employment at seven major retailers after a third-party “personality” test deemed him a high risk based on its algorithmic classification.\nMany jurisdictions are using “predictive policing” to shift resources to likely “hot spots.” O’Neill says that depending on how data is fed into the system, this could lead to discovery of more minor crimes and a “feedback loop” which stigmatizes poor communities.\nSome courts rely on computer-ranked formulas to determine jail sentences and parole, which may discriminate against minorities by taking into account “risk” factors such as their neighborhoods and friend or family links to crime.\nIn the world of finance, brokers “scrape” data from online and other sources in new ways to make decisions on credit or insurance. This too often amplifies prejudice against the disadvantaged, O’Neil argues.\nHer findings were echoed in a White House report last year warning that algorithmic systems “are not infallible—they rely on the imperfect inputs, logic, probability, and people who design them.”\nThe report noted that data systems can ideally help weed out human bias but warned against algorithms “systematically disadvantaging certain groups.”\nZeynep Tufekci, a University of North Carolina professor who studies technology and society, said automated decisions are often based on data collected about people, sometimes without their knowledge. “These computational systems can infer all sorts of things about you from your digital crumbs,” Tufekci said in a recent TED lecture. “They can infer your sexual orientation, your personality traits, your political leanings. They have predictive power with high levels of accuracy.”\nSuch insights may be useful in certain contexts—such as helping medical professionals diagnose postpartum depression—but unfair in others, she said. Part of the problem, she said, stems from asking computers to answer questions that have no single right answer. “They are subjective, open-ended and value-laden questions, asking who should the company hire, which update from which friend should you be shown, which convict is more likely to reoffend.”\nFrank Pasquale, a University of Maryland law professor and author of The Black Box Society: The Secret Algorithms That Control Money and Information, shares the same concerns. He suggests one way to remedy unfair effects may be to enforce existing laws on consumer protection or deceptive practices.\nPasquale points at the European Union’s data protection law, set from next year to create a “right of explanation” when consumers are impacted by an algorithmic decision, as a model that could be expanded. This would “either force transparency or it will stop algorithms from being used in certain contexts,” he said.\nAlethea Lange, a policy analyst at the Center for Democracy and Technology, said the E.U. plan “sounds good” but “is really burdensome” and risked proving unworkable in practice. She believes education and discussion may be more important than enforcement in developing fairer algorithms.\nLange said her organization worked with Facebook, for example, to modify a much-criticized formula that allowed advertisers to use “ethnic affinity” in their targeting.\nOthers meanwhile caution that algorithms should not be made a scapegoat for societal ills. “People get angry and they are looking for something to blame,” said Daniel Castro, vice president at the Information Technology and Innovation Foundation. “We are concerned about bias, accountability and ethical decisions but those exist whether you are using algorithms or not.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:ca4122da-fa4b-4064-91e9-fb222fc5cba6>","<urn:uuid:6ac4c8f9-4485-43b7-b764-b48a249516fb>"],"error":null}
{"question":"How does artist Minjung Kim create her unique 'Phasing' series artwork using paper, ink and fire?","answer":"In the 'Phasing' series, the artist follows a three-step process: First, she makes calligraphic strokes using meok (black ink) on a first sheet of paper. Second, on a separate sheet, she traces the outlines of the meok strokes and cuts out these parts by burning them with incense sticks. Finally, she overlaps the two sheets to complete the artwork.","context":["Creating new and dangerous traditions : Four artists challenge the past with unconventional interpretations of East Asian ink paintings\nAmong them, Sun Xun, an up-and-coming Chinese artist, presents 20 spectacular paintings of dashed-off brushstrokes in ink and mixed media and an animated film based on these ink drawings in his solo show titled “Tears of Chiwen.”\nThe exhibition, running until Nov. 5 at Arario Gallery Seoul, east of Gyeongbok Palace, is about the Chinese artist’s impressions of Korea’s history and culture during the period of modernization, Sun said.\n“Chiwen’s tears result from the regret of lost traditions and the joy of new changes at the same time,” he told the reporters at the gallery last week.\n“Chiwen” refers to small sculptures of a dragon or other monsters perched for decoration on the roofs of old Korean and Chinese buildings. Many of the artist’s paintings and animations on display feature the monsters in hybrid forms that originate both from the conventional iconographies and from his imagination influenced by Western culture.\nBelonging to “Balinghou” or the post-’80s generation of China, Sun said he grew up virtually separated from tradition. He added, “Art is not about making something beautiful but is about thinking new and dangerous things.”\nAt Gallery Hyundai near Arario, two solo shows from Korean artists are going on in both its old and new buildings.\nYoo Geun-taek presents 37 of his latest paintings in an exhibition titled “Promenade,” which runs until Sept. 17. The 52-year-old has used mediums typically used in traditional East Asian paintings such as hanji paper, the black ink known as meok and hobun, known as Chinese white, for his paintings. But the results look quite different from traditional East Asian art, as the artist creates unique texture in his paintings by scratching hanji piled thick.\nThe subjects of Yoo’s paintings are also different from those of old Korean and Chinese paintings. His works depict fountains, rooms with mosquito nets, bathtubs with showers and landscapes of low mountains near his studio in northern Seoul.\n“East Asian paintings have never depicted a fountain, as it is against the law of nature in which water flows downward,” Yoo said at the gallery last month. “But I’m greatly interested in fountains, as they have the power of suddenly, dramatically changing an ordinary environment.”\nThe other artist Minjung Kim, 55, who is active in France and the United States, presents about 30 of her latest paintings. The title of her solo show “Paper, Ink and Fire: After the Process,” which runs until Oct. 8, summarizes her works’ mediums.\nIn her “Phasing” series, the artist dashes calligraphic strokes of meok on a first sheet of paper and then, on a second sheet, she traces out the outlines of the meok strokes and cuts out the parts by burning them with incense sticks. Finally, she overlaps the sheets to complete a work.\nOn the other hand, at Artside Gallery, located to the west of Gyeongbok Palace, Jae Hoon Lee presents 12 new paintings and one installation work in a solo show titled “For a Fight on the Green Field,” which runs until Sept. 24.\nIn the unique paintings, which combine East Asian traditional ink painting and Western fresco, the 39-year-old artist tells “the story of hidden links between modernization under the Japanese colonial rule and the contemporary period,” the artist said last month."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:1e883b58-9493-4d44-8243-d5056995d9bb>"],"error":null}
{"question":"How does chronic muscle pain manifest differently in fibromyalgia versus polymyalgia patients?","answer":"In fibromyalgia patients, muscle pain is characterized by heightened sensitivity to both painful and non-painful stimuli, including hypersensitivity to heat, pressure, and even sounds in everyday life. These patients show lower pain thresholds for temperature and pressure stimuli. In contrast, polymyalgia patients experience muscle pain specifically in the neck, shoulder, and hip areas, with 70-95% of patients first noticing stiffness and pain in their shoulder muscles. Notably, polymyalgia muscle pain doesn't typically include the tenderness characteristic of fibromyalgia. Polymyalgia patients particularly struggle with morning stiffness and difficulty raising arms above shoulders, while fibromyalgia patients experience broader pain amplification throughout the body.","context":["Session Title: Fibromyalgia & Other Clinical Pain Syndromes Poster\nSession Type: Poster Session (Sunday)\nSession Time: 9:00AM-11:00AM\nBackground/Purpose: Fibromyalgia (FM) is a condition in which patients are afflicted with unexplained chronic musculoskeletal pain, insomnia and fatigue. The underlying mechanisms of this disorder is only partially understood. FM patients frequently report not only hypersensitivities to noxious but also to non-noxious sensations like light touch, sounds, and smells encountered in everyday life. Despite anecdotal reports of unusual sensory sensitivities of FM patients, there has been little or no empirical validation of these complaints in current research. The presence of such sensitivities may contribute to difficulties in function by creating an additional source of stress, anxiety, and fatigue\nHypothesis: Compared to age-sex matched healthy controls (HC), FM patients are not only hypersensitive to A) noxious heat and pressure but also to B) auditory stimuli across 3 different sound intensity levels\nMethods: FM subjects fulfilled the 1990 FM Criteria and were not taking pain or psychotropic medications. HC had to be without pain and without any medications. All subjects underwent heat and pressure sensory testing at the hand using sensitivity adjusted stimuli that elicited pain rating of 5 ± 1 VAS (0-10). The subjects underwent auditory threshold testing to ensure normal hearing at frequencies between 1,000 and 4,000 Hz using a GSI 61 Clinical Audiometer. Subsequently, they received auditory stimuli of mixed frequencies at intensities that resulted in mild (12.5 dB, moderate 32.5 dB, and intense (67.5 dB) loudness. All tests were performed in triplicates.\nResults: We enrolled 26 HC and 20 FM subjects (all females). Their ages were 52.3 years and 47.5 years, respectively. Avg pain of FM subjects was 4.6 VAS (0-10). Independent t-test demonstrated significantly lower temperature and pressure to achieve 5 VAS for FM subjects than HC (all p < .001). A mixed model ANOVA showed that FM subject required significantly less sound intensity (dB) to rate similar loudness compared to HC (p < .001). Pearson’s product moment correlations demonstrated significant associations between auditory and heat/pressure sensitivity (p < .05)\nConclusion: Patients with FM are more sensitive to sensory stimuli in daily life than patients who do not experience chronic pain. Their hypersensitivity, however, seems to extend beyond heat and pressure to auditory and possibly to taste/smell and visual sensations. Our findings suggest that FM patients not only demonstrate hypersensitivity to painful but also auditory stimuli. Furthermore, auditory hypersensitivity predicted heat/pressure pain hypersensitivity. Besides genetic predisposition other factor could explain our findings, specifically the “Cognitive Activation Theory of Stress (CATS)”, a theoretical model tying together chronic pain and hyperacusis. CATS postulates that sustained arousal and lack of appropriate arousal resolution (i.e., restful sleep) can result in dysfunction of the central nervous system. In particular, chronic arousal and lack of restorative sleep, can lead to increased sensitivity to peripheral neural input, resulting in chronic pain and hyperacusis\nTo cite this abstract in AMA style:Staud R, Tom G, Godfrey M, Robinson M. Hypersensitivity Beyond Pain: Hyperacusis and Hyperalgesia of Patients with Fibromyalgia [abstract]. Arthritis Rheumatol. 2019; 71 (suppl 10). https://acrabstracts.org/abstract/hypersensitivity-beyond-pain-hyperacusis-and-hyperalgesia-of-patients-with-fibromyalgia/. Accessed February 26, 2020.\n« Back to 2019 ACR/ARP Annual Meeting\nACR Meeting Abstracts - https://acrabstracts.org/abstract/hypersensitivity-beyond-pain-hyperacusis-and-hyperalgesia-of-patients-with-fibromyalgia/","Here’s a quick language lesson: “Myalgia,” the second half of “fibromyalgia,” comes from the combination of two Greek combining forms1. The first is “myo,” which means muscle2. The second, a variation on “algo,” “algia,” means pain3. Muscle pain is a hallmark of fibromyalgia, but it’s not the only chronic illness that’s named after it’s primary symptom. Take for example, polymyalgia — a condition that may sound similar to fibromyalgia, but has little in common with it.\nWhat Is Polymyalgia?\n“Polymyalgia rheumatic is an inflammatory condition that causes pain and stiffness primarily involving the shoulder and hip girdles,” Paul Sufka, M.D., a board-certified rheumatologist, told The Mighty. “We don’t know what causes the condition, but it is seen almost exclusively in adults over the age of 50.”\nPolymyalgia rheumatica (PMR) is believed to be an autoimmune condition that causes your immune system to attack your connective tissues4. It typically can be treated effectively with low-dose steroids and has a good prognosis. Unlike fibromyalgia, which is still a relatively controversial diagnosis, there is at least 100 years of medical knowledge behind how to manage polymyalgia1.\nWhat Causes Polymyalgia?\nThe cause of polymyalgia is still a mystery to experts. What we do know is the symptoms may be directly related to the inflammation the condition causes in your muscles4. Beyond that, Dr. Sufka said research suggests polymyalgia may be caused by a combination of genetics and environmental factors, especially infection5, 7. Other evidence suggests polymyalgia might be related to immune system issues or aging in general6.\nWho Gets Polymyalgia?\nPolymyalgia may be caused in part by aging since most people who experience polymyalgia are between 70 and 80 years old, and occurs in almost no one under the age of 501. Polymyalgia affects approximately 50 out of every 100,000 people over the age of 50 worldwide7. Similar to fibromyalgia, polymyalgia is more common in women1. Caucasian people, especially in Northern European and Scandinavian countries or with Viking ancestry are also most likely to develop polymyalgia7.\nWhat Are the Symptoms of Polymyalgia?\nThe core symptoms of polymyalgia include stiffness and pain in your neck, shoulder and hip muscles4. Most people with polymyalgia, about 70 to 95% of patients, will notice stiffness and pain in their shoulder muscles first, which then gradually moves to your neck and hip areas on both sides of your body7, 10. Your symptoms may have a rapid onset and seem to come on out of the blue — you were previously healthy but over a period of two weeks or less found pain and stiffness limit your agility7.\nApproximately 40% of people will experience additional polymyalgia symptoms, including4, 10:\n- Low-grade fever\n- Flu-like symptoms\n- Weight loss\nWith polymyalgia, it’s common to experience difficult stiffness right after you wake up, which can make some activities like putting on your socks and shoes very difficult7, 8. You may also experience stiffness during the day if you’ve been sitting for a long period of time, but once you get moving, you can typically go about your usual routine7. Raising your arms above your shoulders is also a common difficulty with polymyalgia because it can impact your upper arms8.\nMany people who have polymyalgia are at risk for experiencing depression, which may be your first true symptom of the condition before you notice the pain7. This can be exacerbated because developing polymyalgia symptoms suddenly can be scary — you might fear you’ll no longer be able to do the activities you enjoy or have anxiety about aging7. Polymyalgia can worsen your other chronic illness symptoms because of how it affects your system.\nPolymyalgia rheumatica causes muscle pain and stiffness in the neck, shoulder, and hip. Stiffness is most noticeable in the morning or after resting. https://t.co/EUrJfZXYrX #polymyalgia pic.twitter.com/sUEXGtE7m8\n— NIAMS/NIH/DHHS (@NIH_NIAMS) October 17, 2019\nHow Is Polymyalgia Diagnosed?\nThere isn’t a single test that can detect polymyalgia, so your doctor will make a diagnosis based on your symptoms, family medical history, a physical examination and blood tests to measure the level of inflammation in your body and to rule out other potential conditions9. Because polymyalgia is an autoimmune disease and the pain symptoms can look similar to other chronic illnesses, like rheumatoid arthritis or fibromyalgia, ruling out other possibilities is important8.\nWhile no blood test can definitely determine you have polymyalgia, your doctor will likely order want to determine if you have higher than expected levels of inflammation, a reliable sign you may have polymyalgia5, 7, 8. “In most cases, if inflammatory markers are normal, the patient is unlikely to have polymyalgia rheumatica,” said Sufka.\nFor example, your doctor might order an erythrocyte sedimentation rate (ESR) or “sed rate” test, a blood test that measures inflammation in your body by looking at how fast your red blood cells settle at the bottom of a test tube8, 11. A blood test can also measure your C-reactive protein (CRP) levels, a protein made by your liver, which can also indicate higher levels of inflammation.\nHow Is Polymyalgia Treated?\nThe good news is polymyalgia is a very treatable condition by reducing your inflammation4, 5 . For most patients with a polymyalgia diagnosis, this means prescription corticosteroids, particularly prednisone4, 5, 8. With medication, you may see a major improvement in your polymyalgia symptoms in just a few days. If after two to three weeks on the medication you don’t get better, it’s likely you don’t have polymyalgia but another condition8.\n“Most people who develop polymyalgia rheumatica are initially treated with corticosteroids — usually prednisone starting at moderate doses of around 15 or 20 mg per day — and then slowly tapered over several months,” said Sufka. “Most patients have a rapid improvement of their symptoms once prednisone is started.”\nThough corticosteroids are extremely effective, medications like prednisone have some pretty serious side effects that occur in at least 50% of patients, such as irritability and mood swings, facial swelling (or “moon face“), weight gain or hot flashes10, 12. You won’t need to take the corticosteroids for the rest of your life, however8, 10. Your doctor should find the lowest dose of the medication to manage your symptoms and most patients can stop taking corticosteroids after one to three years, depending on the severity of your symptoms8.\nDepending on the course of your illness, there’s also a chance your polymyalgia symptoms can relapse. Sufka said relapse occurs in about 50% of patients5, 10. Research suggests that patients who experience a relapse were more likely to have started treatment on higher doses of corticosteroids or faster medication tapers at the end of treatment10. Sufka said newer research suggests the “immunosuppressive medication methotrexate” can also be used to treat polymyalgia and prevent relapses5.\nWhat’s the Difference Between Polymyalgia and Fibromyalgia?\nApart from their name and main symptom, fibromyalgia and polymyalgia have little in common. Even the muscle pain might be slightly different — some experts suggest polymyalgia muscle pain doesn’t include the tenderness you may experience with fibromyalgia muscle pain15. The way each condition affects your body is very distinct and the treatment methods are also very different5, 13, 14.\n“Sometimes differentiating these two conditions can be difficult, especially because there isn’t an extremely specific blood test for polymyalgia rheumatica, and there isn’t a blood test for fibromyalgia,” said Sufka, adding:\nThe main difference is that polymyalgia rheumatica is an inflammatory condition, and fibromyalgia is a non-inflammatory condition. Because of this, polymyalgia rheumatica responds to treatment with anti-inflammatory medications. … Alternatively, fibromyalgia is a non-inflammatory condition caused by the nervous system that results in pain amplification, and is associated with fatigue, ‘brain fog’ and sleep problems.\nWhile it’s possible to have polymyalgia and fibromyalgia at the same time, the bigger risk when you’re diagnosed with polymyalgia is developing another inflammatory condition called giant cell arteritis.\nPolymyalgia rheumatica gives muscle PAIN without TENDERNESS.\nFibromyalgia and statin induced myositis is associated with tenderness. Fibromyalgia has very severe SLEEP DISTURBANCE\n— Conrad Fischer (@SeeFisch) July 23, 2019\nWhat Is Giant Cell Arteritis?\n“Giant cell arteritis (GCA) is another inflammatory condition that causes inflammation of the large blood vessels, most frequently affecting the head and scalp,” Sufka said. “The most common symptom of GCA is headache, as well as jaw pain that worsens with chewing, tenderness when touching the scalp over the temples. In some cases, GCA affects the arteries going to the eyes, which can result in blindness.”\nApproximately 10 to 20% of people diagnosed with polymyalgia will develop GCA7, 10. About 40 to 60% of people diagnosed with GCA have polymyalgia, though polymyalgia is two-thirds more common10.\nThe reason that polymyalgia and GCA are linked is unclear, but some experts believe both illnesses could be related to similar genes and may be different ways the same disease process manifests7. You can develop GCA before, during or after your polymyalgia diagnosis10. GCA is a serious condition that your doctor should monitor for regularly if you have polymyalgia10.\nOur #longread for the #weekend: Giant cell arteritis and polymyalgia rheumatica: current challenges and opportunities\nRead it for #free here https://t.co/ploWrPdIzF #SharedIt pic.twitter.com/oLfSULffsb\n— NatRevRheumatol (@NatRevRheumatol) November 3, 2018\nIf you’re concerned about your pain symptoms, check with your doctor. Even if you’re diagnosed with fibromyalgia — and have a hard time finding ways to manage your symptoms — know there are many treatment options out there to help with chronic pain, fatigue and your other fibro symptoms. To learn more, check out The Mighty’s Guide to Fibromyalgia, and post on The Mighty using the hashtag #Fibromyalgia to connect with others who have been there.\nYou can also check out these other Mighty stories that have helped others dealing with chronic illness and chronic pain:\n- How to Survive the Side Effects of Prednisone\n- 7 Things Your Doctor Won’t Tell You When You’re Diagnosed With Chronic Illness\n- 15 Tips for Coping With Chronic Pain\n- The Top 10 Ways I Combat Chronic Pain\n- Tsai, Y. (2018). PAIN MANAGEMENT: Polymyalgia and fibromyalgia are different. The Daytona Beach News-Journal. Retrieved from: https://www.news-journalonline.com/entertainmentlife/20181231/pain-management-polymyalgia-and-fibromyalgia–are-different\n- Dictionary.com. (2002). Myo-. Retrieved from https://www.dictionary.com/browse/myo-.\n- Dictionary.com. (2002). Algia. Retrieved from https://www.dictionary.com/browse/algia?s=t.\n- Clauw, D. (n.d.). Difference Between Fibromyalgia and Polymyalgia. Retrieved from https://www.arthritis.org/living-with-arthritis/tools-resources/expert-q-a/fibromyalgia-questions/polymyalgia-fibromyalgia.php.\n- Sufka, Paul. (2019). What is polymyalgia versus fibromyalgia? [Email interview].\n- National Institute of Arthritis and Musculoskeletal and Skin Diseases. (2016). Polymyalgia Rheumatica: Causes. Retrieved from https://www.niams.nih.gov/health-topics/polymyalgia-rheumatica#tab-causes.\n- Milchert , M., & Brzosko, M. (2017). Diagnosis of polymyalgia rheumatica usually means a favourable outcome for your patient. Indian Journal of Medical Research, 145(5), 593–600. doi: 10.4103/ijmr.IJMR_298_17\n- Sufka, P. (2019). Polymyalgia Rheumatica. Retrieved from https://www.rheumatology.org/I-Am-A/Patient-Caregiver/Diseases-Conditions/Polymyalgia-Rheumatica.\n- National Institute of Arthritis and Musculoskeletal and Skin Diseases. (2016). Polymyalgia Rheumatica: Diagnosis. Retrieved from https://www.niams.nih.gov/health-topics/polymyalgia-rheumatica#tab-diagnosis\n- Salvarani, C. G., Cantini, F. G., & Hunder, G. G. (2008). Polymyalgia rheumatica and giant-cell arteritis. The Lancet Seminar, 372(9634), 234–245. doi: 10.1016/S0140-6736(08)61077-6\n- U.S. National Library of Medicine. (n.d.). Erythrocyte Sedimentation Rate (ESR): MedlinePlus Lab Test Information. Retrieved from https://medlineplus.gov/lab-tests/erythrocyte-sedimentation-rate-esr/.\n- Wyant, P. (2018, November 16). 15 ‘Embarrassing’ Side Effects of Prednisone We Don’t Talk About. Retrieved from https://themighty.com/2018/11/embarrassing-prednisone-side-effects/.\n- The Mighty. (2019, April 1). What Is Fibromyalgia? Retrieved from https://themighty.com/2019/04/what-is-fibromyalgia/.\n- Matsumoto, A. (2007, April 24). Polymyalgia and Fibromyalgia. Retrieved from https://www.hopkinsarthritis.org/ask-the-expert/polymyalgia-and-fibromyalgia-2/\n- Fischer, C. [SeeFisch]. (2019, July 22). Polymyalgia rheumatica gives muscle PAIN without TENDERNESS. Fibromyalgia and statin induced myositis is associated with tenderness. Fibromyalgia has very severe SLEEP DISTURBANCE [Tweet]. Retrieved from https://twitter.com/SeeFisch/status/1153468637770080261"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:20294cad-020f-4601-a971-6aebf87637ba>","<urn:uuid:efda6e42-d809-4632-ba23-21875461af48>"],"error":null}
{"question":"I am interested in portrait photography techniques. How do Chuck Close's portraits of Kate Moss compare to Yousuf Karsh's formal portrait style?","answer":"Chuck Close and Yousuf Karsh had contrasting approaches to portrait photography. Close's portrait of Kate Moss was stark and unadorned, showing her without makeup, with unkempt hair, wrinkles under her eyes, and visible freckles and pores - with no airbrushing. In contrast, Karsh employed a formal style using subtle lighting to meticulously model his subjects' faces, creating monumental and idealized presentations that aligned with their public image.","context":["When someone takes our picture, we usually deliver a mile-wide grin, but there's not a smile in the room at the Phillips Collection's photography show in Washington.\nThe exhibit mostly consists of portraits of inner lives, taken by various photographers, and it's about the encounter between the two participants. Susan Behrends Frank curated the small show, called \"Shaping a Modern Identity,\" which is running through Jan. 12.\n\"I see this whole spectrum of a conversation,\" Frank says. \"It's like a dance between the photographer and the subject.\"\nPhotographs were chosen from the collection of Joseph and Charlotte Lichtenberg, and while there are no big grins, there are plenty of poses. In a color series called \"Nomads,\" Andres Serrano invited various people in front of his camera.\n\"He showed up with a plain backdrop, lights, his camera,\" Frank explains. \"He asked these people to pose; he said all he asked of them was to look left or right.\"\nOne of Serrano's subjects — he called himself Sir Leonard — is a majestic, monumental African-American man. He wears an Indiana Jones hat, a fitted tweed overcoat, woolen gloves and a white bandanna around his neck.\n\"But when you look closely,\" Frank says, \"you see it's a restaurant dinner napkin.\"\nSir Leonard was homeless, destitute. Yet for this 1990 picture, he presents himself with style and flair.\nAnother wall has one of the most famous faces in the world — supermodel Kate Moss. For years Moss has defined glamour and beauty on the pages of glossy fashion magazines. But in artist Chuck Close's extreme close-up, she looks like a girl in a dusty little Texas town, maybe walking along a railroad track.\n\"[There's] no makeup on this woman,\" Frank says. \"Her hair appears unkempt; she stares out at us blankly.\"\nThere are wrinkles under Moss' blank eyes, and her skin is a confetti of freckles and pores — no airbrushing to be seen. Moss and Close knew each other, and she trusted him. She had even posed nude for him over the course of various photo shoots.\nYears ago, photographer Richard Avedon said portrait photography was an exercise in \"unearned intimacy,\" with subject and photographer connecting intensely in a brief period of time. But in Close's portrait of Moss, the intimacy has been earned over years.\nPortrait Of An Artist As A Young Woman\nFrida Kahlo is another of the exhibit's female icons. Imogen Cunningham shot the Mexican painter in 1931, when Kahlo was 24. Her husband, Diego Rivera, a legend in his day, brought her with him when he came to paint murals in California.\nThe young Kahlo is fresh and sweet — she hasn't fully grown into the pain and suffering that will etch her face for life. She's wearing her great, artistic clothes — big beads, big earrings — and her eyebrows are not yet fully grown together in the middle. The photo is called Frida Kahlo, Painter and Wife of Diego Rivera.\n\"This was when she was a young woman known primarily as the wife of a painter [who] had a far larger reputation than her own at the time,\" Frank says.\nIt would take another 40 years for Kahlo to be discovered, and celebrated.\nA Guarded Subject\nFinally, there's The Orange Room, a portrait that's large and in color. It was taken by Tina Barney — an American, like most of the photographers in the Phillips show — part of a 1995 series in which Barney photographed aristocrats in their homes. It was called \"The Europeans.\"\nThe Orange Room is a portrait of nondisclosure — or is it? The subject is an elderly gentleman, and we're not told his name. He wears a three-piece gray suit, has white hair and is balding. He sits against an orange wall in an armchair, elbows on the arms. On the wall, there's a small, old painting that could be a saint, and another painting showing a seascape.\nOn a fine antique console nearby, there are various objects (when they're expensive, you say objets): porcelain figures, a glass and gold bowl, other fancy tchotchkes. They're on display, but he is not. Mr. X tents his hands in front of his face so we can't see his mouth\n\"He has put this barrier between Tina Barney, the photographer, and himself ... so that we can't really see who he is,\" Frank says. \" ... I think that what [Barney] was fascinated by was how people responded to her presence that didn't really know her. He's guarding his inner person very carefully.\"\nWe see him guarded, hiding, and we wonder if he's allowing himself to be defined by his things instead of his self. Or is that his self?\nIt's really fascinating: What do we choose to reveal in a photo portrait? What do we hide? And what do our wide, sunny smiles conceal in the pictures we take and pose for with family and friends?\nCopyright 2013 NPR. To see more, visit http://www.npr.org/.","Portraits by Yousuf Karsh\nYousuf Karsh (1908-2002) Canadian photographer, known for portraits of the great personages of his time.\nAs an Armenian in Turkey, the young Karsh endured persecution and privation. At the age of 16 (in 1924) he emigrated to Canada, joining his uncle, who was a photographer in Sherbrooke, Que. From 1928 to 1931 he served as an apprentice to a prominent Boston painter and portrait photographer and briefly attended art school. Returning to Canada in 1932, he was employed by an Ottawa photographer, whose studio Karsh leased after his employer retired. He was appointed official portrait photographer of the Canadian government in 1935. He became a naturalized Canadian citizen in 1947.\nKarsh's portrait of Sir Winston Churchill, made in Ottawa in 1941, brilliantly conveyed\nthe dogged determination of\nBritain's wartime leader and brought Karsh his first real international fame. Karsh went on to photograph an enormous\nnumber of the world's most prominent personalities, including royalty, statesmen, artists, and writers. He also\ncontinued to make portraits of world leaders. Karsh's style as a portraitist was formal. He used subtle lighting to\nmeticulously model his subjects' faces, thereby obtaining a monumental and idealized presentation that was in accord\nwith their public image.\nHis books include Faces of Destiny (1946), Portraits of Greatness (1959), In Search of\nGreatness (1962), Karsh Portfolio (1967), Faces of Our Time (1971), Karsh Portraits (1976), Karsh Canadians\n(1978), Karsh: A Fifty-Year Retrospective (1983),\nKarsh: American Legends (1992),\nKarsh: A Sixty-Year Retrospective (1996),\nYousuf Karsh: Heroes of Light and Shadow\nHis books include Faces of Destiny (1946), Portraits of Greatness (1959), In Search of Greatness (1962), Karsh Portfolio (1967), Faces of Our Time (1971), Karsh Portraits (1976), Karsh Canadians (1978), Karsh: A Fifty-Year Retrospective (1983), Karsh: American Legends (1992), Karsh: A Sixty-Year Retrospective (1996), and Yousuf Karsh: Heroes of Light and Shadow (2001).\nModern reproductions of portraits\nThese high quality color and black and white prints were produced in Switzerland by Imprimerie Jean Genoud SA, Lausanne. They come from one of Yousuf Karsh art books. Usually, there is a short unrelated text or another portrait on the back side but it doesn't effect the main picture. Please refer to a description of the specific print for details. These prints are of poster quality or better.\nA gravure is a photomechanical intaglio process print, developed in the mid-19th Century, in which the image is transferred to the printing plate by using a light sensitized gelatin film surface on a metal plate which is then etched. The photogravure c an reproduce an original painting or photograph with an accuracy of detail and tonal depth unlikely to be surpassed in monochrome printing. The sheet-fed gravure method involves feeding each sheet individually through the printer, with printing only on one side of the page. This prevents one of the most common printing problems -- the show through of material from the other side of the page. In addition, Karsh and the publisher went to considerable length -- the use of a special soft ink and of specially produced (very heavy) paper -- to insure that the final print was as close an approximation of the original photograph as possible. The deep, velvety blacks and the low gloss finish provide a sense of texture that is totally lacking from most reproductions.\nThe magnificent portraits were produced by sheet-fed gravure, a printing process not presently used in North America for this type of work. The world-renowed printing house of Enschede in Haarlem, Holland, which has been turning out fine printing for more than two hundred and fifty years, was entrusted with making and reproduction of the gravure cylinders. The text was first printed by offset lithography on paper especially manufactured for this book in Paris. The gravure printing of the portraits followed. The results are as close to the quality of Yousuf Karsh's originals mat finish prints as has ever been obtained by any printing method A special thermoplastic binding used in gravure portfolio books allows for easy removing individual portraits for mounting and framing.\nGravure prints offered on this site come all from early portfolio books by Yousuf Karsh. They are 30 to 40+ years old. Some of these prints are very difficult to find in a good condition. Due to the soft ink they need to be handled with care. Archival matting and framing under glass is recomended."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:d487c83c-ab03-40ad-b870-9c56f9059bed>","<urn:uuid:2e3da5ab-cd9d-47f5-b962-ee727d1413c5>"],"error":null}
{"question":"德国和美国实施 Daylight Savings Time 的时间有什么区别呢？Which country adopted it first? 👀","answer":"Germany was the first country to enact daylight saving time, implementing it on April 30, 1916 as a wartime effort to conserve electricity. The United States followed later, implementing it on March 31, 1918, also as a wartime conservation measure. Both countries adopted it during World War I for similar purposes - to reduce energy consumption and produce electric power more efficiently during wartime.","context":["By Daniel Donnelly ‘19\nIt’s that time of year again when the days are shorter and it’s dark in the morning during the work commute. Although Daylight Savings began with the goal of conserving fuel during war times, it was influenced by the ideas of William Willet whose goal was to increase the enjoyment of sunlight, according to reporter Olivia Waxman of Time Magazine. Willet would lobby Parliament and England and Germany would eventually pass it.\nDaylight Savings Time has been in effect since World War I by the United States and European countries. It’s initial purpose was to reduce consumption of energy to produce electric power during World War I. At the time, Germany and Austria advanced time by one hour to accomplish this. Other countries including France, Italy, Norway and Sweden also adopted the practice. Eventually Britain and Australia would follow. The United States finally came on board in 1918. The change proved itself to be unpopular with citizens and was eventually repealed in 1919 with a congressional override of the president’s veto. It then became a local option and continued in some New England states. It also continued in some cities including Chicago, New York and Philadelphia.\nDuring World War II, from 1942 to 1945, Franklin Delano Roosevelt began a program named “War Time” that was a year round daylight savings time during World War II, according to webexibits.org after 1945, there was no federal law regarding daylight savings time through 1966, states were free to determine usage, start dates and end dates. This led to confusion for many industries including airlines, railways, bus companies and the broadcasting industry that relied on schedules. Radio, TV and transportation companies had to disseminate a wide variety of divergent schedules.\nDuring the 60’s Daylight Savings Time was very inconsistent varying state by state. The nation’s timekeeper, The Interstate Commerce Commission remained immobilized and the issue was deadlocked. Many businesses were open to its standardization though the issue remained divisive between the indoor and outdoor theater industries. The movie industry opposed it as they reasoned viewers wouldn’t go into theaters when it was sunny outside, rerouted the Time Magazine. The farmers were opposed to it because it reduced the key morning time to harvest crops. Each farmer had a different standard and set of rules that they followed. State and local governments were divisive on the issue depending on local conditions. In the 70’s Nixon signed into law the Emergency Daylight Savings Time Energy Conservation Act of 1973. Later in 1974 the act was implemented and the clocks were set ahead.\nThe transportation industry and the Committee of Time Uniformity both encouraged efforts at standardization. They surveyed the nation by questioning telephone operators about local time and found the matter to be really confusing. The Committee’s next goal was to get a strong supportive story on the front page of the New York Times newspaper. They rallied the general public’s support and their goal was accomplished despite one consequence that every bus driver and passenger on route 2 which was a 35 mile highway from Ohio to West Virginia had to experience seven time changes.\nIn 1966 congress finally decided to intervene and end the confusion by establishing one pattern across the country. The Uniform Time Act of 1966 signed into law by president Lyndon B. Johnson set Daylight Savings Time to begin on the last Sunday of April and end on the last Sunday of October. Any state that wanted to be exempt from Daylight Savings Time could do so by passing their own state law. The Uniform Time Act of 1966 established a Uniform system within each time zone throughout the U.S. and its territories. The only states that were exempted from this were the states in which their legislatures voted to keep the whole state on standard time.\nIn the 70’s Congress revised the law so that if a state has more than one time zone, than the state could exempt in one time zone while allowing the other to observe Daylight Savings Time. The Energy Policy Act of 2005 extended Daylight Savings Time in the United States. Beginning in 2007 though congress held onto the right to revert to the 1986 law if it proved unpopular or if energy savings were significant. From 2007 onward Daylight Savings Time would begin at 2:00 am on the second Sunday of March and end at 2:00 a.m. on the first Sunday of November. European countries in western Europe and others that were apart of the EU would have Daylight Savings Time begin at 1:00am on the last Sunday of March and end at 1:00am on the last Sunday of October. Elsewhere in the world, observance of Daylight Savings Time are highly variable depending on where you live.\nAccording to Time Magazine, the intent of Daylight Savings and its ability to reduce energy consumption is being reconsidered. Although extra daylight is enjoyable, further studies are required to conclude whether daylight savings time reduces energy consumption.","This Day in History: March 31- DST\nThis Day In History: March 31, 1918\nThe concept of daylight saving time (or at least something like it) was first bandied about, albeit jokingly, by that American sage Benjamin Franklin during his sojourn in Paris in 1784. The idea was included in a satirical essay he wrote called ‘An Economical Project” published anonymously in Journal de Paris.\nBy the time Ben was 78, Mr. “Early to Bed and Early to Rise” had changed his tune a bit, and was inspired to write the essay after being woken up at 6 a.m. by the summer sun one too many times. His article offered the tongue-in-cheek practical plus that by rising at dawn, the people of Paris could save oodles of money by “using sunshine instead of candles.”\nHe then suggests a variety of methods to induce people who were “obstinately attached to [the] old custom” of getting up at noon to instead wake up with the sunrise. These included taxing people who have shutters on their windows, rationing candles, and waking people as soon as the sun comes up by ringing church bells and firing cannons.\nBecause of this essay, Franklin is often falsely credited with the invention of daylight saving time. But Ben merely suggested a change in sleeping schedules to give people more daylight hours to work with, not an adjustment to time itself.\nThe modern day version of daylight saving time was first proposed by the New Zealand entomologist George Vernon Hudson in 1895. However, he is rarely given much credit for being the first in this case and is sometimes not even mentioned at all.\nThat’s because the person who generally gets most of the credit, an Englishman named William Willett, in the early 1900s led the first major campaign for daylight saving time with an evangelistic zeal that dominated his life. Inspiration struck while he was out for an early-morning horseback ride around London. He noticed the streets were all pretty much empty with most buildings shuttered up with sleeping occupants, despite that it was quite light out. It then dawned on him that if every April through October everybody turned their clocks ahead by 80 minutes, everyone could enjoy more of the plentiful seasonal sunlight without needing to change their normal schedules.\nIn 1907, Willett published a brochure entitled “The Waste of Daylight” and blew most of his life savings trying to convert the masses over to what he called “summer time.” Parliament turned his idea down year after year, and Willet died in 1915 without ever seeing his pet cause come to fruition.\nOn April 30, 1916, Germany became the first country to enact daylight saving time in a war-time effort to conserve electricity. Great Britain followed suit a few weeks later and introduced “summer time,” as poor William Willett rolled over in his very fresh grave.\nDaylight saving time was first implemented in the United States on this day in history, March 31, 1918, as a war-time conservation tactic as well. At the time, one of the principle uses of electricity and certain other valuable fuel sources in the home was for lighting. Daylight saving time reduced the need for such lighting slightly. That slight reduction across many millions of homes added up to quite a savings on certain resources. So really not a bad idea at the time.\nBecause of our modern, vastly different, energy usage (only about 3.5 percent of our energy usage today goes towards lighting), the effect is now negligible. However, it has been shown to be a profitable thing for many stores, particularly those that sell product related to outdoor leisure activities.\nContrary to the real original intent, most people today believe the initial idea was to give farmers more time to work in their fields. But, in reality, what time it is has little effect on a farmer’s schedule. Farmer’s days were (and to some extent still are) ruled by the sun and the needs of their crop or livestock, not the clock. In fact, for various reasons, farmers led the fight for the repeal of national daylight saving time in 1919. It’s historically been urbanites that favor daylight saving, not country folk.\nAfter daylight saving was done away with on a national level in 1919, the practice became a confusing hodge-podge as some cities, such as New York City and Chicago, decided to continue the practice, while most other places were more than happy to see it go. That is, until World War II, but once the war was over it went back to the same mass confusion we came to know and love in 1919, that Time magazine described in 1963 as “a chaos of clocks.”\nPublic order was restored in 1966 with the introduction of the Uniform Time Act. This defined daylight saving time as extending from the last Sunday in April to the last Sunday in October, rather than letting each region decide the dates. States did have the right to remain on standard time all year if they chose. Out of the fifty states, Arizona and Hawaii remain on standard time year-round.\nIf you liked this article, you might also enjoy subscribing to our new Daily Knowledge YouTube channel, as well as:\n- How Candy Pumpkins and Halloween Helped Change Daylight Saving Time\n- Why We Say “O’Clock”\n- Why the Same Side of the Moon Always Faces the Earth\n- Why We Divide the Day Into Seconds, Minutes, and Hours\n- A Jiffy is Used as an Actual Unit of Time\n|Share the Knowledge!|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:298da6df-94d8-4994-8ba5-6b98f2092076>","<urn:uuid:f04b5b92-ae2c-403b-8480-fb2c4ec9cf10>"],"error":null}
{"question":"Could you explain how damask fabric is used differently in draperies versus jacquard weaving? I'm trying to understand the connection!","answer":"In jacquard weaving, damask is created as a patterned fabric with a ground of one weave and designs in other weaves, particularly using satin and twill variants, resulting in sheen and light reflection. The patterns are always reversible, with the pattern weaves becoming ground weaves on the reverse. When used in draperies, damask is considered a heavier fabric choice that creates a formal look, particularly suitable for living rooms, master bedrooms, or dining rooms. Draperies made from damask are often lined, tailored, and pinch-pleated, stretching from floor to ceiling.","context":["A long time ago, when I posted the difference and between muslin, voile, lawn, and batiste (among other fabrics), someone asked if I could explain the difference between brocade and jacquard. I took a deep breath, and say “Yes, but it will take a while.” It certainly has, because it’s actually quite a big question, and there is so much confusion around it!\nA lot of the confusion come from the fact that while the appearance of brocade has stayed very similar throughout history, the method of creating it has changed drastically. Prior to 1801 brocades were woven on hand operated draw-looms by master weavers, who manually created the elaborate brocade patterns as they were woven in with the help of a drawboy, who stood on a perch above the loom. Then, in 1801 Joseph Marie Jacquard demonstrated a new invention (albeit one based partly on a series of inventions from the 1740s-60s) – a loom which ran on cards with holes punched in them. Each card represented one line of a pattern, with the holes allowing threads to pass through into the pattern, changing the colours and creating a design.\nThe Jacquard loom revolutionised the production of elaborately patterned fabrics. Skilled craftsmen who could read pattern diagrams and manipulate the pattern as it was being woven were no longer needed to weave brocades and other designs, and the Jacquard loom did not require the assistance of an additional drawboy. The new looms could be operated by an unskilled labourer, making richly patterned fabrics faster and cheaper to produce. Jacquard looms were so much easier and cheaper to operate that the old style of looms quickly became obsolete, and within a few decades of Jacquard’s invention almost all elaborate fabrics woven in the West, including brocades, damasks, and richly patterned faux-Kashmiri or ‘Paisley’ shawls, were woven on Jacquard looms.\nIf the punch cards with holes which create a pattern sounds a little like an early computer – it is. The Jacquard loom and its punch card pattern system is considered an important point in the history of the computer. Babbage and Lovelace (the ‘Father of the Computer’ and world’s first computer programmer and first person to envision a computer that did much more than mathematical calculations (also Byron’s daughter), respectively) were familiar with Jacquards loom, and Babbage intended to use punch cards based on the loom punch cards in his Analytical Engine.\nThe jacquard loom was further revolutionised in 1843, with the invention of the dobby loom, which makes simpler patterned fabrics by a method of up to 40 frames which lift according to a programme. The dobby loom was even cheaper to run than the Jacquard, and supplanted it for all simpler patterned weaves. Dobby loom patterns, however, are limited to designs that stretch over 40 threads, whereas designs made on a Jacquard loom are virtually limitless. Today jacquard weaves are achieved not with a Jacquard loom, but rather a Jacquard head which is fitted on to a dobby loom.\nAlmost all modern brocades are woven with a jacquard device, so one could say that all modern brocades are jacquards, but not all jacquards are brocades, because jacquard looms are used to create other weaves, such as brocatelle, damask and tapestry.\nToday the name jacquard usually applies to all weaves that can be achieved with the machine, but it is sometimes used to describe one specific type of fabric woven using a Jacquard loom: a light, soft, draping damask weave (see damask weaves below) of silk, rayon or synthetic fibres, which is why descriptions of jacquard as a fabric sometimes say daft things like ‘similar in appearance to damask’.\nHere are some of the most common weaves achieved on a jacquard loom. Because a jacquard head can produce an almost infinite variety of weaves, there are many fabrics produced with a jacquard loom that don’t fit nicely into one weave category or another, so certain fabrics (particularly brocade & tapestry) can be quite fluid or imprecise in their definition:\nBrocade: These days brocade frequently describes the aesthetic of a fabric, rather than a specific weave. Brocades are fabric with an elaborate embossed or embroidered surface effect, usually with different ground and pattern weaves. The name comes from the Italian brocatto, meaning ’embossed cloth’. Unlike damask, brocades are not reversible. Continuous brocades have the weft threads left loose and floating on the back. Some continuous brocades have the back threads cut away, though the short cut ends are still visible. A discontinuous brocade is one where additional yarns are only woven into the patterned areas, resulting in a smoother back.\nBrocading: brocade or other jacquard weaves with the inclusion of gold or silver coloured threads. It is also called Imperial Brocade.\nBrocade velvet: a patterned velvet with a raised pile and a woven ground (not to be confused with a burnout velvet, where the patterning is achieved by burning out the pile with acid, rather than weaving in the pattern from the start).\nBrocatelle: Similar to brocade, but the patterned areas are more distinct and raised, and the fabric is heavier.\nDamask: Patterned fabrics with a ground of one weave (usually plain, twill or sateen) and designs in other weaves (particularly satin and twill variants), so that the patterned areas have sheen and reflect light, Damasks are always reversible, with the pattern weaves becoming the ground weaves on the reverse (so on a fabric with a plain ground and satin pattern front, the ground would be satin and the pattern plain on the reverse). There are tone-on-tone damasks, with different weaves within the damask creating elaborate floral or geometric patterns, and multicoloured damasks, where the background colours and the pattern colours reverse from front to back. My Polly Oliver jacket is made from a red tone-on-tone jacquard damask.\nMatelassé/Marcella/Piqué: a weave specifically designed to imitate quilting with a characteristic bubbled/blistered raised effect.\nTapestry: In modern terminology ‘tapestry’ just means a fabric woven on a jacquard loom that imitates historical tapestries. It’s a very imprecise term, but it describes a heavy fabric with an elaborate, multicolour weave, usually with the colours reversing on the back of the fabric (for example, a fabric with green leaves on a red ground would have red leaves on a green ground on the reverse), but is thicker, stiffer, and heavier than damask, the reverse may not be as neat and tidy, and is usually woven with thicker yarns than damask or brocade.\nIf you are looking at a modern jacquard weave fabric and trying to determine what it is most likely to be called, ask yourself:\n– Is it reversible, with the pattern a mirror of each other on each side? If so – it’s a damask. If it’s light and drapey, some people might call it a jacquard.\n– Is one side beautiful, and the other side a mess of floating threads? It’s a continuous brocade, unless it’s very heavy, and the pattern is very raised, and then it is a brocatelle.\n– Is one side beautiful, and the other a mess of short, cut threads? Its a discontinuous brocade, unless it’s very heavy, and the pattern is very raised, and then it is a brocatelle.\n– Is one side beautiful, and the other a pattern of coloured stripes? It’s a type of brocade.\n– Is it a brocade with gold and/or silver coloured threads? It’s an imperial brocade/brocade with brocading.\n– Is it quite textured, with puffy, blistered areas on the front and a loose, gauzy support weave on the back? It’s a matelasse/marcelle/piqué\n– Is it really elaborately patterned, quite heavy, and doesn’t fit any of the other descriptions? It’s a tapestry weave.\nJacquard looms can also be used to create elaborately patterned knits, including:\nJacquard hose: socks and stockings with elaborate patterns, such as argyle, herringbone, and other socks with the patterns woven in. Stockings/socks/hose have been knit on jacquard looms since the 1920s, and have gone in and out of popularity since then.\nJacquard sweaters: machine made sweaters with elaborate patterns knitted in. Most ugly Christmas sweaters? Yep. Those are jacquard sweaters. Aztec sweaters – those are jacquard sweaters. Machine knit argyle sweaters are knit on a jacquard loom. Faux Fair Isle and Cowichan sweaters are knit on jacquard looms.\nAnd if you are interested, weaves that are usually done with a dobby loom are birds eye (diaper cloth), crepes, cloche, dotted swiss, double-weaves, honeycomb weaves, simple matelasse/piqué patterns, satins, and elaborate twills, as well as fabrics with small, simple widely spaced designs.\nCant, Jennifer and Fritz, Anne, Consumer Textiles. Melbourne: Oxford University Press. 1988\nCalasibetta, C. M., Tortora, P, and Abling, B (illus.). The Fairchild Dictionary of Fashion (Third Ed). London: Laurence King Publishing Ltd. 2003\nCollier, Billie J. and Tortora, Phyllis G. Understanding Textiles (Sixth ed). Upper Saddle River, New Jersey: Prentice Hall Inc. 2001\nShaeffer, Claire. Claire Shaeffer’s Fabric Sewing Guide. Iola, Wisconsin: Krause Publications. 2008\nWilard, Dana. The Fabric Selector. Millers Point, NSW Australia: Murdoch Books Pty Ltd. 2012","One of the easier and more cost-effective ways to give a room a whole new look includes adding new window treatments. But with so many types and styles of curtains, blinds and drapes to choose from, how can you find the ones that are best for your windows?\nWhat makes shopping for new window treatments even more confusing is the myriad of unfamiliar terms used to describe and classify the different types and styles. (What is a cellular shade, anyway? What’s the difference between a curtain and a drapery? And why is it important to know the depth of a rod pocket?)\nTo make your search easier, we’ve compiled a mini “dictionary” of some of the terms you’ll find to describe common window treatments.\nThese terms describe the different types of window treatments and their basic components or parts used to describe them.\nblinds: Refers to window treatments made of horizontal or vertical slats, kept in place with string, cord or fabric tape. Materials for blinds include plastic, metal, wood and heavy fabric.\ncurtains: Not to be confused with draperies, curtains are unlined, stationary window coverings. Curtains can be hung over windows using a curtain rod or decorative pole, and are most often held back with tiebacks or holdbacks to let in some light. Curtains can be made of most any lightweight, sheer or semi-sheer fabric.\ndrape: Not to be confused with draperies, “drape” refers to the way a fabric hangs on the window.\ndraperies: Draperies are window treatments made of fabric that’s heavier than that usually used for curtains. They can be stationary or mobile, and can be used with fabric tiebacks or fixed holdbacks mounted on either side of a window. Draperies can be lined for the purpose of insulation or light blocking and used with a variety of decorative or non-decorative curtain rods. They also come in many hanging styles. Common materials for draperies include brocade, boucle, chenille, damask, suede and velvet.\nDraperies are often lined, tailored, and pinch-pleated. They usually stretch from floor-to-ceiling, giving them a more formal look. Draperies are most often seen in a living room, master bedroom, or dining room. They often cost more due to the high quality of the fabric, the fact that they are lined, and the beautiful way they \"drape\" from the curtain rod to the floor.\nhand: This refers to the actual “feel” and draping abilities of a fabric; for example, a fabric with a “soft hand” drapes easily and is soft to the touch.\nrod pocket curtains: Also known as a pole top curtain or draperies. rod pocket curtains or draperies have a horizontal sleeve stitched across the top that opens to allow a curtain rod or decorative rod to be slipped through. The curtain or drapery is then arranged to create a soft, gathered look.\nshade: “Shades” can refer to blinds, pleated shades, roller shades and other opaque window coverings that can be adjusted to fully or partially expose or cover a window.\nsheers: Made of lightweight, translucent and finely woven fabrics, sheers can be used alone to obscure a view while letting in plenty of light or under draperies to create a layered look.\nStyles for Every Window:\nOK so you’ve decided whether or not to go with blinds, curtains, shades, or a combination. The fun doesn’t stop there! Each of these very different treatments comes in a variety of styles from casual to formal, traditional to modern, classic to cozy. You can even choose styles based on how much or how little light you want to let in These terms explain the different styles available so you can best decide what works for you.\nascot valance: A triangular top treatment usually used between matching panels. A double rod would be used, with the panels on the inside rod and usually 3 ascots on the outside rod for a finished look. An ascot can also be inserted between the panels on a single rod.\nblackouts: Blackout draperies are lined or coated window panels made from heavyweight fabric. Blackout draperies are designed to block light and insulate windows so that artificially warmed or cooled air doesn’t escape and outside temperatures can’t penetrate the rooms as easily. Blackout draperies also reduce exterior noise.\nblouson/balloon valance: A straight-across valance that is sewn as a pocket that can be filled with tissue paper for a full, puffy look, or left unstuffed for a more tailored look.\ncafé curtains: Also known as tiered curtains or kitchen tiers, these short, straight curtains cover the lower half of a window. Café curtains are usually paired with some sort of top treatment, such as a swag valance.\ncellular shade: Known for a distinctive “honeycomb” fabric construction, cellular shades are multi-layered, pleated shades that trap air to provide a high level of window insulation.\ncrescent valance: A gathered, half-moon valance usually used with jabots or as top treatments to window panels.\ngrommet-top curtains: Grommets (or eyelets) are metal, plastic or rubber rings used to reinforce a hole in fabric. Grommet top curtains and draperies are hung using “grommets” or “eyelets” – metal, plastic or rubber reinforced holes in the top of the fabric through which decorative curtain rod can be threaded, instead of using a rod pocket to hang the window treatment.\ninsert valance: A valance that is usually shorter in width than a regular valance. It is used between two panels or a pair of swags.\npanel: A panel refers to a single curtain or drapery. A conventional window treatment requires two panels, also known as a “panel pair.”\npinch pleats: A pinch pleat is a three-fold, stitched pleat at the top of a formal drapery panel. The draperies generally are hung on a traverse rod using drapery hooks inserted into the back of the pleat.\nroller shade/roll-up shade: Flat fabric, plastic or vinyl shades that roll up onto a cylinder. With roller shades, the cylinder is spring loaded, while with roll-up shades, the shade is drawn up with cords or strings. Roll up shades also can be made of wood.\nRoman shade: This fabric window shade creates a tailored, flat look at the window. The classic Roman shade features a flat face fabric that forms pleats as the shade is raised; these pleats are formed by rings threaded with cords or tapes sewn on the back of the fabric that allow the shade to be raised and lowered.\nswag: Also known as a jabot, it’s a decorative window top treatment that features a soft, curving semicircle centered on the window with fabric hanging down on both sides. Multiple swags can be used on a window to create a highly decorative top treatment; longer swags can also be used alone as a simple window embellishment. Swags can be made from any fabric and are can trimmed with fringe, lace or tassels.\ntab-top curtains: Tab top curtains and draperies are hung from fabric loops or tabs sewn across their tops. A curtain rod or decorative pole is threaded through the tabs, creating a window covering that hangs straight and flat.\ntailoring: The term “tailored” or “tailoring” refers to panels or valances with simple, straight lines that hang straight down from the rod.\nthermal backing: Thermal-backed draperies are coated on the back side of the material with an insulating layer to block light, heat, drafts or sound. They work similarly to thermal-lined draperies, which have a separate lining that acts as the insulating layer. Thermal linings can be obtained separately for regular draperies.\ntie tops: Tie-top curtains and draperies have ribbons or tapes sewn across the top that are used to tie the panel to the rod or to rings and create a casual, homespun look.\nvalances: Valances are decorative window treatments that cover the top part of a window. They can be used as the top layer of a layered window treatment or as alone as a decorative accent.\nCurtains, draperies and even blinds come in a variety of materials that can add weight or lighten up a room, depending on the look you’re going for. Whether a room’s crying out for the luxurious warmth of chenille or the springy lightness of traditional gingham, Here are just a few of the most popular materials, along with their benefits.\nacrylic: Acrylic is a lightweight fabric that looks and feels like wool, but is machine washable, wrinkle-resistant, and won’t fade in the sunlight. Acrylic window treatments are easy to care for and hold up well over time.\nbouclé: Boucle is a woven or knitted fabric made with popular novelty yarn to create a rough, looped or knotted textured surface.\nchenille: A luxuriously soft, textured fabric characterized by a thick pile. Chenille is usually made from cotton or wool, but also can be constructed of acrylic, rayon or olefin.\ndamask: Damask is an elaborately patterned, jacquard-woven fabric constructed from silk, linen, wool, cotton or synthetic fibers. Common design themes in damask fabrics include flowers, leaves, fruit and animal figures. Metallic threads can be added to the pattern for effect.\neyelet: A lightweight curtain fabric decorated with small, embroidered holes; the holes are often laid out in a flower pattern. It is also known as “eyelet lace” and is often used as trim.\ngingham: A casual cotton or cotton/polyester blend fabric that has a small-scale checkerboard design of colored squares alternating with white squares. Gingham frequently is used for tier curtains.\nlace: Lace is a delicate, ornamental fabric woven in an open, web-like pattern, often combined with different types of embroidery.\nlinen: Linen is a flat-woven fabric made from the fibers of the flax plant. Linen is extremely strong and smooth with a crisp texture. It can also be blended with cotton, silk, and other natural fibers.\npolyester: Polyester is an easy-care, synthetic fiber that’s machine washable, dries quickly, is wrinkle-resistant and takes dye easily. Polyester is often blended with cotton or with other synthetic fibers.\nrayon: A versatile, semi-synthetic fiber made from cellulose, rayon has a shiny finish and superior draping characteristics. Most rayon fabrics need to be dry-cleaned.\nsilk: Silk is a natural fiber that features a soft hand, lustrous appearance and superior draping qualities. Common types of silk include and dupioni.\nsuede: Sueded fabrics include cotton, silk or synthetic fabrics designed with a napped finish to resemble the look and feel of leather.\nvoile: A simple, lightweight, semi-sheer fabric made from cotton, polyester, silk or rayon, voile is a popular as an under-treatment in layered window treatment ensembles.\nIt’s OK To Embellish A Little:\nAdd texture to a room by adding texture to your window treatments! Fabrics don’t have to be plain – textured or patterned weaves, embroidery, embellishments and trim add visual interest to basic fabrics, making your window treatments as much a fashion statement as your furniture or area rugs.\nappliqué: Appliqué refers to a needlework technique in which pieces of fabric are embroidered onto a background fabric to create a design.\nbasketweave: A basketweave is an allover textured design created by an under-and-over weaving process, resembling the weave used to make baskets.\nbox pleat: Box pleats are evenly spaced and stitched double pleats, with fabric folded under on both sides to create a box. Box pleats are often used as a header for draperies.\nburnout: A fabric design produced by dissolving away one or more fibers in a fabric using a weak acid or chemical salt, which destroys some of the fibers to create a relief or silhouette pattern.\nembroidery: Embroidery is decorative stitches used to dress up a base fabric. There are many different types of embroidery used to embellish curtains and draperies, including eyelet, chain stitch, cross stitch, crewel and satin stitch patterns.\njacquard: A jacquard weave creates an intricate woven pattern using multiple levels. Tapestries, brocades and damask fabrics are all jacquard weaves.\nmatelassé: A complex jacquard woven fabric with an embossed, quilted appearance.\nslub or slubbed fabric: Small nubs or bumps in a fabric, woven to create a random, allover texture.\nThe Nuts And Bolts Of Hanging:\nNo need to settle for the traditional traverse rod if you don’t want to (and we explain what that is and how it works, too): today’s window treatment hardware options come in a wide array of decorative and functional styles and colors that are sure to add the perfect finishing touch to your ensemble.\nbrackets: A bracket refers to a piece of hardware attached to a wall or window frame used to support a curtain rod, decorative rod or drapery holdbacks. In wall-mounted brackets, plastic or metal screw anchors are used to install the bracket and add stability and extra support to the rod.\ncafé rod: A café curtain rod is a narrow metal or plastic rod used to hang lightweight curtains that comes in two diameters (½” or ¾”).\ncenter draw: “Center draw” refers to drapery traverse rods that open and close from the center.\nclip rings: Clip rings are small metal, wood or plastic rings with a clip that are used in hanging curtains or draperies. The rings slide onto the drapery pole or curtain rod and the clips attach to the fabric panels. Clip rings can be used with pinch-pleated draperies in place of hooks.\ndrapery hooks: Drapery hooks are inserted into the back of the pleats in pinch pleated draperies. The hooks are then threaded onto a carrier on a traverse rod.\nfinial: A finial is a decorative end piece used to finish or cap the ends of a drapery rod or top of a drapery holdback. Finials come in a variety of shapes, including balls, urns, pineapples, leaves, flowers, scrolls and fleur-de-lis.\nholdbacks: Like fabric tiebacks, holdbacks let curtain or drapery panels to be pulled to the sides of the window and held there. Made of metal, wood, resin or plastic, holdbacks are mounted on the sides of the window and come in virtually any shape and design.\ntension rod: Tension rods are adjustable, spring loaded curtain rods that mount on the inside of a window frame or between two walls. Most tension rods are telescoping rods with rubber tips, which anchor the rod and eliminate the need for tools when installing.\ntiebacks: Tiebacks are slim strips or loops of fabric that fasten drapery or curtain panels to the sides of the window. The most common type of holdback (see above), tiebacks are often made of a fabric that matches the window panel. They can be trimmed with tassels or fringe to create a more decorative look.\ntraverse rod: Traverse rods are drapery rods (usually hidden) that allow the panels to “traverse,” or open and close” across the window. They are usually hung on drapery pins or hooks, which are threaded onto small carriers on the inner side of the rod, allowing the carriers to slide on a draw cord to open or close the draperies.\nBack to Top"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:71116ca2-fff0-49ac-97b7-f1c039c56a6a>","<urn:uuid:aa2927a2-698d-40aa-8613-0292c2119fdf>"],"error":null}
{"question":"I'm studying marine biology and computing - could you compare the concept of distributivity in Horner's Rule with the size distribution patterns of California fish species?","answer":"While both involve distribution concepts, they are quite different. Horner's Rule deals with mathematical distributivity, where an operation distributes over collections and involves binary operators, algebras, and functors. In contrast, the size distribution of California fish species shows actual physical patterns - ranging from small fish like the 2-3 inch red-marked species to much larger ones like the 1.5-3.5 foot fish with canine teeth. The mathematical distributivity is about operations preserving structure, while the fish size distribution reflects natural biological variation.","context":["This is a continuation of my previous post on Horner’s Rule, and in particular, of the discussion there about distributivity in the datatype-generic version of the Maximum Segment Sum problem:\nthe essential property behind Horner’s Rule is one of distributivity. In the datatype-generic case, we will model this as follows. We are given an -algebra [for a binary shape functor ], and a -algebra [for a collection monad ]; you might think of these as “datatype-generic product” and “collection sum”, respectively. Then there are two different methods of computing a result from an structure: we can either distribute the structure over the collection(s) of s, compute the “product” of each structure, and then compute the “sum” of the resulting products; or we can “sum” each collection, then compute the “product” of the resulting structure. Distributivity of “product” over “sum” is the property that these two different methods agree, as illustrated in the following diagram.\nFor example, with adding all the integers in an -structure, and finding the maximum of a (non-empty) collection, the diagram commutes.\nThere’s a bit of hand-waving above to justify the claim that this is really a kind of distributivity. What does it have to do with the common-or-garden equation\nstating distributivity of one binary operator over another? That question is the subject of this post.\nDistributing over effects\nRecall that distributes the shape functor over the monad in its second argument; this is the form of distribution over effects that crops up in the datatype-generic Maximum Segment Sum problem. More generally, this works for any idiom ; this will be important below.\nGeneralizing in another direction, one might think of distributing over an idiom in both arguments of the bifunctor, via an operator , which is to say, , natural in the . This is the method of the subclass of that Bruno Oliveira and I used in our Essence of the Iterator Pattern paper; informally, it requires just that has a finite ordered sequence of “element positions”. Given , one can define .\nThat traversability (or equivalently, distributivity over effects) for a bifunctor is definable for any idiom, not just any monad, means that one can also conveniently define an operator for any traversable unary functor . This is because the constant functor (which takes any to ) is an idiom: the method returns the empty list, and idiomatic application appends two lists. Then one can define\nwhere makes a singleton list. For a traversable bifunctor , we define where is the diagonal functor; that is, , natural in the . (No constant functor is a monad, except in trivial categories, so this convenient definition of contents doesn’t work monadically. Of course, one can use a writer monad, but this isn’t quite so convenient, because an additional step is needed to extract the output.)\nOne important axiom of that I made recent use of in a paper with Richard Bird on Effective Reasoning about Effectful Traversals is that it should be “natural in the contents”: it should leave shape unchanged, and depend on contents only up to the extent of their ordering. Say that a natural transformation between traversable functors and “preserves contents” if . Then, in the case of unary functors, the formalization of “naturality in the contents” requires to respect content-preserving :\nIn particular, itself preserves contents, and so we expect\nFolding a structure\nHappily, the same generic operation provides a datatype-generic means to “fold” over the elements of an -structure. Given a binary operator and an initial value , we can define an -algebra —that is, a function —by\n(This is a slight specialization of the presentation of the datatype-generic MSS problem from last time; there we had . The specialization arises because we are hoping to define such an given a homogeneous binary operator . On the other hand, the introduction of the initial value is no specialization, as we needed such a value for the “product” of an empty “segment” anyway.)\nIncidentally, I believe that this “generic folding” construction is exactly what is intended in Ross Paterson’s Data.Foldable library.\nSumming a collection\nThe other ingredient we need is an -algebra . We already decided last time to\nstick to reductions—s of the form for associative binary operator ; then we also have distribution over choice: . Note also that we prohibited empty collections in , so we do not need a unit for .\nOn account of being an algebra for the collection monad , we also get a singleton rule .\nReduction to distributivity for lists\nOne of the take-home messages in the Effective Reasoning about Effectful Traversals paper is that it helps to reduce a traversal problem for datatypes in general to a more specific one about lists, exploiting the “naturality in contents” property of traversability. We’ll use that tactic for the distributivity property in the datatype-generic version Horner’s Rule.\nIn this diagram, the perimeter is the commuting diagram given at the start of this post—the diagram we have to justify. Face (1) is the definition of in terms of . Faces (2) and (3) are the expansion of as generic folding of an -structure. Face (4) follows from being an -algebra, and hence being a left-inverse of . Face (5) is an instance of the naturality property of . Face (6) is the property that respects the contents-preserving transformation . Therefore, the whole diagram commutes if Face (7) does—so let’s look at Face (7)!\nDistributivity for lists\nHere’s Face (7) again:\nDemonstrating that this diagram commutes is not too difficult, because both sides turn out to be list folds.\nAround the left and bottom edges, we have a fold after a map , which automatically fuses to , where is defined by\nAround the top and right edges we have the composition . If we can write as an instance of , we can then use the fusion law for\nto prove that this composition equals .\nIn fact, there are various equivalent ways of writing as an instance of . The definition given by Conor McBride and Ross Paterson in their original paper on idioms looked like the identity function, but with added idiomness:\nIn the special case that the idiom is a monad, it can be written in terms of (aka ) and :\nBut we’ll use a third definition:\nNow, for the base case we have\nas required. For the inductive step, we have:\nwhich completes the fusion proof, modulo the wish about distributivity for :\nDistributivity for cartesian product\nAs for that wish about distributivity for :\nwhich discharges the proof obligation about distributivity for cartesian product, but again modulo two symmetric wishes about distributivity for collections:\nDistributivity for collections\nFinally, the proof obligations about distributivity for collections are easily discharged, by induction over the size of the (finite!) collection, provided that the binary operator distributes over in the familiar sense. The base case is for a singleton collection, ie in the image of (because we disallowed empty collections); this case follows from the fact that is an -algebra. The inductive step is for a collection of the form with both strictly smaller than the whole (so, if the monad is idempotent, disjoint, or at least not nested); this requires the distribution of the algebra over choice , together with the familiar distribution of over .\nSo, the datatype-generic distributivity for -structures of collections that we used for the Maximum Segment Sum problem reduced to distributivity for lists of collections, which reduced to the cartesian product of collections, which reduced to that for pairs. That’s a much deeper hierarchy than I was expecting; can it be streamlined?","SIZE: 16-30 in. max 39 in.\nDISTINCTIVE FEATURES: 1. Short cirrus centered near tip of snout. 2. Prominent cirrus above each eye. 3. Notch in spinous dorsal fin after third or forth spine.\nMarbled earthtones; can change color. Unscaled bulbous head and stout body. Common S. California and north to southern BC.\nSIZE: 2-3 in. max 4 in.\nDISTINCTIVE FEATURES: 1. Red markings, often encircle a dark spot on first two dorsal fin segments. 2. Broad band of scales between dorsal fin and lateral line. 3. Small cirri on snout and larger ones between eyes, especially on males.\nMottled, spotted and blotched earthtone shades; usually several saddle markings on back. Abundant Alaska and Washington, occasional to rare to S. California.\nSIZE: 1 1/2 - 4 in. max. 6 in.\nDISTINCTIVE FEATURES: 1. Black eye. 2. Black edge on dorsal fin.\nDark to pale tan. Common S. California to central BC.\nSIZE: 3/4 - 2 in. max. 2 1/2 in.\nDISTINCTIVE FEATURES: Brilliant red. 1. Four to nine electric-blue bars.\nForedorsal fin tall, especially in males. Rear body bars thinner. Abundant to common S. and central California.\nSIZE: 6-16 in. max. 2 ft.\nDISTINCTIVE FEATURES: 1. Elongated head with pointed, slightly upturned snout. 2. Forked tail (all other kelpfish have rounded tail).\nColor and markings vary greatly, includes shades of yellow. Common S. California, rare to the north.\nSIZE: 2 - 3 1/2 in. max. 4 in.\nDISTINCTIVE FEATURES: 1. Foredorsal fin same height as soft dorsal and first few spines have flexible bent tips. 2. Rays of raised rear dorsal fin evenly spaces. 3. Pale rounded blotch extends onto cheek from lower rear quarter of eye.\nBlotched, barred and striped in shades of red to maroon, lavender, orange, tan; tiny white to pale spots cover body; color change to match background. Abundant around islands in S. California; occasional coastal areas in S. California.\nSIZE: 4-6 in. max. 10 in.\nDISTINCTIVE FEATURES: 1. Five to six bold, dark (usually red) bars encircle fins and body. 2. Pointed snout. 3. Two pair of cirri - one above each eye and another midway between eyes and dorsal fin.\nUndercolor pale, red bars often shaded with brown. Mature males often turn nearly black during winter mating season. Occasional southern California to BC.\nSIZE: 10-18 in. max. 2 ft.\nDISTINCTIVE FEATURES: MALE: 1. Blue irregular spots on head and forebody, outlined by a few small, dark reddish brown spots. FEMALE: 2. Speckled with red-brown to gold spots over bluish white, fins generally yellow.\nPair of small cirri above eyes and another tiny pair between eyes and dorsal fin. Five lateral lines. Abundant to common Alaska to central California, rare S. California.\nSIZE: 1 1/2 - 3 1/2 ft. max 5 ft.\nDISTINCTIVE FEATURES: 1. Large mouth with prominent canine teeth. 2. Long, even spinous dorsal separated by notch just before soft rear dorsal.\nNumerous dark spots and several darkish blotches. Can lighten and darken with background. After mating, males guard masses of white eggs; guarding males often dark black. Occasional Alaska through California.\nSIZE: 6-12 in. max. 14 1/2 in.\nDISTINCTIVE FEATURES: 1. Dark crescent-shaped marking on tail near base followed by large spot form \"C-O\". 2. Usually dark prominent midbody spot with pale patch behind. 3. Large, protruding bulbous eyes near tip of snout.\nMottled, spotted and blotched in rich shades of brown with occasional white spots. Occasional S. California to SE Alaska.\nSIZE: 1-2 ft. max. 3 ft. (terminal phase male), 6-12 in. (initial phase)\nDISTINCTIVE FEATURES: 1. White chin in both phases. TP: Red to orange or pink midbody; dark head and rear body. IP: Pink to reddish brown.\nTwo distinct color phases, terminal and initial. Older TPs develop a bulbous limp on nape. Occasional to uncommon central California; formally common but numbers greatly reduced by overfishing, especially spearfishing of large TP.\nSIZE: 3-8 in. max 10 in.\nDISTINCTIVE FEATURES: Yellow to orange to orange-brown. 1. Large black spot on tail base.\nWhite belly. Cigar shaped. Abundant to common S. California; occasional to N. California.\nSIZE: 12-14 in. max. 15 in. (terminal phase), 5-12 in. (initial phase)\nDISTINCTIVE FEATURES: TP: 1. Dark bar behind pectoral fin. IP: 2. Dark areas on scales form spotted stripe on upper side.\nTP generally >12inches, green to lue-green and occasionally orange. IP typically between 5 and 12 inches, yellow to orange or orangish brown. Occasional S. California.\nSIZE: 4-8 in. max. 1 ft.\nDISTINCTIVE FEATURES: Blue-gray. 1. Black spots on scales primarily scattered from mid-body to tail.\nUsually blue border on dorsal, anal, and tail fins. Typically swim in open water in large aggregations. Abundant to common S. California, occasional central California.\nSIZE: 2-10 in. max. 14 in.\nDISTINCTIVE FEATURES: Brilliant orange. 1. Display numerous iridescent blue spots until about 6 inches long.\nThin, oval shaped body; tail deeply notched between large rounded lobes. Aggressively territorial. Protected from any type of harvest as the California state marine fish. Abundant S. California; occasional to rare central California.\nSIZE: 5 - 12 1/2 in. max. 15 1/2 in.\nDISTINCTIVE FEATURES: 1. About nine dusky bars on body 2. Narrow blue stripe along base of anal fin. 3. Patch of large scales between pectoral fin and ventral fin.\nThin-bodied; football shaped profile. Silvery, tinted with shades of orange to reddish brown. Large dark lips. Occasional central to S. California; uncommon to rare N. California.\nSIZE: 3 1/2 - 7 in. max. 8 1/2 in.\nDISTINCTIVE FEATURES: 1. Snout pointed with upward jutting lower jaw. 2. Darkish areas on scales align to form several dusky stripes above midlateral line. 3. Head concave above eyes.\nThin-bodied.. Silvery with coppery sheen, often with bluish spots. Abundant to common N. California to B.C.; uncommon central and S. California.\nSIZE: 6-14 in. max. 17 1/4 in.\nDISTINCTIVE FEATURES: 1. Dusky to dark bar below front portion of soft dorsal fin. 2. Black spot behind corner of mouth.\nThin-bodied; football-shaped profile. Silvery gray, occasionally brownish. Deeply forked tail. Common central California to B.C.\nSIZE: 6-20 in. max 26 in.\nDISTINCTIVE FEATURES: 1. One to three white spots on back. 2. Bright blue to blue-green eyes.\nRounded, football shaped profile. Olive-green, often shaded with blue. Common S. California; occasional to uncommon north to Oregon.\nSIZE: 3-4 in. max. 6 in.\nDISTINCTIVE FEATURES: 1. Black eye ring. 2. Black bar from mid-dorsal fin to base of tail.\nSilver head; silver-yellow body. Black ring around snout. Black bar from in front of dorsal fin to above eye."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:abc7f3af-135f-4b6e-a5f4-e99b032ffec3>","<urn:uuid:b4e622e9-4b11-4ce8-82b4-9b4b00a5d310>"],"error":null}
{"question":"What methods can be used to remove the old dark spots on the wood when replacing an axe handle?","answer":"The dark spots where the helve rubs against the eye can be removed in several ways. For beginners, using a four-in-hand or wood rasp is the best and easiest option. More experienced users can use various sanding methods. Professionals often recommend using a five-inch disc chucked into a drill with sticky-back sandpaper - 80-grit for heavy-duty removal or 120-grit for wood removal. The sanding should begin right below the bottom of the rub line and move toward the shoulder.","context":["If you regularly use an axe, you’ll find at some point that you’ll need to learn how to replace an axe handle. If you break the handle, you may be tempted to replace the axe, but replacing the handle itself is actually a simple process and one that will allow you to get plenty of more use out of your favorite tool. In this guide, I’ll walk you through the process and explain just how easy handle replacement is.\nRemoving the Old Handle\nBefore you can install the new handle, you need to remove the old one. Basically, for this step, you’ll need to cut the old handle off right below the eye, driving the wood out in the same direction that it was inserted. Make sure that you wear eye protection during this process.\nOnce the handle has been removed, now is the time to clean the axe head and closely check it for any flaws, before you install the new handle.\nFitting the New Handle\nThe next step is determining how the new handle will fit onto the head of the axe. If you shove the handle into the eye of the axe does it slide to the shoulder of the handle? Are you feeling some resistance because the handle is a little larger than the eye?\nIn order to get the handle fixed onto the head you’ll need to know how much material needs to be removed. You can figure this out by using a piece of paper and placing it over the eye. Next, rub the paper with your thumb. Old dirt and oil from your hands will show the edges, giving you the shape that you want for the replacement handle. This clearly shows you how much bigger the new handle is than the eye.\nBegin by removing a little material, slowly, using a rasp or draw knife. Continue this process until the handle begins to fit. Once the handle can fit into the eye just a little, you’ll want to turn the head upside down, hammering the haft’s bottom using a hammer or wooden mallet. The axe head should slowly begin to move up the handle. Expect it to stop going at some point.\nThe next task is getting the head off the haft, which can be accomplished in a couple of ways. The first method you can try is hitting the head of the axe with a dead blow hammer or a small mallet. You’ll strike the bottom until it slides off the handle. Another option is clamping the head in a vise, then placing a drift on the helve, followed by hammering the drift until the shaft slides out.\nOnce the handle pops out, you’ll notice some dark spots on the wood. This will show you where the helve is rubbing against the eye. This discoloration is totally normal. Basically, the dark spots need to be removed in order for the head to continue to move down towards the shoulder. In some cases, you’ll find that the insides of the head are clean, which can make it tricky to see where the eye is rubbing. However, in most cases, the axe will have some rust inside, so you’ll be able to identify the spots easily.\nBegin removing the wood that’s below the rubbed spots. You can do this in a number of ways. If you’re new to the process, then the best and easiest option is to use a four-in-hand or a wood rasp. If you have experience, then you can try a variety of sanding methods.\nMany pros recommend using a five-inch disc that will chuck into a drill and can be combined with sandpaper with a sticky back. Eighty-grit is ideal for heavy-duty removal, yet one hundred and twenty grit will work the best for wood removal. Begin right below the bottom of the rub line, sanding toward the shoulder.\nIf the handle you’re using has more of a pronounced shoulder, then you can sand it off in order to create a wedge effect as the head slides down the shaft.\nKeep working on getting the head down the helve until you’re right by the line. Once the head is at the right point, you’ll notice that a portion of the handle will protrude from the top. Here, you’ll trace the top of the eye using a pencil before you prepare the handle for the kerf cut.\nThis step will take plenty of patience. The work here will be slow going, however, it’s necessary to take your time here in order to avoid damaging your axe and to ensure the proper fit.\nPrepping the Kerf\nOnce the head has moved far enough down the handle it will be time to ensure the wedge and kerf cut are ready. You can remove the handle and check out the pencil line from the last step. If you were to make a cut using this line, the top of the haft would rest flush with the top of the axe eye. Some people like the handle to slightly protrude around a quarter of an inch. Take a ruler and measure this length above your line, creating a new cut line. You can use a coping saw to cut the line to make it simpler, allowing for a more accurate kerf depth.\nThe cut down the centerline of the handle is called the kerf. This is where the wooden wedge will be driven. Once the helve has been cut, take a look at the depth of the kerf cut and what its relation is to the line where the head stops. Most people prefer the kerf to be around 2/3s of the depth of the head in order to promote a stronger wedging. Be sure to mark each side of the kerf at the correct depth, cutting to that line. You can use a saw that’s slightly wider for this step since a wider kerf is preferred over a narrow one.\nInstalling the Wedge\nCorrect wedge installation can make the entire handle replacement process, or break it. Because of this, it’s very important that you take some extra care here. The majority of the time the wedge that comes with a handle will be the right size. You’ll size up the length of the wedge in relation to the length of the eye, marking the length where the wedge will fill the eye completely. You will then start trimming any excess material either by using a belt sander or with the help of a coping saw.\nThe next step is measuring the depth of the wedge in relation to the depth of the kerf, then drawing a line across. This will be the line that sits flush with the handle if it bottomed out.\nyou can use some wood glue on your wedge to prevent the wedge from backing out. However, it can make it difficult if the wedge ever needs to be replaced. There are pros and cons associated with using glue, however, many people have found that using the glue works the best. You can now pound the head of the axe back onto the helve, clamping it in a vise. Make sure you evenly apply a layer of glue on each side of the wedge, then hammer it using a wooden mallet.\nOnce the wedge has been installed you’ll need to give the glue a couple of hours to dry, then you can trim off any excess wedge that’s close to the haft using a coping saw.\nOiling and Sanding\nNow you’re ready to oil the handle. You can use one hundred and twenty grit sandpaper, sanding the haft by hand. Doing so will smooth it out, will remove oil from your hands, and allows oil to be easily absorbed into the wood. The type of oil you use is entirely up to you, but linseed oil can be a great option since it’s very effective and affordable.\nWear some disposable gloves and pour a little oil into your hand, rubbing it liberally over the length of the handle and the head. Allow it to dry for several minutes and then remove any excess oil with a paper towel.\nNow that you have a new handle for your axe, you’ll want to put it to the test. You can do so by seeing how your axe performs in a variety of conditions.\nLearning how to replace an axe handle consists of many steps, but it’s a fairly simple process after you’ve done it the first time. If you want your new handle to last, then make sure you store it in a dry warm place. However, make sure the location isn’t too hot or dry since this can cause the handle to shrink. In wet weather, your handle will be susceptible to rot, and the head can develop rust over time. Always take the time to properly store your axe. Doing so will significantly lengthen the lifespan of both the axe head and the handle, saving you the time and work that comes with restoring your axe to its former glory."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:e34ba18a-63bc-4229-9292-903a6fd53f89>"],"error":null}
{"question":"How do the investor protection mechanisms compare between Colombia-UK investments and China's Belt and Road Initiative projects?","answer":"Colombia-UK investments and China's BRI projects have different protection mechanisms. For UK-Colombia investments, there is a Bilateral Investment Treaty (BIT) that provides compensation for losses due to war or armed conflicts, guarantees fair compensation in case of expropriation, and includes a dispute resolution clause allowing investors to call for an arbitration panel. For Chinese BRI investments, protection is provided through Sinosure, a state-funded insurance company that offers Overseas Investment Insurance covering up to 95% of investments for risks related to expropriation, war, political riot, breach of contract, and exchange risks, with a maximum tenor of 20 years.","context":["25 May Legal framework for investments between the UK and Colombia\nColombian economy is the third most stable in south and Central America. The World Bank has described Colombia as a top country in Latin America and sixth in the world for investor protection and it has the second most flexible labor market in Latin America (Banco de la Republica). It is recognized as being highly diverse, due to the abundance in natural resources. The signature of the peace treaty promises to correct historically large problems such as drug trafficking and terrorism. With the country’s new adjustments, large areas of land have been introduced into the market, mostly for agriculture, renewable energy and mineral extraction.\nEnglish companies have been present in Colombia since the 1920s and with the modernization of the Colombian economy in the first years of the 21st century, new opportunities were opened in the service sector with British companies, mostly insurance, consulting, auditing and risk management firms.\nSince 2002 the UK has become the second largest investor in Colombia after the United States. During this period, fifty-five companies have provided 9,000 employment positions in different regions of Colombia. The main areas of British investment are:\n- In the pharmaceutical sector, large companies like Astra Zeneca and Glaxo Smithkline operate.\n- In consumer goods, multinationals in the market of soft drinks, sweets and confections, such as Cadbury.\n- In services and consultancy companies like Compass, baker Tilly, price waterhouse coopers, Steer Davies Gleave. It is also important to mention that Ernst & young has developed in Colombia one of its best markets in South America.\nColombia and the UK have a fair and transparent legal framework for the promotion of investment that includes minimum standards recognized by international law. Colombia and UK have signed two treaties:\n- A bilateral investment treaty (BIT) intended to create and maintain favorable conditions for the investments of investors of one Contracting Party in the territory of the other Contracting Party.\n- An agreement to avoid double taxation.\nThe BIT is an article that compensates British investors in case their investment is lost to acts related to war, which means that one of the biggest fears in the investing process with Colombia is covered by an international agreement. The treaty’s article specifies that in case of loss occurred, due to war or armed conflicts, investors shall receive from the host country, as regards restitution, compensation or other arrangement, no less favorable than that which that Contracting Party grants to its own nationals or companies or to nationals or companies of any third State. The resulting payments shall be freely transferable and payable in a freely convertible currency.\nOne of the fundamental points of the treaty is the resolution of disputes clause. If there is a disagreement between the investor and the host state, the investor is in every right to call for an arbitration panel where investor and state will resolve their dispute in equal footing, that is, where the state is divested from is sovereign immunity.\nAccording to article 6 of the BIT, expropriation is prohibited without fair compensation. Expropriation is only permitted if the investment affects public or social interest, but there is a fundamental requirement of fair compensation. To grant the genuine value of the investment, the judge who grants the compensation must see the value of the investment immediately before the expropriation measures are taken or publicly communicated. The affected company shall have the right to prompt review by the judicial authority.\nIt is of great value for investors to understand that the BIT and the Colombian judicial system, guarantee national treatment for them. The foreign investor shall not be treated less favorably than a national investor. Colombia reserves the right to create or maintain restrictions regarding the granting of national treatment in the sectors of public services, supply of goods and services of the public sector, automotive assembly and a few others. Lastly, investors should know that they can receive payments in pesos (Colombian currency) but are free to transfer it into pounds without any interference or restriction.\nThe second treaty, fundamental for investment relations, is the agreement to avoid double taxation between Colombia and UK. The instrument aims to avoid double taxation on Dividends, capital gaining, royalties and labor income. Further information of this treaty will be developed in a future article.\nJose A. Abusaid","China’s Overseas Investment Insurance – Sinosure\nFor the majority of projects along the Belt and Road Initiative (BRI) four layers of actors can be identified: Lender, Credit Insurance, Construction Company, and Project Developer. The lenders are often familiar names such as China Development Bank (CDB), Industrial and Commercial Bank of China (ICBC) or the Export-Import Bank of China. The provider of Credit Insurance – Sinosure – however is a rather unknown name despite its wide appreance among the BRI projects. Nevertheless, Sinosure is a significant player in the making of projects along the BRI since it provides a safety net critical to Chinese investors. Therefore, we want take a closer look at the Credit Insurarer Sinosure and its engagement within the BRI.\nWhat is Sinosure?\nThe China Export Credit & Insurance Corporation (Sinosure) is a state-funded insurance company established in 2001 – shortly after China’s accession to the World Trade Organization (WTO). Sinosure was established by merging the Export Credit Insurance Departments of the People’s Insurance Company of China (PICC) and the Export-Import Bank of China (Exim) in order to support China’s foreign and trade development and cooperation. It is China’s only national policy-oriented export credit insurance agency and seeks to encourage Chinese enterprises to invest overseas. Sinosure’s main products and services include medium and long-term export credit insurance, overseas investment insurance, short-term export credit insurance, domestic trade credit insurance, bonds and guarantees related to export credit insurance, accounts receivable management, and information consultation.\nThe company states its own mission and responsibility to support the Belt and Road Initiative, Made in China 20205 as well as Small Businesses. The business principle of Sinosure is: “By means of export credit insurance against non-payment risks, promote Chinese exports of goods, technologies and services, especially high-tech and high-value-added capital products such as mechanical and electrical products, thus further support foreign trade and economic cooperation, economic growth, employment, and international balance of payments.”\nAs of the end of 2017, the cumulative domestic and foreign trade and investment supported by Sinosure exceeded USD 3.3 trillion. As of 2017 the company has provided about 70,000 companies going abroad with insurances, risk management and financing services. According to Sinosure claims paid amounted to USD 10.8 billion by the end of 2017. Sinosure also cooperated with more than 200 banks to facilitate export finance and helped export companies obtain loans. Over the years Sinosure has received multiple capital injections through the Central Huijin Investment Ltd., a state-owned investment company, which are understood as a endorsment of Sinosure’s support of Chinese companies going out strategies.\nSinosure has over the years reached various agreements with international companies. Here some examples: In 2016, Sinosure and Euler Hermes Kreditversicherung, concluded a cooperation agreement in the field of state export credit guarantees. In 2011 Sinosure and J.P. Morgan reached an export credit insurance agreement. In 2009 Deutsche Bank signed a Memorandum of Understanding (MoU) with Sinosure to strengthen cooperation between both organizations on Sinosure’s guaranteed products, which support the cross-border trade for Chinese companies. Already in 2008 Export Credits Guarantee Department (ECGD), the UK’s official export credit agency signed an agreement with Sinosure.\nThough Sinosure receives a lot of government support it is not immune to losses or corruption allegations. In 2008 the Head of Sinosure, Tang Ruoxin, was removed from post and party membership and arrested in August 2008 over allegations of accepting bribes and later sentenced to 14 years in prison. The decision was made by the China Insurance Regulatory Commission (CIRC), China’s Insurance watchdog. Tang Ruoxin, who has been in the top position of the company since its establishment in 2001 and served as Communist Party Secretary, allegedly offered credit guarantee to the Shanghai Hongsheng (Norcent) Technology Co, who lied to the bank and defaulted on a USD 245 million bank loan. According to CIRC huge government assets were lost due to Tang’s abuse of power. In 2011, after the uprising in Libya, Sinosure had to pay high insurance claims to Chinese companies with investments in the country. Also in 2013, Dai Chunning, a deputy general manager at Sinosure was detained for investigation for serious discipline violations. The investigation concluded in June 2014 that Dai “embezzled massive amounts of public funds with his associates, took massive bribes personally or in association with others, abused his position of power for his personal interests and those of related companies” and that he “committed adultery.”\nSinosure and the Belt and Road Initiative\nChinese financial institutions have so far provided a total of more than USD 440 billion for Belt and Road projects, Yi Gang, governor of the People’s Bank of China, said in a speech at the second Belt and Road Forum in April 2019. As China’s only national policy-oriented export credit insurance agency, Sinosure actively and openly supports the foreign activities of Chinese companies and thus the Belt and Road Initiative and has provided insurance to the most part of the aforementioned USD 440 billion.\nMore often than not, Sinosure’s involvement in a project is what gives it the green light. The safety net Sinosure offers is critical to Chinese investors, especially in markets with high uncertainties. Banks would rarely say yes to an overseas project without the “Go” from Sinosure to assure that political and market risks associated with projects far away from home are covered.\nSinosure offers a variety of Insurance Products, ranging from Export Credit Insurance and Overseas Investment Insurance to Bond Guarantee. Most important for BRI projects are the Export Credit Insurance as well as the Overseas Investment Insurance. The Overseas Investment Insurance is available for equity and debt investments made by Chinese enterprises in projects outside of China. If the equity/debt receiving project company faces payment defaults, Sinosure will reimburse the Chinese equity shareholder and/or loan provider respectively. The maximum insured percentage of the equity or debt provided is 95 percent with a maximum tenor of 20 years. Sinosure covers risk related to expropriation, war and political riot, breach of contract as well as exchange risks. Both overseas project companies financed by Chinese investors as well as overseas investment projects of Chinese companies are eligible to be covered by the Overseas Investment Insurance. The Export Credit Insurance covers risks in relation to the collection of deferred payments. The tenor is between 2 to 15 years and the maximum insured percentage is 90 percent.\nAccording to the Sinosure official website the company has, by the end of the third quarter of 2017, covered about 1,300 projects along the BRI. The total insured amount on export and investment to Belt and Road countries, by the end of 2017, is stated at nearly USD 510 billion with nearly USD 2 billion paid out in claims and more than 2,300 issued insurance policies.\nSinosure has reached multiple agreements with international private sector actors along the lines of supporting the Belt and Road Initiative. Just in April 2019, Sinosure reached multiple agreements with international companies. It has signed an agreement with the Singapore-based Oversea-Chinese Banking Corporation Limited (OCBC) to support Green Belt and Road Projects such as renewable energy, pollution prevention, clean transport, sustainable water management, and green buildings. Also in April 2019, it signed an MoU with the Standard Chartered Bank to deepen collaboration to facilitate the Belt and Road Initiative as well as a cooperation agreement with the Development Bank of Singapore (DBS Bank).\nOverseas investment, especially in developing countries, means risks and uncertainties. Sinosure offers a safety net for Chinese companies operating abroad and is therefore a significant actor for BRI projects. Sinosure’s importance, also for coining future BRI projects, cannot be underestimated since without Sinosure’s “Go”, the future of a propossed project can be at stake.\nSinosure and its role for the BRI hasn’t received much attention. But the insurance companies activities should be watched closer. For one Sinosure could have the capability to influence the types of new projects that will emerge along the BRI corridors. As of now Sinosure does not pay closer attention to environmental risks. Therefore, there is a big potential of positively influencing Sinosure in this reagrd. In addition, Sinosure is also involved in projects with opaque awarding practices. Especially with regard to the corruption allegations of the past, Sinosure should be lobbyed towards more transparent standards and practices. Consequently, it is important to pay closer attention to China’s only policy-oriented export credit insurnce agency."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:e910ce24-b10d-4a9a-a21c-e5b3a5798937>","<urn:uuid:efc83c89-a5a8-45af-932b-88b72ad3ab35>"],"error":null}
{"question":"Amateur potter here! ¿Los artesanos de Kurinuki y los maestros del Torno alfarero necesitan el mismo tipo de arcilla para sus creaciones? Need to know for my next piece.","answer":"While both Kurinuki and wheel-throwing techniques require clay with proper moisture content, their specific requirements differ. For Kurinuki, the clay must be soft and moist enough to carve easily, but not too wet to prevent cracking during the carving process. For wheel throwing, the clay needs to contain some water to maintain plasticity - enough to allow the clay sheets to slide past each other when force is applied on the wheel, but not so much that it becomes too soft and unworkable or turns into slip. In both cases, the clay's plasticity comes from water molecules between clay layers connected by hydrogen bonds, allowing the clay to take and hold shape.","context":["DISCOVER THE ART OF KURINUKI\nCeramics is one of the oldest art forms in the world. Since prehistoric times, mankind has used clay to create utensils, decorative objects and works of art. The Kurinuki technique is one of the most unique and recognized ceramic techniques used in Japan. Through this technique, artisans can create unique and fascinating pieces of great beauty and simplicity. You will learn about a magical sample of Japanese ceramics, its history, characteristics and more. Join us!\nWHAT IS KURINUKI\nIt is a Japanese ceramic technique that involves carving the desired shape into a piece of clay. The name kurinuki can be literally translated as “to hollow”.\nA single lump of clay is used which is carved with the appropriate tools to achieve the desired shape. Instead of creating a work from individual pieces of clay, as is done in some other techniques, the piece is carved from a single block of clay. The result is frankly beautiful.\nTea bowl made with the Kurinuki technique. Made by\nSarah Maelle Ceramique.\nWHAT DOES THE KURINUKI TECHNIQUE CONSIST OF?\nThe Kurinuki technique begins with the selection of the right block of clay. The artisan must ensure that the clay is soft and moist enough to carve easily, but not too wet to prevent cracking during carving.\nOnce the clay is selected, it is cut into the desired basic shape and carved using different tools, such as clay knives, needles and spatulas. As the piece takes shape, small amounts of clay are removed to achieve the desired design.\nArtisans use this technique to create organic shapes and unique textures on the surface of the bowl. The Kurinuki process involves sculpting the clay from the inside of the bowl to the outside, rather than shaping the form from the outside to the inside, as is done in other ceramic techniques.\nFinally, the part can be smoothed and polished to obtain a smooth and uniform finish.\nArtist: Ewe Suchanek\nYUNOMI TYPE KURINUKI BOWL\nArtist: Chanoyu Ceramics\nSMALL YUNOMI STYLE CUP\nArtist: Clarochelle Ceramiste\nLEARN MORE ABOUT ITS ORIGIN AND USES\nThe Kurinuki technique dates back to medieval Japanese times, where it was used to make ceramic objects for the tea ceremony. During the Edo period (1603-1868), the technique was extended to the production of other everyday objects, such as bowls and plates. Despite the popularity of the technique, most objects produced with Kurinuki were made for personal use and were not widely marketed.\nDue to the special nature of this technique, each resulting piece is unique and no two pieces can be made alike. This uniqueness has led many artisans to embrace the Kurinuki technique as an art form to express themselves with unrepeatable creations.\nOne of the most popular applications of this technique is in the creation of Yunomi bowls, which are used in the Japanese tea ceremony.\nThese ceramic bowls have a cylindrical shape and are sometimes wider at the base than at the mouth. They are used for drinking tea and are held with both hands while drinking. Because of their ceremonial use, these pieces are of great cultural importance in Japan.\nAnother type of bowls that are made with Kurinuki are the Chawan, these are less cylindrical and wider.\nArita Porcelain: A priceless marvel of ceramicsWelcome to a journey full of charm that will take you through the fascinating history of Arita pottery. In this article, I invite you to explore the curious aspects that make this Japanese technique a...\nAlfred GuinaroanModeling and creating from the infinity of the raw material captivated me since childhood, a few years ago now, in a small mountain village in Barcelona. My parents are not Spanish, but they fell in love with the beauty of the area and there I was...\nThe Art of Kintsugi: The Inspiring Japanese TechniqueWe are going to explain the secrets of Kintsugi. A technique used in the repair of objects that has become a whole philosophy of life that more and more people are falling in love with. Since...\nKintsugi repair kit: EVERYTHING for this Art in gold.Kintsugi repair kits with gold or other materials such as silver, make it possible for you to practice this art at home, as they allow you to create an artistic piece from a broken object, with...\nNote: This article contains affiliate links that lead to the artists’ stores outside of the Ateologic website. If you buy something from them, we will receive a small commission that will help us to continue our site and in turn continue to support those artists. This of course does not affect the selling price.","Pottery vessels have been made for around 18,000 years. But how does clay extracted from the earth become a colourful pot, and what's the chemistry behind the process?\nThe process of firing a pot creates crosslinks between the hydroxyl groups in the clay\nOxides of the first row transition metals are the main sources of colour in pottery glazes\nMaking things from burnt clay has been part of human experience for many thousands of years. A small figurine of a woman is the earliest known object made of fired earth, dated to almost 30 000 years ago. The earliest known example of a pottery vessel was made around 18 000 years ago.1 Since then, the craft of pottery has developed in all parts of the world, both for the practical purposes of making usable vessels for food and storage, and as expressions of the instinct for art and ritual. About 7000 years ago the Egyptians discovered the art of glazing their pots. Subsequently the Chinese steadily improved kilns and so it was possible to produce more and more highly decorated stoneware and porcelain.\nThe immense strides in the making of pots were the result of patient trial and error by thousands of potters over thousands of years. A scientific approach to the process only became possible in the past two centuries, initially in establishing the compositions of the materials used. More recently, with the development of modern analytical techniques, the elucidation of their structures has been possible. Although much is now known about the materials and their structures, there are so many variables and the structures so complex, that the empirical approach still largely dominates pot making.\nMaking a pot\nForming a pot\nThere are many ways of forming a pot. Hand building is the earliest method and is still widely used. The potter's wheel was invented about 7000 years ago and has become the method of choice for many potters. In all methods it is essential for the clay to contain some water; not too much, because that makes the clay too soft and unworkable, or even turns it into 'slip' - a dispersion of clay in water. The essence of the plasticity is that the clay takes the shape given to it by applying a force and it keeps that shape unless some other force acts on it.\nThis property is readily understood if we consider that the layers are separated by a thin layer of water molecules which are linked to neighbouring layers via hydrogen bonds. These are weak but still significant forces. They are weak enough to allow the clay sheets to slide past each other when some force is applied, but strong enough to keep them in position once the force is removed. In other words, clay can be moulded into a shape, which it then keeps.\nAs the clay dries, water molecules escape from between the clay sheets, so these move closer together (the clay shrinks by 5% or more). The kaolinite hydroxyls become hydrogen bonded to the next layer, forming a stronger, firmer structure ('green ware'). If at this point the clay object is put into water it will disintegrate and can be returned into a workable state.\nThe feldspar group of minerals comprise around 60% of the earth's crust. They are aluminium silicates, also incorporating alkali and/or alkaline earth metals. A typical example is orthoclase, whose approximate composition is given as K2O.Al2O3.6SiO2. Over the geological timescale a great deal of feldspar has been eroded through a weathering process, mainly through the action of water. Although these rocks seem solid and eternal, over millions of years the effect of rain, made slightly acidic by dissolved CO2, does dissolve some of the alkali and alkaline earth metal oxides leaving the silicon and aluminium oxides:\nK2O.Al2O3·6SiO2 + 2H2O → Al2O3.2SiO2.2H2O + K2O(SiO2) + 3SiO2\nFeldspar a clay mineral in solution in the clay\nThe clay is either found where it had been formed or it can be carried by rivers and deposited elsewhere. When transported by water the particles continue to be ground finer and finer by the action of other rocks. They are also separated by size according to what settles out first. As a result, clay is a major component of soil all over the world, with a variety of properties according to the precise conditions that applied during its formation.2\nCrystallographic studies have established that the clay minerals are composed of sheets of tetrahedral silicon dioxide (SiO2) and octahedral aluminium oxide (Al2O3) linked through bridging oxygen atoms. On the aluminium oxide surface of the 'sheets' some of the oxygens are in the form of OH groups and there are OH groups within the structure as well. Broadly speaking, there are two main categories of clay minerals: those with one sheet each of the silicon and aluminium oxides and those with two sheets of silicon oxide enclosing a sheet of aluminium oxide.\nThe important mineral in pottery is kaolinite, which contains 1:1 silicon to aluminium oxides. The crystal structure shows plate-like particles, which are stacked in layers linked by hydrogen bonds. It is through the structure, properties and transformations of kaolinite that we can understand the physical changes involved in the making of a pot. Although there are deposits of virtually pure kaolinite, in practice it is always used as part of a mixture with other minerals, either because it is dug out of the ground in an impure state and used directly or because it is blended with other minerals (eg feldspar and quartz) to achieve the desired properties.2\nFiring the pot\nThe dry pot is then heated to drive off some more water. Once its temperature reaches around 500ºC, the changes in it have become irreversible. At this point the clay is very fragile and crumbly, but it can no longer be reconstituted into the original workable state. This stage is described as the driving off of the so-called chemically bound water:\n[clay]-OH + HO-[clay] → [clay]-O-[clay] + H2O(g)\nThe weak hydrogen bonds are replaced by stronger and shorter oxygen bridges (the clay may shrink a little further). When this happens the clay can no longer be recycled. Linking the neighbouring clay particles is a gradual process and if the firing is stopped at around 500ºC, enough of these crosslinks will be formed to prevent recycling, but not enough to strengthen the piece. At the same time the regular sheet-like crystal structure of kaolinite is being lost and amorphous metakaolinite is formed.3\nGenerally, the pot is first fired to about 1000°C to produce what is known as 'biscuit ware' with very slight further shrinkage. Biscuit ware is quite strong and porous; it readily absorbs water and dries again very easily. It is glazed by spreading a suspension of the glaze solids in water over the pot by pouring, dipping or spraying, and when it is dry, firing it again at the appropriate temperature for the clay and the glaze.\nStoneware vs earthenware\nPots can be classified according to the temperature they have been fired at - earthenware (1000-1150ºC), stoneware and porcelain (>1200ºC). In every case the clay composition has to be so that at the 'maturing temperature' it begins to vitrify and the partial melting of some of its components provides the 'glue' to provide its strength.\nOther chemical changes take place during firing. These include burning off all organic matter often found in many clays, the decomposition of carbonates, which are common ingredients of many glazes, and further crosslinking of metakaolinite to give a three-dimensional network with the elimination of water. This process does not go to completion up to earthenware temperatures,4,5 but at stoneware temperatures all water is gone. It is difficult to believe that water is present in pots fired to earthenware temperatures, but easy to demonstrate:\nTake two cups, one earthenware and one stoneware, and put water in both. Put them into a microwave and run it at full power for 2-3 minutes. The water in both should be hot; the handle of the earthenware cup will also be hot, while that of the stoneware cup will be cold. Since microwave ovens heat water by causing water molecules to move faster, the hot handle on the earthenware cup indicates the presence of free, mobile water molecules.\nAt stoneware temperatures, the metakaolinite undergoes transformation into mullite (3Al2 O3.2SiO2) which forms needle-like crystals, while the feldspar present melts into a glass, binding the mullite crystals together. These two structural changes account for the much greater hardness and strength of stoneware over earthenware.\nMost pots are glazed, ie they are covered by a thin coating of glass. This can be for aesthetic or for practical reasons, usually both. It is particularly important for pots holding food. The glaze usually has three main components:\n- silicon dioxide to provide the main body\n- aluminium oxide to enhance the viscosity of the glaze by crosslinking the silica networks\n- fluxes, generally alkali or alkaline earth metal oxides, to lower the melting point of the mixture to the temperature of firing.2\nIn addition, it is common to include transition metal oxides to provide colour to the glaze.\nA potter needs to consider three important properties of a glaze. These are the texture (rough or smooth), opacity (clear or opaque) and colour. The first two are best considered together in terms of the melting properties of solid mixtures.\nA generalised phase diagram (fig 1) illustrates the issues. Let us consider a mixture of composition indicated by the red line; when it reaches the temperature TE, it begins to melt. As the temperature rises the proportion of solid diminishes and the proportion of liquid increases until the last of the solid melts.\nPoint '1' corresponds to a glaze fully melted at the maturing temperature. Such a glaze is used when all the decoration has been done before glazing and if a smooth, shiny surface is required (fig 2).\nPoint '2' marks the maturing temperature when an opaque glaze is required; this should have a smooth surface (the glaze is mainly liquid), but it should include some solid to scatter the light and provide the opacity. In practice, the temperature in a kiln can vary considerably from the nominal value, which may complicate matters; fig 3 illustrates this well. The cup shown was made to have raised opaque spots on it, but it was placed too close to the heating elements, so the temperature range experienced by the cup straddled the liquid line. The spots on the right of the image were at '1' and those on the left were at '2', as intended.\nThe temperature indicated at point '3' produces a glaze that is still mainly solid with just a small portion melted. Such glazes feel rough to the touch, since the liquid is only enough to stick the solid components together and to the object; these are not recommended for use with food.\nWhile the phase diagram illustrates the general phenomena, it represents a gross oversimplification of the real situation with glazes for two main reasons. One is that most glazes comprise more than two components, and the other is that the diagram presupposes that no chemical changes will take place to or between A and B. This is hardly ever the case, so any phase diagram representing the behaviour of a real glaze would be far more complicated.\nThe main minerals comprising the glazes are colourless. Both SiO2 (quartz) and Al2O3 (corundum) are known in nature in their pure states as white crystalline solids. They are also found in contaminated forms: amethyst and citrine are quartz contaminated with Fe, and ruby is corundum contaminated with Cr.When the contaminants in corundum are Fe, Co, Ti and V they are known as sapphires of various colours. In most glazes the colour is provided by oxides of the first row transition metals; in addition to those already mentioned, copper is also widely used. In pottery, the most common colouring oxides are those of iron, copper and cobalt. Of these, iron seems to be the most versatile; depending on the firing conditions and on what else is present in the glaze, it can give rise to red, yellow, brown, blue and green in various shades.\nUnsurprisingly, the two dominant variables are the oxidation state and the environment of the transition metal ion. Potters using kilns heated by wood, gas or oil, have the option of using reducing conditions for part of the firing. The oxygen supply is restricted and the atmosphere in the kiln becomes rich in CO. If transition metals are present in low concentration in the glaze, they can be reduced to a lower oxidation state. The cup (fig 4) with iron glazes was fired under reducing conditions. The glaze on the inside contains 0.5% iron(III) oxide and the glaze on the outside 10%; the reducing power of the CO was enough to convert the iron in the inner glaze into Fe(II), but the concentration of iron in the outer glaze was just too high.\nThe vessel (fig 5) is glazed with a mixture containing CuO. Under oxidation conditions this would appear in the familiar blue or green of copper compounds. Under reducing conditions the copper is present as a mixture of Cu2O and finely dispersed elemental copper,6 hence the colour observed.\nThe fascination of pottery is that the variables are many, the possibilities are endless, and it offers opportunities for both artistic and scientific creativity. In addition, you can drink your tea out of your creations.\nStephen Breuer is a potter who previously taught chemistry at Lancaster University\n- E Boaretto et al, P. Natl. Acad. Sci., 2009, 106, 9595 (DOI: 10.1073/pnas.0900539106)\n- D Rhodes, Clay and glazes for the potter, 2nd edn, London, UK: A & C Black, 1973\n- G Varga, Építoanyag[Building Materials], 2007, 59, 6\n- S A T Redfern, Clay Miner., 1987, 22, 447 http://bit.ly/M3cstt (pdf)\n- I Stubna, G Varga and A Trnik, Építoanyag[Building Materials], 2006, 58, 6\n- A Paul, Chemistry of glasses, 2nd edn, p336-342, London, Chapman & Hall, 1990"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:9d27e125-15d7-4ee2-9310-b40f74021697>","<urn:uuid:367abf6a-6de4-4215-b4e2-56ca2eda3f81>"],"error":null}
{"question":"I'm researching the historical discovery of Tutankhamun's tomb and related archaeological work in Luxor. What was significant about both the initial opening of Tutankhamun's tomb and the later restoration work done on the tomb of his great-uncle Anen?","answer":"The inner chamber of Tutankhamun's tomb was opened in 1923 at Luxor in the presence of Egyptian government officials and archaeologists led by Lord Carnarvon. As for Anen's tomb, who was Tutankhamun's maternal great uncle, it underwent significant restoration in 2002 through a project backed by the American Research Center in Egypt. The restoration involved repairing deteriorated wall paintings, including impressive reliefs that had suffered from roof collapse, flooding, and vandalization. The project successfully restored several wall reliefs and made the tomb accessible to scholars and visitors, demonstrating how seemingly unsalvageable tombs could be successfully restored.","context":["EVENTS, birthdays and anniversaries on February 17.\n1540: Scotland’s gypsies were given recognition by King James V.\n1813: Prussia’s Frederick William III declared war on France.\n1863: The International Red Cross was founded in Geneva by Swiss philanthropist Jean Henri Dunant. Its original title was The Committee for Aid to Wounded Soldiers.\n1867: The first ship passed through the Suez Canal.\n1868: Scottish Reform Bill introduced.\n1897: Britain rejected Austro-Russian proposal for blockade of Piraeus in Greece.\n1904: Puccini’s opera Madame Butterfly was first produced, in Milan.\n1916: British and French forces captured Germany’s African colony of Cameroon.\n1923: The inner chamber of the tomb of Tutankhamun was opened at Luxor in the presence of officials of the Egyptian government and archaeologists led by Lord Carnarvon.\n1936: British-Irish trade pact ended tariff war.\n1944: White Paper was issued outlining National Health Service for Britain.\n1959: Queen Elizabeth gave Marlborough House to the nation for use as a Commonwealth Centre.\n1965: United States spacecraft Ranger 8 was launched from Cape Kennedy, Florida, and made crash-landing on Moon three days later after sending back more than 7,000 pictures.\n1972: House of Commons voted by a majority of eight in favour of Britain joining the European Common Market.\n1972: Volkswagen cars broke the record held by the Model T Ford by selling the 15,007,034th production model of the Beetle.\n1989: Frozen carcasses of apes, antelopes, monkeys, squirrels and aardvarks were seized by Dutch customs from a cargo vessel en route from Africa to supply restaurants in Belgium and Germany.\n1989: Scientists stated that levels of ozone-depleting gases over the Arctic were 50 times higher than had been predicted by computer.\n1990: East Germany announced it would take down a 600ft section of the Berlin Wall near the Brandenburg Gate, which would be the first section with no official controls.\n1996: In Philadelphia, Pennsylvania, world champion Garry Kasparov beat the Deep Blue supercomputer in a chess match.\n1996: Nasa’s Discovery Program began as the NEAR Shoemaker spacecraft lifted off on the first mission ever to orbit and land on an asteroid, 433 Eros.\n2003: The London Congestion Charge scheme began.\n2006: A mudslide occurred in Southern Leyte, Philippines; the official death toll was set at 1,126.\n2008: Kosovo declared independence.\n2009: Texan billionaire and cricket promoter Sir Allen Stanford was charged over a $8 billion (£5.6 bn) investment fraud. English cricket bosses promptly pulled out of sponsorship talks with Sir Allen.\nRebecca Adlington, double Olympic swimming gold medallist 2008, 26; Michael Bay, film director, 50; Lord Drummond Young, QC, Senator of the College of Justice in Scotland, 65; Andrew Bruce, 11th Earl of Elgin and 15th Earl of Kincardine, Lord Lieutenant of Fife 1987-99, 91; Brenda Fricker, actress, 67; Prunella Gee, actress, 65; Paris Hilton, hotel heiress and actress, 34; Barry Humphries, entertainer (Dame Edna Everage) and author, 81; Michael Jordan, basketball player, 52; Julia McKenzie, actress and singer, 74; Norman Pace, comedian and actor, 62; Lou Diamond Phillips, actor, 53; Ruth Rendell, Baroness Rendell of Babergh, crime novelist, 85; Denise Richards, actress, 44; Patricia Routledge, actress, 86; Renee Russo, actress, 61; Benjamin Whitrow, actor, 78.\nBirths: 1653 Arcangelo Corelli, composer; 1766 Thomas Robert Malthus, economist; 1781 René Laennec, physician who invented the stethoscope; 1842 William Fritze, composer; 1849 Selwyn Image, artist; 1864 Andrew B (Banjo) Paterson, poet, journalist and author (Waltzing Matilda); 1872 Sir Ernest Davis, philanthropist; 1877 Andre Maginot, architect of Maginot Line in France; 1902 Marian Anderson, opera singer; 1934 Sir Alan Bates, actor; 1941 Gene Pitney, country and western singer.\nDeaths: 1673 Molière (Jean-Baptiste Poquelin), French playwright; 1796 James MacPherson, author of the Ossianic poems: Fragments of Ancient Poetry, Collected in the Highlands of Scotland; 1890 Christopher Sholes, inventor of the typewriter; 1909 Geronimo, last Apache chief to surrender; 1934 Albert I, King of the Belgians; 1980 Graham Sutherland, artist; 2010 Kathryn Grayson, film actress; 2013 Richard Briers, actor.","Located atop the highest point of the West Bank’s royal necropolis hill in Luxor, the tomb of Anen belonged to an ancient Egyptian priest that served under the reign of Amenhotep III. Commonly believed to have served in the military prior to joining the priesthood, Anen was known by a number of honorable titles that included ‘Guardian of the Palanquin,’ ‘Second of the Four Prophets of Amun’ and ‘Greatest of Seers.’ Beyond his high spiritual standing, Anen also came from a notable regal line. The brother of Queen Tiye, Anen was not only the brother in-law of Amenhotep III, but a maternal uncle to Akhenaten and a maternal great uncle to Tutankhamun.\nBecause of these familial ties, Anen’s tomb carries substantial historical and scholarly value, and was selected for an intensive restoration project in 2002. Backed by the American Research Center in Egypt, with support from the U.S. Agency for International Development, and directed by archaeologist Lyla Pinch-Brock, the project sought to reverse much of the deterioration that had affected the tomb’s colorful wall paintings and overall structural integrity. Carried out on an intensive schedule in October and November 2002, the project not only restored several impressive wall reliefs – one of which was famously captured as a painting entitled “a jewel in the gebel” in 1929 by Norman de Garis Davies, on display at the Metropolitan Museum of Art – but also has made the tomb accessible to scholars and visitors alike.\nComposed of a main hall and an inner burial chamber, the ‘T’-shaped layout of Anen’s tomb is typical of the 18th dynasty. The surviving wall reliefs depict a series of powerful images, including that of rekhyt birds (lapwings) with their anthropomorphized wings raised in adoration – a symbolic image that was unique to the New Kingdom period and represents bound captives, or the captured enemies of Egypt. A second exquisitely detailed image depicts Amenhotep III and Queen Tiye, receiving tribute from foreign visitors. The bowing figures beneath the thrones represent the Nine Bows, or the leaders of the foreign dynasties dominated by Egypt at the time: Minoa, Babylonia, Libya, Beduin, Mittani (the Assyrians), Kush, Irem (Upper Nubia), Iuntiu-seti (Nubian nomads) and Mentu-nu-setet (coastal Levant). The relief has an incredible amount of movement and symbolism in it, including a cat holding a duck by the neck beneath the throne of Queen Tiye, a leaping monkey and foreign enemies on King Amenhotep III’s foot cushion, literally being crushed under the weight of the pharaoh’s feet.\nThe wall reliefs were in a state of advanced deterioration, due in part to the roof collapsing and filling the tomb with rubble. This allowed for exposure to external elements such as flooding from rain and extreme light and heat. The reliefs had also undergone targeted vandalization, particularly in the case of the depiction of Amenhotep III and Queen Tiye, both of whom had their faces chiseled out. Additionally, sections of the reliefs had been cut from the walls and left discarded in fragments among the tomb’s remaining rubble, likely the result of failed attempts to steal them at some point in the 1930s when looting in the area was especially rife. This had predominantly affected the bottom half of the Amenhotep III and Queen Tiye wall relief, where looters had tried to remove the Nine Bows but ended up shattering seven of the figures.\nThe project conservator, Ewa Parandowska, began work first on the rekhyt relief, and owing to the repetitive nature of the imagery, was able to repair the missing sections of the relief relatively quickly. Fragments belonging to the relief that had been found in the tomb were re-adhered to the wall with specially prepared mortar and the relief as a whole was further stabilized to the wall with a thin layer of mortar that was applied along the edges of the painting. The wall surrounding the relief was reinforced and finished with lime mortar, and followed by a mechanical cleaning of the relief with brushes and scalpels.\nThe conservation of the relief of Amenhotep III and Queen Tiye was a more challenging task and also required some ingenuity on the part of Parandowska. Given the extent of the missing and damaged fragments, Parandowska obtained a high-resolution facsimile based on the Davies painting from the Metropolitan Museum of Art and used it as a guideline to restore the missing sections of the relief. These ‘fill-ins’ are differentiated from the rest of the relief by mimicking an ancient painting technique whereby craftsmen would sketch the relief images in red ink before filling them with color. The relief was also stabilized via injections of glue into the wall, the application of mortar around the edges of the image, and the filling in of any gaps or holes in the wall with a light-colored lime mortar.\nAs a final precaution, a protective display box was constructed over the two restored wall reliefs to protect them from human or environmental damage, and a series of low slanted walls were built up along the top edges of the tomb to divert rainwater. From a conservation perspective, these less invasive solutions were preferable over installing an entirely new roof in the tomb, which would have substantially altered the appearance and materials of its original walls and floor.\nSpeaking on the success of the restoration, project director Pinch-Brock explained that the tomb of Anen is a “good example of what can be done to restore a tomb apparently beyond help.” She further added, “ARCE conservation projects such as this one can open up otherwise inaccessible tombs to scholars, and further our knowledge of Egyptian history.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:9f28408d-dddd-40e9-b6d5-ab117fdb41fd>","<urn:uuid:a1332813-ddd2-476b-bf3f-91e9399b863a>"],"error":null}
{"question":"How do both The Wetlands Institute and Northwest Panay's rescue centers handle staff training for wildlife rehabilitation?","answer":"The Wetlands Institute trains volunteers like Steve and Susan Ahern to rescue terrapins, engaging them in conservation work and teaching them methods to retrieve hatchlings. Similarly, Northwest Panay's rescue centers provide staff with species-specific training on behavioral issues, nutritional requirements, and proper handling techniques. They even sent a local veterinarian, Dr. Enrique Sanchez, to Germany for specialized training in avian medicine.","context":["Stone Harbor, NJ, November 6, 2014 – The Wetlands Institute is thrilled to announce that Joe Grottola, and Steve and Susan Ahern – part of the Storm Drain Terrapin Rescue Team – have been honored with a Disney Conservation Heroes Award from the Disney Worldwide Conservation Fund (DWCF). The award recognizes local citizens for their tireless efforts to save wildlife, protect habitats, and educate communities. There were 19 Conservation Heroes selected from around the globe this year. Recipients were nominated by non-profit environmental organizations, and each honoree and their nominating organization will share a $1,500 award from DWCF.\n“I’ve always found great satisfaction working with Diamondback Terrapins as they are such an interesting species,” stated Joe Grottola. “After 25 years I still get excited every time we pull a terrapin hatchling from a storm drain. Being named a Disney Conservation Hero Award recipient is an honor that can only help to increase awareness of the challenges facing the Diamondback Terrapin. Many local people, even those who’ve lived here for years, don’t realize we have these very unique terrapins living in the saltmarsh.”\nIn 1990, Joe Grottola, first noticed the issue of diamondback terrapin hatchlings becoming trapped in storm drains and developed a method to retrieve these hatchlings. Joe then involved students and educators from Lower Cape May Regional High School in the rescue, care and release of these threatened creatures. In 2010, Steve and Susan Ahern were trained to join him in this work, expanding the project to other New Jersey barrier islands. These three individuals, along with several others, took action to reduce the potential impacts, working on a volunteer basis to rescue terrapins, raise money for terrapin conservation, and engage other volunteers at The Wetlands Institute and students from local schools each year to rescue terrapin hatchlings from storm drains throughout southern New Jersey.\n“Little did we know that rescuing these hatchlings would be such a rewarding experience,” commented Susan and Steve Ahern, who have engaged elementary school students from Sea Isle to help with various projects. “In addition to giving them a second chance, it has given us the opportunity to interact and share information with so many residents living near the salt marsh in Sea Isle City, who do important things every day to protect terrapins in our community. Our efforts have been supported not only by The Wetlands Institute but by our local Environmental Commission, who has partnered with us in several other terrapin protection activities. We are very proud to be part of The Wetlands Institute’s Terrapin Conservation Programs and honored to be recognized by Disney as Conservation Heroes.”\nTheir efforts have been inspiring, and have attracted numerous volunteers to get involved in this community-based conservation project organized by The Wetlands Institute. To date, the project has rescued over 5,000 terrapins from storm drains and released them back into the marsh.\nIn addition to their efforts to rescue terrapins trapped in storm drains, the team has contributed extensively to terrapin conservation efforts in their communities and to the continued success of the terrapin conservation program at The Wetlands Institute.\nThe Disney Worldwide Conservation Fund focuses on protecting wildlife and connecting kids and families with nature. Since 2004, Disney has honored more than 100 leaders around the world for their extraordinary conservation efforts.\nFor information on Disney’s commitment to conserve nature and a complete list of 2014 Conservation Hero Award recipients, visit www.disney.com/conservation.\n# # #\nAbout The Wetlands Institute:\nThe Wetlands Institute is a non-profit organization dedicated to promoting appreciation, understanding and stewardship of wetlands and coastal ecosystems through our programs in research, conservation and education. We inspire visitors of all ages to appreciate and steward wetlands and coastal ecosystems by teaching them the importance of those systems and how they relate to their own lives and well-being. Visit our website at wetlandsinstitute.org to find out more about our programs and mission.","Rescue & rehab Centers\nRehabilitation and release of endangered wildlife at rescue centers in northwest Panay:\nconfiscated/donated birds and other endangered wildlife are rehabilitated till being able to fend for themselves when released\nRehabilitation and release of endangered wildlife at rescue centers in NW Panay: confiscated / donated birds and other endangered wildlife are rehabilitated till being able to fend for themselves when released\nWe foster, rehabilitate, and then release injured animals.\nOne of our most obvious and longest activity is the rehabilitation and release of endangered wildlife at rescue centers in northwest Panay.\nThe animals taking into rehabilitation are either found and reported to PhilinCon or seized by PhilinCon’s rangers from illegal wildlife traitor or poacher. After bringing the injured, sick or orphaned animal to one of the three rehab stations (Mag-aba Wildlife Clinic, Pandan, Antique; Bulanao Rescue Facility, Brgy, Antique; Sibaliw Rehabilitation Station, Brgy, Antique) a veterinarian will determine the extent of injury and the probability of successful rehabilitation. If the animal can make a recovery and be released into the wild, it will be nurtured, trained, and medically taken care of. Otherwise the animal will be still nursed but stays in an animal sanctuary.\nIn the first step basic first aid and physical therapy will be applied to the animal. For this purpose the station staff received training explicit for the unique species found on Panay; they understand behavioral issues, nutritional requirements, and have knowledge about species-specific handling. Day-to-day care is applied to the animals; it includes feeding, physical therapy, exercise, medicating and a pre-release conditioning program. In the program, the animals are familiarized with their natural diet to enable them to survive when released back to the wild. To record the recuperation process, regular medical examinations are executed.\nBefore releasing the animals, the reintroduction into the wild has to be carefully planned. To be released, the animal must be healthy, strong, and have intact wild instincts to survive in the wild. The releasing is not only solely dependent on the animal, it also hinge on the right habitat, location, season, sometimes even the weather. When all conditions are met, the animals are released into the wild.\nWe presently maintain several hornbills and raptors at two locations, Bulanao, Libertad, and Santo Rosario, Pandan.\nIn our three rescue centers, injured animals are treated professionally until they can be released.\nThe animals admitted to each of the three rescue centers are checked for their health, treated professionally, if necessary by a vet, fed adequately and kept as long as necessary until they attain top shape in terms of plumage, health (pre-release health check) and locomotor abilities. Then they are released back into the wild. Hornbills pass through a rehabilitation station in the upland forest to accustom to their natural forest environment, larger raptors exercise sustained power flight in large aviaries prior to release from a lowland station.\nThe facilities have been maintained permanently since 1999.\nThe costs for e.g. one hornbill average around 200 USD annually. This includes food, medicine, caretaking, and health monitoring.\nOur MOA (Memorandum of Agreement) authorizes the project to receive and maintain confiscated, donated, and rescued wildlife for rehabilitation, and later release them back into their former habitats. We presently maintain several hornbills and raptors at two locations, Bulanao, Libertad, and Santo Rosario, Pandan.\nA local DVM, Dr. Enrique Sanchez, had been dispatched to Cologne, Germany, for additional training in avian medicine. Likewise, Filipino staff were trained to tend and care for rehabilitated wildlife in our three facilities, namely in Mag-aba Wildlife Clinic, Pandan, Bulanao Rescue Facility in Brgy. Bulanao, Libertad, both in Antique, and the Sibaliw Rehabilitation Facility in Brgy. Tag-osip, Buruanga, Aklan. After proper health checks, the birds are trained and conditioned for release. In the process, the animals are familiarized with their natural diet to enable them to survive when released back to the wild."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:901dcdfa-7791-48c5-9f12-ef2796a9be9c>","<urn:uuid:c109c59d-24f9-4ecb-993a-f1d483b4912c>"],"error":null}
{"question":"What are the key features of modern offshore wind farm vessel management systems, and what role do unmanned vessels play in supporting these operations?","answer":"Modern offshore wind farm vessel management systems, like Strainstall Marine's system, include modular components such as crew/vessel tracking, fuel consumption monitoring, vessel motion monitoring, intelligent fender systems, and metocean data collection. These systems provide centralized access to measured data, alarms, and historic information for analysis and reporting. Meanwhile, unmanned vessels like those developed by XOCEAN support wind farm operations by conducting seabed surveys, planning turbine locations, monitoring cable burial depth, and performing routine inspections throughout the wind farm's 30-year lifespan. These robotic vessels can be controlled remotely through satellite connections and use hybrid power systems combining lithium-ion batteries, solar arrays, and micro-diesel generators.","context":["New Vessel Motion Monitoring System (VMMS) – developed by James Fisher group company Strainstall Marine – provides accurate motion and impact load monitoring with a data logging capability; Increases safety and operational efficiency during personnel and equipment transfer manoeuvres involving offshore wind farm support vessels; MMS is now undergoing trials on board Tidal Transit’s ‘Eden Rose’ during normal operations off the UK’s east coast.\nStrainstall’s VMMS product significantly improves safety levels during personnel and equipment transfer operations involving offshore installations such as wind turbines. When transporting personnel from shore to the work site (or between work sites) the motion trend displays can be used by the vessel’s master to monitor and quantify its motions, enabling more effective changes in course and speed to be made. During docking operations and the offloading of personnel, the displays can be used to monitor heave levels and provide warning to the master when pre-set danger limits are about to be reached. All transfers can then be halted until conditions improve in order to safeguard personnel and avoid damage to vessels or equipment. Crucially, however, VMMS enables transfers to take place safely during a significantly increased operational window, as support vessel masters can monitor all motion parameters rather than relying on significant wave height as the main limiting factor.\nIn addition to its role in transfer operations, VMMS can also be used as a dedicated health and safety tool, enabling the collection and logging of data for statistical purposes as well as in accident investigations.\n“This trial announced today is a significant step forward in the development of the VMMS product,” said Scott Cruttenden, Strainstall business development manager. “It will allow us to demonstrate the accuracy and effectiveness of this important safety technology in real operational conditions. We’ve already seen impressive results during trials conducted by STL Research, but will now be able to see those results translated into an actual working environment. The system will be trialled on board the ‘Eden Rose’ for one month, after which time all the data collected will be available for analysis and evaluation.”\nThe VMMS provides a live & historic view of vessel heave, pitch and roll as well as GPS, heading and the following additional parameters:\n- Vertical acceleration, slip magnitude (ms-2) at point of transfer\n- Vertical displacement, slip magnitude (m) at point of transfer\n- Lateral acceleration, slip magnitude (ms-2) at point of transfer\n- RMS vertical acceleration (ms-2) at CoG\n- RMS Lateral accelerations (ms-2) at CoG\n- RMS roll angle (°)\nPart of a comprehensive modular package\nThe VMMS is just one of the modular systems that comprise the Strainstall Marine Offshore Wind farm Vessel Management System. This comprehensive package is designed to facilitate the growing demand for better productivity and safety in the offshore wind industry. Being of a modular design enables any number of component systems to be selected, according to the individual clients’ requirements. Many elements are also fully customizable to make a fully tailored package.\nThe system can include:\n- Crew/Vessel Tracking\n- Fuel consumption monitoring\n- Vessel Motion Monitoring System\n- Intelligent (load sensing) fender\n- Metocean data\n- Wave data\n- Vessel performance monitoring\nA central control computer gives full access to all measured data, alarms and historic information for analysis and reporting, which can be used to help streamline and improve operational activities or as a budgeting and forecasting tool.","Uncrewed surface vessels made by Louth business XOCEAN have the potential to transform ocean exploration as well as offshore wind farms by gathering whole oceans’ worth of data.\nTo many the idea of robotic drones patrolling our seas might sound like something out of science fiction, but it is very much technological fact. A young Carlingford-based business called XOCEAN led by James Ives has developed uncrewed surface vessels that are proving to be a win-win in terms of cost, the environment and safety.\nThe vessels allow large businesses such as energy companies to survey the ocean floor to do everything from discovering minerals, maintaining important fibre optic telecoms networks to surveying for suitable locations for offshore wind farms.\n“Offshore wind is probably the fastest growing sector for us. We’ve operated with a large majority of the offshore wind farms around Europe”\nCrucially it means that vessels, which can be controlled onshore, can patrol for longer and do so in a way that won’t endanger life.\nXOCEAN CEO and founder James Ives\n“Our product is data. Every day we are listening to our clients and they are finding other types of data that they would like us to collect”\nIn recent months the company secured €1.7m in grant funding from Blue Economy Window, an EU programme funded by the European Maritime and Fisheries Fund (EMFF) and the Climate Infrastructure and Environment Executive Agency (CINEA).\nThe company said that the funding will support the product market launch in Europe, North America and Asia resulting in over 1,000 direct and indirect jobs in three years.\nAn engineer by trade, Ives previously worked at an ocean technology business for more than a decade before deciding to start XOCEAN in 2017. The opportunity he sees is to capture opportunities in the global ocean economy which is projected to double by 2030. However, a significant knowledge gap remains with 95pc of the world’s oceans still unmapped. By bringing uncrewed vessels to the waves, this knowledge gap could be filled.\n“In my previous company one of the things we used to do was buy a lot of ocean data. Any project you need to do in the ocean you need data to do it. I couldn’t help but think there could be a better way to get that data without having to put a boat out to sea with a crew on board.\n“And the idea was to develop a robotic system that could do the same activity. One benefit of this is safety; anytime you go off shore there is an inherent risk in that. Another benefit is the environment; the system we’ve developed has about 1,000th the emissions of a conventional survey vessel. And another benefit is cost: if we can deliver the data at a really high quality at a lower cost then it is a win-win altogether.\n“And these three points align with the vision of many of our customers who are focused on the environment, cost and safety.”\nSailing the high seas\nTo date, XOCEAN has operated in 14 countries, delivering over 100 projects including seabed surveys on 16 offshore wind farms for clients such as SSE Renewables and Ørsted. In the last 12 months, XOCEAN has doubled the size of its team to 82.\nClients include oil and gas companies such as BP, renewable energy companies and government agencies such as the Canadian Hydrographic Service.\n“Offshore wind is probably the fastest growing sector for us. We’ve operated with a large majority of the offshore wind farms around Europe. An offshore wind farm requires data over a 30-year lifespan from developing the farm, constructing the farm to 25 years of operating the farm to in the end decommissioning the farm. Throughout the entire period it needs data.”\nThe XOCEAN vessels map the seabed to pinpoint accuracy using high-resolution sonar. “This can be used for planning where the turbines are going to be located to planning the route for the cable going to shore. This involves a lot of routine surveys of the seabed and the foundations, depth of burial of the cables and more. A lot of it is about preventative maintenance and management of the wind farm. We use many different types of technology to penetrate the seabed and gather important data.”\nIves describes the XOCEAN vessels as being the size of an average car but half the weight. The company outsources their manufacture and then crams its technology on board to be the brains of the vessel.\nThe boats are festooned with all kinds of wireless technologies from radio to satellite and sonar to give an all-round accuracy of location and data gathering as well as awareness of other boats. “Right now we have three vessels quite a long way out in the North Sea doing survey work. But the pilots who are managing those vessels are sitting at home but have a direct connection to the vessel through a secure satellite connection. So they’re sitting there getting the camera images, the situational awareness data from the boat and more while they’re sitting in today’s world at home with a good broadband connection using a reasonable and affordable laptop.\n“Our product is data. Every day we are listening to our clients and they are finding other types of data that they would like us to collect. In the first year of being in business we did about 17 days of ocean operations. In the second year we did about 88 days of operations. This year we are doing more than 400 days’ worth of operations using our uncrewed vessels.”\nThe boats themselves are hybrid vehicles similar in concept to hybrid vehicles on today’s roads, consisting lithium ion batteries, a solar array and small micro-diesel generators.\nIves said the business is hurtling towards employing 100 people this year. “Things are growing very fast for us. We have delivered more than 100 projects to date and more than 21,000 hours of survey time were performed on our systems. We have operated everywhere from North America to the Caribbean, all around Europe and out into Asia.\n“We’ve tripled our revenues in the past year, even with Covid-19 and working remotely.\n“Ultimately it is the data we gather for our clients, the safety, the cost and the low carbon footprint hits the right buttons for what our clients are looking for.”\nBy John Kennedy (email@example.com)\nPublished: 25 June 2021"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:39a0b658-344b-4a9a-9090-c3f9496017d4>","<urn:uuid:39b48b2c-f938-4a01-a50a-a77e1665e71c>"],"error":null}
{"question":"I'm researching diseases related to fungi. Could you explain how iron uptake affects Rhizopus oryzae's virulence, and what complications it causes in diabetic patients with COVID-19?","answer":"Iron uptake is crucial for Rhizopus oryzae's virulence through its high affinity iron permease (rFTR1) protein. This protein is essential for iron transport in iron-depleted environments and is a key virulence factor. When the relative copy number of rFTR1 is reduced, the fungus's ability to acquire iron and its virulence are compromised. In diabetic patients with COVID-19, mucormycosis caused by Rhizopus can lead to severe complications including orbital apex syndrome, brain infarction, and pansinusitis. The disease progresses rapidly, especially in diabetic patients, and can cause complete ophthalmoplegia, facial pain, and visual impairment. COVID-19 infection may worsen the condition by affecting T-cells and reducing lymphocyte counts, which are important for fighting fungal infections.","context":["|Ibrahim, Ashraf -|\n|Gebremariam, Teclegiorgis -|\n|Lin, Lin -|\n|Luo, Guanpinksheng -|\n|Husseiny, Mohamed -|\n|Fu, Yue -|\n|French, Samuel -|\n|Edwards, JR., John -|\n|Spellberg, Brad -|\nSubmitted to: Molecular Microbiology\nPublication Type: Peer Reviewed Journal\nPublication Acceptance Date: May 17, 2010\nPublication Date: June 9, 2010\nRepository URL: http://hdl.handle.net/10113/47305\nCitation: Ibrahim, A.S., Gebremariam, T., Lin, L., Husseiny, M.I., Skory, C.D., Fu, Y., French, S.W., Edwards, Jr, J.E., Spellberg, B. 2010. The high affinity iron permease is a key virulence factor required for Rhizopus oryzae pathogenesis. Molecular Microbiology. 77(3):587-604. Interpretive Summary: This research demonstrates the importance of a protein involved in iron uptake in the fungus Rhizopus, which has broad interest to the industrial, agricultural, and medical community. Isolates of this organism have the ability to synthesize enzymes and chemicals (e.g. lactic acid or fumaric) on a commercial scale, but are also responsible for food and agricultural crop spoilage and opportunistic (i.e. will not affect healthy individuals) infections. Rhizopus can only grow if it is able to obtain iron from its surroundings using specific transporters that bring the iron into the cell. Understanding how this process works is critical to controlling growth of the organism. This work focused on a specific transport protein called rFTR1 that appears to be the principal mechanism for importing iron. Deletion of the gene required to make this protein prevented growth of the fungus. Additionally, decreasing the amount of this protein had a direct correlation on the ability to take up iron. The results of this study provide valuable information that will be of interest to clinicians and scientists; and are expected to provide new targets for controlling the growth of this important fungus.\nTechnical Abstract: Rhizopus oryzae is the most common cause of mucormycosis, an angioinvasive fungal infection that causes a >/=50% mortality rate despite first-line therapy. Clinical and animal model data clearly demonstrate that the presence of elevated available serum iron predisposes the host to mucormycosis. The high affinity iron permease gene (rFTR1) is required for R. oryzae iron transport in iron-depleted environments, such as those found in the host during infection. To define its role in R. oryzae virulence, we sought to abrogate the function of rFTR1 by gene disruption or by RNA interference. Here we demonstrate that rFTR1 is required for full virulence of R. oryzae in mice. We show that rFTR1 is expressed during infection in diabetic ketoacidotic (DKA) mice. In addition, rFTR1 could be disrupted by double cross-over homologous recombination, but multinucleated R. oryzae could not be forced to segregate to a homokaryotic null allele, regardless of the availability of extracellular iron. Nevertheless, a reduction in the relative copy number of functional rFTR1 demonstrated that it was indispensible for growth, especially in iron-depleted environments. Reduction of the relative copy number of rFTR1 or inhibition of rFTR1 expression compromised the ability of R. oryzae to acquire iron in vitro and reduced its virulence in DKA mice. Importantly, passive immunization with anti-rFtr1p immune sera significantly protected DKA mice from infection with R. oryzae. Thus rFTR1 is a crucial virulence factor for R. oryzae, and passive immunotherapy against rFtr1p is a promising strategy to improve outcomes of these deadly infections.","A middle-aged woman with diabetes presented with left-sided facial pain, complete ptosis and fever of short duration. On presentation, she had hyperglycaemia without ketosis. There was total ophthalmoplegia of the left eye with a visual acuity of 6/36. She incidentally tested positive for COVID-19. CT paranasal sinus and MRI brain revealed left-sided pansinusitis with acute infarct in the left parieto-occipital region without angioinvasion. An emergency functional endoscopic sinus procedure was done, which confirmed mucormycosis on histopathological examination. After 1 week of conventional amphotericin B and antibiotics, repeat CT brain showed improvement in mucosal thickening and sinusitis. This case is a rare presentation of mucormycosis associated with rapid progression to orbital apex syndrome with brain infarction in a patient with non-ketotic diabetes and COVID-19. Early diagnosis and treatment are essential to prevent further end-organ damage. It is also interesting that there was no angioinvasion and transient periarterial inflammation was attributed to brain infarction.\n- tropical medicine (infectious disease)\nThis article is made freely available for use in accordance with BMJ’s website terms and conditions for the duration of the covid-19 pandemic or until otherwise determined by BMJ. You may use, download and print the article for any lawful, non-commercial purpose (including text and data mining) provided that all copyright notices and trade marks are retained.https://bmj.com/coronavirus/usage\nStatistics from Altmetric.com\nMucormycosis is an angioinvasive disease caused by fungi of the order Mucorales like Rhizopus, Mucor, Rhizomucor, Cunninghamella and Absidia. The prevalence of mucormycosis in India is approximately 0.14 cases per 1000 population, about 80 times the prevalence in developed countries.1 COVID-19 infection has been associated with fungal infections. Mucormycosis is more often seen in immunocompromised individuals, and complications of orbital and cerebral involvement are likely in diabetic ketoacidosis and with concomitant use of steroids. The most common risk factor associated with mucormycosis is diabetes mellitus in India.2 In the background of the COVID-19 pandemic, only a limited number of cases of mucormycosis have been reported, but there are no known documented cases of sudden-onset visual loss with incidental COVID-19 infection in a newly detected young non-ketotic diabetic.3\nA middle-aged woman, with newly detected diabetes mellitus (haemoglobin A1c: 12.39%), presented with left eye complete ptosis, facial pain for 5 days and preceding fever for 3 days with no symptoms of rhinitis or sinusitis. Clinical examination revealed tenderness of all sinuses on the left side. There was a complete internal and external ophthalmoplegia of the left eye, with absent left eye direct light reflex and left eye visual acuity of 6/36 (figure 1). The right eye movements and vision were normal. The rest of the neurological examination was within normal limits. The patient was afebrile on presentation. Pulse was 78 bpm, blood pressure was 124/80 mm Hg and oxygen saturation was 98% on room air. Systemic examination was unremarkable, and there was no clinical evidence of ketoacidosis.\nAdmission glucose was 378 mg/dL without ketosis. Serum glycated haemoglobin was 12.39%. A CT paranasal sinus showed total opacification of the left ethmoid, maxillary and frontal sinuses (figure 2). An MRI brain showed acute infarct in the left parieto-occipital lobe with a subperiosteal abscess in the superomedial extraconal aspect of the left orbit (figure 3) with thickening and perineural enhancement of the left optic nerve. There was no thrombus in the left internal carotid artery (ICA); however, there was some periarterial inflammation of the left ICA (figure 4). The total leucocyte count was 10.6 × 109/L with a lymphocyte count of 1696 cells/uL. Serial monitoring showed a falling trend of lymphocyte count from 1696 (day 1), to 1246 (day 5), to 924 (day 10), to 666 (day 14) cells/uL. However, CD4 or CD8 testing could not be done. Serum liver biochemistry was within normal limits. Blood urea was 12.3 mg/dL, and serum creatinine was 0.44 mg/dL. The serum electrolytes were within normal limits. She had a B-positive blood type. Inflammatory markers were C reactive protein, 68.35 mg/L (reference range: ≤6 mg/L); procalcitonin, 0.069 ng/mL (reference range: <0.5 ng/mL); D-dimer, 0.80 μg FEU/mL (reference range: <0.5 μg FEU/mL); and serum ferritin, 180.2 mg/dL (reference range for men, 30–400 ng/mL; women, 13–150 ng/mL). She tested positive incidentally for COVID-19 by real-time PCR (nasal and oropharyngeal swab) on day 2 of hospitalisation using TRUPCR SARS-CoV-2 RT qPCR KIT (version 3.2). Chest X-ray (figure 5) was unremarkable, and the patient never required oxygen therapy. She was classified as having a mild COVID-19 infection. Functional endoscopic sinus surgery (FESS) was performed on an emergency basis due to orbital apex syndrome as she fell into group ‘A’ of endonasal surgery indications4 and showed unhealthy, polypoidal mass and slough in the maxillary, anterior and posterior ethmoid sinuses with pus in the frontal sinus and polypoidal mucosa in the sphenoid sinus. Histopathological analysis of the biopsy sample from ethmoid sinus showed fungal colonies of broad aseptate hyphae at an obtuse angle with periodic acid–Schiff stain, which was consistent with mucormycosis. Fungal culture of the sample obtained after sinus debridement in FESS confirmed mucormycosis (Rhizopus species). However, serological fungal markers and fungal culture sensitivity were not tested. The patient was hospitalised for 17 days. Her sugars were well controlled after the initiation of insulin therapy. However, there was no resolution of ophthalmoplegia or ptosis until she was discharged.\nShe was initiated on conventional amphotericin B (given for 11 days) and aspirin for acute cerebral infarct. Post FESS, CT paranasal sinus imaging was done after 1 week of treatment with antifungal therapy and showed a reduction in the diffuse opacification of the left ethmoid, frontal and maxillary sinuses (figure 6).\nAn active search of literature reviewed few reported rhino-orbitary cases associated with COVID-19.3 5 6 Diabetes mellitus is an independent risk factor for rhino-orbital–cerebral mucormycosis in a meta-analysis of 600 series with 851 cases. The most common species isolated was Rhizopus species, with an overall mortality of 46%.7\nA case of COVID-19 with rhino-orbital mucormycosis coinfection associated with ketoacidosis was reported in a patient with recent-onset diabetes mellitus.8 Pathogenic mechanisms involved in fungal aggressiveness include decreased phagocytic activity, accessible amounts of iron due to the displacement of protons by transferrin in diabetic ketoacidosis and fungal heme oxygenase, which promotes iron absorption for its metabolism.\nIn a case described of severe COVID-19 associated with fungal coinfection, cell counts revealed that there was a progressive increase in white blood cell count and neutrophils while lymphocytes progressively decreased.9 It is hypothesised that SARS-CoV-2 infection may affect CD4+ and CD8+ T-cells, which are highly involved in the pathological process of COVID-19 infection. It has been shown that in severe COVID-19 cases, there is a reduction in the absolute number of lymphocytes and T-cells, which is associated with the worst outcomes. Mucorales-specific T-cells (CD4+ and CD8+) produce cytokines such as interleukin (IL) 4, IL-10, IL-17 and interferon-gamma (IFN-γ) that damage the fungal hyphae. Such specific T-cells were seen only in patients affected by invasive mucormycosis, and they concluded that they could be a useful surrogate diagnostic marker of an invasive fungal disease. It might be speculated that lymphopenia could increase the risk of developing invasive mucormycosis, while the recovery of lymphocyte count could improve the adaptive immune system and induce the production of Mucorales-specific T-cells, which might have a role in controlling the invasive infection.\nThere are a significant number of reports showing alterations in cell-mediated immunity, such as chemotaxis, phagocytosis and cytokine secretion in both type 1 and type 2 diabetics. Individuals with diabetes have been described to have alterations in innate immune system components. Natural killer cell activity is reduced in individuals with diabetes, and more pro-inflammatory M1 macrophages are present. Furthermore, T-cell activity is skewed. Disease severity in patients is due to not only the viral infection but also the host response. Elevated glucose levels may also suppress the antiviral response. In the context of COVID-19, severe disease progression is described by a delay in IFN-γ response with a prolonged hyperinflammatory state and lower CD4 and CD8 cell numbers. Regardless of the involvement of the endothelial cells, the initial delay in IFN-γ response together with the hyperinflammatory response in individuals with diabetes may exacerbate the ‘cytokine storm’ and increase COVID-19 severity. Increased vascular lesions, endothelial inflammation and vasoconstriction associated with endothelial dysfunction put individuals with diabetes at a greater risk for endotheliitis in several organs. Change of vascular tone towards more vasoconstriction can lead to subsequent organ ischaemia, tissue oedema and a procoagulant state. Finally, dysregulated immune cell populations and activity observed in individuals with diabetes play a critical role in aggravating the severity.10\nA case series in the Indian subcontinent reported six cases of rhino-orbital–cerebral mucormycosis following COVID-19 infections.11 The mean duration between the diagnosis of COVID-19 and the development of symptoms of mucormycosis was 15.6±9.6 days. Control of hyperglycaemia, early treatment with liposomal amphotericin B and surgery are essential for the successful management of mucormycosis. Thus, the use of glucocorticoids in mild COVID-19 cases (without hypoxaemia) or the utilisation of higher doses of glucocorticoids should be avoided. Further, in the absence of a clear benefit, drugs targeting immune pathways such as tocilizumab should be discouraged. For successful management of mucormycosis, a high index of clinical suspicion, low threshold for diagnosis in patients with risk factors, neuroimaging and specific diagnostic tests with a coordinated effort from a multidisciplinary team including ophthalmology, otorhinolaryngology, infectious diseases, neurosurgery, critical care, microbiology and pathology department are crucial. A delay of even 6 days in initiating treatment doubles the 30-day mortality from 35% to 66%.11\nSimple tests like vision, pupil, ocular motility and sinus tenderness can be part of routine physical evaluation of a patient with COVID-19 hospitalised with moderate to severe infection or diabetics with COVID-19 or those receiving systemic corticosteroids. Visual prognosis, however, continues to remain poor.\nThus, it is important to have a high index of suspicion for fungal coinfection in patients with COVID-19 presenting with comorbidities. Further, they should undergo immediate imaging studies with an emphasis on the requirement of surgical intervention. There is a need to stress on the judicious use of steroids to avoid flaring up of the fungal infection.\nThis case is an unusual presentation of rapidly developing fungal infection in a patient with non-ketotic diabetes in the background of COVID-19. Severe disease progression in the absence of use of immunosuppressants makes it a rare case.\nAn alteration in the T-cell population in COVID-19 infection is linked to the pathogenesis of fungal infection.\nEarly diagnosis and treatment of mucormycosis that involve antifungal therapy and surgical debridement are necessary to reduce mortality and prevent end-organ damage.\nJudicial use of immunosuppressive therapy in COVID-19 infection should be considered particularly in regard to treatment of fungal coinfections.\nContributors All four authors were involved in patient care directly. Writing the initial manuscript was done by SMR. Selecting appropriate image templates was done by VVK. The necessary corrections and final outcome of the article were done under the guidance of SPS and LS.\nFunding The authors have not declared a specific grant for this research from any funding agency in the public, commercial or not-for-profit sectors.\nCompeting interests None declared.\nProvenance and peer review Not commissioned; externally peer reviewed.\nIf you wish to reuse any or all of this article please use the link below which will take you to the Copyright Clearance Center’s RightsLink service. You will be able to get a quick price and instant permission to reuse the content in many different ways."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:584bfeb0-77a3-4b70-9ffb-248239ae074a>","<urn:uuid:0419cc06-8800-41cd-8467-90adc4377911>"],"error":null}
{"question":"How do you protect a horse's legs during transport vs during medical emergencies? 🚗","answer":"For transport protection, shipping boots or bandages are specifically designed to cover from the knee or hock down to the hoof, protecting the cannon bones, tendons, fetlocks, pasterns, coronets, and heels. Some shipping boots even include hoof guards for extra protection. For medical emergencies, a first-aid kit should include self-sticking bandages, clean leg wraps, and padding materials like Gamgee cloth or leg cottons. The emergency supplies should be kept clean and organized in zip closure bags. Additionally, antiseptic wound cleaners and products like zinc oxide cream should be available to treat any injuries that might occur.","context":["Every stable should have an equine first-aid kit, and if you are going to events, you should also have some first-aid items along. Equine first-aid kits needn't be elaborate. Keep these emergency items in a sturdy box for unexpected horse injuries or illness. You don't need to keep a stock of injectable or oral medications. Unless you are very experienced with reading symptoms medications may mask important indicators your veterinarian will need to diagnose a problem. They are best administered only with veterinarian supervision. But these basic items in your horse first-aid kit should help you take care of the most common problems you can deal with yourself, and help you cope with any injury until the veterinarian arrives.\n01 of 09\nWhat did we do before the availability of these stretchy self-sticking bandages? They seem to have so many uses around the stable. In a first-aid situation use them to keep a dressing in place, or help support stable wraps. There are lots of different name brands such as VetWrap, Co-Flex, and others. There will be little difference between the ones you buy at the tack shop, and the ones you can buy at the pharmacy so chose the ones you can afford.\n02 of 09\nA thermometer will quickly tell you if your horse has an elevated temperature—a sure sign of a health problem that needs attention. You may prefer digital over mercury as you don't need reading glasses or a watch to get an accurate reading. Some models save the last temperature taken, should you forget to write it down. A string and clip feature will help prevent the thermometer from getting 'lost'.\n03 of 09\n04 of 09\nKeep a clean set of leg wraps handy for emergencies. Since you want them to be clean and ready, have an extra set of stable wraps in your horse first aid kit, besides the ones you might be using for non-emergency use.Continue to 5 of 9 below.\n05 of 09\nYou'll need something for padding under leg wraps or cut up for wound dressing such as:\n- Gamgee cloth\n- Disposable diapers (although they are not breathable).\n- Gauze diapers\n- Leg cottons used under stable wraps work as well. The cottons must be kept clean so seal an extra set in a zip closure bag for your first aid kit.\nIn fact, zip closure bags are very handy for keeping your horse's first aid kit organized.\n06 of 09\nA big, inexpensive tub of zinc oxide cream is handy to soothe and protect sunburned noses, help clear up grease heel, and protect and heal minor cuts and nicks. You can find zinc oxide creams in the baby care section of your drugstore. An effective by inexpensive one called Ilhes Paste can also be found alongside where baby diaper and rash creams are sold.\n07 of 09\nEpsom salts are great for drawing out infection. Many people use good old salt water to wash out cuts and scrapes on both four legged and two legged family members. Both are inexpensive items you can buy at the grocery, pharmacy or bulk food store.\n08 of 09\nYou'll find antiseptic wound cleaners such as Hibitane, Betadine or Novalsan scrubs are useful for washing skin infections, cuts, and punctures. These injuries can be encouraged to heal by keeping the skin moist and clean. There is a wide variety of products available. You can choose from all natural products or products containing various medicinal and antibiotic ingredients.Continue to 9 of 9 below.\n09 of 09\nThere are lots of great veterinary first aid books on bookstore shelves. Buy one and read it before an emergency happens. Another book you’ll want is a small notebook to keep track of temperatures or write down things you want to tell the vet and may forget in your worry. Keep both books in your first aid kit with your vet’s number written on the covers. Of course, keep your veterinarian’s number near the phone as well.","Keeping Horse Legs Safe: Bandages, Boots, and Wraps\nBy Sarah Evers Conrad\nWhether a horse is being used for jumping, eventing, dressage, reining, cattle work, trail riding, riding lessons, camp programs, or just as a pleasure horse, one thing is certain – they work hard, and so do their legs. The legs of a horse are certainly amazing. They take on extreme amounts of stress, bear a lot of weight, can move quickly so that the horse can change directions on a dime or jump over an obstacle, and they are one of the most important parts of the horse. Protecting a horse’s legs is imperative in certain situations, especially if the horse is young and still growing.\nHorses that are faced with poor footing, uneven ground, a competition environment, transportation, or have a tendency to stock up (or have their legs swell) while in a stall, can benefit from leg wraps, boots, or bandages. It is important to know when and how to use each kind so that the horse’s legs are protected properly. Using leg wraps and boots incorrectly can cause problems for the horse and could accidentally put more strain on the horse’s legs and cause damage, such as inflammation of the flexor tendon and the flexor tendon sheath, which is sometimes known as the “bandage bow.”\nSkid Boots: Skid boots are for use on the hind legs during work, especially if a horse has a tendency to catch one leg with another leg or hoof. They are popular in western events, such as cutting, reining, and cattle work. Skid boots protect the lower legs, fetlock joint, and pasterns.\nBell Boots: Bell boots fit around and underneath the fetlock and Velcro in place. Some can even be pulled on, and these may be used if the ones with Velcro cause chafing or do not fit the horse well. Proper fit means that the rider can put two fingers between the bell boot and the pastern at the top opening, and they should cover the heel bulbs. Bell boots are used when a horse has a tendency of overstepping/overreaching, which could then cause him to catch the back of his front hoof or coronet and cut or bruise himself. In addition, the horse could pull off a shoe, along with part of the hoof. Horses that have studs on their shoes also benefit from the use of bell boots so that the studs do not injure the horse if he catches himself. Bell boots can also be used during turnout or shipping or when being ridden.\nTendon Boots: These boots have elastic straps across the front and hook closures while padding protects the tendons and ligaments on the sides and backs of the leg from a strike from the back hooves. They are popular among jumpers since the open front helps the horse feel a pole if he strikes it with his foreleg. In addition, the open design allows additional air flow. They are only used on the front legs.\nFetlock Boots: These boots are used to protect the fetlocks on the hind legs and may be used with tendon boots. They are also open in the front.\nSports Medicine Boots: These boots can be used during exercise to protect the muscles and tendons, as well as the pastern and fetlock. Sports medicine boots are most commonly used to protect the horse from muscle and tendon strains and sprains, suspensory injuries, and splints. Many riders tend to only put boots on the front legs. However, the hind legs can also be susceptible to injury. In addition, by booting all four legs, support is even on each leg, and it may help the horse bear weight more evenly.\nSplint Boots or Brushing Boots: Splint boots or brushing boots help prevent injury during exercise, especially if one hoof strikes an opposite leg, and are easier to put on than wraps. They come in handy with horses that are less coordinated or in training for faster events. They can also be used in turnout, especially if a horse is extra exuberant when playing. They sit right on top of the fetlock joint. Fit and proper placement is important to prevent injury. To learn how to put on bell boots, sports medicine boots, and splint boots, check out CHA's Safety Short Video titled \"Fitting Horse Boots\" on YouTube.\nPolo Wraps: Polo wraps are stretchy, available in various colors and lengths, and help protect the horse’s legs from scrapes, bruises, and irritation from dirt, sand, and other types of arena footing. However, polos, also called track wraps, should not be used during trail riding since burrs and small sticks and debris can become attached to them and then cause the horse irritation as they dig into his skin. They are not recommended for use when putting the horse in a stall for a while or in turnout since they can easily become unraveled and torn if the horse steps on them. Many choose polo wraps over boots since they conform to the leg, however, if they become wet, they become really heavy for the horse. This can place additional strain on tendons and ligaments. Applying polo wraps incorrectly can also damage the horse’s leg. Polo wraps should be washed often since a dirty polo wrap can irritate a horse’s legs. For proper placement, check out CHA’s video called “How to Put Polo Wraps and Standing Wraps on Horses.”\nStanding Wraps: Standing wraps, also called stable bandages, consist of padding that is wrapped around the horse’s legs using polo wraps. They help protect the horse’s legs, tendons, and ligaments, while the horse is in a stall. Standing wraps can be beneficial if a horse has a tendency to be restless in the stall, or if the horse’s legs tends to stock up (swell) after exercise or while in a stall. They can also be used in shipping, although shipping boots provide better protection. In addition, they can be used for certain injuries, but this should be at the discretion of a veterinarian. Using a wrap can help keep cuts, wounds, and other injuries clean while they heal. In addition, standing wraps are beneficial when poultices or liniments need to be used, again at the discretion of a veterinarian. A veterinarian should advise on the use of standing wraps with any product, since some products can produce excessive heat, thus causing the horse discomfort or pain if used under a wrap. Standing wraps stretch from the bottom of the knee or hock to below the fetlock and are always used with padding.\nShipping Bandages, Boots, or Wraps: Shipping boots, bandages, and wraps are used when trailering and flying to prevent injuries to the legs. Shipping boots and wraps go from the knee or hock down to the hoof. Shipping boots can provide more protection than shipping wraps since they cover the hock and some even have hoof guards. They protect the cannon bones, tendons, fetlocks, pasterns, coronets, and heels. As with other boots, bandages, and wraps, make sure to clean the horse's legs and the boot and wrap so that the horse does not become irritated from trapped dirt, shavings, or other obstructions. Poorly applied shipping bandages and wraps have the possibility of coming loose and falling off. In addition, they could strain the horse’s legs. Wraps are best for long trips, while boots are great for short trips or for those who do not know how to properly put on a shipping wrap.\nIt is possible to go on and on about wraps, boots, and bandages. There is a plethora of options on the market, and each company may have a unique take on the design. In addition, the names of boots may vary from discipline to discipline and also differ by country.\nThe most important thing about using wraps, boots, and bandages is to apply them properly and to use them in the right situations. Proper fit is important as well. It may take trial and error to find the right options for a particular horse, but the benefits are worth the effort. After all, no rider wants to hear that their horse has turned up lame."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:e02d3c0b-3338-4f75-bbaf-a080f4ee83df>","<urn:uuid:8a82d24c-8475-49bd-bbb3-a04ad31e5b2e>"],"error":null}
{"question":"I'm interested in firewood and forest products. What are the structural and non-structural wood panels commonly used in construction, and what risks do they pose in terms of spreading invasive species?","answer":"Structural panels include plywood and oriented-strand board (OSB). Plywood is mainly used in commercial structures, while OSB is more common in residential construction. Both serve as cladding for roofs, walls, and subfloors. Non-structural panels include medium-density fiberboard (MDF), particleboard, and hardboard, which are valued for dimensional stability and textural consistency. However, when it comes to firewood and wood products, they can pose serious risks by spreading invasive insects and fungi. These pests can travel hundreds of miles in a day through transported wood, compared to their natural spread rate of just a few miles per year. Even small pieces of bark or wood can harbor invasive species, making proper handling and local sourcing crucial for forest protection.","context":["By Alex Anderson, DNR Forest Products Specialist, Rhinelander\nWhen people think of wood-based panel products, plywood and oriented-strand board (OSB) usually come to mind. However, a vast array of panel products whose primary building block is wood do not fall under the auspices of plywood or OSB. Before we explore those, it’s important to understand the distinction between “structural” panels and “non-structural” panels.\nAs the name implies, structural panels are designed to bolster the strength, stiffness and resistance of the items they adhere to. Plywood, generally, is utilized in commercial structures, whereas OSB is more common in residential construction.\nBoth plywood and OSB function as cladding for roofs, walls and subfloors. In Wisconsin, various plywood manufacturers generally produce hardwood plywood for decorative applications, such as furniture, cabinets, pinblocks for pianos and many others. There are also two OSB manufacturers in Wisconsin that utilize OSB as a base for their siding product.\nAlternatively, non-structural panel products are primarily represented by medium-density fiberboard (MDF), particleboard and hardboard. Like OSB and plywood, these panels are prized for their dimensional stability and textural consistency.\nContinue reading “There’s More To Panels Than OSB And Plywood”\nIn early September, the Wisconsin Wood Marketing Team and the Wisconsin Department of Natural Resources’ Forest Products Services program partnered with the Softwood Export Council and Northeastern Lumber Manufacturer’s Association to host a virtual trade seminar with US softwood manufacturers, US building material brokers, and Pakistani wood buyers. The seminar had 28 people in attendance, with the majority being lumber purchasers from Pakistan. Participants from Pakistan gained valuable information about the benefits of using US softwood products and current market trends for wood products in Pakistan.\nContinue reading “Opportunities for US softwood lumber in Pakistan”\nBy Marguerite Rapp, forest health communications specialist, email@example.com, Andrea Diss Torrance, invasive insects program coordinator, firstname.lastname@example.org, and Tim Allen, DATCP forest pest program coordinator and nursery inspector, email@example.com, 715-891-8158\nThis time of year, many Wisconsinites warm up with firewood, whether that’s in a wood stove for the home or a bonfire with family and friends. While firewood is one of the most sustainable heat sources available, the forests that produce it are threatened when firewood infested by invasive species is moved long distances. Fortunately, we can reduce this threat together through responsible use, movement and sale of firewood and wood products.\nContinue reading “Fighting invasives together through responsible firewood practices”\nBy Andrea Diss-Torrance, invasive forest insects program coordinator, Andrea.DissTorrance@wisconsin.gov, 608-516-2223\nMost people know that using locally-sourced firewood helps prevent the spread of invasive pests and diseases. What may be less well known are the processes for finding local sources of firewood or learning where and how you can collect it yourself. During Firewood Awareness Month, we want to share what options are out there so you can take steps to protect the places you love.\nInfested firewood can carry invasive insects and diseases to new places. Buy or gather firewood where you will use it or buy firewood that has been certified as heat-treated and free of pests and diseases. Credit: dontmovefirewood.org.\nContinue reading “It’s Firewood Awareness Month: do you know what your options are?”\nBy Alex Anderson, forest products specialist, Rhinelander\nThe devastation left behind in forested areas after a severe weather event can seem overwhelming. In order to further understanding of how storm-damaged forest and woodlot salvage harvests differ from traditional timber sales, we have compiled information that will, hopefully, help landowners affected by the recent rash of severe weather events in Wisconsin deal with their damaged woodlands more confidently.\nMany of the downed trees from July’s severe weather are red pines (Pinus resinosa). Pines, particularly white pine (Pinus strobus) and red pine in the Lake States region, are susceptible to staining when they are harvested or killed during a weather event but are not processed quickly enough. The discoloration is a result of microscopic fungi that manifest as a pale, blueish stain in the wood often called “blue-stain” or “sap-stain.” Though there is a small, decorative market for blue-stained pine—sometimes referred to as “denim pine”—it is generally undesirable. Hardwood species are also susceptible to blue stain fungi. End coating logs with a wax barrier may reduce the risk of staining and end checking.\nThese logs show blue staining. Continue reading “Salvaging Storm-Damaged Forest Products”\nBy Sabina Dhungana, forest products specialist, Madison\nThe Wisconsin Department of Natural Resources in cooperation with the USDA Forest Service hosted a webinar highlighting the principles of biochar.\nBiochar is an emerging forest product that is derived from woody biomass and other organic feedstocks. The use of biochar has gained considerable interest in the agricultural field, and it presents opportunities for utilizing available biomass sources. Topics covered in the webinar included: Biochar markets and uses, biochar production systems, applications in vegetable growing.\nThe webinar has been archived and can be viewed here.\nBy Logan Wells, forest products specialist, Hayward\nWorkforce development efforts are a key pillar in the Forest Products Services (FPS) program’s strategic direction. These efforts range from teaching and organizing technical workshops on topics that range from rail tie manufacturing to lumber grading. Traditionally, many efforts have focused on training the existing industry workforce. In addition, several current initiatives to raise career awareness in the forest products industry among students include the UW-Stevens Point LEAF Forest Products Kit and the Skills USA woodworking competition.\nTo build on these efforts of raising awareness about the importance of forest products and potential careers, an industry perspective curriculum is being developed by FPS in cooperation with agriculture teachers, industry partners and LEAF staff. The first installment of the curriculum will focus on the hardwood lumber sector and will be a full weeks’ worth of material. The first class is devoted to establishing basic information about the industry and process of turning logs into lumber. A general overview of the terms, products, jobs and sawmill equipment will be the focus of the first day. The second and third days will allow students to learn and practice the hands-on skill of lumber grading. Lumber grading is one of the most important skillsets in the hardwood industry. Teaching an abbreviated form of lumber grading will provide students a chance to practice applied math and critical thinking in a real-world application. The fourth day will be devoted to learning about the different types of defects in lumber and their causes along with a review exercise for the week. The final day will include resources for classes to connect with a guest speaker, participate in a mill tour or learn about other opportunities to continue to explore the hardwood lumber industry.\nThese materials will target freshman through junior level students and be taught in Career and Technical Education (CTE) classes like industrial technology, woodworking, agriculture or forestry/natural resources. The lumber grading program will be piloted with several schools this fall and be available in late Spring 2020. Eventually this model of industry perspective curriculum will be expanded to highlight information and skills of other sectors of the forest products industry, including logging and trucking. If you have thoughts or questions, or if would like to learn more about the program, please contact Logan Wells at Logan.Wells@wisconsin.gov.\nA Wisconsin-based real estate development firm, New Land Enterprises, plans to construct a seven-story mass timber office building on a vacant site in downtown Milwaukee. The building was designed by Korb + Associates Architects and would be the tallest of such structures in Wisconsin.\nWhat is mass timber?\nMass timber is a category of structural framing styles typically characterized by the use of large, solid wood panels for wall, floor, and roof construction. Examples include: glued-laminated timber (glulam), nail-laminated timber (NLT), cross-laminated timber (CLT), and dowel-laminated timber (DLT).\nContinue reading “Mass Timber Proposed in Milwaukee Building”\nAre you considering sawing railroad ties but don’t know where to start? Do you wish to gain a better understanding of log selection and manufacturing as it relates to tie grades and markets? The Wisconsin DNR, in partnership with the Wisconsin Wood Marketing Team, will host a one-day workshop on railroad tie manufacturing on September 18, 2018.\nThis course will cover the basics of tie grades, defect limitations, log selection and overall feasibility of sawing ties. The workshop will conclude with a hands-on grading exercise and discussion.\nHead sawyers, lumber inspectors, mill owners, managers, salespeople, loggers, foresters and others interested in railroad tie manufacturing are encouraged to attend this informative seminar to be held at Northcentral Technical College, Wood Technology Center for Excellence in Antigo, Wisconsin.\nRegistration will be taken online (see the online registration portal here) or by mail (see the mail-in registration form here). The registration fee of $20 includes lunch.\nContact Collin Buntrock (608-286-9083, Collin.Buntrock@Wisconsin.gov) for more information.\nA delegation of six forestry businesses from Wisconsin, along with representatives from the Department of Agriculture, Trade, and Consumer Protection and Scott Lyon from the Department of Natural Resources, participated in a trade mission to China on March 20-April 4, 2018. The purpose was to explore and expand markets for Wisconsin’s forest products. Continue reading “Wisconsin delegation explores China market”","A crackling fire is a big part of most family camping expeditions. After all, a marshmallow roasted over a stove burner just isn’t the same. But building that fire may pose a risk to the forest shading your campsite — even if you do everything Smokey Bear tells you to do.\nThe firewood you feed the fire may be a greater risk to the forest than the flames.\nDozens of invasive insects, fungi and disease accidentally brought to North America are ravaging swaths of woodland from sea to shining sea. The emerald ash borer — a beetle that got here from Asia in shipping crates and pallets made of infested wood — has killed tens of millions of ash trees in North America since its discovery here in 2002. The redbay ambrosia beetle, which causes laurel wilt disease, is marching through Georgia and Florida.\n“You have the same problem of invasive species everywhere, but you have different species in each region of the country,” says Leigh Greenwood, the Don’t Move Firewood campaign manager with The Nature Conservancy.\nNative forest ecosystems have complex checks and balances that combat native insect populations and plant diseases. Imported bugs are often resistant to these natural controls, causing greater harm than native pests. And the destructive insects and diseases often hitch a ride on firewood, speeding the spread of the devastation.\nThe emerald ash borer flies further on its own that most beetles, says Greenwood, but still moves just two or three miles a year.\n“But when you move firewood, it can move hundreds of miles in a day,” she says.\nThe goldspotted oak borer, a beetle native to Mexico and southeastern Arizona, has killed more than 20,000 oak trees in the Cleveland National Forest and Cuyamaca Rancho State Park in eastern San Diego County in California. The beetle got a ride across the natural barriers of desert in shipments of firewood from Mexico, says Greenwood.\n“Bringing in your own firewood seems like the smart and economical thing to do,” says James Johnson of the Georgia Forestry Commission. “In reality, just a few microscopic fungus spores or tiny insects hiding in non-local firewood can wreak havoc on our native environment.”\nAnd don’t think burning all the wood in the campfire will prevent the spread of insects or fungi.\n“Even a small chip of bark containing invasive insect larvae can fall unnoticed to the ground,” says Johnson. “A sudden rainstorm can wash fungus spores off wood or out of your pickup, so the danger is very real.”\nExperts say campers should burn only local firewood bought at, or near, the campground. A rule of thumb, Greenwood says, is don’t haul firewood more than 50 miles and 10 miles or less is best. Many states across New England, the Midwest and Appalachians have laws restricting movement of firewood with rules, regulations, and quarantines that clearly state how far is too far. Campgrounds in national parks ban firewood from outside the area. Failure to comply may result in a citation or seizure of firewood."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:171e3ab9-52da-4057-b6f5-64cf61f6e930>","<urn:uuid:a6bfd08d-366a-4f2a-8e22-af4ebb15c7eb>"],"error":null}
{"question":"How do the patient/subject safety protocols differ between the Channel-X-ray device and NOMAD Pro 2 Veterinary?","answer":"The Channel-X-ray device strictly prohibits checking humans or other living creatures, being exclusively designed for baggage and items inspection, with a dose per inspection of less than 1.5 μGy/h. In contrast, the NOMAD Pro 2 Veterinary is specifically designed for use on living subjects (veterinary patients), incorporating safety features like lead aprons and thyroid shields to protect the patient from backscatter radiation that can reflect from the patient's jaw, while maintaining radiation exposure within safe limits.","context":["Channel-X-ray device is consist of baggage handling section, X beam source and control section, signal acquisition and transfer, image processing and electrical control section. Channel-X-ray device is with the help of the transfer belt is checked baggage-feed track channel. Items go to Channel, detection unit the information sent to the control unit, the control unit trigger X-ray source launch X rays. X-ray after collimator, formed a very narrow fan beam to penetrate on a conveyor belt of baggage items falling onto the detector, the detector to receive the X-ray into electrical signals, and these are very weak current signal is zoomed in quantified, through a universal serial bus transfer to industrial control computer for further processing, the complex arithmetic and image processing, high-quality images.\nThe product has the following characteristics, provides users with a fast, convenient service.\n*Environmental friendly lead-free Curtain: Lead curtain looks after a specific processing, wear, and environmentally friendly, increase the using time\n*The Network Interface: can be connected to your LAN, multiple terminal and check Baggage\n*Ray more secure:-ray emission control to avoid misleading launch\n*The key to shutdown control: off, simply turn the key, the device auto safety shut-down, simple and convenient.\n*The image Dynamic processing functions: Some function keys in the dynamic, real-time processing\nThe works shown below:\nBasic Safety Rules\nAlthough channel-X-ray device easy-to-operate, and we continue to recommend that you in before you turn on the computer read through manual, and to comply with the following rules:\n1,More than 6 months without the use of the equipment, please do not turn it on! Must be a professional technician to X X-ray generator to re-start, it may cause X X-ray generator damage!\n2,The operating channel of X-ray device, you should learn more about the application of radiation protection regulations!\n3,If the other person to operate your channel-X-ray device, you must make sure that he is a qualified operator for all safety instructions, laws and regulations.\n4,The channel of X-ray installation of the equipment, electrical connections, as well as the mechanical and electrical components of the replacement must only be handled by an experienced professional technicians .\n5,If a channel-X-ray device in case parts, cable, or belt is damaged, it must be stopped immediately!\n6,Channel-X-ray device Housing Panel and Parts can only be eligible for the professional technician, open!\n7,No modification, change the security of the system in any part. For installation of the equipment, debugging, or repairs may only be done by trained and qualified personnel.\n8,Channel-X-ray device can only be used to check items! Do not check for human or other creature!\n9,Prohibiting sitting or standing on the belt.\n10,Prohibits the start channel-X-ray device, any part of the body to enter the channel!\n11,Ensure that the baggage detection channel or outlet port is not the stack! If the baggage blocking the Check the channel, the channel clean before first should be shut down.\n12,To prevent the fluid flowing to the machine, in such a case should be immediately shut down.\n13,The channel-X-ray device and display the thermal port cannot be blocked!\n14,Before the system works must be grounded. The power supply socket and install on-site must have a reliable grounding configuration.\n15,The device, try to avoid standing in the Channel exports, near to the entrance.\n16,Lead curtain damage or open, you cannot get the device.\n17,Although X-ray dose is very small, but non-members of the staff or to be as far as possible away from the device.\n|Product Name||X-ray inspection machine|\nConveyor Max Load\nDose per Inspection\nless than 1.5 μGy/h\n|Film Safety||Guarantee ISO1600 Film|\n|X-ray Leakage||less than 1 μGy/h (at a distance of 5cm from frame house)|\n|Storage Temperature/Humidity||-20°C-60°C/20%-95%(non-condensing )|\n|Power Consumption||less than 0.5KW|\nL-Shaped Photodiode Array (monoenergetic) , 16bit Deep\nHigh-Resolution Color, LCD Accord, 19 inch\nEdge enhancement, image strengthening, image lightening, reducing darkening,\nimage returning, image retrieval\nImage grey level\nImage max resolution\n24bit for processing real-time\nStorage 10000 pictures in real-time\n0.4 to 0.5mA\nCooling / Duty Cycle\nOil Cooling /100%\nChannel-X-ray device uses the latest imaging technology, the higher resolution, and the image sharper, can quickly and efficiently test a variety of dangerous goods, is widely used in government agencies, embassies, airports, conference centers, expo center, tourist attractions, sports and cultural facilities, post office, shopping centers, hotels, schools, and other security applications, suitable for bags, suitcases, bags, and other classes.","NOMAD Pro 2™ Veterinary – Safety\nX-ray systems typically generate stray radiation when fired. Backscatter radiation from a patient’s jaw can reflect back at the operator or to other parts of the patient. Because of this, lead aprons and/or thyroid shields are usually used to protect the patient, and conventional X-ray generators are operated remotely, in order to protect the operator.\nHowever, NOMAD Pro 2 Veterinary is designed with safety in mind. The external backscatter shield and internal radiation shielding are specially designed to protect the operator from radiation exposure. In fact, the exposure from using NOMAD Pro 2 Veterinary is less than 1% of allowed occupational doses. NOMAD Pro 2 Veterinary is safe!\n- Standards for Protection Against Radiation, 10 CFR 20 (US Federal Standards), 1994 (see also NCRP Report No. 116)\n- NCRP Report No. 145 (National Council on Radiation Protection and Measurements), p7-9\n- “Estimated Cosmic Radiation Doses for Flight Personnel”, Feng YJ et al, Space Medicine and Medical Engineering, 15(4) 2002, p265-9\n- Normalized average assumes 7,200 exposures per year, and the average length of exposure for D-speed=0.50 seconds, F-speed=0.25 seconds, digital sensor=0.20 seconds\nAdditional references: Occupational Dose Limit: Suggested State Regulations for Control of Radiation (SSRCR), Section D.1201; Occupational Dose Limit Requiring Dosimetry: SSRCR, Section D.1502; General Public Dose Limit: SSRCR, Section D.1301\nStudies and Reports\nCommentary: ‘Not all handheld x-ray systems are created equal’\nJoel E. Gray, Ph.D., is the founder of DIQUAD, an organization dedicated to improving image quality and film processing in dentistry while lowering patient dose. Dr. Gray is a medical physicist with 20-years experience in clinical medical imaging at Mayo Clinic (Rochester, Minnesota). Click on his picture to read his commentary regarding NOMAD™ devices vs. foreign-made devices.\n|NOMAD Safety Flier\nTo download a copy of the NOMAD Safety Flier , please click here.\n|NOMAD – Dosimetry Study\nTo download a copy of Image Quality and Radiation Dose Comparison for Intraoral Radiography: Hand-held, Battery Powered versus Conventional X-ray Systems, please click here.\nHow does it work?\nAs mentioned, NOMAD Pro 2 Veterinary employs an external backscatter shield to protect the operator from reflected radiation. This shield produces a cone-shaped protection zone extending behind it. At the position where the operator stands, the zone has a diameter of over 6 feet, enough to protect the operator’s entire body.\nIn addition, radiation shielding inside NOMAD Pro 2 Veterinary ensures that leakage from the X-ray source itself is virtually eliminated, protecting the operator’s hands and other areas close to the device.\nHow much exposure is too much?\nRadiation is all around us, all the time. It varies with your lifestyle and where you live. Click here to find out how much exposure you receive in everyday living.\nThe average dose per person from all sources is about 360 mrems per year. International Standards allow exposure to as much as 5,000 mrems a year for those who work with and around radioactive material. You can see how NOMAD, even with heavy use, provides minimal exposure for the operator.\nContact Aribex to learn more about how NOMAD Handheld X-ray Systems can advance your level of patient care."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:1c216d1f-1627-4e8b-8c84-ef81ebad80c7>","<urn:uuid:47b7ab83-e3fa-47d3-a710-2460ac8225b9>"],"error":null}